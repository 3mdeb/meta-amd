From 76a4b0178057e8bbdf5bb1ffe8707d1e36116e3b Mon Sep 17 00:00:00 2001
From: Ayyappa Chandolu <Ayyappa.Chandolu@amd.com>
Date: Wed, 15 Feb 2017 19:39:35 +0530
Subject: [PATCH 1718/1722] drm/amdgpu: Disable/Enable kfd specific changes in
 amdgpu based on compile time option

The intention is to ensure ROCm specific changes does not affect the functionality
of the gfx stack. This is a temporary change and should go away when gfx and compute
can co-exist without stability issues.

Signed-off-by: Ayyappa Chandolu <Ayyappa.Chandolu@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/Makefile                |   7 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu.h                |  27 +++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c             |   4 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_device.c         |  12 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_display.c        |   2 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h            |   4 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c            |  24 ++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c            |  23 ++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c             |   2 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c            |   9 ++
 drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c             |  46 ++++++
 drivers/gpu/drm/amd/amdgpu/amdgpu_object.c         |  16 ++
 drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c             |   8 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c          |   8 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c           |  19 +++
 drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h           |   2 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c            |  48 +++++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h            |   2 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c             | 167 ++++++++++++++++++++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h             |  12 +-
 drivers/gpu/drm/amd/amdgpu/ci_dpm.c                |   8 +
 drivers/gpu/drm/amd/amdgpu/ci_dpm.h                |   4 +
 drivers/gpu/drm/amd/amdgpu/cik.c                   |   3 +
 drivers/gpu/drm/amd/amdgpu/cik_sdma.c              |   6 +
 drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c              |   2 +
 drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c              |  12 +-
 drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c              |  18 +++
 drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c              |  22 +++
 drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c             |   9 +-
 drivers/gpu/drm/amd/amdgpu/vi.c                    |  10 ++
 drivers/gpu/drm/amd/powerplay/amd_powerplay.c      |   4 +
 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c   |  12 +-
 .../gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c  |   9 ++
 .../gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c  |  11 +-
 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c  |   9 +-
 drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h  |   6 +-
 drivers/gpu/drm/amd/powerplay/inc/hwmgr.h          |   6 +-
 37 files changed, 554 insertions(+), 39 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/Makefile b/drivers/gpu/drm/amd/amdgpu/Makefile
index ddd4755..5b06dc4 100644
--- a/drivers/gpu/drm/amd/amdgpu/Makefile
+++ b/drivers/gpu/drm/amd/amdgpu/Makefile
@@ -30,7 +30,7 @@ amdgpu-y += amdgpu_device.o amdgpu_kms.o \
 	atombios_encoders.o amdgpu_sa.o atombios_i2c.o \
 	amdgpu_prime.o amdgpu_vm.o amdgpu_ib.o amdgpu_pll.o \
 	amdgpu_ucode.o amdgpu_bo_list.o amdgpu_ctx.o amdgpu_sync.o \
-	amdgpu_gtt_mgr.o amdgpu_vram_mgr.o amdgpu_amdkfd_fence.o
+	amdgpu_gtt_mgr.o amdgpu_vram_mgr.o
 
 # add asic specific block
 amdgpu-$(CONFIG_DRM_AMDGPU_CIK)+= cik.o cik_ih.o kv_smc.o kv_dpm.o \
@@ -88,11 +88,14 @@ amdgpu-y += \
 	vce_v3_0.o
 
 # add amdkfd interfaces
+ifneq ($(CONFIG_HSA_AMD),)
 amdgpu-y += \
 	 amdgpu_amdkfd.o \
 	 amdgpu_amdkfd_gfx_v7.o \
 	 amdgpu_amdkfd_gfx_v8.o \
-	 amdgpu_amdkfd_gpuvm.o 
+	 amdgpu_amdkfd_gpuvm.o \
+	 amdgpu_amdkfd_fence.o
+endif
 
 # add cgs
 amdgpu-y += amdgpu_cgs.o
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu.h b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
index 96acdc5..9632905 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
@@ -90,7 +90,9 @@ extern int amdgpu_vm_size;
 extern int amdgpu_vm_block_size;
 extern int amdgpu_vm_fault_stop;
 extern int amdgpu_vm_debug;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 extern int amdgpu_vm_update_context;
+#endif
 extern int amdgpu_dal;
 extern int amdgpu_sched_jobs;
 extern int amdgpu_sched_hw_submission;
@@ -105,8 +107,9 @@ extern char *amdgpu_disable_cu;
 extern char *amdgpu_virtual_display;
 extern unsigned amdgpu_pp_feature_mask;
 extern int amdgpu_vram_page_split;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 extern unsigned amdgpu_sdma_phase_quantum;
-
+#endif
 #define AMDGPU_WAIT_IDLE_TIMEOUT_IN_MS	        3000
 #define AMDGPU_MAX_USEC_TIMEOUT			100000	/* 100 ms */
 #define AMDGPU_FENCE_JIFFIES_TIMEOUT		(HZ / 2)
@@ -168,7 +171,9 @@ struct amdgpu_cs_parser;
 struct amdgpu_job;
 struct amdgpu_irq_src;
 struct amdgpu_fpriv;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 struct kfd_vm_fault_info;
+#endif
 
 enum amdgpu_cp_irq {
 	AMDGPU_CP_IRQ_GFX_EOP = 0,
@@ -391,6 +396,7 @@ struct amdgpu_bo_va {
 
 #define AMDGPU_GEM_DOMAIN_MAX		0x3
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 struct amdgpu_gem_object {
 	struct drm_gem_object		base;
 	struct list_head		list;
@@ -398,6 +404,7 @@ struct amdgpu_gem_object {
 };
 
 struct kgd_mem;
+#endif
 
 struct amdgpu_bo {
 	/* Protected by tbo.reserved */
@@ -414,14 +421,18 @@ struct amdgpu_bo {
 	u64				metadata_flags;
 	void				*metadata;
 	u32				metadata_size;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	/* GEM objects refereing to this BO */
 	struct list_head	gem_objects;
-
+#endif
 	/* list of all virtual address to which this bo
 	 * is associated to
 	 */
 	struct list_head		va;
 	/* Constant after initialization */
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	struct drm_gem_object           gem_base;
+#endif
 	struct amdgpu_bo		*parent;
 	struct amdgpu_bo		*shadow;
 
@@ -429,9 +440,15 @@ struct amdgpu_bo {
 	struct amdgpu_mn		*mn;
 	struct list_head		mn_list;
 	struct list_head		shadow_list;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
 	struct kgd_mem			*kfd_bo;
+#endif	
 };
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #define gem_to_amdgpu_bo(gobj) container_of((gobj), struct amdgpu_gem_object, base)->bo
+#else
+#define gem_to_amdgpu_bo(gobj) container_of((gobj), struct amdgpu_bo, gem_base)
+#endif
 
 void amdgpu_gem_object_free(struct drm_gem_object *obj);
 int amdgpu_gem_object_open(struct drm_gem_object *obj,
@@ -446,10 +463,12 @@ struct drm_gem_object *amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
 struct dma_buf *amdgpu_gem_prime_export(struct drm_device *dev,
 					struct drm_gem_object *gobj,
 					int flags);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 struct drm_gem_object *
 amdgpu_gem_prime_foreign_bo(struct amdgpu_device *adev, struct amdgpu_bo *bo);
 struct drm_gem_object *amdgpu_gem_prime_import(struct drm_device *dev,
 					       struct dma_buf *dma_buf);
+#endif
 int amdgpu_gem_prime_pin(struct drm_gem_object *obj);
 void amdgpu_gem_prime_unpin(struct drm_gem_object *obj);
 struct reservation_object *amdgpu_gem_prime_res_obj(struct drm_gem_object *);
@@ -590,8 +609,10 @@ struct amdgpu_mc {
 	uint32_t		vram_type;
 	uint32_t                srbm_soft_reset;
 	struct amdgpu_mode_mc_save save;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	struct kfd_vm_fault_info *vm_fault_info;
 	atomic_t		vm_fault_info_updated;
+#endif
 };
 
 /*
@@ -865,11 +886,13 @@ struct amdgpu_gca_config {
 struct amdgpu_cu_info {
 	uint32_t number; /* total active CU number */
 	uint32_t ao_cu_mask;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	uint32_t simd_per_cu;
 	uint32_t max_waves_per_simd;
 	uint32_t wave_front_size;
 	uint32_t max_scratch_slots_per_cu;
 	uint32_t lds_size;
+#endif
 	uint32_t bitmap[4][4];
 };
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
index 167df00..616c0e5 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
@@ -1370,7 +1370,11 @@ amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,
 		struct amdgpu_bo_list_entry *lobj;
 
 		lobj = &parser->bo_list->array[i];
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)		
 		if (!lobj->bo_va || amdgpu_ttm_adev(lobj->bo_va->bo->tbo.bdev) != parser->adev)
+#else
+		if (!lobj->bo_va)
+#endif
 			continue;
 
 		list_for_each_entry(mapping, &lobj->bo_va->valids, list) {
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
index 940aa2a..683c291 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
@@ -52,9 +52,9 @@
 #include "bif/bif_4_1_d.h"
 #include <linux/pci.h>
 #include <linux/firmware.h>
-
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #include "amdgpu_amdkfd.h"
-
+#endif
 static int amdgpu_debugfs_regs_init(struct amdgpu_device *adev);
 static void amdgpu_debugfs_regs_cleanup(struct amdgpu_device *adev);
 
@@ -952,8 +952,10 @@ static bool amdgpu_check_pot_argument(int arg)
  */
 static void amdgpu_check_arguments(struct amdgpu_device *adev)
 {
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	struct sysinfo si;
 	int phys_ram_gb, amdgpu_vm_size_aligned;
+#endif
 
 	if (amdgpu_sched_jobs < 4) {
 		dev_warn(adev->dev, "sched jobs (%d) must be at least 4\n",
@@ -974,6 +976,7 @@ static void amdgpu_check_arguments(struct amdgpu_device *adev)
 		}
 	}
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	/* Compute the GPU VM space only if the user
 	 * hasn't changed it from the default.
 	 */
@@ -994,6 +997,7 @@ static void amdgpu_check_arguments(struct amdgpu_device *adev)
 
 		amdgpu_vm_size = amdgpu_vm_size_aligned;
 	}
+#endif
 
 
 	if (!amdgpu_check_pot_argument(amdgpu_vm_size)) {
@@ -1997,7 +2001,9 @@ int amdgpu_device_suspend(struct drm_device *dev, bool suspend, bool fbcon)
 		drm_modeset_unlock_all(dev);
 	}
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	amdgpu_amdkfd_suspend(adev);
+#endif
 
 	/* unpin the front buffers and cursors */
 	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
@@ -2138,9 +2144,11 @@ int amdgpu_device_resume(struct drm_device *dev, bool resume, bool fbcon)
 			}
 		}
 	}
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	r = amdgpu_amdkfd_resume(adev);
 	if (r)
 		return r;
+#endif
 
 	/* blat the mode back in */
 	if (fbcon) {
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
index 2b80709..5cd5270 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
@@ -211,11 +211,13 @@ int amdgpu_crtc_page_flip(struct drm_crtc *crtc,
 	obj = new_amdgpu_fb->obj;
 	new_abo = gem_to_amdgpu_bo(obj);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	if (amdgpu_ttm_adev(new_abo->tbo.bdev) != adev) {
 		DRM_ERROR("Foreign BOs not allowed in the display engine\n");
 		r = -EINVAL;
 		goto cleanup;
 	}
+#endif
 
 	/* pin the new buffer */
 	r = amdgpu_bo_reserve(new_abo, false);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h
index d6ab799..26183ec 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h
@@ -273,6 +273,7 @@ struct amdgpu_dpm_funcs {
 	int (*set_sclk_od)(struct amdgpu_device *adev, uint32_t value);
 	int (*get_mclk_od)(struct amdgpu_device *adev);
 	int (*set_mclk_od)(struct amdgpu_device *adev, uint32_t value);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	int (*reset_power_profile_state)(struct amdgpu_device *adev,
 			struct pp_profile *request);
 	int (*get_power_profile_state)(struct amdgpu_device *adev,
@@ -281,6 +282,7 @@ struct amdgpu_dpm_funcs {
 			struct pp_profile *request);
 	int (*switch_power_profile)(struct amdgpu_device *adev,
 			enum pp_profile_type type);
+#endif
 	int (*check_state_equal)(struct amdgpu_device *adev,
 				struct amdgpu_ps *cps,
 				struct amdgpu_ps *rps,
@@ -391,6 +393,7 @@ struct amdgpu_dpm_funcs {
 #define amdgpu_dpm_set_mclk_od(adev, value) \
 	((adev)->powerplay.pp_funcs->set_mclk_od((adev)->powerplay.pp_handle, value))
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #define amdgpu_dpm_reset_power_profile_state(adev, request) \
 	((adev)->powerplay.pp_funcs->reset_power_profile_state(\
 			(adev)->powerplay.pp_handle, request))
@@ -406,6 +409,7 @@ struct amdgpu_dpm_funcs {
 #define amdgpu_dpm_switch_power_profile(adev, type) \
 	((adev)->powerplay.pp_funcs->switch_power_profile(\
 			(adev)->powerplay.pp_handle, type))
+#endif
 
 #define amdgpu_dpm_dispatch_task(adev, event_id, input, output)		\
 	(adev)->powerplay.pp_funcs->dispatch_tasks((adev)->powerplay.pp_handle, (event_id), (input), (output))
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
index 8b4bdb4..d9bcb30 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
@@ -82,11 +82,17 @@ int amdgpu_runtime_pm = -1;
 unsigned amdgpu_ip_block_mask = 0xffffffff;
 int amdgpu_bapm = -1;
 int amdgpu_deep_color = 0;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 int amdgpu_vm_size = -1;
+#else
+int amdgpu_vm_size = 64;
+#endif
 int amdgpu_vm_block_size = -1;
 int amdgpu_vm_fault_stop = 0;
 int amdgpu_vm_debug = 0;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 int amdgpu_vm_update_context = 2;
+#endif
 int amdgpu_vram_page_split = 1024;
 int amdgpu_exp_hw_support = 0;
 int amdgpu_dal = -1;
@@ -102,8 +108,9 @@ unsigned amdgpu_pg_mask = 0xffffffff;
 char *amdgpu_disable_cu = NULL;
 char *amdgpu_virtual_display = NULL;
 unsigned amdgpu_pp_feature_mask = 0xffffffff;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 unsigned amdgpu_sdma_phase_quantum = 32;
-
+#endif
 MODULE_PARM_DESC(vramlimit, "Restrict VRAM for testing, in megabytes");
 module_param_named(vramlimit, amdgpu_vram_limit, int, 0600);
 
@@ -170,8 +177,10 @@ module_param_named(vm_fault_stop, amdgpu_vm_fault_stop, int, 0444);
 MODULE_PARM_DESC(vm_debug, "Debug VM handling (0 = disabled (default), 1 = enabled)");
 module_param_named(vm_debug, amdgpu_vm_debug, int, 0644);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 MODULE_PARM_DESC(vm_update_context, "VM update using CPU on large BAR. (0 = never, 1 = Graphics only, 2 = Compute only (default), 3 = Both");
 module_param_named(vm_update_context, amdgpu_vm_update_context, int, 0444);
+#endif
 
 MODULE_PARM_DESC(vram_page_split, "Number of pages after we split VRAM allocations (default 1024, -1 = disable)");
 module_param_named(vram_page_split, amdgpu_vram_page_split, int, 0444);
@@ -218,10 +227,10 @@ module_param_named(disable_cu, amdgpu_disable_cu, charp, 0444);
 MODULE_PARM_DESC(virtual_display,
 		 "Enable virtual display feature (the virtual_display will be set like xxxx:xx:xx.x,x;xxxx:xx:xx.x,x)");
 module_param_named(virtual_display, amdgpu_virtual_display, charp, 0444);
-
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 MODULE_PARM_DESC(sdma_phase_quantum, "SDMA context switch phase quantum (x 1K GPU clock cycles, 0 = no change (default 32))");
 module_param_named(sdma_phase_quantum, amdgpu_sdma_phase_quantum, uint, 0444);
-
+#endif
 static const struct pci_device_id pciidlist[] = {
 #ifdef  CONFIG_DRM_AMDGPU_SI
 	{0x1002, 0x6780, PCI_ANY_ID, PCI_ANY_ID, 0, 0, CHIP_TAHITI},
@@ -713,7 +722,11 @@ static struct drm_driver kms_driver = {
 	.prime_handle_to_fd = drm_gem_prime_handle_to_fd,
 	.prime_fd_to_handle = drm_gem_prime_fd_to_handle,
 	.gem_prime_export = amdgpu_gem_prime_export,
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	.gem_prime_import = amdgpu_gem_prime_import,
+#else
+	.gem_prime_import = drm_gem_prime_import,
+#endif
 	.gem_prime_pin = amdgpu_gem_prime_pin,
 	.gem_prime_unpin = amdgpu_gem_prime_unpin,
 	.gem_prime_res_obj = amdgpu_gem_prime_res_obj,
@@ -769,8 +782,9 @@ static int __init amdgpu_init(void)
 	pdriver = &amdgpu_kms_pci_driver;
 	driver->num_ioctls = amdgpu_max_kms_ioctl;
 	amdgpu_register_atpx_handler();
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
 	amdgpu_amdkfd_init();
-
+#endif
 	/* let modprobe override vga console setting */
 	return drm_pci_init(driver, pdriver);
 
@@ -786,7 +800,9 @@ error_sync:
 
 static void __exit amdgpu_exit(void)
 {
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	amdgpu_amdkfd_fini();
+#endif
 	drm_pci_exit(driver, pdriver);
 	amdgpu_unregister_atpx_handler();
 	amdgpu_sync_fini();
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
index 5d5099f..6e7329e 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
@@ -33,6 +33,7 @@
 
 void amdgpu_gem_object_free(struct drm_gem_object *gobj)
 {
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	struct amdgpu_gem_object *aobj;
 
 	aobj = container_of((gobj), struct amdgpu_gem_object, base);
@@ -47,6 +48,16 @@ void amdgpu_gem_object_free(struct drm_gem_object *gobj)
 	amdgpu_bo_unref(&aobj->bo);
 	drm_gem_object_release(&aobj->base);
 	kfree(aobj);
+#else
+	struct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);
+
+	if (robj) {
+		if (robj->gem_base.import_attach)
+			drm_prime_gem_destroy(&robj->gem_base, robj->tbo.sg);
+		amdgpu_mn_unregister(robj);
+		amdgpu_bo_unref(&robj);
+	}
+#endif
 }
 
 int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,
@@ -55,7 +66,9 @@ int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,
 				struct drm_gem_object **obj)
 {
 	struct amdgpu_bo *robj;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	struct amdgpu_gem_object *gobj;
+#endif
 	unsigned long max_size;
 	int r;
 
@@ -90,7 +103,9 @@ retry:
 		}
 		return r;
 	}
-
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	*obj = &robj->gem_base;
+#else
 	gobj = kzalloc(sizeof(struct amdgpu_gem_object), GFP_KERNEL);
 	if (unlikely(!gobj)) {
 		amdgpu_bo_unref(&robj);
@@ -107,7 +122,7 @@ retry:
 	list_add(&gobj->list, &robj->gem_objects);
 	gobj->bo = robj;
 	*obj = &gobj->base;
-
+#endif
 
 	return 0;
 }
@@ -690,7 +705,11 @@ int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
 		struct drm_amdgpu_gem_create_in info;
 		void __user *out = (void __user *)(long)args->value;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 		info.bo_size = amdgpu_bo_size(robj);
+#else
+		info.bo_size = robj->gem_base.size;
+#endif
 		info.alignment = robj->tbo.mem.page_alignment << PAGE_SHIFT;
 		info.domains = robj->prefered_domains;
 		info.domain_flags = robj->flags;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c
index 3ab4c65..e8443df 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c
@@ -169,9 +169,11 @@ restart_ih:
 	while (adev->irq.ih.rptr != wptr) {
 		u32 ring_index = adev->irq.ih.rptr >> 2;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 		/* Before dispatching irq to IP blocks, send it to amdkfd */
 		amdgpu_amdkfd_interrupt(adev,
 				(const void *) &adev->irq.ih.ring[ring_index]);
+#endif
 
 		entry.iv_entry = (const uint32_t *)
 			&adev->irq.ih.ring[ring_index];
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
index 530abdf..b547241 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
@@ -65,7 +65,9 @@ int amdgpu_driver_unload_kms(struct drm_device *dev)
 		pm_runtime_forbid(dev->dev);
 	}
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	amdgpu_amdkfd_device_fini(adev);
+#endif
 
 	amdgpu_acpi_fini(adev);
 
@@ -126,9 +128,11 @@ int amdgpu_driver_load_kms(struct drm_device *dev, unsigned long flags)
 				"Error during ACPI methods call\n");
 	}
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	amdgpu_amdkfd_load_interface(adev);
 	amdgpu_amdkfd_device_probe(adev);
 	amdgpu_amdkfd_device_init(adev);
+#endif
 
 	if (amdgpu_device_is_px(dev)) {
 		pm_runtime_use_autosuspend(dev->dev);
@@ -597,10 +601,15 @@ int amdgpu_driver_open_kms(struct drm_device *dev, struct drm_file *file_priv)
 		goto out_suspend;
 	}
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	r = amdgpu_vm_init(adev, &fpriv->vm,
 		!!(amdgpu_vm_update_context &
 		AMDGPU_VM_USE_CPU_UPDATE_FOR_GRAPHICS_MASK),
 		false);
+#else
+	r = amdgpu_vm_init(adev, &fpriv->vm);
+#endif
+
 	if (r) {
 		kfree(fpriv);
 		goto out_suspend;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c
index 5c9a829..b512c4e 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c
@@ -35,19 +35,23 @@
 #include <drm/drm.h>
 
 #include "amdgpu.h"
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #include "amdgpu_amdkfd.h"
 
 enum amdgpu_mn_type {
 	AMDGPU_MN_TYPE_GFX,
 	AMDGPU_MN_TYPE_HSA,
 };
+#endif
 
 struct amdgpu_mn {
 	/* constant after initialisation */
 	struct amdgpu_device	*adev;
 	struct mm_struct	*mm;
 	struct mmu_notifier	mn;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	enum amdgpu_mn_type	type;
+#endif
 
 	/* only used on destruction */
 	struct work_struct	work;
@@ -87,9 +91,11 @@ static void amdgpu_mn_destroy(struct work_struct *work)
 		list_for_each_entry_safe(bo, next_bo, &node->bos, mn_list) {
 			bo->mn = NULL;
 			list_del_init(&bo->mn_list);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 			if (rmn->type == AMDGPU_MN_TYPE_HSA)
 				amdgpu_amdkfd_cancel_restore_mem(
 					adev, bo->kfd_bo);
+#endif
 		}
 		kfree(node);
 	}
@@ -196,10 +202,17 @@ static void amdgpu_mn_invalidate_page(struct mmu_notifier *mn,
  * We block for all BOs between start and end to be idle and
  * unmap them by move them into system domain again.
  */
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static void amdgpu_mn_invalidate_range_start_gfx(struct mmu_notifier *mn,
 						 struct mm_struct *mm,
 						 unsigned long start,
 						 unsigned long end)
+#else
+static void amdgpu_mn_invalidate_range_start(struct mmu_notifier *mn,
+					     struct mm_struct *mm,
+					     unsigned long start,
+					     unsigned long end)
+#endif
 {
 	struct amdgpu_mn *rmn = container_of(mn, struct amdgpu_mn, mn);
 	struct interval_tree_node *it;
@@ -222,6 +235,13 @@ static void amdgpu_mn_invalidate_range_start_gfx(struct mmu_notifier *mn,
 	mutex_unlock(&rmn->lock);
 }
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+static const struct mmu_notifier_ops amdgpu_mn_ops = {
+	.release = amdgpu_mn_release,
+	.invalidate_page = amdgpu_mn_invalidate_page,
+	.invalidate_range_start = amdgpu_mn_invalidate_range_start,
+};
+#else
 /**
  * amdgpu_mn_invalidate_range_start_hsa - callback to notify about mm change
  *
@@ -328,6 +348,7 @@ static const struct mmu_notifier_ops amdgpu_mn_ops[] = {
  * alignment. Use these bits to make a unique key from the mm pointer
  * and notifier type. */
 #define AMDGPU_MN_KEY(mm, type) ((unsigned long)(mm) + (type))
+#endif
 
 /**
  * amdgpu_mn_get - create notifier context
@@ -337,12 +358,18 @@ static const struct mmu_notifier_ops amdgpu_mn_ops[] = {
  *
  * Creates a notifier context for current->mm.
  */
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev,
 				       enum amdgpu_mn_type type)
+#else
+static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)
+#endif
 {
 	struct mm_struct *mm = current->mm;
 	struct amdgpu_mn *rmn;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	unsigned long key = AMDGPU_MN_KEY(mm, type);
+#endif
 	int r;
 
 	mutex_lock(&adev->mn_lock);
@@ -351,8 +378,13 @@ static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev,
 		return ERR_PTR(-EINTR);
 	}
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	hash_for_each_possible(adev->mn_hash, rmn, node, key)
 		if (AMDGPU_MN_KEY(rmn->mm, rmn->type) == key)
+#else
+	hash_for_each_possible(adev->mn_hash, rmn, node, (unsigned long)mm)
+		if (rmn->mm == mm)
+#endif
 			goto release_locks;
 
 	rmn = kzalloc(sizeof(*rmn), GFP_KERNEL);
@@ -363,8 +395,12 @@ static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev,
 
 	rmn->adev = adev;
 	rmn->mm = mm;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	rmn->type = type;
 	rmn->mn.ops = &amdgpu_mn_ops[type];
+#else
+	rmn->mn.ops = &amdgpu_mn_ops;
+#endif
 	mutex_init(&rmn->lock);
 	rmn->objects = RB_ROOT;
 
@@ -372,7 +408,11 @@ static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev,
 	if (r)
 		goto free_rmn;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	hash_add(adev->mn_hash, &rmn->node, AMDGPU_MN_KEY(mm, type));
+#else
+	hash_add(adev->mn_hash, &rmn->node, (unsigned long)mm);
+#endif
 
 release_locks:
 	up_write(&mm->mmap_sem);
@@ -401,14 +441,20 @@ int amdgpu_mn_register(struct amdgpu_bo *bo, unsigned long addr)
 {
 	unsigned long end = addr + amdgpu_bo_size(bo) - 1;
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	enum amdgpu_mn_type type =
 		bo->kfd_bo ? AMDGPU_MN_TYPE_HSA : AMDGPU_MN_TYPE_GFX;
+#endif
 	struct amdgpu_mn *rmn;
 	struct amdgpu_mn_node *node = NULL;
 	struct list_head bos;
 	struct interval_tree_node *it;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	rmn = amdgpu_mn_get(adev, type);
+#else
+	rmn = amdgpu_mn_get(adev);
+#endif
 	if (IS_ERR(rmn))
 		return PTR_ERR(rmn);
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
index dec0783..0b3fbb0 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
@@ -36,7 +36,9 @@
 #include <drm/drm_cache.h>
 #include "amdgpu.h"
 #include "amdgpu_trace.h"
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #include "amdgpu_amdkfd.h"
+#endif
 
 
 static u64 amdgpu_get_vis_part_size(struct amdgpu_device *adev,
@@ -93,10 +95,15 @@ static void amdgpu_ttm_bo_destroy(struct ttm_buffer_object *tbo)
 
 	bo = container_of(tbo, struct amdgpu_bo, tbo);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	if (bo->kfd_bo)
 		amdgpu_amdkfd_unreserve_system_memory_limit(bo);
+#endif
 	amdgpu_update_memory_usage(adev, &bo->tbo.mem, NULL);
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	drm_gem_object_release(&bo->gem_base);
+#endif
 	amdgpu_bo_unref(&bo->parent);
 	if (!list_empty(&bo->shadow_list)) {
 		mutex_lock(&adev->shadow_list_lock);
@@ -346,9 +353,18 @@ int amdgpu_bo_create_restricted(struct amdgpu_device *adev,
 	if (bo == NULL)
 		return -ENOMEM;
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	r = drm_gem_object_init(adev->ddev, &bo->gem_base, size);
+	if (unlikely(r)) {
+		kfree(bo);
+		return r;
+	}
+#endif
 	INIT_LIST_HEAD(&bo->shadow_list);
 	INIT_LIST_HEAD(&bo->va);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	INIT_LIST_HEAD(&bo->gem_objects);
+#endif
 	bo->prefered_domains = domain & (AMDGPU_GEM_DOMAIN_VRAM |
 					 AMDGPU_GEM_DOMAIN_GTT |
 					 AMDGPU_GEM_DOMAIN_CPU |
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c
index 2d3bf6f..4fb54537 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c
@@ -576,6 +576,7 @@ fail:
 	return count;
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static ssize_t amdgpu_get_pp_power_profile(struct device *dev,
 		char *buf, struct pp_profile *query)
 {
@@ -744,6 +745,7 @@ static ssize_t amdgpu_set_pp_compute_power_profile(struct device *dev,
 
 	return amdgpu_set_pp_power_profile(dev, buf, count, &request);
 }
+#endif
 
 static DEVICE_ATTR(power_dpm_state, S_IRUGO | S_IWUSR, amdgpu_get_dpm_state, amdgpu_set_dpm_state);
 static DEVICE_ATTR(power_dpm_force_performance_level, S_IRUGO | S_IWUSR,
@@ -772,12 +774,14 @@ static DEVICE_ATTR(pp_sclk_od, S_IRUGO | S_IWUSR,
 static DEVICE_ATTR(pp_mclk_od, S_IRUGO | S_IWUSR,
 		amdgpu_get_pp_mclk_od,
 		amdgpu_set_pp_mclk_od);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static DEVICE_ATTR(pp_gfx_power_profile, S_IRUGO | S_IWUSR,
 		amdgpu_get_pp_gfx_power_profile,
 		amdgpu_set_pp_gfx_power_profile);
 static DEVICE_ATTR(pp_compute_power_profile, S_IRUGO | S_IWUSR,
 		amdgpu_get_pp_compute_power_profile,
 		amdgpu_set_pp_compute_power_profile);
+#endif
 
 static ssize_t amdgpu_hwmon_show_temp(struct device *dev,
 				      struct device_attribute *attr,
@@ -1386,6 +1390,7 @@ int amdgpu_pm_sysfs_init(struct amdgpu_device *adev)
 		DRM_ERROR("failed to create device file pp_mclk_od\n");
 		return ret;
 	}
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	ret = device_create_file(adev->dev,
 			&dev_attr_pp_gfx_power_profile);
 	if (ret) {
@@ -1400,6 +1405,7 @@ int amdgpu_pm_sysfs_init(struct amdgpu_device *adev)
 				"pp_compute_power_profile\n");
 		return ret;
 	}
+#endif
 
 	ret = amdgpu_debugfs_pm_init(adev);
 	if (ret) {
@@ -1429,10 +1435,12 @@ void amdgpu_pm_sysfs_fini(struct amdgpu_device *adev)
 	device_remove_file(adev->dev, &dev_attr_pp_dpm_pcie);
 	device_remove_file(adev->dev, &dev_attr_pp_sclk_od);
 	device_remove_file(adev->dev, &dev_attr_pp_mclk_od);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	device_remove_file(adev->dev,
 			&dev_attr_pp_gfx_power_profile);
 	device_remove_file(adev->dev,
 			&dev_attr_pp_compute_power_profile);
+#endif
 }
 
 void amdgpu_pm_compute_clocks(struct amdgpu_device *adev)
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c
index f1dd1c2..5676e21 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c
@@ -65,7 +65,9 @@ amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
 	struct reservation_object *resv = attach->dmabuf->resv;
 	struct amdgpu_device *adev = dev->dev_private;
 	struct amdgpu_bo *bo;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	struct amdgpu_gem_object *gobj;
+#endif
 	int ret;
 
 	ww_mutex_lock(&resv->lock, NULL);
@@ -74,6 +76,9 @@ amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
 	ww_mutex_unlock(&resv->lock);
 	if (ret)
 		return ERR_PTR(ret);
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	return &bo->gem_base;
+#else
 
 	gobj = kzalloc(sizeof(struct amdgpu_gem_object), GFP_KERNEL);
 	if (unlikely(!gobj)) {
@@ -92,6 +97,7 @@ amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
 	gobj->bo = amdgpu_bo_ref(bo);
 
 	return &gobj->base;
+#endif
 }
 
 int amdgpu_gem_prime_pin(struct drm_gem_object *obj)
@@ -141,6 +147,7 @@ struct dma_buf *amdgpu_gem_prime_export(struct drm_device *dev,
 	return drm_gem_prime_export(dev, gobj, flags);
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 struct drm_gem_object *
 amdgpu_gem_prime_foreign_bo(struct amdgpu_device *adev, struct amdgpu_bo *bo)
 {
@@ -199,3 +206,4 @@ struct drm_gem_object *amdgpu_gem_prime_import(struct drm_device *dev,
 
 	return drm_gem_prime_import(dev, dma_buf);
 }
+#endif
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c
index 65ec707..25b1de4 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c
@@ -31,7 +31,9 @@
 #include <drm/drmP.h>
 #include "amdgpu.h"
 #include "amdgpu_trace.h"
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #include "amdgpu_amdkfd.h"
+#endif
 
 struct amdgpu_sync_entry {
 	struct hlist_node	node;
@@ -84,6 +86,9 @@ static bool amdgpu_sync_same_dev(struct amdgpu_device *adev, struct fence *f)
  */
 static void *amdgpu_sync_get_owner(struct fence *f)
 {
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	struct amd_sched_fence *s_fence = to_amd_sched_fence(f);
+#else
 	struct amd_sched_fence *s_fence;
 	struct amdgpu_amdkfd_fence *kfd_fence;
 
@@ -91,12 +96,15 @@ static void *amdgpu_sync_get_owner(struct fence *f)
 		return AMDGPU_FENCE_OWNER_UNDEFINED;
 
 	s_fence = to_amd_sched_fence(f);
+#endif
 	if (s_fence)
 		return s_fence->owner;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	kfd_fence = to_amdgpu_amdkfd_fence(f);
 	if (kfd_fence)
 		return AMDGPU_FENCE_OWNER_KFD;
+#endif
 
 	return AMDGPU_FENCE_OWNER_UNDEFINED;
 }
@@ -197,10 +205,14 @@ int amdgpu_sync_resv(struct amdgpu_device *adev,
 		return -EINVAL;
 
 	f = reservation_object_get_excl(resv);
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	r = amdgpu_sync_fence(adev, sync, f);
+#else
 	fence_owner = amdgpu_sync_get_owner(f);
 	if (fence_owner != AMDGPU_FENCE_OWNER_KFD ||
 			owner != AMDGPU_FENCE_OWNER_VM)
 		r = amdgpu_sync_fence(adev, sync, f);
+#endif
 
 	flist = reservation_object_get_list(resv);
 	if (!flist || r)
@@ -209,15 +221,20 @@ int amdgpu_sync_resv(struct amdgpu_device *adev,
 	for (i = 0; i < flist->shared_count; ++i) {
 		f = rcu_dereference_protected(flist->shared[i],
 					      reservation_object_held(resv));
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 		fence_owner = amdgpu_sync_get_owner(f);
 		if (fence_owner == AMDGPU_FENCE_OWNER_KFD &&
 				owner == AMDGPU_FENCE_OWNER_VM)
 			continue;
+#endif
 
 		if (amdgpu_sync_same_dev(adev, f)) {
 			/* VM updates are only interesting
 			 * for other VM updates and moves.
 			 */
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+			fence_owner = amdgpu_sync_get_owner(f);
+#endif
 			if ((owner != AMDGPU_FENCE_OWNER_UNDEFINED) &&
 			    (fence_owner != AMDGPU_FENCE_OWNER_UNDEFINED) &&
 			    ((owner == AMDGPU_FENCE_OWNER_VM) !=
@@ -313,6 +330,7 @@ struct fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync)
 	return NULL;
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 int amdgpu_sync_wait(struct amdgpu_sync *sync)
 {
 	struct amdgpu_sync_entry *e;
@@ -331,6 +349,7 @@ int amdgpu_sync_wait(struct amdgpu_sync *sync)
 
 	return 0;
 }
+#endif
 
 /**
  * amdgpu_sync_free - free the sync object
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h
index 6c9c489..172e8d8 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h
@@ -49,7 +49,9 @@ int amdgpu_sync_resv(struct amdgpu_device *adev,
 struct fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,
 				     struct amdgpu_ring *ring);
 struct fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 int amdgpu_sync_wait(struct amdgpu_sync *sync);
+#endif
 void amdgpu_sync_free(struct amdgpu_sync *sync);
 int amdgpu_sync_init(void);
 void amdgpu_sync_fini(void);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
index 65b7e93..46711be 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
@@ -44,8 +44,9 @@
 #include <linux/debugfs.h>
 #include "amdgpu.h"
 #include "bif/bif_4_1_d.h"
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #include "amdgpu_amdkfd.h"
-
+#endif
 #define DRM_FILE_PAGE_OFFSET (0x100000000ULL >> PAGE_SHIFT)
 
 static int amdgpu_ttm_debugfs_init(struct amdgpu_device *adev);
@@ -233,6 +234,9 @@ static void amdgpu_evict_flags(struct ttm_buffer_object *bo,
 
 static int amdgpu_verify_access(struct ttm_buffer_object *bo, struct file *filp)
 {
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	struct amdgpu_bo *abo = container_of(bo, struct amdgpu_bo, tbo);
+#else
 	struct amdgpu_bo *abo;
 	struct drm_file *file_priv;
 	struct amdgpu_gem_object *gobj;
@@ -246,10 +250,14 @@ static int amdgpu_verify_access(struct ttm_buffer_object *bo, struct file *filp)
 	if (!abo || abo->kfd_bo || !filp)
 		return 0;
 	file_priv = filp->private_data;
+#endif
 
 	if (amdgpu_ttm_tt_get_usermm(bo->ttm))
 		return -EPERM;
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	return drm_vma_node_verify_access(&abo->gem_base.vma_node, filp);
+#else
 	ww_mutex_lock(&abo->tbo.resv->lock, NULL);
 	list_for_each_entry(gobj, &abo->gem_objects, list) {
 		if (gobj->base.dev != file_priv->minor->dev)
@@ -261,6 +269,7 @@ static int amdgpu_verify_access(struct ttm_buffer_object *bo, struct file *filp)
 	ww_mutex_unlock(&abo->tbo.resv->lock);
 
 	return -EPERM;
+#endif
 }
 
 static void amdgpu_move_null(struct ttm_buffer_object *bo,
@@ -611,7 +620,11 @@ struct amdgpu_ttm_tt {
 	struct amdgpu_device	*adev;
 	u64			offset;
 	uint64_t		userptr;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	struct task_struct	*usertask;
+#else
+	struct mm_struct        *usermm;
+#endif
 	uint32_t		userflags;
 	spinlock_t              guptasklock;
 	struct list_head        guptasks;
@@ -632,7 +645,11 @@ int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)
 		unsigned long end = gtt->userptr + ttm->num_pages * PAGE_SIZE;
 		struct vm_area_struct *vma;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 		vma = find_vma(gtt->usertask->mm, gtt->userptr);
+#else
+		vma = find_vma(gtt->usermm, gtt->userptr);
+#endif
 		if (!vma || vma->vm_file || vma->vm_end < end)
 			return -EPERM;
 	}
@@ -648,9 +665,13 @@ int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)
 		list_add(&guptask.list, &gtt->guptasks);
 		spin_unlock(&gtt->guptasklock);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 		r = get_user_pages(gtt->usertask, gtt->usertask->mm, userptr, num_pages,
 				   write, 0, p, NULL);
-
+#else
+		r = get_user_pages(current, current->mm, userptr, num_pages,
+				   write, 0, p, NULL);
+#endif
 		spin_lock(&gtt->guptasklock);
 		list_del(&guptask.list);
 		spin_unlock(&gtt->guptasklock);
@@ -982,7 +1003,11 @@ int amdgpu_ttm_tt_set_userptr(struct ttm_tt *ttm, uint64_t addr,
 		return -EINVAL;
 
 	gtt->userptr = addr;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	gtt->usertask = current->group_leader;
+#else
+	gtt->usermm = current->mm;
+#endif
 	gtt->userflags = flags;
 	spin_lock_init(&gtt->guptasklock);
 	INIT_LIST_HEAD(&gtt->guptasks);
@@ -998,10 +1023,14 @@ struct mm_struct *amdgpu_ttm_tt_get_usermm(struct ttm_tt *ttm)
 	if (gtt == NULL)
 		return NULL;
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	return gtt->usermm;
+#else
 	if (gtt->usertask == NULL)
 		return NULL;
 
 	return gtt->usertask->mm;
+#endif
 }
 
 bool amdgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
@@ -1131,10 +1160,12 @@ static struct list_head *amdgpu_ttm_swap_lru_tail(struct ttm_buffer_object *tbo)
 static bool amdgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
 					    const struct ttm_place *place)
 {
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	struct reservation_object_list *flist;
 	struct fence *f;
 	int i;
-	
+#endif
+
 	if (bo->mem.mem_type == TTM_PL_VRAM &&
 	    bo->mem.start == AMDGPU_BO_INVALID_OFFSET) {
 		unsigned long num_pages = bo->mem.num_pages;
@@ -1152,7 +1183,9 @@ static bool amdgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
 
 		return false;
 	}
-
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	return ttm_bo_eviction_valuable(bo, place);
+#else
 	/* Don't evict this BO if it's outside of the
 	 * requested placement range
 	 */
@@ -1174,6 +1207,7 @@ static bool amdgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
 	}
 
 	return true;
+#endif
 }
 
 static struct ttm_bo_driver amdgpu_bo_driver = {
@@ -1348,6 +1382,7 @@ void amdgpu_ttm_set_active_vram_size(struct amdgpu_device *adev, u64 size)
 	man->size = size >> PAGE_SHIFT;
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static struct vm_operations_struct amdgpu_ttm_vm_ops;
 static const struct vm_operations_struct *ttm_vm_ops /* = NULL;
 						      * (appease checkpatch) */;
@@ -1483,6 +1518,7 @@ int amdgpu_bo_mmap(struct file *filp, struct vm_area_struct *vma,
 
 	return 0;
 }
+#endif
 
 int amdgpu_mmap(struct file *filp, struct vm_area_struct *vma)
 {
@@ -1497,7 +1533,11 @@ int amdgpu_mmap(struct file *filp, struct vm_area_struct *vma)
 	if (adev == NULL)
 		return -EINVAL;
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	return ttm_bo_mmap(filp, vma, &adev->mman.bdev);
+#else
 	return amdgpu_bo_mmap(filp, vma, &adev->mman.bdev);
+#endif
 }
 
 int amdgpu_copy_buffer(struct amdgpu_ring *ring,
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h
index d67f219..cf0f0a7 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h
@@ -87,6 +87,8 @@ int amdgpu_fill_buffer(struct amdgpu_bo *bo,
 int amdgpu_mmap(struct file *filp, struct vm_area_struct *vma);
 bool amdgpu_ttm_is_bound(struct ttm_tt *ttm);
 int amdgpu_ttm_bind(struct ttm_buffer_object *bo, struct ttm_mem_reg *bo_mem);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 int amdgpu_bo_mmap(struct file *filp, struct vm_area_struct *vma,
                    struct ttm_bo_device *bdev);
+#endif				   
 #endif
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index c5fa912..c44aae2 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -59,8 +59,10 @@ struct amdgpu_pte_update_params {
 	struct amdgpu_device *adev;
 	/* address where to copy page table entries from */
 	uint64_t src;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	/* DMA addresses to use for mapping */
 	dma_addr_t *pages_addr;
+#endif
 	/* indirect buffer to fill with commands */
 	struct amdgpu_ib *ib;
 	/* Function which actually does the update */
@@ -69,10 +71,13 @@ struct amdgpu_pte_update_params {
 		     uint32_t flags);
 	/* indicate update pt or its shadow */
 	bool shadow;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
 	/* Kernel pointer of PD/PT BO that needs to be updated */
 	void *kptr;
+#endif	
 };
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 /**
  * write_pte_using_cpu - update the page tables using CPU
  *
@@ -106,6 +111,7 @@ static void write_pte_using_cpu(struct amdgpu_device *adev,
 	mb();
 	amdgpu_gart_flush_gpu_tlb(adev, 0);
 }
+#endif
 
 /**
  * amdgpu_vm_num_pde - return the number of page directory entries
@@ -533,10 +539,14 @@ static void amdgpu_vm_do_set_ptes(struct amdgpu_pte_update_params *params,
 {
 	trace_amdgpu_vm_set_ptes(pe, addr, count, incr, flags);
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	if (count < 3) {
+#else
 	if (params->kptr) {
 		write_pte_using_cpu(params->adev, params, pe, addr,
 			count, incr, flags);
 	} else if (count < 3) {
+#endif
 		amdgpu_vm_write_pte(params->adev, params->ib, pe,
 				    addr | flags, count, incr);
 
@@ -580,10 +590,21 @@ static void amdgpu_vm_do_copy_ptes(struct amdgpu_pte_update_params *params,
  * Look up the physical address of the page that the pte resolves
  * to and return the pointer for the page table entry.
  */
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
+#else
+static uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
+#endif
 {
 	uint64_t result;
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	/* page table offset */
+	result = pages_addr[addr >> PAGE_SHIFT];
+
+	/* in case cpu page size != gpu page size*/
+	result |= addr & (~PAGE_MASK);
+#else
 	if (pages_addr) {
 		/* page table offset */
 		result = pages_addr[addr >> PAGE_SHIFT];
@@ -595,6 +616,7 @@ uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
 		/* No mapping required */
 		result = addr;
 	}
+#endif
 
 	result &= 0xFFFFFFFFFFFFF000ULL;
 
@@ -638,6 +660,26 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 	/* assume the worst case */
 	ndw += vm->max_pde_used * 6;
 
+	memset(&params, 0, sizeof(params));
+	params.adev = adev;
+
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	pd_addr = amdgpu_bo_gpu_offset(vm->page_directory);
+	if (shadow) {
+		r = amdgpu_ttm_bind(&shadow->tbo, &shadow->tbo.mem);
+		if (r)
+			return r;
+		shadow_addr = amdgpu_bo_gpu_offset(shadow);
+		ndw *= 2;
+	} else {
+		shadow_addr = 0;
+	}
+
+	r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
+	if (r)
+		return r;
+	params.ib = &job->ibs[0];
+#else
 	/* If is_vm_update_mode_cpu flag is set, then try to update PD table
 	 * using CPU. If failed, fallback to use SDMA
 	 */
@@ -673,10 +715,9 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 		if (r)
 			return r;
 
-		memset(&params, 0, sizeof(params));
-		params.adev = adev;
 		params.ib = &job->ibs[0];
 	}
+#endif
 	/* walk over the address space and update the page directory */
 	for (pt_idx = 0; pt_idx <= vm->max_pde_used; ++pt_idx) {
 		struct amdgpu_bo *bo = vm->page_tables[pt_idx].bo;
@@ -759,8 +800,14 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 		fence_put(vm->page_directory_fence);
 		vm->page_directory_fence = fence_get(fence);
 		fence_put(fence);
-	} else if (params.kptr)
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	}
+#else
+	} else if (params.kptr) {
 		amdgpu_bo_kunmap(vm->page_directory);
+	}
+#endif
+
 	return 0;
 
 error_free:
@@ -780,10 +827,17 @@ error_free:
  *
  * Update the page tables in the range @start - @end using SDMA
  */
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static void amdgpu_vm_update_ptes_sdma(struct amdgpu_pte_update_params *params,
 				  struct amdgpu_vm *vm,
 				  uint64_t start, uint64_t end,
 				  uint64_t dst, uint32_t flags)
+#else
+static void amdgpu_vm_update_ptes(struct amdgpu_pte_update_params *params,
+				  struct amdgpu_vm *vm,
+				  uint64_t start, uint64_t end,
+				  uint64_t dst, uint32_t flags)
+#endif
 {
 	const uint64_t mask = AMDGPU_VM_PTE_COUNT - 1;
 
@@ -860,6 +914,7 @@ static void amdgpu_vm_update_ptes_sdma(struct amdgpu_pte_update_params *params,
 		     AMDGPU_GPU_PAGE_SIZE, flags);
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 /*
  * amdgpu_vm_frag_ptes - add fragment information to PTEs
  *
@@ -947,6 +1002,78 @@ static void amdgpu_vm_frag_ptes(struct amdgpu_pte_update_params	*params,
 	}
 }
 
+#else
+
+/*
+ * amdgpu_vm_frag_ptes - add fragment information to PTEs
+ *
+ * @params: see amdgpu_pte_update_params definition
+ * @vm: requested vm
+ * @start: first PTE to handle
+ * @end: last PTE to handle
+ * @dst: addr those PTEs should point to
+ * @flags: hw mapping flags
+ */
+static void amdgpu_vm_frag_ptes(struct amdgpu_pte_update_params	*params,
+				struct amdgpu_vm *vm,
+				uint64_t start, uint64_t end,
+				uint64_t dst, uint32_t flags)
+{
+	/**
+	 * The MC L1 TLB supports variable sized pages, based on a fragment
+	 * field in the PTE. When this field is set to a non-zero value, page
+	 * granularity is increased from 4KB to (1 << (12 + frag)). The PTE
+	 * flags are considered valid for all PTEs within the fragment range
+	 * and corresponding mappings are assumed to be physically contiguous.
+	 *
+	 * The L1 TLB can store a single PTE for the whole fragment,
+	 * significantly increasing the space available for translation
+	 * caching. This leads to large improvements in throughput when the
+	 * TLB is under pressure.
+	 *
+	 * The L2 TLB distributes small and large fragments into two
+	 * asymmetric partitions. The large fragment cache is significantly
+	 * larger. Thus, we try to use large fragments wherever possible.
+	 * Userspace can support this by aligning virtual base address and
+	 * allocation size to the fragment size.
+	 */
+
+	/* SI and newer are optimized for 64KB */
+	uint64_t frag_flags = AMDGPU_PTE_FRAG(AMDGPU_LOG2_PAGES_PER_FRAG);
+	uint64_t frag_align = 1 << AMDGPU_LOG2_PAGES_PER_FRAG;
+
+	uint64_t frag_start = ALIGN(start, frag_align);
+	uint64_t frag_end = end & ~(frag_align - 1);
+
+	/* system pages are non continuously */
+	if (params->src || !(flags & AMDGPU_PTE_VALID) ||
+	    (frag_start >= frag_end)) {
+
+		amdgpu_vm_update_ptes(params, vm, start, end, dst, flags);
+		return;
+	}
+
+	/* handle the 4K area at the beginning */
+	if (start != frag_start) {
+		amdgpu_vm_update_ptes(params, vm, start, frag_start,
+				      dst, flags);
+		dst += (frag_start - start) * AMDGPU_GPU_PAGE_SIZE;
+	}
+
+	/* handle the area in the middle */
+	amdgpu_vm_update_ptes(params, vm, frag_start, frag_end, dst,
+			      flags | frag_flags);
+
+	/* handle the 4K area at the end */
+	if (frag_end != end) {
+		dst += (frag_end - frag_start) * AMDGPU_GPU_PAGE_SIZE;
+		amdgpu_vm_update_ptes(params, vm, frag_end, end, dst, flags);
+	}
+}
+
+#endif
+
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 /**
  * amdgpu_vm_update_ptes_cpu - make sure that page tables are valid
  *
@@ -1000,6 +1127,7 @@ static int amdgpu_vm_update_ptes_cpu(struct amdgpu_pte_update_params
 	}
 	return 0;
 }
+#endif
 
 /**
  * amdgpu_vm_bo_update_mapping - update a mapping in the vm page table
@@ -1038,13 +1166,12 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 	memset(&params, 0, sizeof(params));
 	params.adev = adev;
 	params.src = src;
-
 	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
+
 
-	memset(&params, 0, sizeof(params));
-	params.adev = adev;
-	params.src = src;
 	params.pages_addr = pages_addr;
+
 	/* If update flag is set to CPU, then try to update PT entries directly
 	 * by CPU. If failed, fallback to SDMA update.
 	 */
@@ -1071,7 +1198,7 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 	}
 
 fallback_sdma_update:
-	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
+#endif
 
 	/* sync to everything on unmapping */
 	if (!(flags & AMDGPU_PTE_VALID))
@@ -1407,7 +1534,11 @@ int amdgpu_vm_clear_invalids(struct amdgpu_device *adev,
 	}
 	spin_unlock(&vm->status_lock);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	if (bo_va && sync)
+#else
+	if (bo_va)
+#endif
 		r = amdgpu_sync_fence(adev, sync, bo_va->last_pt_update);
 
 	return r;
@@ -1542,9 +1673,13 @@ int amdgpu_vm_bo_map(struct amdgpu_device *adev,
 		r = amdgpu_bo_create(adev, AMDGPU_VM_PTE_COUNT * 8,
 				     AMDGPU_GPU_PAGE_SIZE, true,
 				     AMDGPU_GEM_DOMAIN_VRAM,
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 				     vm->is_vm_update_mode_cpu ?
 					AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED :
 					AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+#else
+				     AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+#endif
 				     AMDGPU_GEM_CREATE_SHADOW |
 				     AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS |
 				     AMDGPU_GEM_CREATE_VRAM_CLEARED,
@@ -1692,8 +1827,12 @@ void amdgpu_vm_bo_invalidate(struct amdgpu_device *adev,
  *
  * Init @vm fields.
  */
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 		   bool vm_update_mode, bool is_kfd_vm)
+#else
+int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm)
+#endif
 {
 	const unsigned align = min(AMDGPU_VM_PTB_ALIGN_SIZE,
 		AMDGPU_VM_PTE_COUNT * 8);
@@ -1733,17 +1872,23 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 	if (r)
 		goto err;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	vm->is_vm_update_mode_cpu = vm_update_mode;
 	DRM_DEBUG_DRIVER("VM update mode is %s\n",
 			 vm_update_mode ? "CPU" : "SDMA");
+#endif
 
 	vm->page_directory_fence = NULL;
 
 	r = amdgpu_bo_create(adev, pd_size, align, true,
 			     AMDGPU_GEM_DOMAIN_VRAM,
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 			     vm->is_vm_update_mode_cpu ?
 				AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED :
 				AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+#else
+			     AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+#endif
 			     AMDGPU_GEM_CREATE_SHADOW |
 			     AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS |
 			     AMDGPU_GEM_CREATE_VRAM_CLEARED,
@@ -1758,6 +1903,7 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 	vm->last_eviction_counter = atomic64_read(&adev->num_evictions);
 	amdgpu_bo_unreserve(vm->page_directory);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	vm->is_kfd_vm = is_kfd_vm;
 	if (is_kfd_vm) {
 		mutex_lock(&adev->vm_manager.lock);
@@ -1774,6 +1920,7 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 
 		mutex_unlock(&adev->vm_manager.lock);
 	}
+#endif
 
 	return 0;
 
@@ -1805,6 +1952,7 @@ void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 	struct amdgpu_bo_va_mapping *mapping, *tmp;
 	int i;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	if (vm->is_kfd_vm) {
 		mutex_lock(&adev->vm_manager.lock);
 
@@ -1822,6 +1970,7 @@ void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 
 		mutex_unlock(&adev->vm_manager.lock);
 	}
+#endif
 
 	amd_sched_entity_fini(vm->entity.sched, &vm->entity);
 
@@ -1882,7 +2031,9 @@ void amdgpu_vm_manager_init(struct amdgpu_device *adev)
 	atomic_set(&adev->vm_manager.vm_pte_next_ring, 0);
 	atomic64_set(&adev->vm_manager.client_counter, 0);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	adev->vm_manager.n_kfd_vms = 0;
+#endif
 }
 
 /**
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
index 19e135d..773c2d8 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
@@ -122,10 +122,12 @@ struct amdgpu_vm {
 	/* client id */
 	u64                     client_id;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	/* Flag to indicate if VM tables are updated by CPU or GPU (SDMA) */
 	bool			is_vm_update_mode_cpu : 1;
 	/* Flag to indicate if this is a KFD VM */
 	bool			is_kfd_vm : 1;
+#endif
 };
 
 struct amdgpu_vm_id {
@@ -173,14 +175,20 @@ struct amdgpu_vm_manager {
 	/* client id counter */
 	atomic64_t				client_counter;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	/* Number of KFD VMs, used for detecting KFD activity */
 	unsigned				n_kfd_vms;
+#endif
 };
 
 void amdgpu_vm_manager_init(struct amdgpu_device *adev);
 void amdgpu_vm_manager_fini(struct amdgpu_device *adev);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		   bool vm_update_mode, bool is_kfd_vm);
+		  bool vm_update_mode, bool is_kfd_vm);
+#else
+int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm);
+#endif
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 void amdgpu_vm_get_pd_bo(struct amdgpu_vm *vm,
 			 struct list_head *validated,
@@ -195,7 +203,9 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 		      struct amdgpu_job *job);
 int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job);
 void amdgpu_vm_reset_id(struct amdgpu_device *adev, unsigned vm_id);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr);
+#endif
 int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 				    struct amdgpu_vm *vm);
 int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
diff --git a/drivers/gpu/drm/amd/amdgpu/ci_dpm.c b/drivers/gpu/drm/amd/amdgpu/ci_dpm.c
index a69f39d..e2b0361 100644
--- a/drivers/gpu/drm/amd/amdgpu/ci_dpm.c
+++ b/drivers/gpu/drm/amd/amdgpu/ci_dpm.c
@@ -3673,6 +3673,7 @@ static int ci_find_boot_level(struct ci_single_dpm_table *table,
 	return ret;
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static void ci_save_default_power_profile(struct amdgpu_device *adev)
 {
 	struct ci_power_info *pi = ci_get_pi(adev);
@@ -3706,6 +3707,7 @@ static void ci_save_default_power_profile(struct amdgpu_device *adev)
 	pi->gfx_power_profile = pi->default_gfx_power_profile;
 	pi->compute_power_profile = pi->default_compute_power_profile;
 }
+#endif
 
 static int ci_init_smc_table(struct amdgpu_device *adev)
 {
@@ -3852,7 +3854,9 @@ static int ci_init_smc_table(struct amdgpu_device *adev)
 	if (ret)
 		return ret;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	ci_save_default_power_profile(adev);
+#endif
 
 	return 0;
 }
@@ -6715,6 +6719,7 @@ static int ci_dpm_set_mclk_od(struct amdgpu_device *adev, uint32_t value)
 	return 0;
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static int ci_dpm_get_power_profile_state(struct amdgpu_device *adev,
 		struct pp_profile *query)
 {
@@ -6926,6 +6931,7 @@ static int ci_dpm_switch_power_profile(struct amdgpu_device *adev,
 
 	return 0;
 }
+#endif
 
 const struct amd_ip_funcs ci_dpm_ip_funcs = {
 	.name = "ci_dpm",
@@ -6967,10 +6973,12 @@ static const struct amdgpu_dpm_funcs ci_dpm_funcs = {
 	.set_sclk_od = ci_dpm_set_sclk_od,
 	.get_mclk_od = ci_dpm_get_mclk_od,
 	.set_mclk_od = ci_dpm_set_mclk_od,
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	.get_power_profile_state = ci_dpm_get_power_profile_state,
 	.set_power_profile_state = ci_dpm_set_power_profile_state,
 	.reset_power_profile_state = ci_dpm_reset_power_profile_state,
 	.switch_power_profile = ci_dpm_switch_power_profile,
+#endif
 	.check_state_equal = ci_check_state_equal,
 	.get_vce_clock_state = amdgpu_get_vce_clock_state,
 };
diff --git a/drivers/gpu/drm/amd/amdgpu/ci_dpm.h b/drivers/gpu/drm/amd/amdgpu/ci_dpm.h
index 46d4130..4e18fd7 100644
--- a/drivers/gpu/drm/amd/amdgpu/ci_dpm.h
+++ b/drivers/gpu/drm/amd/amdgpu/ci_dpm.h
@@ -25,7 +25,9 @@
 
 #include "amdgpu_atombios.h"
 #include "ppsmc.h"
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #include "amd_powerplay.h"
+#endif
 
 #define SMU__NUM_SCLK_DPM_STATE  8
 #define SMU__NUM_MCLK_DPM_LEVELS 6
@@ -297,12 +299,14 @@ struct ci_power_info {
 	u32 t_min;
 	u32 fan_ctrl_default_mode;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	/* power profile */
 	struct pp_profile gfx_power_profile;
 	struct pp_profile compute_power_profile;
 	struct pp_profile default_gfx_power_profile;
 	struct pp_profile default_compute_power_profile;
 	enum pp_profile_type current_power_profile;
+#endif
 };
 
 #define CISLANDS_VOLTAGE_CONTROL_NONE                   0x0
diff --git a/drivers/gpu/drm/amd/amdgpu/cik.c b/drivers/gpu/drm/amd/amdgpu/cik.c
index 2023aa3..9be7bab 100644
--- a/drivers/gpu/drm/amd/amdgpu/cik.c
+++ b/drivers/gpu/drm/amd/amdgpu/cik.c
@@ -1818,6 +1818,9 @@ static int cik_common_suspend(void *handle)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
+	amdgpu_amdkfd_suspend(adev);
+#endif
 	return cik_common_hw_fini(adev);
 }
 
diff --git a/drivers/gpu/drm/amd/amdgpu/cik_sdma.c b/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
index 43307ed..22c4533 100644
--- a/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
+++ b/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
@@ -341,6 +341,7 @@ static void cik_sdma_rlc_stop(struct amdgpu_device *adev)
 	/* XXX todo */
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 /**
  * cik_ctx_switch_enable - stop the async dma engines context switch
  *
@@ -397,6 +398,7 @@ static void cik_ctx_switch_enable(struct amdgpu_device *adev, bool enable)
 		WREG32(mmSDMA0_CNTL + sdma_offsets[i], f32_cntl);
 	}
 }
+#endif
 
 /**
  * cik_sdma_enable - stop the async dma engines
@@ -594,8 +596,10 @@ static int cik_sdma_start(struct amdgpu_device *adev)
 
 	/* halt the engine before programing */
 	cik_sdma_enable(adev, false);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	/* enable sdma ring preemption */
 	cik_ctx_switch_enable(adev, true);
+#endif
 
 	/* start the gfx rings and rlc compute queues */
 	r = cik_sdma_gfx_resume(adev);
@@ -1040,7 +1044,9 @@ static int cik_sdma_hw_fini(void *handle)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	cik_ctx_switch_enable(adev, false);
+#endif
 	cik_sdma_enable(adev, false);
 
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
index 1b0275b..f9e6764 100644
--- a/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
@@ -5313,6 +5313,7 @@ static void gfx_v7_0_get_cu_info(struct amdgpu_device *adev)
 
 	cu_info->number = active_cu_number;
 	cu_info->ao_cu_mask = ao_cu_mask;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	cu_info->simd_per_cu = NUM_SIMD_PER_CU;
 	switch (adev->asic_type) {
 	case CHIP_KAVERI:
@@ -5327,6 +5328,7 @@ static void gfx_v7_0_get_cu_info(struct amdgpu_device *adev)
 				adev->asic_type);
 		break;
 	}
+#endif
 }
 
 const struct amdgpu_ip_block_version gfx_v7_0_ip_block =
diff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
index 19781c0..34ba867 100644
--- a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
@@ -1703,7 +1703,12 @@ static int gfx_v8_0_do_edc_gpr_workarounds(struct amdgpu_device *adev)
 		DRM_ERROR("amdgpu: fence wait failed (%d).\n", r);
 		goto fail;
 	}
-#if 0
+
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
+	/* read back registers to clear the counters */
+	for (i = 0; i < ARRAY_SIZE(sec_ded_counter_registers); i++)
+		RREG32(sec_ded_counter_registers[i]);
+#else
 	tmp = REG_SET_FIELD(tmp, GB_EDC_MODE, DED_MODE, 2);
 	tmp = REG_SET_FIELD(tmp, GB_EDC_MODE, PROP_FED, 1);
 	WREG32(mmGB_EDC_MODE, tmp);
@@ -1711,11 +1716,12 @@ static int gfx_v8_0_do_edc_gpr_workarounds(struct amdgpu_device *adev)
 	tmp = RREG32(mmCC_GC_EDC_CONFIG);
 	tmp = REG_SET_FIELD(tmp, CC_GC_EDC_CONFIG, DIS_EDC, 0) | 1;
 	WREG32(mmCC_GC_EDC_CONFIG, tmp);
-#endif
+
 
 	/* read back registers to clear the counters */
 	for (i = 0; i < ARRAY_SIZE(sec_ded_counter_registers); i++)
 		RREG32(sec_ded_counter_registers[i]);
+#endif
 
 fail:
 	amdgpu_ib_free(adev, &ib, NULL);
@@ -6783,6 +6789,7 @@ static void gfx_v8_0_get_cu_info(struct amdgpu_device *adev)
 
 	cu_info->number = active_cu_number;
 	cu_info->ao_cu_mask = ao_cu_mask;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	cu_info->simd_per_cu = NUM_SIMD_PER_CU;
 	switch (adev->asic_type) {
 	case CHIP_CARRIZO:
@@ -6799,6 +6806,7 @@ static void gfx_v8_0_get_cu_info(struct amdgpu_device *adev)
 		dev_warn(adev->dev, "CU info asic_type [0x%x] not supported\n",
 					adev->asic_type);
 	}
+#endif
 }
 
 const struct amdgpu_ip_block_version gfx_v8_0_ip_block =
diff --git a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c b/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
index 01f454d..470d2e4 100644
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
@@ -27,7 +27,9 @@
 #include "cik.h"
 #include "gmc_v7_0.h"
 #include "amdgpu_ucode.h"
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #include "amdgpu_amdkfd.h"
+#endif
 
 #include "bif/bif_4_1_d.h"
 #include "bif/bif_4_1_sh_mask.h"
@@ -330,8 +332,10 @@ static int gmc_v7_0_mc_init(struct amdgpu_device *adev)
 {
 	u32 tmp;
 	int chansize, numchan;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	uint64_t gart_size_aligned;
 	struct sysinfo si;
+#endif
 
 	/* Get VRAM informations */
 	tmp = RREG32(mmMC_ARB_RAMCFG);
@@ -384,6 +388,13 @@ static int gmc_v7_0_mc_init(struct amdgpu_device *adev)
 	if (adev->mc.visible_vram_size > adev->mc.real_vram_size)
 		adev->mc.visible_vram_size = adev->mc.real_vram_size;
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	/* unless the user had overridden it, set the gart
+	 * size equal to the 1024 or vram, whichever is larger.
+	 */
+	if (amdgpu_gart_size == -1)
+		adev->mc.gtt_size = max((1024ULL << 20), adev->mc.mc_vram_size);
+#else
 	/* Unless the user has overridden it, compute the GART size */
 	if (amdgpu_gart_size == -1) {
 		/* Maximum GTT size is limited by the GART table size
@@ -415,6 +426,7 @@ static int gmc_v7_0_mc_init(struct amdgpu_device *adev)
 
 		adev->mc.gtt_size = gart_size_aligned;
 	}
+#endif
 	else
 		adev->mc.gtt_size = (uint64_t)amdgpu_gart_size << 20;
 
@@ -704,11 +716,13 @@ static int gmc_v7_0_vm_init(struct amdgpu_device *adev)
 	} else
 		adev->vm_manager.vram_base_offset = 0;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	adev->mc.vm_fault_info = kmalloc(sizeof(struct kfd_vm_fault_info),
 					GFP_KERNEL);
 	if (!adev->mc.vm_fault_info)
 		return -ENOMEM;
 	atomic_set(&adev->mc.vm_fault_info_updated, 0);
+#endif
 
 	return 0;
 }
@@ -737,7 +751,9 @@ static void gmc_v7_0_vm_decode_fault(struct amdgpu_device *adev,
 				     u32 status, u32 addr, u32 mc_client)
 {
 	u32 mc_id;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	struct kfd_vm_fault_info *info = adev->mc.vm_fault_info;
+#endif
 	u32 vmid = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS, VMID);
 	u32 protections = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,
 					PROTECTIONS);
@@ -753,6 +769,7 @@ static void gmc_v7_0_vm_decode_fault(struct amdgpu_device *adev,
 			     MEMORY_CLIENT_RW) ?
 	       "write" : "read", block, mc_client, mc_id);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	if (!atomic_read(&adev->mc.vm_fault_info_updated)) {
 		info->vmid = vmid;
 		info->mc_id = mc_id;
@@ -764,6 +781,7 @@ static void gmc_v7_0_vm_decode_fault(struct amdgpu_device *adev,
 		mb();
 		atomic_set(&adev->mc.vm_fault_info_updated, 1);
 	}
+#endif
 }
 
 
diff --git a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c b/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
index c4bd6fe..060770b7 100644
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
@@ -25,7 +25,9 @@
 #include "amdgpu.h"
 #include "gmc_v8_0.h"
 #include "amdgpu_ucode.h"
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 #include "amdgpu_amdkfd.h"
+#endif
 
 #include "gmc/gmc_8_1_d.h"
 #include "gmc/gmc_8_1_sh_mask.h"
@@ -417,8 +419,10 @@ static int gmc_v8_0_mc_init(struct amdgpu_device *adev)
 {
 	u32 tmp;
 	int chansize, numchan;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	uint64_t gart_size_aligned;
 	struct sysinfo si;
+#endif
 
 	/* Get VRAM informations */
 	tmp = RREG32(mmMC_ARB_RAMCFG);
@@ -459,10 +463,12 @@ static int gmc_v8_0_mc_init(struct amdgpu_device *adev)
 		break;
 	}
 	adev->mc.vram_width = numchan * chansize;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	/* FIXME: The above calculation is outdated.
 	 * For HBM provide a temporary fix */
 	if (adev->mc.vram_type == AMDGPU_VRAM_TYPE_HBM)
 		adev->mc.vram_width = AMDGPU_VRAM_TYPE_HBM_WIDTH;
+#endif
 
 	/* Could aper size report 0 ? */
 	adev->mc.aper_base = pci_resource_start(adev->pdev, 0);
@@ -476,6 +482,13 @@ static int gmc_v8_0_mc_init(struct amdgpu_device *adev)
 	if (adev->mc.visible_vram_size > adev->mc.real_vram_size)
 		adev->mc.visible_vram_size = adev->mc.real_vram_size;
 
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+	/* unless the user had overridden it, set the gart
+	 * size equal to the 1024 or vram, whichever is larger.
+	 */
+	if (amdgpu_gart_size == -1)
+		adev->mc.gtt_size = max((1024ULL << 20), adev->mc.mc_vram_size);
+#else
 	/* Unless the user has overridden it, compute the GART size */
 	if (amdgpu_gart_size == -1) {
 		/* Maximum GTT size is limited by the GART table size
@@ -507,6 +520,7 @@ static int gmc_v8_0_mc_init(struct amdgpu_device *adev)
 
 		adev->mc.gtt_size = gart_size_aligned;
 	}
+#endif
 	else
 		adev->mc.gtt_size = (uint64_t)amdgpu_gart_size << 20;
 
@@ -835,11 +849,13 @@ static int gmc_v8_0_vm_init(struct amdgpu_device *adev)
 	} else
 		adev->vm_manager.vram_base_offset = 0;
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	adev->mc.vm_fault_info = kmalloc(sizeof(struct kfd_vm_fault_info),
 					GFP_KERNEL);
 	if (!adev->mc.vm_fault_info)
 		return -ENOMEM;
 	atomic_set(&adev->mc.vm_fault_info_updated, 0);
+#endif
 
 	return 0;
 }
@@ -868,7 +884,9 @@ static void gmc_v8_0_vm_decode_fault(struct amdgpu_device *adev,
 				     u32 status, u32 addr, u32 mc_client)
 {
 	u32 mc_id;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	struct kfd_vm_fault_info *info = adev->mc.vm_fault_info;
+#endif
 	u32 vmid = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS, VMID);
 	u32 protections = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,
 					PROTECTIONS);
@@ -884,6 +902,7 @@ static void gmc_v8_0_vm_decode_fault(struct amdgpu_device *adev,
 			     MEMORY_CLIENT_RW) ?
 	       "write" : "read", block, mc_client, mc_id);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	if (!atomic_read(&adev->mc.vm_fault_info_updated)) {
 		info->vmid = vmid;
 		info->mc_id = mc_id;
@@ -895,6 +914,7 @@ static void gmc_v8_0_vm_decode_fault(struct amdgpu_device *adev,
 		mb();
 		atomic_set(&adev->mc.vm_fault_info_updated, 1);
 	}
+#endif
 }
 
 static int gmc_v8_0_convert_vram_type(int mc_seq_vram_type)
@@ -1032,9 +1052,11 @@ static int gmc_v8_0_sw_init(void *handle)
 		adev->vm_manager.enabled = true;
 	}
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	/* Allow BIF to recode atomics to PCIe 3.0 AtomicOps.
 	 */
 	pci_enable_atomic_request(adev->pdev);
+#endif
 
 	return r;
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c b/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c
index a02ac58..a1183c1 100644
--- a/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c
@@ -551,8 +551,10 @@ static void sdma_v3_0_rlc_stop(struct amdgpu_device *adev)
  */
 static void sdma_v3_0_ctx_switch_enable(struct amdgpu_device *adev, bool enable)
 {
-	u32 f32_cntl, phase_quantum = 0;
 	int i;
+	u32 f32_cntl;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
+	u32 phase_quantum = 0;
 
 	if (amdgpu_sdma_phase_quantum) {
 		unsigned value = amdgpu_sdma_phase_quantum;
@@ -577,12 +579,14 @@ static void sdma_v3_0_ctx_switch_enable(struct amdgpu_device *adev, bool enable)
 			value << SDMA0_PHASE0_QUANTUM__VALUE__SHIFT |
 			unit  << SDMA0_PHASE0_QUANTUM__UNIT__SHIFT;
 	}
+#endif
 
 	for (i = 0; i < adev->sdma.num_instances; i++) {
 		f32_cntl = RREG32(mmSDMA0_CNTL + sdma_offsets[i]);
 		if (enable) {
 			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
 					AUTO_CTXSW_ENABLE, 1);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
 					ATC_L1_ENABLE, 1);
 			if (amdgpu_sdma_phase_quantum) {
@@ -591,11 +595,14 @@ static void sdma_v3_0_ctx_switch_enable(struct amdgpu_device *adev, bool enable)
 				WREG32(mmSDMA0_PHASE1_QUANTUM + sdma_offsets[i],
 				       phase_quantum);
 			}
+#endif
 		} else {
 			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
 					AUTO_CTXSW_ENABLE, 0);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
 					ATC_L1_ENABLE, 1);
+#endif
 		}
 
 		WREG32(mmSDMA0_CNTL + sdma_offsets[i], f32_cntl);
diff --git a/drivers/gpu/drm/amd/amdgpu/vi.c b/drivers/gpu/drm/amd/amdgpu/vi.c
index c15734f..4bfb763 100644
--- a/drivers/gpu/drm/amd/amdgpu/vi.c
+++ b/drivers/gpu/drm/amd/amdgpu/vi.c
@@ -965,6 +965,16 @@ static int vi_common_early_init(void *handle)
 			AMD_CG_SUPPORT_SDMA_LS |
 			AMD_CG_SUPPORT_VCE_MGCG;
 		adev->pg_flags = 0;
+#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
+		/* rev0 hardware requires workarounds to support PG */
+		if (adev->rev_id != 0x00) {
+			adev->pg_flags |= AMD_PG_SUPPORT_GFX_PG |
+				AMD_PG_SUPPORT_GFX_SMG |
+				AMD_PG_SUPPORT_GFX_PIPELINE |
+				AMD_PG_SUPPORT_UVD |
+				AMD_PG_SUPPORT_VCE;
+		}
+#endif
 		adev->external_rev_id = adev->rev_id + 0x1;
 		break;
 	case CHIP_STONEY:
diff --git a/drivers/gpu/drm/amd/powerplay/amd_powerplay.c b/drivers/gpu/drm/amd/powerplay/amd_powerplay.c
index 42349cf..d4e5eca 100644
--- a/drivers/gpu/drm/amd/powerplay/amd_powerplay.c
+++ b/drivers/gpu/drm/amd/powerplay/amd_powerplay.c
@@ -828,6 +828,7 @@ static int pp_dpm_set_mclk_od(void *handle, uint32_t value)
 	return hwmgr->hwmgr_func->set_mclk_od(hwmgr, value);
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static int pp_dpm_reset_power_profile_state(void *handle,
 		struct pp_profile *request)
 {
@@ -962,6 +963,7 @@ static int pp_dpm_switch_power_profile(void *handle,
 
 	return 0;
 }
+#endif
 
 static int pp_dpm_read_sensor(void *handle, int idx, int32_t *value)
 {
@@ -1023,10 +1025,12 @@ const struct amd_powerplay_funcs pp_dpm_funcs = {
 	.set_sclk_od = pp_dpm_set_sclk_od,
 	.get_mclk_od = pp_dpm_get_mclk_od,
 	.set_mclk_od = pp_dpm_set_mclk_od,
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	.reset_power_profile_state = pp_dpm_reset_power_profile_state,
 	.get_power_profile_state = pp_dpm_get_power_profile_state,
 	.set_power_profile_state = pp_dpm_set_power_profile_state,
 	.switch_power_profile = pp_dpm_switch_power_profile,	
+#endif
 	.read_sensor = pp_dpm_read_sensor,
 	.get_vce_clock_state = pp_dpm_get_vce_clock_state,
 };
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c
index 5a24361..9044949 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c
@@ -2954,7 +2954,7 @@ static int fiji_populate_vr_config(struct pp_hwmgr *hwmgr,
 
 	return 0;
 }
-
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static int fiji_save_default_power_profile(struct pp_hwmgr *hwmgr)
 {
 	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
@@ -2992,6 +2992,7 @@ static int fiji_save_default_power_profile(struct pp_hwmgr *hwmgr)
 
 	return 0;
 }
+#endif
 
 /**
 * Initializes the SMC table and uploads it
@@ -3200,9 +3201,9 @@ static int fiji_init_smc_table(struct pp_hwmgr *hwmgr)
 			data->sram_end);
 	PP_ASSERT_WITH_CODE(0 == result,
 			"Failed to upload dpm data to SMC memory!", return result);
-
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	fiji_save_default_power_profile(hwmgr);
-
+#enduf
 	return 0;
 }
 
@@ -5585,7 +5586,7 @@ static int fiji_set_mclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
 
 	return 0;
 }
-
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static int fiji_populate_requested_graphic_levels(struct pp_hwmgr *hwmgr,
 		struct pp_profile *request)
 {
@@ -5683,6 +5684,7 @@ static int fiji_set_power_profile_state(struct pp_hwmgr *hwmgr,
 
 	return result;
 }
+#endif
 
 static const struct pp_hwmgr_func fiji_hwmgr_funcs = {
 	.backend_init = &fiji_hwmgr_backend_init,
@@ -5728,7 +5730,9 @@ static const struct pp_hwmgr_func fiji_hwmgr_funcs = {
 	.set_sclk_od = fiji_set_sclk_od,
 	.get_mclk_od = fiji_get_mclk_od,
 	.set_mclk_od = fiji_set_mclk_od,
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
 	.set_power_profile_state = fiji_set_power_profile_state,
+#endif	
 };
 
 int fiji_hwmgr_init(struct pp_hwmgr *hwmgr)
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c b/drivers/gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c
index 4d7b44f..bd25a64 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c
@@ -146,6 +146,7 @@ int phm_disable_dynamic_state_management(struct pp_hwmgr *hwmgr)
 
 int phm_force_dpm_levels(struct pp_hwmgr *hwmgr, enum amd_dpm_forced_level level)
 {
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	int ret = 0;
 
 	PHM_FUNC_CHECK(hwmgr);
@@ -168,6 +169,14 @@ int phm_force_dpm_levels(struct pp_hwmgr *hwmgr, enum amd_dpm_forced_level level
 	}
 
 	return ret;
+#else
+	PHM_FUNC_CHECK(hwmgr);
+
+	if (hwmgr->hwmgr_func->force_dpm_level != NULL)
+		return hwmgr->hwmgr_func->force_dpm_level(hwmgr, level);
+
+	return 0;
+#endif
 }
 
 int phm_apply_state_adjust_rules(struct pp_hwmgr *hwmgr,
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c
index ba7be6d..2684cc0 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c
@@ -2014,7 +2014,7 @@ static int polaris10_populate_avfs_parameters(struct pp_hwmgr *hwmgr)
 	}
 	return result;
 }
-
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static void polaris10_save_default_power_profile(struct pp_hwmgr *hwmgr)
 {
 	struct polaris10_hwmgr *data =
@@ -2051,6 +2051,7 @@ static void polaris10_save_default_power_profile(struct pp_hwmgr *hwmgr)
 	hwmgr->gfx_power_profile = hwmgr->default_gfx_power_profile;
 	hwmgr->compute_power_profile = hwmgr->default_compute_power_profile;
 }
+#endif
 
 /**
 * Initializes the SMC table and uploads it
@@ -2264,9 +2265,9 @@ static int polaris10_init_smc_table(struct pp_hwmgr *hwmgr)
 			data->sram_end);
 	PP_ASSERT_WITH_CODE(0 == result,
 			"Failed to upload dpm data to SMC memory!", return result);
-
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	polaris10_save_default_power_profile(hwmgr);
-
+#endif
 	return 0;
 }
 
@@ -5277,6 +5278,7 @@ static int polaris10_set_mclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
 	return 0;
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static int polaris10_populate_requested_graphic_levels(struct pp_hwmgr *hwmgr,
 		struct pp_profile *request)
 {
@@ -5375,6 +5377,7 @@ static int polaris10_set_power_profile_state(struct pp_hwmgr *hwmgr,
 
 	return result;
 }
+#endif
 
 static const struct pp_hwmgr_func polaris10_hwmgr_funcs = {
 	.backend_init = &polaris10_hwmgr_backend_init,
@@ -5421,7 +5424,9 @@ static const struct pp_hwmgr_func polaris10_hwmgr_funcs = {
 	.set_sclk_od = polaris10_set_sclk_od,
 	.get_mclk_od = polaris10_get_mclk_od,
 	.set_mclk_od = polaris10_set_mclk_od,
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
 	.set_power_profile_state = polaris10_set_power_profile_state,
+#endif	
 };
 
 int polaris10_hwmgr_init(struct pp_hwmgr *hwmgr)
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c
index c825e1a..82d6200 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c
@@ -2880,6 +2880,7 @@ int tonga_populate_smc_initial_state(struct pp_hwmgr *hwmgr,
 	return 0;
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static void tonga_save_default_power_profile(struct pp_hwmgr *hwmgr)
 {
 	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
@@ -2915,6 +2916,7 @@ static void tonga_save_default_power_profile(struct pp_hwmgr *hwmgr)
 	hwmgr->gfx_power_profile = hwmgr->default_gfx_power_profile;
 	hwmgr->compute_power_profile = hwmgr->default_compute_power_profile;
 }
+#endif
 
 /**
  * Initializes the SMC table and uploads it
@@ -3151,8 +3153,9 @@ static int tonga_init_smc_table(struct pp_hwmgr *hwmgr)
 	PP_ASSERT_WITH_CODE(0 == result,
 		"Failed to upload dpm data to SMC memory!", return result;);
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	tonga_save_default_power_profile(hwmgr);
-
+#endif
 	return result;
 }
 
@@ -6344,6 +6347,7 @@ static int tonga_set_mclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
 	return 0;
 }
 
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 static int tonga_populate_requested_graphic_levels(struct pp_hwmgr *hwmgr,
 		struct pp_profile *request)
 {
@@ -6438,6 +6442,7 @@ static int tonga_set_power_profile_state(struct pp_hwmgr *hwmgr,
 
 	return result;
 }
+#endif
 
 static const struct pp_hwmgr_func tonga_hwmgr_funcs = {
 	.backend_init = &tonga_hwmgr_backend_init,
@@ -6484,7 +6489,9 @@ static const struct pp_hwmgr_func tonga_hwmgr_funcs = {
 	.set_sclk_od = tonga_set_sclk_od,
 	.get_mclk_od = tonga_get_mclk_od,
 	.set_mclk_od = tonga_set_mclk_od,
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
 	.set_power_profile_state = tonga_set_power_profile_state,
+#endif	
 };
 
 int tonga_hwmgr_init(struct pp_hwmgr *hwmgr)
diff --git a/drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h b/drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h
index c6c21cf..43007581 100644
--- a/drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h
+++ b/drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h
@@ -295,7 +295,7 @@ struct pp_states_info {
 	uint32_t nums;
 	uint32_t states[16];
 };
-
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 enum pp_profile_type {
 	PP_GFX_PROFILE = 0,
 	PP_COMPUTE_PROFILE,
@@ -309,7 +309,7 @@ struct pp_profile {
 	uint8_t up_hyst;
 	uint8_t down_hyst;
 };
-
+#endif
 #define PP_GROUP_MASK        0xF0000000
 #define PP_GROUP_SHIFT       28
 
@@ -374,6 +374,7 @@ struct amd_powerplay_funcs {
 	int (*set_sclk_od)(void *handle, uint32_t value);
 	int (*get_mclk_od)(void *handle);
 	int (*set_mclk_od)(void *handle, uint32_t value);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
 	int (*reset_power_profile_state)(void *handle,
 			struct pp_profile *request);
 	int (*get_power_profile_state)(void *handle,
@@ -382,6 +383,7 @@ struct amd_powerplay_funcs {
 			struct pp_profile *request);
 	int (*switch_power_profile)(void *handle,
 			enum pp_profile_type type);	
+#endif			
 	int (*read_sensor)(void *handle, int idx, int32_t *value);
 	struct amd_vce_state* (*get_vce_clock_state)(void *handle, unsigned idx);
 };
diff --git a/drivers/gpu/drm/amd/powerplay/inc/hwmgr.h b/drivers/gpu/drm/amd/powerplay/inc/hwmgr.h
index 5f4734f..38b2f50 100644
--- a/drivers/gpu/drm/amd/powerplay/inc/hwmgr.h
+++ b/drivers/gpu/drm/amd/powerplay/inc/hwmgr.h
@@ -357,8 +357,10 @@ struct pp_hwmgr_func {
 	int (*set_sclk_od)(struct pp_hwmgr *hwmgr, uint32_t value);
 	int (*get_mclk_od)(struct pp_hwmgr *hwmgr);
 	int (*set_mclk_od)(struct pp_hwmgr *hwmgr, uint32_t value);
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
 	int (*set_power_profile_state)(struct pp_hwmgr *hwmgr,
-			struct pp_profile *request);	
+			struct pp_profile *request);
+#endif 				
 	int (*read_sensor)(struct pp_hwmgr *hwmgr, int idx, int32_t *value);
 };
 
@@ -651,12 +653,14 @@ struct pp_hwmgr {
 	struct pp_power_state    *uvd_ps;
 	struct amd_pp_display_configuration display_config;
 	uint32_t feature_mask;
+#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
 	/* power profile */
 	struct pp_profile gfx_power_profile;
 	struct pp_profile compute_power_profile;
 	struct pp_profile default_gfx_power_profile;
 	struct pp_profile default_compute_power_profile;
 	enum pp_profile_type current_power_profile;
+#endif	
 };
 
 
-- 
2.7.4

