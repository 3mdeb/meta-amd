From 0389ba9b0eaa14b5f0e753e4208bc272072835ea Mon Sep 17 00:00:00 2001
From: Kalyan Alle <kalyan.alle@amd.com>
Date: Thu, 25 Aug 2016 11:59:45 +0530
Subject: [PATCH 02/12] drm/amd: Indentation of the code

Spaces are converted to tabs whereever necessary.

Signed-off-by: Kalyan Alle <kalyan.alle@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu.h                | 361 ++++++++-------
 drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c            |  18 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c   |   6 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c           |   4 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c        |  58 +--
 drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c             | 506 ++++++++++-----------
 drivers/gpu/drm/amd/amdgpu/amdgpu_device.c         |  14 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_display.c        |   3 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h            |   4 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c          |   2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c            |  34 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c             |  56 +--
 drivers/gpu/drm/amd/amdgpu/amdgpu_job.c            | 231 +++++-----
 drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c            |  10 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_object.c         |  20 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c          |   2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c           |   6 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c            |  61 +--
 drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c            | 140 +++---
 drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c            |  65 +--
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c             | 394 ++++++++--------
 drivers/gpu/drm/amd/amdgpu/cik_sdma.c              |  14 +-
 drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c              |  12 +-
 drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c              | 406 ++++++++---------
 drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c             |  14 +-
 drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c             |  14 +-
 drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c              |  12 +-
 drivers/gpu/drm/amd/amdgpu/vi.c                    |  24 +-
 .../drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c    |   2 +
 drivers/gpu/drm/amd/dal/dc/core/dc.c               |   2 +-
 drivers/gpu/drm/amd/dal/dc/core/dc_resource.c      |  11 +-
 drivers/gpu/drm/amd/dal/dc/core/dc_stream.c        |   3 +-
 drivers/gpu/drm/amd/dal/dc/inc/core_types.h        |   2 +-
 .../gpu/drm/amd/dal/modules/freesync/freesync.c    |  14 +-
 drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c     |   2 +-
 35 files changed, 1270 insertions(+), 1257 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu.h b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
index c105e68..551f763 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
+ *	    Alex Deucher
+ *	    Jerome Glisse
  */
 #ifndef __AMDGPU_H__
 #define __AMDGPU_H__
@@ -90,7 +90,7 @@ extern int amdgpu_powerplay;
 extern unsigned amdgpu_pcie_gen_cap;
 extern unsigned amdgpu_pcie_lane_cap;
 
-#define AMDGPU_WAIT_IDLE_TIMEOUT_IN_MS	        3000
+#define AMDGPU_WAIT_IDLE_TIMEOUT_IN_MS		3000
 #define AMDGPU_MAX_USEC_TIMEOUT			100000	/* 100 ms */
 #define AMDGPU_FENCE_JIFFIES_TIMEOUT		(HZ / 2)
 /* AMDGPU_IB_POOL_SIZE must be a power of 2 */
@@ -112,7 +112,7 @@ extern unsigned amdgpu_pcie_lane_cap;
 #define AMDGPU_VA_RESERVED_SIZE			(8 << 20)
 
 /* hard reset data */
-#define AMDGPU_ASIC_RESET_DATA                  0x39d5e86b
+#define AMDGPU_ASIC_RESET_DATA			0x39d5e86b
 
 /* reset flags */
 #define AMDGPU_RESET_GFX			(1 << 0)
@@ -131,14 +131,14 @@ extern unsigned amdgpu_pcie_lane_cap;
 #define AMDGPU_RESET_VCE			(1 << 13)
 #define AMDGPU_RESET_VCE1			(1 << 14)
 
-#define AMDGPU_PG_SUPPORT_GFX_QUICK_MG         (1 << 11)
-#define AMDGPU_PG_SUPPORT_GFX_PIPELINE         (1 << 12)
+#define AMDGPU_PG_SUPPORT_GFX_QUICK_MG		(1 << 11)
+#define AMDGPU_PG_SUPPORT_GFX_PIPELINE		(1 << 12)
 
-#define AMDGPU_PG_SUPPORT_GFX_PG                (1 << 0)
-#define AMDGPU_PG_SUPPORT_GFX_SMG               (1 << 1)
-#define AMDGPU_PG_SUPPORT_GFX_DMG               (1 << 2)
-#define AMDGPU_PG_SUPPORT_CP                    (1 << 5)
-#define AMDGPU_PG_SUPPORT_RLC_SMU_HS            (1 << 7)
+#define AMDGPU_PG_SUPPORT_GFX_PG		(1 << 0)
+#define AMDGPU_PG_SUPPORT_GFX_SMG		(1 << 1)
+#define AMDGPU_PG_SUPPORT_GFX_DMG		(1 << 2)
+#define AMDGPU_PG_SUPPORT_CP			(1 << 5)
+#define AMDGPU_PG_SUPPORT_RLC_SMU_HS		(1 << 7)
 
 /* GFX current status */
 #define AMDGPU_GFX_NORMAL_MODE			0x00000000L
@@ -374,16 +374,16 @@ struct amdgpu_fence_driver {
 /* some special values for the owner field */
 #define AMDGPU_FENCE_OWNER_UNDEFINED	((void*)0ul)
 #define AMDGPU_FENCE_OWNER_VM		((void*)1ul)
-#define AMDGPU_CLIENT_ID_RESERVED       2
+#define AMDGPU_CLIENT_ID_RESERVED	2
 
-#define AMDGPU_FENCE_FLAG_64BIT         (1 << 0)
-#define AMDGPU_FENCE_FLAG_INT           (1 << 1)
+#define AMDGPU_FENCE_FLAG_64BIT		(1 << 0)
+#define AMDGPU_FENCE_FLAG_INT		(1 << 1)
 
 struct amdgpu_user_fence {
 	/* write-back bo */
-	struct amdgpu_bo 	*bo;
+	struct amdgpu_bo	*bo;
 	/* write-back address offset to bo start */
-	uint32_t                offset;
+	uint32_t		offset;
 };
 
 int amdgpu_fence_driver_init(struct amdgpu_device *adev);
@@ -414,7 +414,7 @@ struct amdgpu_mman_lru {
 };
 
 struct amdgpu_mman {
-	struct ttm_bo_global_ref        bo_global_ref;
+	struct ttm_bo_global_ref	bo_global_ref;
 	struct drm_global_reference	mem_global_ref;
 	struct ttm_bo_device		bdev;
 	bool				mem_global_referenced;
@@ -428,11 +428,11 @@ struct amdgpu_mman {
 	/* buffer handling */
 	const struct amdgpu_buffer_funcs	*buffer_funcs;
 	struct amdgpu_ring			*buffer_funcs_ring;
-        /* Scheduler entity for buffer moves */
-        struct amd_sched_entity                 entity;
+	/* Scheduler entity for buffer moves */
+	struct amd_sched_entity			entity;
 
-        /* custom LRU management */
-        struct amdgpu_mman_lru                  log2_size[AMDGPU_TTM_LRU_SIZE];
+	/* custom LRU management */
+	struct amdgpu_mman_lru			log2_size[AMDGPU_TTM_LRU_SIZE];
 };
 
 int amdgpu_copy_buffer(struct amdgpu_ring *ring,
@@ -463,7 +463,7 @@ struct amdgpu_bo_va_mapping {
 struct amdgpu_bo_va {
 	/* protected by bo being reserved */
 	struct list_head		bo_list;
-	struct fence		        *last_pt_update;
+	struct fence			*last_pt_update;
 	unsigned			ref_count;
 
 	/* protected by vm mutex and spinlock */
@@ -484,8 +484,8 @@ struct amdgpu_bo {
 	/* Protected by gem.mutex */
 	struct list_head		list;
 	/* Protected by tbo.reserved */
-        u32                             prefered_domains;
-        u32                             allowed_domains;
+	u32				prefered_domains;
+	u32				allowed_domains;
 	struct ttm_place		placements[AMDGPU_GEM_DOMAIN_MAX + 1];
 	struct ttm_placement		placement;
 	struct ttm_buffer_object	tbo;
@@ -519,8 +519,7 @@ void amdgpu_gem_object_close(struct drm_gem_object *obj,
 				struct drm_file *file_priv);
 unsigned long amdgpu_gem_timeout(uint64_t timeout_ns);
 struct sg_table *amdgpu_gem_prime_get_sg_table(struct drm_gem_object *obj);
-struct drm_gem_object *
-amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
+struct drm_gem_object *amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
 				 struct dma_buf_attachment *attach,
 				 struct sg_table *sg);
 struct dma_buf *amdgpu_gem_prime_export(struct drm_device *dev,
@@ -579,7 +578,7 @@ struct amdgpu_sa_bo {
 	struct amdgpu_sa_manager	*manager;
 	unsigned			soffset;
 	unsigned			eoffset;
-	struct fence		        *fence;
+	struct fence			*fence;
 };
 
 /*
@@ -602,7 +601,7 @@ int amdgpu_mode_dumb_mmap(struct drm_file *filp,
  */
 struct amdgpu_sync {
 	DECLARE_HASHTABLE(fences, 4);
-	struct fence	        *last_vm_update;
+	struct fence		*last_vm_update;
 };
 
 void amdgpu_sync_create(struct amdgpu_sync *sync);
@@ -614,7 +613,7 @@ int amdgpu_sync_resv(struct amdgpu_device *adev,
 		     void *owner);
 bool amdgpu_sync_is_idle(struct amdgpu_sync *sync);
 int amdgpu_sync_cycle_fences(struct amdgpu_sync *dst, struct amdgpu_sync *src,
-                            struct fence *fence);
+			     struct fence *fence);
 struct fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync);
 int amdgpu_sync_wait(struct amdgpu_sync *sync);
 void amdgpu_sync_free(struct amdgpu_sync *sync);
@@ -678,10 +677,10 @@ struct amdgpu_mc {
 	unsigned		vram_width;
 	u64			real_vram_size;
 	int			vram_mtrr;
-	u64                     gtt_base_align;
-	u64                     mc_mask;
-	const struct firmware   *fw;	/* MC firmware */
-	uint32_t                fw_version;
+	u64			gtt_base_align;
+	u64			mc_mask;
+	const struct firmware	*fw;	/* MC firmware */
+	uint32_t		fw_version;
 	struct amdgpu_irq_src	vm_fault;
 	uint32_t		vram_type;
 };
@@ -691,23 +690,23 @@ struct amdgpu_mc {
  */
 typedef enum _AMDGPU_DOORBELL_ASSIGNMENT
 {
-	AMDGPU_DOORBELL_KIQ                     = 0x000,
-	AMDGPU_DOORBELL_HIQ                     = 0x001,
-	AMDGPU_DOORBELL_DIQ                     = 0x002,
-	AMDGPU_DOORBELL_MEC_RING0               = 0x010,
-	AMDGPU_DOORBELL_MEC_RING1               = 0x011,
-	AMDGPU_DOORBELL_MEC_RING2               = 0x012,
-	AMDGPU_DOORBELL_MEC_RING3               = 0x013,
-	AMDGPU_DOORBELL_MEC_RING4               = 0x014,
-	AMDGPU_DOORBELL_MEC_RING5               = 0x015,
-	AMDGPU_DOORBELL_MEC_RING6               = 0x016,
-	AMDGPU_DOORBELL_MEC_RING7               = 0x017,
-	AMDGPU_DOORBELL_GFX_RING0               = 0x020,
-	AMDGPU_DOORBELL_sDMA_ENGINE0            = 0x1E0,
-	AMDGPU_DOORBELL_sDMA_ENGINE1            = 0x1E1,
-	AMDGPU_DOORBELL_IH                      = 0x1E8,
-	AMDGPU_DOORBELL_MAX_ASSIGNMENT          = 0x3FF,
-	AMDGPU_DOORBELL_INVALID                 = 0xFFFF
+	AMDGPU_DOORBELL_KIQ			= 0x000,
+	AMDGPU_DOORBELL_HIQ			= 0x001,
+	AMDGPU_DOORBELL_DIQ			= 0x002,
+	AMDGPU_DOORBELL_MEC_RING0		= 0x010,
+	AMDGPU_DOORBELL_MEC_RING1		= 0x011,
+	AMDGPU_DOORBELL_MEC_RING2		= 0x012,
+	AMDGPU_DOORBELL_MEC_RING3		= 0x013,
+	AMDGPU_DOORBELL_MEC_RING4		= 0x014,
+	AMDGPU_DOORBELL_MEC_RING5		= 0x015,
+	AMDGPU_DOORBELL_MEC_RING6		= 0x016,
+	AMDGPU_DOORBELL_MEC_RING7		= 0x017,
+	AMDGPU_DOORBELL_GFX_RING0		= 0x020,
+	AMDGPU_DOORBELL_sDMA_ENGINE0		= 0x1E0,
+	AMDGPU_DOORBELL_sDMA_ENGINE1		= 0x1E1,
+	AMDGPU_DOORBELL_IH			= 0x1E8,
+	AMDGPU_DOORBELL_MAX_ASSIGNMENT		= 0x3FF,
+	AMDGPU_DOORBELL_INVALID			= 0xFFFF
 } AMDGPU_DOORBELL_ASSIGNMENT;
 
 struct amdgpu_doorbell {
@@ -751,9 +750,9 @@ struct amdgpu_ib {
 	uint32_t			length_dw;
 	uint64_t			gpu_addr;
 	uint32_t			*ptr;
-	struct amdgpu_user_fence        *user;
-        unsigned                        vm_id;
-        uint64_t                        vm_pd_addr;
+	struct amdgpu_user_fence	*user;
+	unsigned			vm_id;
+	uint64_t			vm_pd_addr;
 	uint32_t			gds_base, gds_size;
 	uint32_t			gws_base, gws_size;
 	uint32_t			oa_base, oa_size;
@@ -772,24 +771,24 @@ enum amdgpu_ring_type {
 
 extern const struct amd_sched_backend_ops amdgpu_sched_ops;
 int amdgpu_job_alloc(struct amdgpu_device *adev, unsigned num_ibs,
-                      struct amdgpu_job **job, struct amdgpu_vm *vm);
+		     struct amdgpu_job **job, struct amdgpu_vm *vm);
 int amdgpu_job_alloc_with_ib(struct amdgpu_device *adev, unsigned size,
-                      struct amdgpu_job **job);
+			     struct amdgpu_job **job);
 
 void amdgpu_job_free(struct amdgpu_job *job);
 void amdgpu_job_free_func(struct kref *refcount);
 int amdgpu_job_submit(struct amdgpu_job *job, struct amdgpu_ring *ring,
-                      struct amd_sched_entity *entity, void *owner,
-                      struct fence **f);
+		      struct amd_sched_entity *entity, void *owner,
+		      struct fence **f);
 void amdgpu_job_timeout_func(struct work_struct *work);
 
 struct amdgpu_ring {
 	struct amdgpu_device		*adev;
 	const struct amdgpu_ring_funcs	*funcs;
 	struct amdgpu_fence_driver	fence_drv;
-	struct amd_gpu_scheduler 	sched;
+	struct amd_gpu_scheduler	sched;
 
-	spinlock_t              fence_lock;
+	spinlock_t		fence_lock;
 	struct amdgpu_bo	*ring_obj;
 	volatile uint32_t	*ring;
 	unsigned		rptr_offs;
@@ -815,7 +814,7 @@ struct amdgpu_ring {
 	unsigned		wptr_offs;
 	unsigned		next_rptr_offs;
 	unsigned		fence_offs;
-	uint64_t last_fence_context;
+	uint64_t 		last_fence_context;
 	enum amdgpu_ring_type	type;
 	char			name[16];
 	unsigned		cond_exe_offs;
@@ -859,9 +858,8 @@ struct amdgpu_ring {
 #define AMDGPU_VM_FAULT_STOP_ALWAYS	2
 
 struct amdgpu_vm_pt {
-        struct amdgpu_bo_list_entry     entry;
-        uint64_t                        addr;
-
+	struct amdgpu_bo_list_entry	entry;
+	uint64_t			addr;
 };
 
 struct amdgpu_vm {
@@ -889,36 +887,36 @@ struct amdgpu_vm {
 	struct amdgpu_vm_pt	*page_tables;
 
 	/* for id and flush management per ring */
-        struct amdgpu_vm_id     *ids[AMDGPU_MAX_RINGS];
+	struct amdgpu_vm_id	*ids[AMDGPU_MAX_RINGS];
 
 	/* protecting freed */
 	spinlock_t		freed_lock;
 
-        /* Scheduler entity for page table updates */
-        struct amd_sched_entity entity;
+	/* Scheduler entity for page table updates */
+	struct amd_sched_entity entity;
  
-        /* client id */
-        u64                     client_id;
+	/* client id */
+	u64			client_id;
 };
 
 struct amdgpu_vm_id {
 	struct list_head	list;
-        struct fence            *first;
-        struct amdgpu_sync      active;
-        struct fence            *last_flush;
-        struct amdgpu_ring      *last_user;
-        atomic64_t              owner;
+	struct fence		*first;
+	struct amdgpu_sync	active;
+	struct fence		*last_flush;
+	struct amdgpu_ring	*last_user;
+	atomic64_t		owner;
 
-        uint64_t                pd_gpu_addr;
-        /* last flushed PD/PT update */
-        struct fence            *flushed_updates;
+	uint64_t		pd_gpu_addr;
+	/* last flushed PD/PT update */
+	struct fence		*flushed_updates;
 
-        uint32_t                gds_base;
-        uint32_t                gds_size;
-        uint32_t                gws_base;
-        uint32_t                gws_size;
-        uint32_t                oa_base;
-        uint32_t                oa_size;
+	uint32_t		gds_base;
+	uint32_t		gds_size;
+	uint32_t		gws_base;
+	uint32_t		gws_size;
+	uint32_t		oa_base;
+	uint32_t		oa_size;
 };
 
 struct amdgpu_vm_manager {
@@ -926,7 +924,7 @@ struct amdgpu_vm_manager {
 	struct mutex				lock;
 	unsigned				num_ids;
 	struct list_head			ids_lru;
-        struct amdgpu_vm_id                     ids[AMDGPU_NUM_VM];
+	struct amdgpu_vm_id			ids[AMDGPU_NUM_VM];
 
 	uint32_t				max_pfn;
 	/* vram base address for page table entry  */
@@ -934,12 +932,12 @@ struct amdgpu_vm_manager {
 	/* is vm enabled? */
 	bool					enabled;
 	/* vm pte handling */
-	const struct amdgpu_vm_pte_funcs        *vm_pte_funcs;
-        struct amdgpu_ring                      *vm_pte_rings[AMDGPU_MAX_RINGS];
-        unsigned                                vm_pte_num_rings;
-        atomic_t                                vm_pte_next_ring;
-        /* client id counter */
-        atomic64_t                              client_counter;
+	const struct amdgpu_vm_pte_funcs	*vm_pte_funcs;
+	struct amdgpu_ring			*vm_pte_rings[AMDGPU_MAX_RINGS];
+	unsigned				vm_pte_num_rings;
+	atomic_t				vm_pte_next_ring;
+	/* client id counter */
+	atomic64_t				client_counter;
 };
 
 void amdgpu_vm_manager_init(struct amdgpu_device *adev);
@@ -947,23 +945,23 @@ void amdgpu_vm_manager_fini(struct amdgpu_device *adev);
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 struct amdgpu_bo_list_entry *amdgpu_vm_get_bos(struct amdgpu_device *adev,
-                                               struct amdgpu_vm *vm,
-                                               struct list_head *validated,
-                                               struct list_head *duplicates);
+					       struct amdgpu_vm *vm,
+					       struct list_head *validated,
+					       struct list_head *duplicates);
 void amdgpu_vm_get_pd_bo(struct amdgpu_vm *vm,
-                         struct list_head *validated,
-                         struct amdgpu_bo_list_entry *entry);
+			 struct list_head *validated,
+			 struct amdgpu_bo_list_entry *entry);
 void amdgpu_vm_get_pt_bos(struct amdgpu_vm *vm, struct list_head *duplicates);
 void amdgpu_vm_move_pt_bos_in_lru(struct amdgpu_device *adev,
-                                  struct amdgpu_vm *vm);
+				  struct amdgpu_vm *vm);
 int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
-                      struct amdgpu_sync *sync, struct fence *fence,
-                      unsigned *vm_id, uint64_t *vm_pd_addr);
+		      struct amdgpu_sync *sync, struct fence *fence,
+		      unsigned *vm_id, uint64_t *vm_pd_addr);
 int amdgpu_vm_flush(struct amdgpu_ring *ring,
-                    unsigned vm_id, uint64_t pd_addr,
-                    uint32_t gds_base, uint32_t gds_size,
-                    uint32_t gws_base, uint32_t gws_size,
-                    uint32_t oa_base, uint32_t oa_size);
+		    unsigned vm_id, uint64_t pd_addr,
+		    uint32_t gds_base, uint32_t gds_size,
+		    uint32_t gws_base, uint32_t gws_size,
+		    uint32_t oa_base, uint32_t oa_size);
 void amdgpu_vm_reset_id(struct amdgpu_device *adev, unsigned vm_id);
 uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr);
 int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
@@ -1000,17 +998,16 @@ struct amdgpu_ctx_ring {
 	uint64_t		sequence;
 	struct fence		**fences;
 	struct amd_sched_entity	entity;
-
 	/* client id */
-	u64                     client_id;
+	u64			client_id;
 };
 
 struct amdgpu_ctx {
 	struct kref		refcount;
-	struct amdgpu_device    *adev;
+	struct amdgpu_device	*adev;
 	unsigned		reset_counter;
 	spinlock_t		ring_lock;
-	struct fence            **fences;
+	struct fence		**fences;
 	struct amdgpu_ctx_ring	rings[AMDGPU_MAX_RINGS];
 };
 
@@ -1055,7 +1052,7 @@ struct amdgpu_bo_list {
 	struct amdgpu_bo *gds_obj;
 	struct amdgpu_bo *gws_obj;
 	struct amdgpu_bo *oa_obj;
-        unsigned first_userptr;
+	unsigned first_userptr;
 	unsigned num_entries;
 	struct amdgpu_bo_list_entry *array;
 };
@@ -1080,19 +1077,19 @@ struct amdgpu_rlc {
 	struct amdgpu_bo	*save_restore_obj;
 	uint64_t		save_restore_gpu_addr;
 	volatile uint32_t	*sr_ptr;
-	const u32               *reg_list;
-	u32                     reg_list_size;
+	const u32		*reg_list;
+	u32			reg_list_size;
 	/* for clear state */
 	struct amdgpu_bo	*clear_state_obj;
 	uint64_t		clear_state_gpu_addr;
 	volatile uint32_t	*cs_ptr;
 	const struct cs_section_def   *cs_data;
-	u32                     clear_state_size;
+	u32			clear_state_size;
 	/* for cp tables */
 	struct amdgpu_bo	*cp_table_obj;
 	uint64_t		cp_table_gpu_addr;
 	volatile uint32_t	*cp_table_ptr;
-	u32                     cp_table_size;
+	u32			cp_table_size;
 
 	/* safe mode for updating CG/PG state */
 	bool in_safe_mode;
@@ -1126,7 +1123,7 @@ struct amdgpu_mec {
  */
 struct amdgpu_scratch {
 	unsigned		num_reg;
-	uint32_t                reg_base;
+	uint32_t		reg_base;
 	bool			free[32];
 	uint32_t		reg[32];
 };
@@ -1196,18 +1193,18 @@ struct amdgpu_gfx {
 	struct amdgpu_irq_src		priv_reg_irq;
 	struct amdgpu_irq_src		priv_inst_irq;
 	/* gfx status */
-	uint32_t gfx_current_status;
+	uint32_t			gfx_current_status;
 	/* ce ram size*/
-	unsigned ce_ram_size;
+	unsigned			ce_ram_size;
 };
 
 int amdgpu_ib_get(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 		  unsigned size, struct amdgpu_ib *ib);
 void amdgpu_ib_free(struct amdgpu_device *adev, struct amdgpu_ib *ib,
-                    struct fence *f);
+		    struct fence *f);
 int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
-                       struct amdgpu_ib *ib, struct fence *last_vm_update,
-                       struct amdgpu_job *job, struct fence **f);
+		       struct amdgpu_ib *ib, struct fence *last_vm_update,
+		       struct amdgpu_job *job, struct fence **f);
 int amdgpu_ib_pool_init(struct amdgpu_device *adev);
 void amdgpu_ib_pool_fini(struct amdgpu_device *adev);
 int amdgpu_ib_ring_tests(struct amdgpu_device *adev);
@@ -1244,34 +1241,34 @@ struct amdgpu_cs_parser {
 	unsigned		nchunks;
 	struct amdgpu_cs_chunk	*chunks;
 
-       /* scheduler job object */
-        struct amdgpu_job       *job;
+	/* scheduler job object */
+	struct amdgpu_job	*job;
  
-       /* buffer objects */
-         struct ww_acquire_ctx           ticket;
-         struct amdgpu_bo_list           *bo_list;
-         struct amdgpu_bo_list_entry     vm_pd;
-         struct list_head                validated;
-         struct fence                    *fence;
-         uint64_t                        bytes_moved_threshold;
-         uint64_t                        bytes_moved;
+	/* buffer objects */
+	struct ww_acquire_ctx		ticket;
+	struct amdgpu_bo_list		*bo_list;
+	struct amdgpu_bo_list_entry	vm_pd;
+	struct list_head		validated;
+	struct fence			*fence;
+	uint64_t			bytes_moved_threshold;
+	uint64_t			bytes_moved;
 
 	/* user fence */
 	struct amdgpu_bo_list_entry	uf_entry;
 };
 
 struct amdgpu_job {
-	struct amd_sched_job    base;
+	struct amd_sched_job	base;
 	struct amdgpu_device	*adev;
-        struct amdgpu_vm        *vm;
-        struct amdgpu_ring      *ring;
-        struct amdgpu_sync      sync;
+	struct amdgpu_vm	*vm;
+	struct amdgpu_ring	*ring;
+	struct amdgpu_sync	sync;
 	struct amdgpu_ib	*ibs;
-        struct fence            *fence; /* the hw fence */
+	struct fence		*fence; /* the hw fence */
 	uint32_t		num_ibs;
 	void			*owner;
-        uint64_t                        fence_context;
-	struct amdgpu_user_fence uf;
+	uint64_t		fence_context;
+	struct amdgpu_user_fence	 uf;
 };
 #define to_amdgpu_job(sched_job)		\
 		container_of((sched_job), struct amdgpu_job, base)
@@ -1279,14 +1276,14 @@ struct amdgpu_job {
 static inline u32 amdgpu_get_ib_value(struct amdgpu_cs_parser *p,
 				      uint32_t ib_idx, int idx)
 {
-        return p->job->ibs[ib_idx].ptr[idx];
+	return p->job->ibs[ib_idx].ptr[idx];
 }
 
 static inline void amdgpu_set_ib_value(struct amdgpu_cs_parser *p,
 				       uint32_t ib_idx, int idx,
 				       uint32_t value)
 {
-        p->job->ibs[ib_idx].ptr[idx] = value;
+	p->job->ibs[ib_idx].ptr[idx] = value;
 }
 
 /*
@@ -1339,8 +1336,8 @@ enum amdgpu_dpm_event_src {
 #define AMDGPU_MAX_VCE_LEVELS 6
 
 enum amdgpu_vce_level {
-	AMDGPU_VCE_LEVEL_AC_ALL = 0,     /* AC, All cases */
-	AMDGPU_VCE_LEVEL_DC_EE = 1,      /* DC, entropy encoding */
+	AMDGPU_VCE_LEVEL_AC_ALL = 0,	 /* AC, All cases */
+	AMDGPU_VCE_LEVEL_DC_EE = 1,	 /* DC, entropy encoding */
 	AMDGPU_VCE_LEVEL_DC_LL_LOW = 2,  /* DC, low latency queue, res <= 720 */
 	AMDGPU_VCE_LEVEL_DC_LL_HIGH = 3, /* DC, low latency queue, 1080 >= res > 720 */
 	AMDGPU_VCE_LEVEL_DC_GP_LOW = 4,  /* DC, general purpose queue, res <= 720 */
@@ -1367,11 +1364,11 @@ struct amdgpu_dpm_thermal {
 	/* thermal interrupt work */
 	struct work_struct work;
 	/* low temperature threshold */
-	int                min_temp;
+	int		   min_temp;
 	/* high temperature threshold */
-	int                max_temp;
+	int		   max_temp;
 	/* was last interrupt low to high or high to low */
-	bool               high_to_low;
+	bool		   high_to_low;
 	/* interrupt source */
 	struct amdgpu_irq_src	irq;
 };
@@ -1574,26 +1571,26 @@ struct amdgpu_dpm_funcs {
 };
 
 struct amdgpu_dpm {
-	struct amdgpu_ps        *ps;
+	struct amdgpu_ps	*ps;
 	/* number of valid power states */
-	int                     num_ps;
+	int			num_ps;
 	/* current power state that is active */
-	struct amdgpu_ps        *current_ps;
+	struct amdgpu_ps	*current_ps;
 	/* requested power state */
-	struct amdgpu_ps        *requested_ps;
+	struct amdgpu_ps	*requested_ps;
 	/* boot up power state */
-	struct amdgpu_ps        *boot_ps;
+	struct amdgpu_ps	*boot_ps;
 	/* default uvd power state */
-	struct amdgpu_ps        *uvd_ps;
+	struct amdgpu_ps	*uvd_ps;
 	/* vce requirements */
 	struct amdgpu_vce_state vce_states[AMDGPU_MAX_VCE_LEVELS];
 	enum amdgpu_vce_level vce_level;
 	enum amd_pm_state_type state;
 	enum amd_pm_state_type user_state;
-	u32                     platform_caps;
-	u32                     voltage_response_time;
-	u32                     backbias_response_time;
-	void                    *priv;
+	u32			platform_caps;
+	u32			voltage_response_time;
+	u32			backbias_response_time;
+	void			*priv;
 	u32			new_active_crtcs;
 	int			new_active_crtc_count;
 	u32			current_active_crtcs;
@@ -1611,9 +1608,9 @@ struct amdgpu_dpm {
 	bool power_control;
 	bool ac_power;
 	/* special states active */
-	bool                    thermal_active;
-	bool                    uvd_active;
-	bool                    vce_active;
+	bool			thermal_active;
+	bool			uvd_active;
+	bool			vce_active;
 	/* thermal handling */
 	struct amdgpu_dpm_thermal thermal;
 	/* forced levels */
@@ -1622,28 +1619,28 @@ struct amdgpu_dpm {
 
 struct amdgpu_pm {
 	struct mutex		mutex;
-	u32                     current_sclk;
-	u32                     current_mclk;
-	u32                     default_sclk;
-	u32                     default_mclk;
+	u32			current_sclk;
+	u32			current_mclk;
+	u32			default_sclk;
+	u32			default_mclk;
 	struct amdgpu_i2c_chan *i2c_bus;
 	/* internal thermal controller on rv6xx+ */
 	enum amdgpu_int_thermal_type int_thermal_type;
-	struct device	        *int_hwmon_dev;
+	struct device		*int_hwmon_dev;
 	/* fan control parameters */
-	bool                    no_fan;
-	u8                      fan_pulses_per_revolution;
-	u8                      fan_min_rpm;
-	u8                      fan_max_rpm;
+	bool			no_fan;
+	u8			fan_pulses_per_revolution;
+	u8			fan_min_rpm;
+	u8			fan_max_rpm;
 	/* dpm */
-	bool                    dpm_enabled;
-	bool                    sysfs_initialized;
-	struct amdgpu_dpm       dpm;
+	bool			dpm_enabled;
+	bool			sysfs_initialized;
+	struct amdgpu_dpm	dpm;
 	const struct firmware	*fw;	/* SMC firmware */
-	uint32_t                fw_version;
+	uint32_t		fw_version;
 	const struct amdgpu_dpm_funcs *funcs;
-	uint32_t                pcie_gen_mask;
-	uint32_t                pcie_mlw_mask;
+	uint32_t		pcie_gen_mask;
+	uint32_t		pcie_mlw_mask;
 	struct amd_pp_display_configuration pm_display_cfg;/* set by DAL */
 };
 
@@ -1653,19 +1650,19 @@ void amdgpu_get_pcie_info(struct amdgpu_device *adev);
  * UVD
  */
 #define AMDGPU_DEFAULT_UVD_HANDLES     10
-#define AMDGPU_MAX_UVD_HANDLES         40
-#define AMDGPU_UVD_STACK_SIZE          (200*1024)
-#define AMDGPU_UVD_HEAP_SIZE           (256*1024)
-#define AMDGPU_UVD_SESSION_SIZE                (50*1024)
+#define AMDGPU_MAX_UVD_HANDLES	       40
+#define AMDGPU_UVD_STACK_SIZE	       (200*1024)
+#define AMDGPU_UVD_HEAP_SIZE	       (256*1024)
+#define AMDGPU_UVD_SESSION_SIZE		       (50*1024)
 #define AMDGPU_UVD_FIRMWARE_OFFSET     256
 
 struct amdgpu_uvd {
 	struct amdgpu_bo	*vcpu_bo;
 	void			*cpu_addr;
 	uint64_t		gpu_addr;
-        void                    *saved_bo;
-        unsigned                max_handles;
 	unsigned		fw_version;
+	void			*saved_bo;
+	unsigned		max_handles;
 	atomic_t		handles[AMDGPU_MAX_UVD_HANDLES];
 	struct drm_file		*filp[AMDGPU_MAX_UVD_HANDLES];
 	struct delayed_work	idle_work;
@@ -1673,7 +1670,7 @@ struct amdgpu_uvd {
 	struct amdgpu_ring	ring;
 	struct amdgpu_irq_src	irq;
 	bool			address_64_bit;
-        struct amd_sched_entity entity;
+	struct amd_sched_entity entity;
 };
 
 /*
@@ -1718,7 +1715,7 @@ struct amdgpu_sdma {
 	struct amdgpu_sdma_instance instance[AMDGPU_MAX_SDMA_INSTANCES];
 	struct amdgpu_irq_src	trap_irq;
 	struct amdgpu_irq_src	illegal_inst_irq;
-	int 			num_instances;
+	int			num_instances;
 };
 
 /*
@@ -1975,11 +1972,11 @@ struct amdgpu_device {
 	bool				shutdown;
 	bool				need_dma32;
 	bool				accel_working;
-	struct work_struct 		reset_work;
+	struct work_struct		reset_work;
 	struct notifier_block		acpi_nb;
 	struct amdgpu_i2c_chan		*i2c_bus[AMDGPU_MAX_I2C_BUS];
 	struct amdgpu_debugfs		debugfs[AMDGPU_DEBUGFS_MAX_COMPONENTS];
-	unsigned 			debugfs_count;
+	unsigned			debugfs_count;
 #if defined(CONFIG_DEBUG_FS)
 	struct dentry			*debugfs_regs[AMDGPU_DEBUGFS_MAX_COMPONENTS];
 #endif
@@ -1987,7 +1984,7 @@ struct amdgpu_device {
 	struct amdgpu_atcs		atcs;
 	struct mutex			srbm_mutex;
 	/* GRBM index mutex. Protects concurrent access to GRBM index */
-	struct mutex                    grbm_idx_mutex;
+	struct mutex			grbm_idx_mutex;
 	struct dev_pm_domain		vga_pm_domain;
 	bool				have_disp_power_ref;
 
@@ -2023,12 +2020,12 @@ struct amdgpu_device {
 	spinlock_t audio_endpt_idx_lock;
 	amdgpu_block_rreg_t		audio_endpt_rreg;
 	amdgpu_block_wreg_t		audio_endpt_wreg;
-	void __iomem                    *rio_mem;
+	void __iomem			*rio_mem;
 	resource_size_t			rio_mem_size;
 	struct amdgpu_doorbell		doorbell;
 
 	/* clock/pll info */
-	struct amdgpu_clock            clock;
+	struct amdgpu_clock	       clock;
 
 	/* MC */
 	struct amdgpu_mc		mc;
@@ -2110,7 +2107,7 @@ struct amdgpu_device {
 	u64 gart_pin_size;
 
 	/* amdkfd interface */
-	struct kfd_dev          *kfd;
+	struct kfd_dev		*kfd;
 
 	struct amdgpu_virtualization virtualization;
 };
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c
index 252edba..043ba60 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c
@@ -35,17 +35,17 @@
 
 #include "acp_gfx_if.h"
 
-#define ACP_TILE_ON_MASK                0x03
-#define ACP_TILE_OFF_MASK               0x02
-#define ACP_TILE_ON_RETAIN_REG_MASK     0x1f
-#define ACP_TILE_OFF_RETAIN_REG_MASK    0x20
+#define ACP_TILE_ON_MASK		0x03
+#define ACP_TILE_OFF_MASK		0x02
+#define ACP_TILE_ON_RETAIN_REG_MASK	0x1f
+#define ACP_TILE_OFF_RETAIN_REG_MASK	0x20
 
-#define ACP_TILE_P1_MASK                0x3e
-#define ACP_TILE_P2_MASK                0x3d
-#define ACP_TILE_DSP0_MASK              0x3b
-#define ACP_TILE_DSP1_MASK              0x37
+#define ACP_TILE_P1_MASK		0x3e
+#define ACP_TILE_P2_MASK		0x3d
+#define ACP_TILE_DSP0_MASK		0x3b
+#define ACP_TILE_DSP1_MASK		0x37
 
-#define ACP_TILE_DSP2_MASK              0x2f
+#define ACP_TILE_DSP2_MASK		0x2f
 
 #define ACP_DMA_REGS_END		0x146c0
 #define ACP_I2S_PLAY_REGS_START		0x14840
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
index 45d93bb..8162152 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
@@ -28,7 +28,7 @@ struct amdgpu_atpx_functions {
 struct amdgpu_atpx {
 	acpi_handle handle;
 	struct amdgpu_atpx_functions functions;
-        bool is_hybrid;
+	bool is_hybrid;
 };
 
 static struct amdgpu_atpx_priv {
@@ -65,11 +65,11 @@ bool amdgpu_has_atpx(void) {
 	return amdgpu_atpx_priv.atpx_detected;
 }
 bool amdgpu_has_atpx_dgpu_power_cntl(void) {
-         return amdgpu_atpx_priv.atpx.functions.power_cntl;
+	return amdgpu_atpx_priv.atpx.functions.power_cntl;
 }
 
 bool amdgpu_is_atpx_hybrid(void) {
-         return amdgpu_atpx_priv.atpx.is_hybrid;
+	return amdgpu_atpx_priv.atpx.is_hybrid;
 }
 
 /**
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c
index 99ca75b..4c803ed 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
+ *	    Alex Deucher
+ *	    Jerome Glisse
  */
 #include <drm/drmP.h>
 #include "amdgpu.h"
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c
index db36de6..35d0856 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c
@@ -91,7 +91,7 @@ static int amdgpu_bo_list_set(struct amdgpu_device *adev,
 	struct amdgpu_bo *gws_obj = adev->gds.gws_gfx_bo;
 	struct amdgpu_bo *oa_obj = adev->gds.oa_gfx_bo;
 
-        unsigned last_entry = 0, first_userptr = num_entries;
+	unsigned last_entry = 0, first_userptr = num_entries;
 	unsigned i;
 	int r;
 
@@ -101,43 +101,43 @@ static int amdgpu_bo_list_set(struct amdgpu_device *adev,
 	memset(array, 0, num_entries * sizeof(struct amdgpu_bo_list_entry));
 
 	for (i = 0; i < num_entries; ++i) {
-                struct amdgpu_bo_list_entry *entry;
+		struct amdgpu_bo_list_entry *entry;
 		struct drm_gem_object *gobj;
-                struct amdgpu_bo *bo;
-                struct mm_struct *usermm;
+		struct amdgpu_bo *bo;
+		struct mm_struct *usermm;
 
 		gobj = drm_gem_object_lookup(adev->ddev, filp, info[i].bo_handle);
-                if (!gobj) {
-                        r = -ENOENT;
+		if (!gobj) {
+			r = -ENOENT;
 			goto error_free;
-                }
+		}
 
-                bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));
+		bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));
 		drm_gem_object_unreference_unlocked(gobj);
 
-                usermm = amdgpu_ttm_tt_get_usermm(bo->tbo.ttm);
-                if (usermm) {
-                        if (usermm != current->mm) {
-                                amdgpu_bo_unref(&bo);
-                                r = -EPERM;
-                                goto error_free;
-                        }
-                        entry = &array[--first_userptr];
-                } else {
-                        entry = &array[last_entry++];
-                }
-                
+		usermm = amdgpu_ttm_tt_get_usermm(bo->tbo.ttm);
+		if (usermm) {
+			if (usermm != current->mm) {
+				amdgpu_bo_unref(&bo);
+				r = -EPERM;
+				goto error_free;
+			}
+			entry = &array[--first_userptr];
+		} else {
+			entry = &array[last_entry++];
+		}
+
 		entry->robj = bo;
-                entry->priority = min(info[i].bo_priority,
-                                      AMDGPU_BO_LIST_MAX_PRIORITY);
+		entry->priority = min(info[i].bo_priority,
+				      AMDGPU_BO_LIST_MAX_PRIORITY);
 		entry->tv.bo = &entry->robj->tbo;
 		entry->tv.shared = true;
 
-                if (entry->robj->prefered_domains == AMDGPU_GEM_DOMAIN_GDS)
+		if (entry->robj->prefered_domains == AMDGPU_GEM_DOMAIN_GDS)
 			gds_obj = entry->robj;
-                if (entry->robj->prefered_domains == AMDGPU_GEM_DOMAIN_GWS)
+		if (entry->robj->prefered_domains == AMDGPU_GEM_DOMAIN_GWS)
 			gws_obj = entry->robj;
-                if (entry->robj->prefered_domains == AMDGPU_GEM_DOMAIN_OA)
+		if (entry->robj->prefered_domains == AMDGPU_GEM_DOMAIN_OA)
 			oa_obj = entry->robj;
 
 		trace_amdgpu_bo_list_set(list, entry->robj);
@@ -151,17 +151,17 @@ static int amdgpu_bo_list_set(struct amdgpu_device *adev,
 	list->gds_obj = gds_obj;
 	list->gws_obj = gws_obj;
 	list->oa_obj = oa_obj;
-        list->first_userptr = first_userptr;
+	list->first_userptr = first_userptr;
 	list->array = array;
 	list->num_entries = num_entries;
 
 	return 0;
 
 error_free:
-        while (i--)
-                amdgpu_bo_unref(&array[i].robj);
+	while (i--)
+		amdgpu_bo_unref(&array[i].robj);
 	drm_free_large(array);
-        return r;
+	return r;
 }
 
 struct amdgpu_bo_list *
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
index bff83e6..f983846 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
@@ -30,45 +30,45 @@
 #include "amdgpu.h"
 #include "amdgpu_trace.h"
 
-#define AMDGPU_CS_MAX_PRIORITY         32u
-#define AMDGPU_CS_NUM_BUCKETS          (AMDGPU_CS_MAX_PRIORITY + 1)
+#define AMDGPU_CS_MAX_PRIORITY		32u
+#define AMDGPU_CS_NUM_BUCKETS		(AMDGPU_CS_MAX_PRIORITY + 1)
 
 /* This is based on the bucket sort with O(n) time complexity.
  * An item with priority "i" is added to bucket[i]. The lists are then
  * concatenated in descending order.
  */
 struct amdgpu_cs_buckets {
-       struct list_head bucket[AMDGPU_CS_NUM_BUCKETS];
+	struct list_head bucket[AMDGPU_CS_NUM_BUCKETS];
 };
 
 static void amdgpu_cs_buckets_init(struct amdgpu_cs_buckets *b)
 {
-       unsigned i;
+	unsigned i;
 
-       for (i = 0; i < AMDGPU_CS_NUM_BUCKETS; i++)
-               INIT_LIST_HEAD(&b->bucket[i]);
+	for (i = 0; i < AMDGPU_CS_NUM_BUCKETS; i++)
+		INIT_LIST_HEAD(&b->bucket[i]);
 }
 
 static void amdgpu_cs_buckets_add(struct amdgpu_cs_buckets *b,
-                                 struct list_head *item, unsigned priority)
+				  struct list_head *item, unsigned priority)
 {
-       /* Since buffers which appear sooner in the relocation list are
-        * likely to be used more often than buffers which appear later
-        * in the list, the sort mustn't change the ordering of buffers
-        * with the same priority, i.e. it must be stable.
-        */
-       list_add_tail(item, &b->bucket[min(priority, AMDGPU_CS_MAX_PRIORITY)]);
+	/* Since buffers which appear sooner in the relocation list are
+	 * likely to be used more often than buffers which appear later
+	 * in the list, the sort mustn't change the ordering of buffers
+	 * with the same priority, i.e. it must be stable.
+	 */
+	list_add_tail(item, &b->bucket[min(priority, AMDGPU_CS_MAX_PRIORITY)]);
 }
 
 static void amdgpu_cs_buckets_get_list(struct amdgpu_cs_buckets *b,
-                                      struct list_head *out_list)
+				       struct list_head *out_list)
 {
-       unsigned i;
+	unsigned i;
 
-       /* Connect the sorted buckets in the output list. */
-       for (i = 0; i < AMDGPU_CS_NUM_BUCKETS; i++) {
-               list_splice(&b->bucket[i], out_list);
-       }
+	/* Connect the sorted buckets in the output list. */
+	for (i = 0; i < AMDGPU_CS_NUM_BUCKETS; i++) {
+		list_splice(&b->bucket[i], out_list);
+	}
 }
 
 int amdgpu_cs_get_ring(struct amdgpu_device *adev, u32 ip_type,
@@ -128,7 +128,7 @@ int amdgpu_cs_get_ring(struct amdgpu_device *adev, u32 ip_type,
 }
 
 static int amdgpu_cs_user_fence_chunk(struct amdgpu_cs_parser *p,
-                                      struct amdgpu_user_fence *uf,
+				      struct amdgpu_user_fence *uf,
 				      struct drm_amdgpu_cs_chunk_fence *fence_data)
 {
 	struct drm_gem_object *gobj;
@@ -140,19 +140,19 @@ static int amdgpu_cs_user_fence_chunk(struct amdgpu_cs_parser *p,
 	if (gobj == NULL)
 		return -EINVAL;
 
-        uf->bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));
-        uf->offset = fence_data->offset;
+	uf->bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));
+	uf->offset = fence_data->offset;
 
-        if (amdgpu_ttm_tt_get_usermm(uf->bo->tbo.ttm)) {
+	if (amdgpu_ttm_tt_get_usermm(uf->bo->tbo.ttm)) {
 		drm_gem_object_unreference_unlocked(gobj);
 		return -EINVAL;
 	}
 
-        p->uf_entry.robj = amdgpu_bo_ref(uf->bo);
+	p->uf_entry.robj = amdgpu_bo_ref(uf->bo);
 	p->uf_entry.priority = 0;
 	p->uf_entry.tv.bo = &p->uf_entry.robj->tbo;
 	p->uf_entry.tv.shared = true;
-        p->uf_entry.user_pages = NULL;
+	p->uf_entry.user_pages = NULL;
 
 	drm_gem_object_unreference_unlocked(gobj);
 	return 0;
@@ -160,12 +160,12 @@ static int amdgpu_cs_user_fence_chunk(struct amdgpu_cs_parser *p,
 
 int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p, void *data)
 {
-        struct amdgpu_fpriv *fpriv = p->filp->driver_priv;
-        struct amdgpu_vm *vm = &fpriv->vm;
+	struct amdgpu_fpriv *fpriv = p->filp->driver_priv;
+	struct amdgpu_vm *vm = &fpriv->vm;
 	union drm_amdgpu_cs *cs = data;
 	uint64_t *chunk_array_user;
 	uint64_t *chunk_array;
-        struct amdgpu_user_fence uf = {};
+	struct amdgpu_user_fence uf = {};
 	unsigned size, num_ibs = 0;
 	int i;
 	int ret;
@@ -183,15 +183,15 @@ int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p, void *data)
 		goto free_chunk;
 	}
  
-        p->bo_list = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle);
+	p->bo_list = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle);
 
 	/* get chunks */
-        INIT_LIST_HEAD(&p->validated);
+	INIT_LIST_HEAD(&p->validated);
 	chunk_array_user = (uint64_t __user *)(unsigned long)(cs->in.chunks);
 	if (copy_from_user(chunk_array, chunk_array_user,
 			   sizeof(uint64_t)*cs->in.num_chunks)) {
 		ret = -EFAULT;
-                goto put_bo_list;
+		goto put_bo_list;
 	}
 
 	p->nchunks = cs->in.num_chunks;
@@ -199,7 +199,7 @@ int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p, void *data)
 			    GFP_KERNEL);
 	if (!p->chunks) {
 		ret = -ENOMEM;
-                goto put_bo_list;
+		goto put_bo_list;
 	}
 
 	for (i = 0; i < p->nchunks; i++) {
@@ -244,7 +244,7 @@ int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p, void *data)
 				goto free_partial_kdata;
 			}
 
-                        ret = amdgpu_cs_user_fence_chunk(p, &uf, (void *)p->chunks[i].kdata);
+			ret = amdgpu_cs_user_fence_chunk(p, &uf, (void *)p->chunks[i].kdata);
 			if (ret)
 				goto free_partial_kdata;
 
@@ -259,11 +259,11 @@ int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p, void *data)
 		}
 	}
 
-        ret = amdgpu_job_alloc(p->adev, num_ibs, &p->job, vm);
-        if (ret)
-                goto free_all_kdata;
+	ret = amdgpu_job_alloc(p->adev, num_ibs, &p->job, vm);
+	if (ret)
+		goto free_all_kdata;
 
-        p->job->uf = uf;
+	p->job->uf = uf;
 
 	kfree(chunk_array);
 	return 0;
@@ -276,7 +276,7 @@ free_partial_kdata:
 	kfree(p->chunks);
 put_bo_list:
        if (p->bo_list)
-               amdgpu_bo_list_put(p->bo_list);
+	       amdgpu_bo_list_put(p->bo_list);
 	amdgpu_ctx_put(p->ctx);
 free_chunk:
 	kfree(chunk_array);
@@ -301,18 +301,18 @@ static u64 amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev)
 	 *
 	 * - From 0 to one half of used VRAM, the threshold decreases
 	 *   linearly.
-	 *         __________________
-	 * 1/4 of -|\               |
-	 * VRAM    | \              |
-	 *         |  \             |
-	 *         |   \            |
-	 *         |    \           |
-	 *         |     \          |
-	 *         |      \         |
-	 *         |       \________|1 MB
-	 *         |----------------|
-	 *    VRAM 0 %             100 %
-	 *         used            used
+	 *	   __________________
+	 * 1/4 of -|\		    |
+	 * VRAM    | \		    |
+	 *	   |  \		    |
+	 *	   |   \	    |
+	 *	   |	\	    |
+	 *	   |	 \	    |
+	 *	   |	  \	    |
+	 *	   |	   \________|1 MB
+	 *	   |----------------|
+	 *    VRAM 0 %		   100 %
+	 *	   used		   used
 	 *
 	 * Note: It's a threshold, not a limit. The threshold must be crossed
 	 * for buffer relocations to stop, so any buffer of an arbitrary size
@@ -340,64 +340,64 @@ int amdgpu_cs_list_validate(struct amdgpu_cs_parser *p,
 			    struct list_head *validated)
 {
 	struct amdgpu_bo_list_entry *lobj;
-        u64 initial_bytes_moved;
+	u64 initial_bytes_moved;
 	int r;
 
 	list_for_each_entry(lobj, validated, tv.head) {
-                struct amdgpu_bo *bo = lobj->robj;
-                bool binding_userptr = false;
-                struct mm_struct *usermm;
-                uint32_t domain;
-
-                usermm = amdgpu_ttm_tt_get_usermm(bo->tbo.ttm);
-                if (usermm && usermm != current->mm)
-                        return -EPERM;
-
-               /* Check if we have user pages and nobody bound the BO already */
-                if (lobj->user_pages && bo->tbo.ttm->state != tt_bound) {
-                       size_t size = sizeof(struct page *);
-
-                       size *= bo->tbo.ttm->num_pages;
-                       memcpy(bo->tbo.ttm->pages, lobj->user_pages, size);
-                       binding_userptr = true;
-               }
-
-                if (bo->pin_count)
-                        continue;
-
-               /* Avoid moving this one if we have moved too many buffers
-                * for this IB already.
-                *
-                * Note that this allows moving at least one buffer of
-                * any size, because it doesn't take the current "bo"
-                * into account. We don't want to disallow buffer moves
-                * completely.
-                */
-                if (p->bytes_moved <= p->bytes_moved_threshold)
-                        domain = bo->prefered_domains;
-                else
-                        domain = bo->allowed_domains;
+		struct amdgpu_bo *bo = lobj->robj;
+		bool binding_userptr = false;
+		struct mm_struct *usermm;
+		uint32_t domain;
+
+		usermm = amdgpu_ttm_tt_get_usermm(bo->tbo.ttm);
+		if (usermm && usermm != current->mm)
+			return -EPERM;
+
+		/* Check if we have user pages and nobody bound the BO already */
+		if (lobj->user_pages && bo->tbo.ttm->state != tt_bound) {
+			size_t size = sizeof(struct page *);
+
+			size *= bo->tbo.ttm->num_pages;
+			memcpy(bo->tbo.ttm->pages, lobj->user_pages, size);
+			binding_userptr = true;
+		}
+
+		if (bo->pin_count)
+			continue;
+
+		/* Avoid moving this one if we have moved too many buffers
+		 * for this IB already.
+		 *
+		 * Note that this allows moving at least one buffer of
+		 * any size, because it doesn't take the current "bo"
+		 * into account. We don't want to disallow buffer moves
+		 * completely.
+		 */
+		if (p->bytes_moved <= p->bytes_moved_threshold)
+			domain = bo->prefered_domains;
+		else
+			domain = bo->allowed_domains;
  
-        retry:
-                amdgpu_ttm_placement_from_domain(bo, domain);
-                initial_bytes_moved = atomic64_read(&bo->adev->num_bytes_moved);
-                r = ttm_bo_validate(&bo->tbo, &bo->placement, true, false);
-                p->bytes_moved += atomic64_read(&bo->adev->num_bytes_moved) -
-                               initial_bytes_moved;
+	retry:
+		amdgpu_ttm_placement_from_domain(bo, domain);
+		initial_bytes_moved = atomic64_read(&bo->adev->num_bytes_moved);
+		r = ttm_bo_validate(&bo->tbo, &bo->placement, true, false);
+		p->bytes_moved += atomic64_read(&bo->adev->num_bytes_moved) -
+			       initial_bytes_moved;
  
-                if (unlikely(r)) {
-                        if (r != -ERESTARTSYS && domain != bo->allowed_domains) {
-                                domain = bo->allowed_domains;
+		if (unlikely(r)) {
+			if (r != -ERESTARTSYS && domain != bo->allowed_domains) {
+				domain = bo->allowed_domains;
 
-                                goto retry;
+				goto retry;
 			}
 			return r;
 		}
 
-               if (binding_userptr) {
-                       drm_free_large(lobj->user_pages);
-                       lobj->user_pages = NULL;
-                }
+		if (binding_userptr) {
+			drm_free_large(lobj->user_pages);
+			lobj->user_pages = NULL;
+		}
 	}
 	return 0;
 }
@@ -405,132 +405,132 @@ int amdgpu_cs_list_validate(struct amdgpu_cs_parser *p,
 static int amdgpu_cs_parser_relocs(struct amdgpu_cs_parser *p)
 {
 	struct amdgpu_fpriv *fpriv = p->filp->driver_priv;
-        struct amdgpu_cs_buckets buckets;
-        struct amdgpu_bo_list_entry *e;
+	struct amdgpu_cs_buckets buckets;
+	struct amdgpu_bo_list_entry *e;
 	struct list_head duplicates;
 	bool need_mmap_lock = false;
-        unsigned i, tries = 10;
-        int  r;
+	unsigned i, tries = 10;
+	int  r;
 	
 	if (p->bo_list) {
-                need_mmap_lock = p->bo_list->first_userptr !=
-                        p->bo_list->num_entries;
-                amdgpu_cs_buckets_init(&buckets);
-                for (i = 0; i < p->bo_list->num_entries; i++)
-                        amdgpu_cs_buckets_add(&buckets, &p->bo_list->array[i].tv.head,
-                                                                 p->bo_list->array[i].priority);
-
-               amdgpu_cs_buckets_get_list(&buckets, &p->validated);
+		need_mmap_lock = p->bo_list->first_userptr !=
+			p->bo_list->num_entries;
+		amdgpu_cs_buckets_init(&buckets);
+		for (i = 0; i < p->bo_list->num_entries; i++)
+			amdgpu_cs_buckets_add(&buckets, &p->bo_list->array[i].tv.head,
+								 p->bo_list->array[i].priority);
+
+	       amdgpu_cs_buckets_get_list(&buckets, &p->validated);
 	}
 
-        INIT_LIST_HEAD(&duplicates);
-        amdgpu_vm_get_pd_bo(&fpriv->vm, &p->validated, &p->vm_pd);
+	INIT_LIST_HEAD(&duplicates);
+	amdgpu_vm_get_pd_bo(&fpriv->vm, &p->validated, &p->vm_pd);
 
-        if (p->job->uf.bo)
+	if (p->job->uf.bo)
 		list_add(&p->uf_entry.tv.head, &p->validated);
 
 	if (need_mmap_lock)
 		down_read(&current->mm->mmap_sem);
 
-        while (1) {
-                struct list_head need_pages;
-                unsigned i;
+	while (1) {
+		struct list_head need_pages;
+		unsigned i;
 
-                r = ttm_eu_reserve_buffers(&p->ticket, &p->validated, true,
-                                           &duplicates);
-                if (unlikely(r != 0))
-                        goto error_free_pages;
+		r = ttm_eu_reserve_buffers(&p->ticket, &p->validated, true,
+					   &duplicates);
+		if (unlikely(r != 0))
+			goto error_free_pages;
  
-                /* Without a BO list we don't have userptr BOs */
-                if (!p->bo_list)
-                        break;
+		/* Without a BO list we don't have userptr BOs */
+		if (!p->bo_list)
+			break;
  
-                INIT_LIST_HEAD(&need_pages);
-                for (i = p->bo_list->first_userptr;
-                     i < p->bo_list->num_entries; ++i) {
+		INIT_LIST_HEAD(&need_pages);
+		for (i = p->bo_list->first_userptr;
+		     i < p->bo_list->num_entries; ++i) {
  
-                        e = &p->bo_list->array[i];
+			e = &p->bo_list->array[i];
  
-                        if (amdgpu_ttm_tt_userptr_invalidated(e->robj->tbo.ttm,
-                                 &e->user_invalidated) && e->user_pages) {
+			if (amdgpu_ttm_tt_userptr_invalidated(e->robj->tbo.ttm,
+				 &e->user_invalidated) && e->user_pages) {
  
-                                /* We acquired a page array, but somebody
-                                * invalidated it. Free it an try again
-                                */
-                                release_pages(e->user_pages,
-                                             e->robj->tbo.ttm->num_pages,
-                                             false);
-                               drm_free_large(e->user_pages);
-                               e->user_pages = NULL;
-                       }
-
-                       if (e->robj->tbo.ttm->state != tt_bound &&
-                           !e->user_pages) {
-                               list_del(&e->tv.head);
-                               list_add(&e->tv.head, &need_pages);
-
-                               amdgpu_bo_unreserve(e->robj);
-                       }
-               }
-
-                if (list_empty(&need_pages))
-                       break;
+				/* We acquired a page array, but somebody
+				 * invalidated it. Free it an try again
+				 */
+				release_pages(e->user_pages,
+					      e->robj->tbo.ttm->num_pages,
+					      false);
+				drm_free_large(e->user_pages);
+				e->user_pages = NULL;
+			}
+
+			if (e->robj->tbo.ttm->state != tt_bound &&
+			    !e->user_pages) {
+				list_del(&e->tv.head);
+				list_add(&e->tv.head, &need_pages);
+
+				amdgpu_bo_unreserve(e->robj);
+			}
+		}
+
+		if (list_empty(&need_pages))
+			break;
  
-               /* Unreserve everything again. */
-                ttm_eu_backoff_reservation(&p->ticket, &p->validated);
-
-               /* We tried to often, just abort */
-                if (!--tries) {
-                       r = -EDEADLK;
-                        goto error_free_pages;
-                }
+		/* Unreserve everything again. */
+		ttm_eu_backoff_reservation(&p->ticket, &p->validated);
+
+		/* We tried to often, just abort */
+		if (!--tries) {
+			r = -EDEADLK;
+			goto error_free_pages;
+		}
  
-                /* Fill the page arrays for all useptrs. */
-                list_for_each_entry(e, &need_pages, tv.head) {
-                        struct ttm_tt *ttm = e->robj->tbo.ttm;
+		/* Fill the page arrays for all useptrs. */
+		list_for_each_entry(e, &need_pages, tv.head) {
+			struct ttm_tt *ttm = e->robj->tbo.ttm;
  
-                        e->user_pages = drm_calloc_large(ttm->num_pages,
-                                                         sizeof(struct page*));
-                        if (!e->user_pages) {
-                                r = -ENOMEM;
-                                goto error_free_pages;
-                        }
+			e->user_pages = drm_calloc_large(ttm->num_pages,
+							 sizeof(struct page*));
+			if (!e->user_pages) {
+				r = -ENOMEM;
+				goto error_free_pages;
+			}
  
-                        r = amdgpu_ttm_tt_get_user_pages(ttm, e->user_pages);
-                        if (r) {
-                                drm_free_large(e->user_pages);
-                                e->user_pages = NULL;
-                                goto error_free_pages;
-                        }
-                }
+			r = amdgpu_ttm_tt_get_user_pages(ttm, e->user_pages);
+			if (r) {
+				drm_free_large(e->user_pages);
+				e->user_pages = NULL;
+				goto error_free_pages;
+			}
+		}
  
-                /* And try again. */
-                list_splice(&need_pages, &p->validated);
-        }
+		/* And try again. */
+		list_splice(&need_pages, &p->validated);
+	}
 
-        amdgpu_vm_get_pt_bos(&fpriv->vm, &duplicates);
+	amdgpu_vm_get_pt_bos(&fpriv->vm, &duplicates);
 
-        p->bytes_moved_threshold = amdgpu_cs_get_threshold_for_moves(p->adev);
-        p->bytes_moved = 0;
+	p->bytes_moved_threshold = amdgpu_cs_get_threshold_for_moves(p->adev);
+	p->bytes_moved = 0;
  
-        r = amdgpu_cs_list_validate(p, &duplicates);
+	r = amdgpu_cs_list_validate(p, &duplicates);
 	if (r)
 		goto error_validate;
 
-        r = amdgpu_cs_list_validate(p, &p->validated);
-        if (r)
-                goto error_validate;
+	r = amdgpu_cs_list_validate(p, &p->validated);
+	if (r)
+		goto error_validate;
 
-        if (p->bo_list) {
-                struct amdgpu_vm *vm = &fpriv->vm;
-                unsigned i;
+	if (p->bo_list) {
+		struct amdgpu_vm *vm = &fpriv->vm;
+		unsigned i;
  
-                for (i = 0; i < p->bo_list->num_entries; i++) {
-                        struct amdgpu_bo *bo = p->bo_list->array[i].robj;
+		for (i = 0; i < p->bo_list->num_entries; i++) {
+			struct amdgpu_bo *bo = p->bo_list->array[i].robj;
  
-                        p->bo_list->array[i].bo_va = amdgpu_vm_bo_find(vm, bo);
-                }
-        }
+			p->bo_list->array[i].bo_va = amdgpu_vm_bo_find(vm, bo);
+		}
+	}
 
 error_validate:
 	if (r) {
@@ -540,23 +540,23 @@ error_validate:
 
 error_free_pages:
 
-        if (need_mmap_lock)
-                up_read(&current->mm->mmap_sem);
+	if (need_mmap_lock)
+		up_read(&current->mm->mmap_sem);
 
-       if (p->bo_list) {
-                for (i = p->bo_list->first_userptr;
-                     i < p->bo_list->num_entries; ++i) {
-                        e = &p->bo_list->array[i];
+	if (p->bo_list) {
+		for (i = p->bo_list->first_userptr;
+		     i < p->bo_list->num_entries; ++i) {
+			e = &p->bo_list->array[i];
  
-                        if (!e->user_pages)
-                                continue;
+			if (!e->user_pages)
+				continue;
  
-                        release_pages(e->user_pages,
-                                      e->robj->tbo.ttm->num_pages,
-                                      false);
-                        drm_free_large(e->user_pages);
-                }
-        }
+			release_pages(e->user_pages,
+				      e->robj->tbo.ttm->num_pages,
+				      false);
+			drm_free_large(e->user_pages);
+		}
+	}
 
 	return r;
 }
@@ -568,7 +568,7 @@ static int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)
 
 	list_for_each_entry(e, &p->validated, tv.head) {
 		struct reservation_object *resv = e->robj->tbo.resv;
-                r = amdgpu_sync_resv(p->adev, &p->job->sync, resv, p->filp);
+		r = amdgpu_sync_resv(p->adev, &p->job->sync, resv, p->filp);
 
 		if (r)
 			return r;
@@ -609,8 +609,8 @@ static void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser, int error, bo
 	for (i = 0; i < parser->nchunks; i++)
 		drm_free_large(parser->chunks[i].kdata);
 	kfree(parser->chunks);
-        if (parser->job)
-                amdgpu_job_free(parser->job);
+	if (parser->job)
+		amdgpu_job_free(parser->job);
 	amdgpu_bo_unref(&parser->uf_entry.robj);
 }
 
@@ -626,7 +626,7 @@ static int amdgpu_bo_vm_update_pte(struct amdgpu_cs_parser *p,
 	if (r)
 		return r;
 
-        r = amdgpu_sync_fence(adev, &p->job->sync, vm->page_directory_fence);
+	r = amdgpu_sync_fence(adev, &p->job->sync, vm->page_directory_fence);
 	if (r)
 		return r;
 
@@ -652,14 +652,14 @@ static int amdgpu_bo_vm_update_pte(struct amdgpu_cs_parser *p,
 				return r;
 
 			f = bo_va->last_pt_update;
-                        r = amdgpu_sync_fence(adev, &p->job->sync, f);
+			r = amdgpu_sync_fence(adev, &p->job->sync, f);
 			if (r)
 				return r;
 		}
 
 	}
 
-        r = amdgpu_vm_clear_invalids(adev, vm, &p->job->sync);
+	r = amdgpu_vm_clear_invalids(adev, vm, &p->job->sync);
 
 	if (amdgpu_vm_debug && p->bo_list) {
 		/* Invalidate all BOs to test for userspace bugs */
@@ -677,26 +677,25 @@ static int amdgpu_bo_vm_update_pte(struct amdgpu_cs_parser *p,
 }
 
 static int amdgpu_cs_ib_vm_chunk(struct amdgpu_device *adev,
-                                 struct amdgpu_cs_parser *p)
+				 struct amdgpu_cs_parser *p)
 {
-        struct amdgpu_fpriv *fpriv = p->filp->driver_priv;
+	struct amdgpu_fpriv *fpriv = p->filp->driver_priv;
 	struct amdgpu_vm *vm = &fpriv->vm;
-        struct amdgpu_ring *ring = p->job->ring;
+	struct amdgpu_ring *ring = p->job->ring;
 	int i, r;
 
 	/* Only for UVD/VCE VM emulation */
-        if (ring->funcs->parse_cs) {
-                for (i = 0; i < p->job->num_ibs; i++) {
-                        r = amdgpu_ring_parse_cs(ring, p, i);
-
+	if (ring->funcs->parse_cs) {
+		for (i = 0; i < p->job->num_ibs; i++) {
+			r = amdgpu_ring_parse_cs(ring, p, i);
 			if (r)
 				return r;
 		}
 	}
 
-        r = amdgpu_bo_vm_update_pte(p, vm);
+	r = amdgpu_bo_vm_update_pte(p, vm);
 	if (!r)
-                amdgpu_cs_sync_rings(p);
+		amdgpu_cs_sync_rings(p);
 
 	return r;
 }
@@ -719,14 +718,14 @@ static int amdgpu_cs_ib_fill(struct amdgpu_device *adev,
 	int i, j;
 	int r;
 
-        for (i = 0, j = 0; i < parser->nchunks && j < parser->job->num_ibs; i++) {
+	for (i = 0, j = 0; i < parser->nchunks && j < parser->job->num_ibs; i++) {
 		struct amdgpu_cs_chunk *chunk;
 		struct amdgpu_ib *ib;
 		struct drm_amdgpu_cs_chunk_ib *chunk_ib;
 		struct amdgpu_ring *ring;
 
 		chunk = &parser->chunks[i];
-                ib = &parser->job->ibs[j];
+		ib = &parser->job->ibs[j];
 		chunk_ib = (struct drm_amdgpu_cs_chunk_ib *)chunk->kdata;
 
 		if (chunk->chunk_id != AMDGPU_CHUNK_ID_IB)
@@ -738,10 +737,10 @@ static int amdgpu_cs_ib_fill(struct amdgpu_device *adev,
 		if (r)
 			return r;
 
-                if (parser->job->ring && parser->job->ring != ring)
-                        return -EINVAL;
+		if (parser->job->ring && parser->job->ring != ring)
+			return -EINVAL;
  
-                parser->job->ring = ring;
+		parser->job->ring = ring;
 
 		if (ring->funcs->parse_cs) {
 			struct amdgpu_bo_va_mapping *m;
@@ -771,7 +770,7 @@ static int amdgpu_cs_ib_fill(struct amdgpu_device *adev,
 			offset = ((uint64_t)m->it.start) * AMDGPU_GPU_PAGE_SIZE;
 			kptr += chunk_ib->va_start - offset;
 
-                        r =  amdgpu_ib_get(adev, NULL, chunk_ib->ib_bytes, ib);
+			r =  amdgpu_ib_get(adev, NULL, chunk_ib->ib_bytes, ib);
 			if (r) {
 				DRM_ERROR("Failed to get ib !\n");
 				return r;
@@ -780,7 +779,7 @@ static int amdgpu_cs_ib_fill(struct amdgpu_device *adev,
 			memcpy(ib->ptr, kptr, chunk_ib->ib_bytes);
 			amdgpu_bo_kunmap(aobj);
 		} else {
-                        r =  amdgpu_ib_get(adev, vm, 0, ib);
+			r =  amdgpu_ib_get(adev, vm, 0, ib);
 			if (r) {
 				DRM_ERROR("Failed to get ib !\n");
 				return r;
@@ -799,7 +798,7 @@ static int amdgpu_cs_ib_fill(struct amdgpu_device *adev,
 		struct amdgpu_bo *gds = parser->bo_list->gds_obj;
 		struct amdgpu_bo *gws = parser->bo_list->gws_obj;
 		struct amdgpu_bo *oa = parser->bo_list->oa_obj;
-                struct amdgpu_ib *ib = &parser->job->ibs[0];
+		struct amdgpu_ib *ib = &parser->job->ibs[0];
 
 		if (gds) {
 			ib->gds_base = amdgpu_bo_gpu_offset(gds);
@@ -815,16 +814,16 @@ static int amdgpu_cs_ib_fill(struct amdgpu_device *adev,
 		}
 	}
 	/* wrap the last IB with user fence */
-        if (parser->job->uf.bo) {
-                struct amdgpu_ib *ib = &parser->job->ibs[parser->job->num_ibs - 1];
+	if (parser->job->uf.bo) {
+		struct amdgpu_ib *ib = &parser->job->ibs[parser->job->num_ibs - 1];
 
 		/* UVD & VCE fw doesn't support user fences */
-                if (parser->job->ring->type == AMDGPU_RING_TYPE_UVD ||
-                    parser->job->ring->type == AMDGPU_RING_TYPE_VCE)
+		if (parser->job->ring->type == AMDGPU_RING_TYPE_UVD ||
+		    parser->job->ring->type == AMDGPU_RING_TYPE_VCE)
 
 			return -EINVAL;
 
-                ib->user = &parser->job->uf;
+		ib->user = &parser->job->uf;
 	}
 
 	return 0;
@@ -873,8 +872,8 @@ static int amdgpu_cs_dependencies(struct amdgpu_device *adev,
 				return r;
 
 			} else if (fence) {
-                                r = amdgpu_sync_fence(adev, &p->job->sync,
-                                                      fence);
+				r = amdgpu_sync_fence(adev, &p->job->sync,
+						      fence);
 				fence_put(fence);
 				amdgpu_ctx_put(ctx);
 				if (r)
@@ -889,28 +888,27 @@ static int amdgpu_cs_dependencies(struct amdgpu_device *adev,
 static int amdgpu_cs_submit(struct amdgpu_cs_parser *p,
 			    union drm_amdgpu_cs *cs)
 {
-        struct amdgpu_ring *ring = p->job->ring;
-        struct fence *fence;
+	struct amdgpu_ring *ring = p->job->ring;
+	struct fence *fence;
 	struct amdgpu_job *job;
 	int r;
 
-        job = p->job;
-        p->job = NULL;
+	job = p->job;
+	p->job = NULL;
 
-        r = amd_sched_job_init(&job->base, &ring->sched,
-                                                &p->ctx->rings[ring->idx].entity,
-                                                amdgpu_job_timeout_func,
-                                                amdgpu_job_free_func,
-                                                p->filp, &fence);
-        if (r) {
-	
- 	         amdgpu_job_free(job);
+	r = amd_sched_job_init(&job->base, &ring->sched,
+						&p->ctx->rings[ring->idx].entity,
+						amdgpu_job_timeout_func,
+						amdgpu_job_free_func,
+						p->filp, &fence);
+	if (r) {
+		amdgpu_job_free(job);
 		return r;
 	}
 
-        job->owner = p->filp;
-        p->fence = fence_get(fence);
-        cs->out.handle = amdgpu_ctx_add_fence(p->ctx, ring, fence);
+	job->owner = p->filp;
+	p->fence = fence_get(fence);
+	cs->out.handle = amdgpu_ctx_add_fence(p->ctx, ring, fence);
 	job->ibs[job->num_ibs - 1].sequence = cs->out.handle;
 	
 	trace_amdgpu_cs_ioctl(job);
@@ -940,7 +938,7 @@ int amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)
 		r = amdgpu_cs_handle_lockup(adev, r);
 		return r;
 	}
-        r = amdgpu_cs_parser_relocs(&parser);
+	r = amdgpu_cs_parser_relocs(&parser);
 	if (r == -ENOMEM)
 		DRM_ERROR("Not enough memory for command submission!\n");
 	else if (r && r != -ERESTARTSYS)
@@ -959,14 +957,14 @@ int amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)
 	if (r)
 		goto out;
 
-        for (i = 0; i < parser.job->num_ibs; i++)
+	for (i = 0; i < parser.job->num_ibs; i++)
 		trace_amdgpu_cs(&parser, i);
 
 	r = amdgpu_cs_ib_vm_chunk(adev, &parser);
 	if (r)
 		goto out;
 
-        r = amdgpu_cs_submit(&parser, cs);
+	r = amdgpu_cs_submit(&parser, cs);
 
 out:
 	amdgpu_cs_parser_fini(&parser, r, reserved_buffers);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
index 5f43c3e..6077ec6 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
+ *	    Alex Deucher
+ *	    Jerome Glisse
  */
 #include <linux/console.h>
 #include <linux/slab.h>
@@ -379,7 +379,7 @@ static void amdgpu_doorbell_fini(struct amdgpu_device *adev)
 
 /**
  * amdgpu_doorbell_get_kfd_info - Report doorbell configuration required to
- *                                setup amdkfd
+ *				  setup amdkfd
  *
  * @adev: amdgpu_device pointer
  * @aperture_base: output returning doorbell aperture base physical address
@@ -454,7 +454,7 @@ static int amdgpu_wb_init(struct amdgpu_device *adev)
 
 	if (adev->wb.wb_obj == NULL) {
 		r = amdgpu_bo_create(adev, AMDGPU_MAX_WB * 4, PAGE_SIZE, true,
-				     AMDGPU_GEM_DOMAIN_GTT, 0,  NULL, NULL,
+				     AMDGPU_GEM_DOMAIN_GTT, 0,	NULL, NULL,
 				     &adev->wb.wb_obj);
 		if (r) {
 			dev_warn(adev->dev, "(%d) create WB bo failed\n", r);
@@ -1003,7 +1003,7 @@ static void amdgpu_check_arguments(struct amdgpu_device *adev)
  * @pdev: pci dev pointer
  * @state: vga_switcheroo state
  *
- * Callback for the switcheroo driver.  Suspends or resumes the
+ * Callback for the switcheroo driver.	Suspends or resumes the
  * the asics before or after it is powered up using ACPI methods.
  */
 static void amdgpu_switcheroo_set_state(struct pci_dev *pdev, enum vga_switcheroo_state state)
@@ -1040,7 +1040,7 @@ static void amdgpu_switcheroo_set_state(struct pci_dev *pdev, enum vga_switchero
  *
  * @pdev: pci dev pointer
  *
- * Callback for the switcheroo driver.  Check of the switcheroo
+ * Callback for the switcheroo driver.	Check of the switcheroo
  * state can be changed.
  * Returns true if the state can be changed, false if not.
  */
@@ -1447,7 +1447,7 @@ int amdgpu_device_init(struct amdgpu_device *adev,
 	adev->mman.buffer_funcs = NULL;
 	adev->mman.buffer_funcs_ring = NULL;
 	adev->vm_manager.vm_pte_funcs = NULL;
-        adev->vm_manager.vm_pte_num_rings = 0;
+	adev->vm_manager.vm_pte_num_rings = 0;
 	adev->gart.gart_funcs = NULL;
 	adev->fence_context = fence_context_alloc(AMDGPU_MAX_RINGS);
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
index bd8fabc..61eebc5 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
@@ -131,8 +131,9 @@ static void amdgpu_flip_work_func(struct work_struct *__work)
 				 vblank->framedur_ns / 1000,
 				 vblank->linedur_ns / 1000, stat, vpos, hpos);
 
-	/* do the flip (mmio) */
+	/* Do the flip (mmio) */
 	adev->mode_info.funcs->page_flip(adev, work->crtc_id, work->base);
+
 	/* set the flip status */
 	amdgpuCrtc->pflip_status = AMDGPU_FLIP_SUBMITTED;
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h
index 3738a96..caa38dd 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h
@@ -23,8 +23,8 @@
 #ifndef __AMDGPU_DPM_H__
 #define __AMDGPU_DPM_H__
 
-#define R600_SSTU_DFLT                               0
-#define R600_SST_DFLT                                0x00C8
+#define R600_SSTU_DFLT				     0
+#define R600_SST_DFLT				     0x00C8
 
 /* XXX are these ok? */
 #define R600_TEMP_RANGE_MIN (90 * 1000)
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c
index 50e95ab..c9fdc86 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c
@@ -616,7 +616,7 @@ static int amdgpu_debugfs_fence_info(struct seq_file *m, void *data)
 		seq_printf(m, "--- ring %d (%s) ---\n", i, ring->name);
 		seq_printf(m, "Last signaled fence 0x%08x\n",
 			   atomic_read(&ring->fence_drv.last_seq));
-		seq_printf(m, "Last emitted        0x%08x\n",
+		seq_printf(m, "Last emitted	   0x%08x\n",
 			   ring->fence_drv.sync_seq);
 	}
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
index 75edc9d..155bad9 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
+ *	    Alex Deucher
+ *	    Jerome Glisse
  */
 #include <linux/ktime.h>
 #include <linux/pagemap.h>
@@ -274,8 +274,8 @@ int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
 		goto handle_lockup;
 
 	bo = gem_to_amdgpu_bo(gobj);
-        bo->prefered_domains = AMDGPU_GEM_DOMAIN_GTT;
-        bo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;
+	bo->prefered_domains = AMDGPU_GEM_DOMAIN_GTT;
+	bo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;
 	r = amdgpu_ttm_tt_set_userptr(bo->tbo.ttm, args->addr, args->flags);
 	if (r)
 		goto release_object;
@@ -343,7 +343,7 @@ int amdgpu_mode_dumb_mmap(struct drm_file *filp,
 		return -ENOENT;
 	}
 	robj = gem_to_amdgpu_bo(gobj);
-        if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||
+	if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||
 	    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
 		drm_gem_object_unreference_unlocked(gobj);
 		return -EPERM;
@@ -483,7 +483,7 @@ static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
 				    struct amdgpu_bo_va *bo_va, uint32_t operation)
 {
 	struct ttm_validate_buffer tv, *entry;
-        struct amdgpu_bo_list_entry vm_pd;
+	struct amdgpu_bo_list_entry vm_pd;
 	struct ww_acquire_ctx ticket;
 	struct list_head list, duplicates;
 	unsigned domain;
@@ -496,14 +496,14 @@ static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
 	tv.shared = true;
 	list_add(&tv.head, &list);
 
-        amdgpu_vm_get_pd_bo(bo_va->vm, &list, &vm_pd);
+	amdgpu_vm_get_pd_bo(bo_va->vm, &list, &vm_pd);
 
 	/* Provide duplicates to avoid -EALREADY */
 	r = ttm_eu_reserve_buffers(&ticket, &list, true, &duplicates);
 	if (r)
-                goto error_print;
+		goto error_print;
  
-        amdgpu_vm_get_pt_bos(bo_va->vm, &duplicates);
+	amdgpu_vm_get_pt_bos(bo_va->vm, &duplicates);
 	list_for_each_entry(entry, &list, head) {
 		domain = amdgpu_mem_type_to_domain(entry->bo->mem.mem_type);
 		/* if anything is swapped out don't swap it in here,
@@ -663,7 +663,7 @@ int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
 
 		info.bo_size = robj->gem_base.size;
 		info.alignment = robj->tbo.mem.page_alignment << PAGE_SHIFT;
-                info.domains = robj->prefered_domains;
+		info.domains = robj->prefered_domains;
 		info.domain_flags = robj->flags;
 		amdgpu_bo_unreserve(robj);
 		if (copy_to_user(out, &info, sizeof(info)))
@@ -671,17 +671,17 @@ int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
 		break;
 	}
 	case AMDGPU_GEM_OP_SET_PLACEMENT:
-                if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
+		if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
 			r = -EPERM;
 			amdgpu_bo_unreserve(robj);
 			break;
 		}
-                robj->prefered_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
-                                                        AMDGPU_GEM_DOMAIN_GTT |
-                                                        AMDGPU_GEM_DOMAIN_CPU);
-                robj->allowed_domains = robj->prefered_domains;
-                if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
-                        robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+		robj->prefered_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
+							AMDGPU_GEM_DOMAIN_GTT |
+							AMDGPU_GEM_DOMAIN_CPU);
+		robj->allowed_domains = robj->prefered_domains;
+		if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
+			robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
  
 		amdgpu_bo_unreserve(robj);
 		break;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c
index b987f47..58f2ec1 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c
@@ -22,9 +22,9 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
- *          Christian König
+ *	    Alex Deucher
+ *	    Jerome Glisse
+ *	    Christian König
  */
 #include <linux/seq_file.h>
 #include <linux/slab.h>
@@ -111,19 +111,19 @@ void amdgpu_ib_free(struct amdgpu_device *adev, struct amdgpu_ib *ib,
  * the resource descriptors will be already in cache when the draw is
  * processed.  To accomplish this, the userspace driver submits two
  * IBs, one for the CE and one for the DE.  If there is a CE IB (called
- * a CONST_IB), it will be put on the ring prior to the DE IB.  Prior
+ * a CONST_IB), it will be put on the ring prior to the DE IB.	Prior
  * to SI there was just a DE IB.
  */
 int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
-                       struct amdgpu_ib *ibs, struct fence *last_vm_update,
-                       struct amdgpu_job *job, struct fence **f)
+		       struct amdgpu_ib *ibs, struct fence *last_vm_update,
+		       struct amdgpu_job *job, struct fence **f)
 {
-        struct amdgpu_device *adev = ring->adev;
+	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib *ib = &ibs[0];
-        uint64_t fence_context = 0, old = ring->last_fence_context;
+	uint64_t fence_context = 0, old = ring->last_fence_context;
 	struct fence *hwf;
-        struct amdgpu_vm *vm = NULL;
-        unsigned i, patch_offset = ~0;
+	struct amdgpu_vm *vm = NULL;
+	unsigned i, patch_offset = ~0;
 
 	int r = 0;
 
@@ -156,17 +156,18 @@ int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
 
 	if (vm) {
 		/* do context switch */
-                r = amdgpu_vm_flush(ring, ib->vm_id, ib->vm_pd_addr,
-                                    ib->gds_base, ib->gds_size,
-                                    ib->gws_base, ib->gws_size,
-                                    ib->oa_base, ib->oa_size);
-                if (r) {
-                        amdgpu_ring_undo(ring);
-                        return r;
-                    }
+		r = amdgpu_vm_flush(ring, ib->vm_id, ib->vm_pd_addr,
+				    ib->gds_base, ib->gds_size,
+				    ib->gws_base, ib->gws_size,
+				    ib->oa_base, ib->oa_size);
+		if (r) {
+			amdgpu_ring_undo(ring);
+			return r;
+		    }
 		}
-		if (ring->funcs->emit_hdp_flush)
-			amdgpu_ring_emit_hdp_flush(ring);
+
+	if (ring->funcs->emit_hdp_flush)
+		amdgpu_ring_emit_hdp_flush(ring);
 
 	/* always set cond_exec_polling to CONTINUE */
 	*ring->cond_exe_cpu_addr = 1;
@@ -176,10 +177,10 @@ int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
 
 		amdgpu_ring_emit_ib(ring, ib, (i == 0 && old != fence_context));
 	}
-        ring->last_fence_context = fence_context;
+	ring->last_fence_context = fence_context;
 
-        if (ring->funcs->emit_hdp_invalidate)
-                amdgpu_ring_emit_hdp_invalidate(ring);
+	if (ring->funcs->emit_hdp_invalidate)
+		amdgpu_ring_emit_hdp_invalidate(ring);
 
 	r = amdgpu_fence_emit(ring, &hwf);
 	if (r) {
@@ -198,11 +199,12 @@ int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
 		amdgpu_ring_emit_fence(ring, addr, ib->sequence,
 				       AMDGPU_FENCE_FLAG_64BIT);
 	}
-	if(f)
-        *f = fence_get(hwf);
 
-        if (patch_offset != ~0 && ring->funcs->patch_cond_exec)
-                amdgpu_ring_patch_cond_exec(ring, patch_offset);
+	if (f)
+		*f = fence_get(hwf);
+
+	if (patch_offset != ~0 && ring->funcs->patch_cond_exec)
+		amdgpu_ring_patch_cond_exec(ring, patch_offset);
 
 	amdgpu_ring_commit(ring);
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c
index 50257ad..de521d8 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c
@@ -37,71 +37,72 @@ static void amdgpu_job_free_handler(struct work_struct *ws)
 void amdgpu_job_timeout_func(struct work_struct *work)
 {
 	struct amdgpu_job *job = container_of(work, struct amdgpu_job, base.work_tdr.work);
+
 	DRM_ERROR("ring %s timeout, last signaled seq=%u, last emitted seq=%u\n",
-				job->base.sched->name,
-				(uint32_t)atomic_read(&job->ring->fence_drv.last_seq),
-				job->ring->fence_drv.sync_seq);
+		  job->base.sched->name,
+		  (uint32_t)atomic_read(&job->ring->fence_drv.last_seq),
+		  job->ring->fence_drv.sync_seq);
 
 	amd_sched_job_put(&job->base);
 }
 
 int amdgpu_job_alloc(struct amdgpu_device *adev, unsigned num_ibs,
-                    struct amdgpu_job **job, struct amdgpu_vm *vm)
+		     struct amdgpu_job **job, struct amdgpu_vm *vm)
 {
-       size_t size = sizeof(struct amdgpu_job);
+	size_t size = sizeof(struct amdgpu_job);
 
-       if (num_ibs == 0)
-               return -EINVAL;
+	if (num_ibs == 0)
+		return -EINVAL;
 
-       size += sizeof(struct amdgpu_ib) * num_ibs;
+	size += sizeof(struct amdgpu_ib) * num_ibs;
 
-       *job = kzalloc(size, GFP_KERNEL);
-       if (!*job)
-               return -ENOMEM;
+	*job = kzalloc(size, GFP_KERNEL);
+	if (!*job)
+		return -ENOMEM;
 
-       (*job)->adev = adev;
-       (*job)->vm = vm;
-       (*job)->ibs = (void *)&(*job)[1];
-       (*job)->num_ibs = num_ibs;
-       INIT_WORK(&(*job)->base.work_free_job, amdgpu_job_free_handler);
+	(*job)->adev = adev;
+	(*job)->vm = vm;
+	(*job)->ibs = (void *)&(*job)[1];
+	(*job)->num_ibs = num_ibs;
+	INIT_WORK(&(*job)->base.work_free_job, amdgpu_job_free_handler);
 
-       amdgpu_sync_create(&(*job)->sync);
+	amdgpu_sync_create(&(*job)->sync);
 
-       return 0;
+	return 0;
 }
 
 int amdgpu_job_alloc_with_ib(struct amdgpu_device *adev, unsigned size,
-                            struct amdgpu_job **job)
+			     struct amdgpu_job **job)
 {
-       int r;
+	int r;
 
-       r = amdgpu_job_alloc(adev, 1, job, NULL);
-       if (r)
-               return r;
+	r = amdgpu_job_alloc(adev, 1, job, NULL);
+	if (r)
+		return r;
 
-       r = amdgpu_ib_get(adev, NULL, size, &(*job)->ibs[0]);
-       if (r)
-               kfree(*job);
+	r = amdgpu_ib_get(adev, NULL, size, &(*job)->ibs[0]);
+	if (r)
+		kfree(*job);
 
-       return r;
+	return r;
 }
 
 void amdgpu_job_free(struct amdgpu_job *job)
 {
-       unsigned i;
-       struct fence *f;
-       /* use sched fence if available */
-       f = (job->base.s_fence)? &job->base.s_fence->base : job->fence;
+	unsigned i;
+	struct fence *f;
+	/* use sched fence if available */
+	f = (job->base.s_fence)? &job->base.s_fence->base : job->fence;
 
-       for (i = 0; i < job->num_ibs; ++i)
-               amdgpu_sa_bo_free(job->adev, &job->ibs[i].sa_bo, f);
-       fence_put(job->fence);
+	for (i = 0; i < job->num_ibs; ++i)
+		amdgpu_sa_bo_free(job->adev, &job->ibs[i].sa_bo, f);
+	fence_put(job->fence);
 
-       amdgpu_bo_unref(&job->uf.bo);
-       amdgpu_sync_free(&job->sync);
+	amdgpu_bo_unref(&job->uf.bo);
+	amdgpu_sync_free(&job->sync);
 
-       if (!job->base.use_sched)
-                kfree(job);
+	if (!job->base.use_sched)
+		 kfree(job);
 }
 
 void amdgpu_job_free_func(struct kref *refcount)
@@ -111,97 +112,97 @@ void amdgpu_job_free_func(struct kref *refcount)
 }
 
 int amdgpu_job_submit(struct amdgpu_job *job, struct amdgpu_ring *ring,
-                      struct amd_sched_entity *entity, void *owner,
-                      struct fence **f)
+		      struct amd_sched_entity *entity, void *owner,
+		      struct fence **f)
 {
-       struct fence *fence;
-       int r;
-       job->ring = ring;
-       if (!f)
-               return -EINVAL;
-
-       r = amd_sched_job_init(&job->base, &ring->sched,
-                                                       entity,
-                                                       amdgpu_job_timeout_func,
-                                                       amdgpu_job_free_func,
-                                                       owner, &fence);
-       if (r)
-               return r;
-
-       job->owner = owner;
-       *f = fence_get(fence);
-       amd_sched_entity_push_job(&job->base);
-
-       return 0;
+	struct fence *fence;
+	int r;
+	job->ring = ring;
+	if (!f)
+		return -EINVAL;
+
+	r = amd_sched_job_init(&job->base, &ring->sched,
+						       entity,
+						       amdgpu_job_timeout_func,
+						       amdgpu_job_free_func,
+						       owner, &fence);
+	if (r)
+		return r;
+
+	job->owner = owner;
+	*f = fence_get(fence);
+	amd_sched_entity_push_job(&job->base);
+
+	return 0;
 }
 
 static struct fence *amdgpu_job_dependency(struct amd_sched_job *sched_job)
 {
-       struct amdgpu_job *job = to_amdgpu_job(sched_job);
-       struct amdgpu_vm *vm = job->vm;
-       struct fence *fence = amdgpu_sync_get_fence(&job->sync);
-
-       if (fence == NULL && vm && !job->ibs->vm_id) {
-               struct amdgpu_ring *ring = job->ring;
-               unsigned i, vm_id;
-               uint64_t vm_pd_addr;
-               int r;
-
-               r = amdgpu_vm_grab_id(vm, ring, &job->sync,
-                                      &job->base.s_fence->base,
-                                      &vm_id, &vm_pd_addr);
-               if (r)
-                       DRM_ERROR("Error getting VM ID (%d)\n", r);
-               else {
-                       for (i = 0; i < job->num_ibs; ++i) {
-                               job->ibs[i].vm_id = vm_id;
-                               job->ibs[i].vm_pd_addr = vm_pd_addr;
-                       }
-               }
-
-               fence = amdgpu_sync_get_fence(&job->sync);
-       }
-
-       return fence;
+	struct amdgpu_job *job = to_amdgpu_job(sched_job);
+	struct amdgpu_vm *vm = job->vm;
+	struct fence *fence = amdgpu_sync_get_fence(&job->sync);
+
+	if (fence == NULL && vm && !job->ibs->vm_id) {
+		struct amdgpu_ring *ring = job->ring;
+		unsigned i, vm_id;
+		uint64_t vm_pd_addr;
+		int r;
+
+		r = amdgpu_vm_grab_id(vm, ring, &job->sync,
+				      &job->base.s_fence->base,
+				      &vm_id, &vm_pd_addr);
+		if (r)
+			DRM_ERROR("Error getting VM ID (%d)\n", r);
+		else {
+			for (i = 0; i < job->num_ibs; ++i) {
+				job->ibs[i].vm_id = vm_id;
+				job->ibs[i].vm_pd_addr = vm_pd_addr;
+			}
+		}
+
+		fence = amdgpu_sync_get_fence(&job->sync);
+	}
+
+	return fence;
 }
 
 static struct fence *amdgpu_job_run(struct amd_sched_job *sched_job)
 {
-       struct fence *fence = NULL;
-       struct amdgpu_job *job;
-       int r;
-
-       if (!sched_job) {
-               DRM_ERROR("job is null\n");
-               return NULL;
-       }
-       job = to_amdgpu_job(sched_job);
-
-       r = amdgpu_sync_wait(&job->sync);
-       if (r) {
-               DRM_ERROR("failed to sync wait (%d)\n", r);
-               return NULL;
-       }
-
-       trace_amdgpu_sched_run_job(job);
-       r = amdgpu_ib_schedule(job->ring, job->num_ibs, job->ibs,
-                              job->sync.last_vm_update, job, &fence);
-       if (r) {
-               DRM_ERROR("Error scheduling IBs (%d)\n", r);
-               goto err;
-       }
+	struct fence *fence = NULL;
+	struct amdgpu_job *job;
+	int r;
+
+	if (!sched_job) {
+		DRM_ERROR("job is null\n");
+		return NULL;
+	}
+	job = to_amdgpu_job(sched_job);
+
+	r = amdgpu_sync_wait(&job->sync);
+	if (r) {
+		DRM_ERROR("failed to sync wait (%d)\n", r);
+		return NULL;
+	}
+
+	trace_amdgpu_sched_run_job(job);
+	r = amdgpu_ib_schedule(job->ring, job->num_ibs, job->ibs,
+			       job->sync.last_vm_update, job, &fence);
+	if (r) {
+		DRM_ERROR("Error scheduling IBs (%d)\n", r);
+		goto err;
+	}
 
 err:
-       job->fence = fence;
-       amdgpu_job_free(job);
-       return fence;
+	job->fence = fence;
+	amdgpu_job_free(job);
+	return fence;
 }
 
 const struct amd_sched_backend_ops amdgpu_sched_ops = {
-       .dependency = amdgpu_job_dependency,
-       .run_job = amdgpu_job_run,
-       .begin_job = amd_sched_job_begin,
-       .finish_job = amd_sched_job_finish,
+	.dependency = amdgpu_job_dependency,
+	.run_job = amdgpu_job_run,
+	.begin_job = amd_sched_job_begin,
+	.finish_job = amd_sched_job_finish,
 };
 
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
index d4e1e86..78bd8b1 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
+ *	    Alex Deucher
+ *	    Jerome Glisse
  */
 #include <drm/drmP.h>
 #include "amdgpu.h"
@@ -448,7 +448,7 @@ static int amdgpu_info_ioctl(struct drm_device *dev, void *data, struct drm_file
 			dev_info.max_memory_clock = adev->pm.default_mclk * 10;
 		}
 		dev_info.enabled_rb_pipes_mask = adev->gfx.config.backend_enable_mask;
-                dev_info.num_rb_pipes = adev->gfx.config.max_backends_per_se;
+		dev_info.num_rb_pipes = adev->gfx.config.max_backends_per_se;
 		dev_info.num_hw_gfx_contexts = adev->gfx.config.max_hw_contexts;
 		dev_info._pad = 0;
 		dev_info.ids_flags = 0;
@@ -566,8 +566,8 @@ void amdgpu_driver_postclose_kms(struct drm_device *dev,
 
 	amdgpu_ctx_mgr_fini(&fpriv->ctx_mgr);
 
-        amdgpu_uvd_free_handles(adev, file_priv);
-        amdgpu_vce_free_handles(adev, file_priv);
+	amdgpu_uvd_free_handles(adev, file_priv);
+	amdgpu_vce_free_handles(adev, file_priv);
 
 	amdgpu_vm_fini(adev, &fpriv->vm);
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
index b91ff33..7ecea83 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
@@ -251,15 +251,15 @@ int amdgpu_bo_create_restricted(struct amdgpu_device *adev,
 	bo->adev = adev;
 	INIT_LIST_HEAD(&bo->list);
 	INIT_LIST_HEAD(&bo->va);
-        bo->prefered_domains = domain & (AMDGPU_GEM_DOMAIN_VRAM |
-                                         AMDGPU_GEM_DOMAIN_GTT |
-                                         AMDGPU_GEM_DOMAIN_CPU |
-                                         AMDGPU_GEM_DOMAIN_GDS |
-                                         AMDGPU_GEM_DOMAIN_GWS |
-                                         AMDGPU_GEM_DOMAIN_OA);
-        bo->allowed_domains = bo->prefered_domains;
-        if (!kernel && bo->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
-                bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+	bo->prefered_domains = domain & (AMDGPU_GEM_DOMAIN_VRAM |
+					 AMDGPU_GEM_DOMAIN_GTT |
+					 AMDGPU_GEM_DOMAIN_CPU |
+					 AMDGPU_GEM_DOMAIN_GDS |
+					 AMDGPU_GEM_DOMAIN_GWS |
+					 AMDGPU_GEM_DOMAIN_OA);
+	bo->allowed_domains = bo->prefered_domains;
+	if (!kernel && bo->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
+		bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
 
 	bo->flags = flags;
 
@@ -373,7 +373,7 @@ int amdgpu_bo_pin_restricted(struct amdgpu_bo *bo, u32 domain,
 	int r, i;
 	unsigned fpfn, lpfn;
 
-        if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))
+	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))
 		return -EPERM;
 
 	if (WARN_ON_ONCE(min_offset > max_offset))
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c
index b4fe303..7700dc2 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c
@@ -118,7 +118,7 @@ struct dma_buf *amdgpu_gem_prime_export(struct drm_device *dev,
 {
 	struct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);
 
-        if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))
+	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))
 		return ERR_PTR(-EPERM);
 
 	return drm_gem_prime_export(dev, gobj, flags);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c
index 1b0b7ae..7e79370 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c
@@ -22,9 +22,9 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
- *          Christian König
+ *	    Alex Deucher
+ *	    Jerome Glisse
+ *	    Christian König
  */
 #include <linux/seq_file.h>
 #include <linux/slab.h>
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
index b602052..5994dfc 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
@@ -77,8 +77,8 @@ static void amdgpu_ttm_mem_global_release(struct drm_global_reference *ref)
 static int amdgpu_ttm_global_init(struct amdgpu_device *adev)
 {
 	struct drm_global_reference *global_ref;
-        struct amdgpu_ring *ring;
-        struct amd_sched_rq *rq;
+	struct amdgpu_ring *ring;
+	struct amd_sched_rq *rq;
 	int r;
 
 	adev->mman.mem_global_referenced = false;
@@ -108,16 +108,16 @@ static int amdgpu_ttm_global_init(struct amdgpu_device *adev)
 		return r;
 	}
 
-        ring = adev->mman.buffer_funcs_ring;
-        rq = &ring->sched.sched_rq[AMD_SCHED_PRIORITY_KERNEL];
-        r = amd_sched_entity_init(&ring->sched, &adev->mman.entity,
-                                  rq, amdgpu_sched_jobs);
-        if (r != 0) {
-                DRM_ERROR("Failed setting up TTM BO move run queue.\n");
-                drm_global_item_unref(&adev->mman.mem_global_ref);
-                drm_global_item_unref(&adev->mman.bo_global_ref.ref);
-                return r;
-        }
+	ring = adev->mman.buffer_funcs_ring;
+	rq = &ring->sched.sched_rq[AMD_SCHED_PRIORITY_KERNEL];
+	r = amd_sched_entity_init(&ring->sched, &adev->mman.entity,
+				  rq, amdgpu_sched_jobs);
+	if (r != 0) {
+		DRM_ERROR("Failed setting up TTM BO move run queue.\n");
+		drm_global_item_unref(&adev->mman.mem_global_ref);
+		drm_global_item_unref(&adev->mman.bo_global_ref.ref);
+		return r;
+	}
 
 	adev->mman.mem_global_referenced = true;
 
@@ -127,8 +127,8 @@ static int amdgpu_ttm_global_init(struct amdgpu_device *adev)
 static void amdgpu_ttm_global_fini(struct amdgpu_device *adev)
 {
 	if (adev->mman.mem_global_referenced) {
-                amd_sched_entity_fini(adev->mman.entity.sched,
-                                      &adev->mman.entity);
+		amd_sched_entity_fini(adev->mman.entity.sched,
+				      &adev->mman.entity);
 		drm_global_item_unref(&adev->mman.bo_global_ref.ref);
 		drm_global_item_unref(&adev->mman.mem_global_ref);
 		adev->mman.mem_global_referenced = false;
@@ -514,8 +514,8 @@ struct amdgpu_ttm_tt {
 	uint64_t		userptr;
 	struct mm_struct	*usermm;
 	uint32_t		userflags;
-	spinlock_t              guptasklock;
-	struct list_head        guptasks;
+	spinlock_t		guptasklock;
+	struct list_head	guptasks;
 	atomic_t		mmu_invalidations;
 };
 
@@ -638,13 +638,13 @@ static int amdgpu_ttm_backend_bind(struct ttm_tt *ttm,
 	uint32_t flags = amdgpu_ttm_tt_pte_flags(gtt->adev, ttm, bo_mem);
 	int r;
 
-        if (gtt->userptr) {
-                r = amdgpu_ttm_tt_pin_userptr(ttm);
-                if (r) {
-                        DRM_ERROR("failed to pin userptr\n");
-                        return r;
-                }
-        }
+	if (gtt->userptr) {
+		r = amdgpu_ttm_tt_pin_userptr(ttm);
+		if (r) {
+			DRM_ERROR("failed to pin userptr\n");
+			return r;
+		}
+	}
 	gtt->offset = (unsigned long)(bo_mem->start << PAGE_SHIFT);
 	if (!ttm->num_pages) {
 		WARN(1, "nothing to bind %lu pages for mreg %p back %p!\n",
@@ -833,9 +833,9 @@ struct mm_struct *amdgpu_ttm_tt_get_usermm(struct ttm_tt *ttm)
 	struct amdgpu_ttm_tt *gtt = (void *)ttm;
 
 	if (gtt == NULL)
-                return NULL;
+		return NULL;
 
-        return gtt->usermm;
+	return gtt->usermm;
 }
 
 bool amdgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
@@ -973,7 +973,7 @@ static struct ttm_bo_driver amdgpu_bo_driver = {
 	.io_mem_free = &amdgpu_ttm_io_mem_free,
 	.lru_removal = &amdgpu_ttm_lru_removal,
 	.lru_tail = &amdgpu_ttm_lru_tail,
-        .swap_lru_tail = &amdgpu_ttm_swap_lru_tail,
+	.swap_lru_tail = &amdgpu_ttm_swap_lru_tail,
 };
 
 int amdgpu_ttm_init(struct amdgpu_device *adev)
@@ -1149,6 +1149,7 @@ int amdgpu_copy_buffer(struct amdgpu_ring *ring,
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_job *job;
+
 	uint32_t max_bytes;
 	unsigned num_loops, num_dw;
 	unsigned i;
@@ -1161,12 +1162,13 @@ int amdgpu_copy_buffer(struct amdgpu_ring *ring,
 	/* for IB padding */
 	while (num_dw & 0x7)
 		num_dw++;
+
 	r = amdgpu_job_alloc_with_ib(adev, num_dw * 4, &job);
 	if (r) 
 		return r;
 
 	if (resv) {
-                r = amdgpu_sync_resv(adev, &job->sync, resv,
+		r = amdgpu_sync_resv(adev, &job->sync, resv,
 				     AMDGPU_FENCE_OWNER_UNDEFINED);
 		if (r) {
 			DRM_ERROR("sync failed (%d).\n", r);
@@ -1187,13 +1189,14 @@ int amdgpu_copy_buffer(struct amdgpu_ring *ring,
 	amdgpu_ring_pad_ib(ring, &job->ibs[0]);
 	WARN_ON(job->ibs[0].length_dw > num_dw);
 	r = amdgpu_job_submit(job, ring, &adev->mman.entity,
-                              AMDGPU_FENCE_OWNER_UNDEFINED, fence);
+			      AMDGPU_FENCE_OWNER_UNDEFINED, fence);
 	if (r)
 		goto error_free;
 
 	return 0;
+
 error_free:
-        amdgpu_job_free(job);
+	amdgpu_job_free(job);
 	return r;
 }
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c
index efb07fe..74f0019 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c
@@ -95,8 +95,8 @@ static void amdgpu_uvd_idle_work_handler(struct work_struct *work);
 
 int amdgpu_uvd_sw_init(struct amdgpu_device *adev)
 {
-        struct amdgpu_ring *ring;
-        struct amd_sched_rq *rq;
+	struct amdgpu_ring *ring;
+	struct amd_sched_rq *rq;
 	unsigned long bo_size;
 	const char *fw_name;
 	const struct common_firmware_header *hdr;
@@ -220,14 +220,14 @@ int amdgpu_uvd_sw_init(struct amdgpu_device *adev)
 
 	amdgpu_bo_unreserve(adev->uvd.vcpu_bo);
  
-        ring = &adev->uvd.ring;
-        rq = &ring->sched.sched_rq[AMD_SCHED_PRIORITY_NORMAL];
-        r = amd_sched_entity_init(&ring->sched, &adev->uvd.entity,
-                                  rq, amdgpu_sched_jobs);
-        if (r != 0) {
-                DRM_ERROR("Failed setting up UVD run queue.\n");
-                return r;
-        }
+	ring = &adev->uvd.ring;
+	rq = &ring->sched.sched_rq[AMD_SCHED_PRIORITY_NORMAL];
+	r = amd_sched_entity_init(&ring->sched, &adev->uvd.entity,
+				  rq, amdgpu_sched_jobs);
+	if (r != 0) {
+		DRM_ERROR("Failed setting up UVD run queue.\n");
+		return r;
+	}
 
 	for (i = 0; i < adev->uvd.max_handles; ++i) {
 		atomic_set(&adev->uvd.handles[i], 0);
@@ -246,18 +246,20 @@ int amdgpu_uvd_sw_fini(struct amdgpu_device *adev)
 	int r;
 	
 	kfree(adev->uvd.saved_bo);
-        amd_sched_entity_fini(&adev->uvd.ring.sched, &adev->uvd.entity);
+
+	amd_sched_entity_fini(&adev->uvd.ring.sched, &adev->uvd.entity);
 	
 	if (adev->uvd.vcpu_bo) {
-	r = amdgpu_bo_reserve(adev->uvd.vcpu_bo, false);
-	if (!r) {
-		amdgpu_bo_kunmap(adev->uvd.vcpu_bo);
-		amdgpu_bo_unpin(adev->uvd.vcpu_bo);
-		amdgpu_bo_unreserve(adev->uvd.vcpu_bo);
+		r = amdgpu_bo_reserve(adev->uvd.vcpu_bo, false);
+		if (!r) {
+			amdgpu_bo_kunmap(adev->uvd.vcpu_bo);
+			amdgpu_bo_unpin(adev->uvd.vcpu_bo);
+			amdgpu_bo_unreserve(adev->uvd.vcpu_bo);
+		}
+
+		amdgpu_bo_unref(&adev->uvd.vcpu_bo);
 	}
 
-	amdgpu_bo_unref(&adev->uvd.vcpu_bo);
-    }
 	amdgpu_ring_fini(&adev->uvd.ring);
 
 	release_firmware(adev->uvd.fw);
@@ -267,30 +269,30 @@ int amdgpu_uvd_sw_fini(struct amdgpu_device *adev)
 
 int amdgpu_uvd_suspend(struct amdgpu_device *adev)
 {
-        unsigned size;
-        void *ptr;
-        int i;
+	unsigned size;
+	void *ptr;
+	int i;
 
 	if (adev->uvd.vcpu_bo == NULL)
 		return 0;
 
-        for (i = 0; i < adev->uvd.max_handles; ++i)
-                if (atomic_read(&adev->uvd.handles[i]))
-                        break;
+	for (i = 0; i < adev->uvd.max_handles; ++i)
+		if (atomic_read(&adev->uvd.handles[i]))
+			break;
  
-        if (i == AMDGPU_MAX_UVD_HANDLES)
-                return 0;
+	if (i == AMDGPU_MAX_UVD_HANDLES)
+		return 0;
 	
 	cancel_delayed_work_sync(&adev->uvd.idle_work);
-        
+
 	size = amdgpu_bo_size(adev->uvd.vcpu_bo);
-        ptr = adev->uvd.cpu_addr;
-        
+	ptr = adev->uvd.cpu_addr;
+
 	adev->uvd.saved_bo = kmalloc(size, GFP_KERNEL);
-        if (!adev->uvd.saved_bo)
-                return -ENOMEM;
+	if (!adev->uvd.saved_bo)
+		return -ENOMEM;
  
-        memcpy(adev->uvd.saved_bo, ptr, size);
+	memcpy(adev->uvd.saved_bo, ptr, size);
 
 	return 0;
 }
@@ -310,18 +312,18 @@ int amdgpu_uvd_resume(struct amdgpu_device *adev)
 		memcpy(ptr, adev->uvd.saved_bo, size);
 		kfree(adev->uvd.saved_bo);
 		adev->uvd.saved_bo = NULL;
-        } else {
-                const struct common_firmware_header *hdr;
-                unsigned offset;
+	} else {
+		const struct common_firmware_header *hdr;
+		unsigned offset;
  
-                hdr = (const struct common_firmware_header *)adev->uvd.fw->data;
-                offset = le32_to_cpu(hdr->ucode_array_offset_bytes);
-                memcpy(adev->uvd.cpu_addr, (adev->uvd.fw->data) + offset,
-                        (adev->uvd.fw->size) - offset);
-                size -= le32_to_cpu(hdr->ucode_size_bytes);
-                ptr += le32_to_cpu(hdr->ucode_size_bytes);
+		hdr = (const struct common_firmware_header *)adev->uvd.fw->data;
+		offset = le32_to_cpu(hdr->ucode_array_offset_bytes);
+		memcpy(adev->uvd.cpu_addr, (adev->uvd.fw->data) + offset,
+			(adev->uvd.fw->size) - offset);
+		size -= le32_to_cpu(hdr->ucode_size_bytes);
+		ptr += le32_to_cpu(hdr->ucode_size_bytes);
 		memset(ptr, 0, size);
-        }
+	}
 
 	return 0;
 }
@@ -331,15 +333,15 @@ void amdgpu_uvd_free_handles(struct amdgpu_device *adev, struct drm_file *filp)
 	struct amdgpu_ring *ring = &adev->uvd.ring;
 	int i, r;
 
-        for (i = 0; i < adev->uvd.max_handles; ++i) {
+	for (i = 0; i < adev->uvd.max_handles; ++i) {
 		uint32_t handle = atomic_read(&adev->uvd.handles[i]);
 		if (handle != 0 && adev->uvd.filp[i] == filp) {
 			struct fence *fence;
 
 			amdgpu_uvd_note_usage(adev);
 
-                        r = amdgpu_uvd_get_destroy_msg(ring, handle,
-                                                      false, &fence);
+			r = amdgpu_uvd_get_destroy_msg(ring, handle,
+						       false, &fence);
 			if (r) {
 				DRM_ERROR("Error destroying UVD (%d)!\n", r);
 				continue;
@@ -591,7 +593,7 @@ static int amdgpu_uvd_cs_msg(struct amdgpu_uvd_cs_ctx *ctx,
 		amdgpu_bo_kunmap(bo);
 
 		/* try to alloc a new handle */
-                for (i = 0; i < adev->uvd.max_handles; ++i) {
+		for (i = 0; i < adev->uvd.max_handles; ++i) {
 			if (atomic_read(&adev->uvd.handles[i]) == handle) {
 				DRM_ERROR("Handle 0x%x already in use!\n", handle);
 				return -EINVAL;
@@ -614,7 +616,7 @@ static int amdgpu_uvd_cs_msg(struct amdgpu_uvd_cs_ctx *ctx,
 			return r;
 
 		/* validate the handle */
-                for (i = 0; i < adev->uvd.max_handles; ++i) {
+		for (i = 0; i < adev->uvd.max_handles; ++i) {
 			if (atomic_read(&adev->uvd.handles[i]) == handle) {
 				if (adev->uvd.filp[i] != ctx->parser->filp) {
 					DRM_ERROR("UVD handle collision detected!\n");
@@ -629,7 +631,7 @@ static int amdgpu_uvd_cs_msg(struct amdgpu_uvd_cs_ctx *ctx,
 
 	case 2:
 		/* it's a destroy msg, free the handle */
-                for (i = 0; i < adev->uvd.max_handles; ++i)
+		for (i = 0; i < adev->uvd.max_handles; ++i)
 			atomic_cmpxchg(&adev->uvd.handles[i], handle, 0);
 		amdgpu_bo_kunmap(bo);
 		return 0;
@@ -739,7 +741,7 @@ static int amdgpu_uvd_cs_pass2(struct amdgpu_uvd_cs_ctx *ctx)
 static int amdgpu_uvd_cs_reg(struct amdgpu_uvd_cs_ctx *ctx,
 			     int (*cb)(struct amdgpu_uvd_cs_ctx *ctx))
 {
-        struct amdgpu_ib *ib = &ctx->parser->job->ibs[ctx->ib_idx];
+	struct amdgpu_ib *ib = &ctx->parser->job->ibs[ctx->ib_idx];
 	int i, r;
 
 	ctx->idx++;
@@ -785,7 +787,7 @@ static int amdgpu_uvd_cs_reg(struct amdgpu_uvd_cs_ctx *ctx,
 static int amdgpu_uvd_cs_packets(struct amdgpu_uvd_cs_ctx *ctx,
 				 int (*cb)(struct amdgpu_uvd_cs_ctx *ctx))
 {
-        struct amdgpu_ib *ib = &ctx->parser->job->ibs[ctx->ib_idx];
+	struct amdgpu_ib *ib = &ctx->parser->job->ibs[ctx->ib_idx];
 	int r;
 
 	for (ctx->idx = 0 ; ctx->idx < ib->length_dw; ) {
@@ -827,7 +829,7 @@ int amdgpu_uvd_ring_parse_cs(struct amdgpu_cs_parser *parser, uint32_t ib_idx)
 		[0x00000003]	=	2048,
 		[0x00000004]	=	0xFFFFFFFF,
 	};
-        struct amdgpu_ib *ib = &parser->job->ibs[ib_idx];
+	struct amdgpu_ib *ib = &parser->job->ibs[ib_idx];
 	int r;
 
 	if (ib->length_dw % 16) {
@@ -861,8 +863,7 @@ int amdgpu_uvd_ring_parse_cs(struct amdgpu_cs_parser *parser, uint32_t ib_idx)
 }
 
 static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,
-                              bool direct, struct fence **fence)
-
+			       bool direct, struct fence **fence)
 {
 	struct ttm_validate_buffer tv;
 	struct ww_acquire_ctx ticket;
@@ -892,6 +893,7 @@ static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,
 	r = ttm_bo_validate(&bo->tbo, &bo->placement, true, false);
 	if (r)
 		goto err;
+
 	r = amdgpu_job_alloc_with_ib(adev, 64, &job);
 	if (r)
 		goto err;
@@ -908,19 +910,19 @@ static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,
 		ib->ptr[i] = PACKET2(0);
 	ib->length_dw = 16;
 
-        if (direct) {
-                r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
-                job->fence = f;
-                if (r)
-                        goto err_free;
+	if (direct) {
+		r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
+		job->fence = f;
+		if (r)
+			goto err_free;
 
-                amdgpu_job_free(job);
-        } else {
-                r = amdgpu_job_submit(job, ring, &adev->uvd.entity,
-                                      AMDGPU_FENCE_OWNER_UNDEFINED, &f);
-                if (r)
-                        goto err_free;
-        }
+		amdgpu_job_free(job);
+	} else {
+		r = amdgpu_job_submit(job, ring, &adev->uvd.entity,
+				      AMDGPU_FENCE_OWNER_UNDEFINED, &f);
+		if (r)
+			goto err_free;
+	}
 
 	ttm_eu_fence_buffer_objects(&ticket, &head, f);
 
@@ -930,8 +932,10 @@ static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,
 	fence_put(f);
 
 	return 0;
+
 err_free:
 	amdgpu_job_free(job);
+
 err:
 	ttm_eu_backoff_reservation(&ticket, &head);
 	return r;
@@ -986,11 +990,11 @@ int amdgpu_uvd_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 	amdgpu_bo_kunmap(bo);
 	amdgpu_bo_unreserve(bo);
 
-        return amdgpu_uvd_send_msg(ring, bo, true, fence);
+	return amdgpu_uvd_send_msg(ring, bo, true, fence);
 }
 
 int amdgpu_uvd_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
-                              bool direct, struct fence **fence)
+			       bool direct, struct fence **fence)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_bo *bo;
@@ -1028,7 +1032,7 @@ int amdgpu_uvd_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
 	amdgpu_bo_kunmap(bo);
 	amdgpu_bo_unreserve(bo);
 
-        return amdgpu_uvd_send_msg(ring, bo, direct, fence);
+	return amdgpu_uvd_send_msg(ring, bo, direct, fence);
 }
 
 static void amdgpu_uvd_idle_work_handler(struct work_struct *work)
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c
index 7aa7342..e03b454 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c
@@ -51,7 +51,7 @@
 #define FIRMWARE_FIJI		"amdgpu/fiji_vce.bin"
 #define FIRMWARE_STONEY		"amdgpu/stoney_vce.bin"
 #define FIRMWARE_POLARIS10	"amdgpu/polaris10_vce.bin"
-#define FIRMWARE_POLARIS11         "amdgpu/polaris11_vce.bin"
+#define FIRMWARE_POLARIS11	   "amdgpu/polaris11_vce.bin"
 
 #ifdef CONFIG_DRM_AMDGPU_CIK
 MODULE_FIRMWARE(FIRMWARE_BONAIRE);
@@ -78,8 +78,8 @@ static void amdgpu_vce_idle_work_handler(struct work_struct *work);
  */
 int amdgpu_vce_sw_init(struct amdgpu_device *adev, unsigned long size)
 {
-        struct amdgpu_ring *ring;
-        struct amd_sched_rq *rq;
+	struct amdgpu_ring *ring;
+	struct amd_sched_rq *rq;
 	const char *fw_name;
 	const struct common_firmware_header *hdr;
 	unsigned ucode_version, version_major, version_minor, binary_id;
@@ -183,14 +183,14 @@ int amdgpu_vce_sw_init(struct amdgpu_device *adev, unsigned long size)
 	}
 
 
-       ring = &adev->vce.ring[0];
-       rq = &ring->sched.sched_rq[AMD_SCHED_PRIORITY_NORMAL];
-       r = amd_sched_entity_init(&ring->sched, &adev->vce.entity,
-                                 rq, amdgpu_sched_jobs);
-       if (r != 0) {
-               DRM_ERROR("Failed setting up VCE run queue.\n");
-               return r;
-       }
+	ring = &adev->vce.ring[0];
+	rq = &ring->sched.sched_rq[AMD_SCHED_PRIORITY_NORMAL];
+	r = amd_sched_entity_init(&ring->sched, &adev->vce.entity,
+				  rq, amdgpu_sched_jobs);
+	if (r != 0) {
+		DRM_ERROR("Failed setting up VCE run queue.\n");
+		return r;
+	}
 
 	for (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i) {
 		atomic_set(&adev->vce.handles[i], 0);
@@ -212,7 +212,7 @@ int amdgpu_vce_sw_fini(struct amdgpu_device *adev)
 	if (adev->vce.vcpu_bo == NULL)
 		return 0;
 
-        amd_sched_entity_fini(&adev->vce.ring[0].sched, &adev->vce.entity);
+	amd_sched_entity_fini(&adev->vce.ring[0].sched, &adev->vce.entity);
 
 	amdgpu_bo_unref(&adev->vce.vcpu_bo);
 
@@ -362,7 +362,7 @@ void amdgpu_vce_free_handles(struct amdgpu_device *adev, struct drm_file *filp)
 
 		amdgpu_vce_note_usage(adev);
 
-                r = amdgpu_vce_get_destroy_msg(ring, handle, false, NULL);
+		r = amdgpu_vce_get_destroy_msg(ring, handle, false, NULL);
 		if (r)
 			DRM_ERROR("Error destroying VCE handle (%d)!\n", r);
 
@@ -396,6 +396,7 @@ int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 		return r;
 
 	ib = &job->ibs[0];	
+
 	dummy = ib->gpu_addr + 1024;
 
 	/* stitch together an VCE create msg */
@@ -435,16 +436,17 @@ int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 	for (i = ib->length_dw; i < ib_size_dw; ++i)
 		ib->ptr[i] = 0x0;
 
-        r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
-                 job->fence = f;
+	r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
+	job->fence = f;
 	if (r)
 		goto err;
        
-	 amdgpu_job_free(job);
+	amdgpu_job_free(job);
 	if (fence)
 		*fence = fence_get(f);
 	fence_put(f);
 	return 0;
+
 err:
 	amdgpu_job_free(job);
 	return r;
@@ -461,11 +463,11 @@ err:
  * Close up a stream for HW test or if userspace failed to do so
  */
 int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
-                               bool direct, struct fence **fence)
+			       bool direct, struct fence **fence)
 {
 	const unsigned ib_size_dw = 1024;
 	struct amdgpu_job *job;
-        struct amdgpu_ib *ib;
+	struct amdgpu_ib *ib;
 	struct fence *f = NULL;
 	uint64_t dummy;
 	int i, r;
@@ -495,24 +497,25 @@ int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
 	for (i = ib->length_dw; i < ib_size_dw; ++i)
 		ib->ptr[i] = 0x0;
 
-       if (direct) {
-                r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
+	if (direct) {
+		r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
 		job->fence = f;
-                if (r)
-                        goto err;
+		if (r)
+			goto err;
 
-                amdgpu_job_free(job);
-        } else {
-                r = amdgpu_job_submit(job, ring, &ring->adev->vce.entity,
-                                      AMDGPU_FENCE_OWNER_UNDEFINED, &f);
-                if (r)
-                        goto err;
-        }
+		amdgpu_job_free(job);
+	} else {
+		r = amdgpu_job_submit(job, ring, &ring->adev->vce.entity,
+				      AMDGPU_FENCE_OWNER_UNDEFINED, &f);
+		if (r)
+			goto err;
+	}
 
 	if (fence)
 		*fence = fence_get(f);
 	fence_put(f);
 	return 0;
+
 err:
 	amdgpu_job_free(job);
 	return r;
@@ -616,7 +619,7 @@ static int amdgpu_vce_validate_handle(struct amdgpu_cs_parser *p,
  */
 int amdgpu_vce_ring_parse_cs(struct amdgpu_cs_parser *p, uint32_t ib_idx)
 {
-        struct amdgpu_ib *ib = &p->job->ibs[ib_idx];
+	struct amdgpu_ib *ib = &p->job->ibs[ib_idx];
 	unsigned fb_idx = 0, bs_idx = 0;
 	int session_idx = -1;
 	bool destroyed = false;
@@ -848,7 +851,7 @@ int amdgpu_vce_ring_test_ib(struct amdgpu_ring *ring)
 		goto error;
 	}
 
-        r = amdgpu_vce_get_destroy_msg(ring, 1, true, &fence);
+	r = amdgpu_vce_get_destroy_msg(ring, 1, true, &fence);
 	if (r) {
 		DRM_ERROR("amdgpu: failed to get destroy ib (%d).\n", r);
 		goto error;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index 7eee8ae..a583f36 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
+ *	    Alex Deucher
+ *	    Jerome Glisse
  */
 #include <drm/drmP.h>
 #include <drm/amdgpu_drm.h>
@@ -87,15 +87,15 @@ static unsigned amdgpu_vm_directory_size(struct amdgpu_device *adev)
  * validate for command submission.
  */
 void amdgpu_vm_get_pd_bo(struct amdgpu_vm *vm,
-                        struct list_head *validated,
-                        struct amdgpu_bo_list_entry *entry)
+			 struct list_head *validated,
+			 struct amdgpu_bo_list_entry *entry)
 {
-        entry->robj = vm->page_directory;
-        entry->priority = 0;
-        entry->tv.bo = &vm->page_directory->tbo;
-        entry->tv.shared = true;
-        entry->user_pages = NULL;
-        list_add(&entry->tv.head, validated);
+	entry->robj = vm->page_directory;
+	entry->priority = 0;
+	entry->tv.bo = &vm->page_directory->tbo;
+	entry->tv.shared = true;
+	entry->user_pages = NULL;
+	list_add(&entry->tv.head, validated);
 }
 
 /**
@@ -110,16 +110,16 @@ void amdgpu_vm_get_pd_bo(struct amdgpu_vm *vm,
  */
 void amdgpu_vm_get_pt_bos(struct amdgpu_vm *vm, struct list_head *duplicates)
 {
-        unsigned i;
+	unsigned i;
 
 	/* add the vm page table to the list */
-        for (i = 0; i <= vm->max_pde_used; ++i) {
-                struct amdgpu_bo_list_entry *entry = &vm->page_tables[i].entry;
+	for (i = 0; i <= vm->max_pde_used; ++i) {
+		struct amdgpu_bo_list_entry *entry = &vm->page_tables[i].entry;
  
-                if (!entry->robj)
+		if (!entry->robj)
 			continue;
 
-                list_add(&entry->tv.head, duplicates);
+		list_add(&entry->tv.head, duplicates);
 	}
 }
 
@@ -132,21 +132,21 @@ void amdgpu_vm_get_pt_bos(struct amdgpu_vm *vm, struct list_head *duplicates)
  * Move the PT BOs to the tail of the LRU.
  */
 void amdgpu_vm_move_pt_bos_in_lru(struct amdgpu_device *adev,
-                                  struct amdgpu_vm *vm)
+				  struct amdgpu_vm *vm)
 {
-        struct ttm_bo_global *glob = adev->mman.bdev.glob;
-        unsigned i;
+	struct ttm_bo_global *glob = adev->mman.bdev.glob;
+	unsigned i;
  
-        spin_lock(&glob->lru_lock);
-        for (i = 0; i <= vm->max_pde_used; ++i) {
-                struct amdgpu_bo_list_entry *entry = &vm->page_tables[i].entry;
+	spin_lock(&glob->lru_lock);
+	for (i = 0; i <= vm->max_pde_used; ++i) {
+		struct amdgpu_bo_list_entry *entry = &vm->page_tables[i].entry;
  
-                if (!entry->robj)
-                        continue;
+		if (!entry->robj)
+			continue;
  
-                ttm_bo_move_to_lru_tail(&entry->robj->tbo);
-        }
-        spin_unlock(&glob->lru_lock);
+		ttm_bo_move_to_lru_tail(&entry->robj->tbo);
+	}
+	spin_unlock(&glob->lru_lock);
 }
 
 /**
@@ -282,55 +282,55 @@ error:
  * Emit a VM flush when it is necessary.
  */
 int amdgpu_vm_flush(struct amdgpu_ring *ring,
-                    unsigned vm_id, uint64_t pd_addr,
-                    uint32_t gds_base, uint32_t gds_size,
-                    uint32_t gws_base, uint32_t gws_size,
-                    uint32_t oa_base, uint32_t oa_size)
+		    unsigned vm_id, uint64_t pd_addr,
+		    uint32_t gds_base, uint32_t gds_size,
+		    uint32_t gws_base, uint32_t gws_size,
+		    uint32_t oa_base, uint32_t oa_size)
 {
-        struct amdgpu_device *adev = ring->adev;
-        struct amdgpu_vm_id *id = &adev->vm_manager.ids[vm_id];
-        bool gds_switch_needed = ring->funcs->emit_gds_switch && (
-                id->gds_base != gds_base ||
-                id->gds_size != gds_size ||
-                id->gws_base != gws_base ||
-                id->gws_size != gws_size ||
-                id->oa_base != oa_base ||
-                id->oa_size != oa_size);
-        int r;
-
-        if (ring->funcs->emit_pipeline_sync && (
-           pd_addr != AMDGPU_VM_NO_FLUSH || gds_switch_needed ||
-                    ring->type == AMDGPU_RING_TYPE_COMPUTE))
-               amdgpu_ring_emit_pipeline_sync(ring);
-
-        if (ring->funcs->emit_vm_flush &&
-            pd_addr != AMDGPU_VM_NO_FLUSH) {
-                struct fence *fence;
+	struct amdgpu_device *adev = ring->adev;
+	struct amdgpu_vm_id *id = &adev->vm_manager.ids[vm_id];
+	bool gds_switch_needed = ring->funcs->emit_gds_switch && (
+		id->gds_base != gds_base ||
+		id->gds_size != gds_size ||
+		id->gws_base != gws_base ||
+		id->gws_size != gws_size ||
+		id->oa_base != oa_base ||
+		id->oa_size != oa_size);
+	int r;
+
+	if (ring->funcs->emit_pipeline_sync && (
+	    pd_addr != AMDGPU_VM_NO_FLUSH || gds_switch_needed ||
+	    ring->type == AMDGPU_RING_TYPE_COMPUTE))
+	       amdgpu_ring_emit_pipeline_sync(ring);
+
+	if (ring->funcs->emit_vm_flush &&
+	    pd_addr != AMDGPU_VM_NO_FLUSH) {
+		struct fence *fence;
 
 		trace_amdgpu_vm_flush(pd_addr, ring->idx, vm_id);
 		amdgpu_ring_emit_vm_flush(ring, vm_id, pd_addr);
  
-                mutex_lock(&adev->vm_manager.lock);
-                if ((id->pd_gpu_addr == pd_addr) && (id->last_user == ring)) {
-                        r = amdgpu_fence_emit(ring, &fence);
-                        if (r) {
-                                mutex_unlock(&adev->vm_manager.lock);
-                                return r;
-                        }
-                        fence_put(id->last_flush);
-                        id->last_flush = fence;
-                }
-                mutex_unlock(&adev->vm_manager.lock);
+		mutex_lock(&adev->vm_manager.lock);
+		if ((id->pd_gpu_addr == pd_addr) && (id->last_user == ring)) {
+			r = amdgpu_fence_emit(ring, &fence);
+			if (r) {
+				mutex_unlock(&adev->vm_manager.lock);
+				return r;
+			}
+			fence_put(id->last_flush);
+			id->last_flush = fence;
+		}
+		mutex_unlock(&adev->vm_manager.lock);
 	}
 
-        if (gds_switch_needed) {
-                id->gds_base = gds_base;
-                id->gds_size = gds_size;
-                id->gws_base = gws_base;
-                id->gws_size = gws_size;
-                id->oa_base = oa_base;
-                id->oa_size = oa_size;
-	       amdgpu_ring_emit_gds_switch(ring, vm_id,
+	if (gds_switch_needed) {
+		id->gds_base = gds_base;
+		id->gds_size = gds_size;
+		id->gws_base = gws_base;
+		id->gws_size = gws_size;
+		id->oa_base = oa_base;
+		id->oa_size = oa_size;
+		amdgpu_ring_emit_gds_switch(ring, vm_id,
 					    gds_base, gds_size,
 					    gws_base, gws_size,
 					    oa_base, oa_size);
@@ -349,14 +349,14 @@ int amdgpu_vm_flush(struct amdgpu_ring *ring,
  */
 void amdgpu_vm_reset_id(struct amdgpu_device *adev, unsigned vm_id)
 {
-       struct amdgpu_vm_id *id = &adev->vm_manager.ids[vm_id];
-
-       id->gds_base = 0;
-       id->gds_size = 0;
-       id->gws_base = 0;
-       id->gws_size = 0;
-       id->oa_base = 0;
-       id->oa_size = 0;
+	struct amdgpu_vm_id *id = &adev->vm_manager.ids[vm_id];
+
+	id->gds_base = 0;
+	id->gds_size = 0;
+	id->gws_base = 0;
+	id->gws_size = 0;
+	id->oa_base = 0;
+	id->oa_size = 0;
 }
 
 /**
@@ -437,17 +437,17 @@ static void amdgpu_vm_update_pages(struct amdgpu_device *adev,
  * need to reserve bo first before calling it.
  */
 static int amdgpu_vm_clear_bo(struct amdgpu_device *adev,
-                              struct amdgpu_vm *vm,
+			      struct amdgpu_vm *vm,
 			      struct amdgpu_bo *bo)
 {
-        struct amdgpu_ring *ring;
+	struct amdgpu_ring *ring;
 	struct fence *fence = NULL;
-        struct amdgpu_job *job;
+	struct amdgpu_job *job;
 	unsigned entries;
 	uint64_t addr;
 	int r;
 
-        ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
+	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
 
 	r = reservation_object_reserve_shared(bo->tbo.resv);
 	if (r)
@@ -460,25 +460,25 @@ static int amdgpu_vm_clear_bo(struct amdgpu_device *adev,
 	addr = amdgpu_bo_gpu_offset(bo);
 	entries = amdgpu_bo_size(bo) / 8;
 
-        r = amdgpu_job_alloc_with_ib(adev, 64, &job);
-        if (r)
+	r = amdgpu_job_alloc_with_ib(adev, 64, &job);
+	if (r)
 		goto error;
-        amdgpu_vm_update_pages(adev, 0, NULL, &job->ibs[0], addr, 0, entries,
-                                0, 0);
-        amdgpu_ring_pad_ib(ring, &job->ibs[0]);
+	amdgpu_vm_update_pages(adev, 0, NULL, &job->ibs[0], addr, 0, entries,
+			       0, 0);
+	amdgpu_ring_pad_ib(ring, &job->ibs[0]);
 
-        WARN_ON(job->ibs[0].length_dw > 64);
-        r = amdgpu_job_submit(job, ring, &vm->entity,
-                              AMDGPU_FENCE_OWNER_VM, &fence);
+	WARN_ON(job->ibs[0].length_dw > 64);
+	r = amdgpu_job_submit(job, ring, &vm->entity,
+			      AMDGPU_FENCE_OWNER_VM, &fence);
 	if (r)
 		goto error_free;
 
-        amdgpu_bo_fence(bo, fence, true);
+	amdgpu_bo_fence(bo, fence, true);
 	fence_put(fence);
 	return 0;
 
 error_free:
-        amdgpu_job_free(job);
+	amdgpu_job_free(job);
 error:
 	return r;
 }
@@ -528,32 +528,32 @@ uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
 int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 				    struct amdgpu_vm *vm)
 {
-        struct amdgpu_ring *ring;
+	struct amdgpu_ring *ring;
 	struct amdgpu_bo *pd = vm->page_directory;
 	uint64_t pd_addr = amdgpu_bo_gpu_offset(pd);
 	uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
 	uint64_t last_pde = ~0, last_pt = ~0;
 	unsigned count = 0, pt_idx, ndw;
-        struct amdgpu_job *job;
+	struct amdgpu_job *job;
 	struct amdgpu_ib *ib;
 	struct fence *fence = NULL;
 
 	int r;
-        ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
+	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
 
 	/* padding, etc. */
 	ndw = 64;
 
 	/* assume the worst case */
 	ndw += vm->max_pde_used * 6;
-        r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
-        if (r)
+	r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
+	if (r)
 		return r;
 
-        ib = &job->ibs[0];
+	ib = &job->ibs[0];
 	/* walk over the address space and update the page directory */
 	for (pt_idx = 0; pt_idx <= vm->max_pde_used; ++pt_idx) {
-                struct amdgpu_bo *bo = vm->page_tables[pt_idx].entry.robj;
+		struct amdgpu_bo *bo = vm->page_tables[pt_idx].entry.robj;
 		uint64_t pde, pt;
 
 		if (bo == NULL)
@@ -588,12 +588,12 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 				       count, incr, AMDGPU_PTE_VALID);
 
 	if (ib->length_dw != 0) {
-                amdgpu_ring_pad_ib(ring, ib);
-                amdgpu_sync_resv(adev, &job->sync, pd->tbo.resv,
-                                 AMDGPU_FENCE_OWNER_VM);
+		amdgpu_ring_pad_ib(ring, ib);
+		amdgpu_sync_resv(adev, &job->sync, pd->tbo.resv,
+				 AMDGPU_FENCE_OWNER_VM);
 		WARN_ON(ib->length_dw > ndw);
-                r = amdgpu_job_submit(job, ring, &vm->entity,
-                                      AMDGPU_FENCE_OWNER_VM, &fence);
+		r = amdgpu_job_submit(job, ring, &vm->entity,
+				      AMDGPU_FENCE_OWNER_VM, &fence);
 		if (r)
 			goto error_free;
 
@@ -601,14 +601,14 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 		fence_put(vm->page_directory_fence);
 		vm->page_directory_fence = fence_get(fence);
 		fence_put(fence);
-        } else {
-                amdgpu_job_free(job);
+	} else {
+		amdgpu_job_free(job);
 	}
 
 	return 0;
 
 error_free:
-        amdgpu_job_free(job);
+	amdgpu_job_free(job);
 	return r;
 }
 
@@ -627,8 +627,8 @@ error_free:
  * Global and local mutex must be locked!
  */
 static void amdgpu_vm_frag_ptes(struct amdgpu_device *adev,
-                                uint64_t src,
-                                dma_addr_t *pages_addr,
+				uint64_t src,
+				dma_addr_t *pages_addr,
 				struct amdgpu_ib *ib,
 				uint64_t pe_start, uint64_t pe_end,
 				uint64_t addr, uint32_t flags)
@@ -661,16 +661,16 @@ static void amdgpu_vm_frag_ptes(struct amdgpu_device *adev,
 
 	unsigned count;
 
-        /* Abort early if there isn't anything to do */
-        if (pe_start == pe_end)
-                return;
+	/* Abort early if there isn't anything to do */
+	if (pe_start == pe_end)
+		return;
 
 	/* system pages are non continuously */
-        if (src || pages_addr || !(flags & AMDGPU_PTE_VALID) ||
-            (frag_start >= frag_end)) {
+	if (src || pages_addr || !(flags & AMDGPU_PTE_VALID) ||
+	    (frag_start >= frag_end)) {
 
 		count = (pe_end - pe_start) / 8;
-                amdgpu_vm_update_pages(adev, src, pages_addr, ib, pe_start,
+		amdgpu_vm_update_pages(adev, src, pages_addr, ib, pe_start,
 				       addr, count, AMDGPU_GPU_PAGE_SIZE,
 				       flags);
 		return;
@@ -713,52 +713,52 @@ static void amdgpu_vm_frag_ptes(struct amdgpu_device *adev,
  * Update the page tables in the range @start - @end.
  */
 static void amdgpu_vm_update_ptes(struct amdgpu_device *adev,
-                                  uint64_t src,
-                                  dma_addr_t *pages_addr,
-                                  struct amdgpu_vm *vm,
-                                  struct amdgpu_ib *ib,
-                                  uint64_t start, uint64_t end,
-                                  uint64_t dst, uint32_t flags)
+				  uint64_t src,
+				  dma_addr_t *pages_addr,
+				  struct amdgpu_vm *vm,
+				  struct amdgpu_ib *ib,
+				  uint64_t start, uint64_t end,
+				  uint64_t dst, uint32_t flags)
 {
-        const uint64_t mask = AMDGPU_VM_PTE_COUNT - 1;
+	const uint64_t mask = AMDGPU_VM_PTE_COUNT - 1;
  
-        uint64_t last_pe_start = ~0, last_pe_end = ~0, last_dst = ~0;
+	uint64_t last_pe_start = ~0, last_pe_end = ~0, last_dst = ~0;
 	uint64_t addr;
 
 	/* walk over the address space and update the page tables */
 	for (addr = start; addr < end; ) {
 		uint64_t pt_idx = addr >> amdgpu_vm_block_size;
-                struct amdgpu_bo *pt = vm->page_tables[pt_idx].entry.robj;
+		struct amdgpu_bo *pt = vm->page_tables[pt_idx].entry.robj;
 		unsigned nptes;
-                uint64_t pe_start;
+		uint64_t pe_start;
 
 		if ((addr & ~mask) == (end & ~mask))
 			nptes = end - addr;
 		else
 			nptes = AMDGPU_VM_PTE_COUNT - (addr & mask);
 
-                pe_start = amdgpu_bo_gpu_offset(pt);
-                pe_start += (addr & mask) * 8;
+		pe_start = amdgpu_bo_gpu_offset(pt);
+		pe_start += (addr & mask) * 8;
 
-                if (last_pe_end != pe_start) {
+		if (last_pe_end != pe_start) {
 
-                        amdgpu_vm_frag_ptes(adev, src, pages_addr, ib,
-                                            last_pe_start, last_pe_end,
-                                            last_dst, flags);
+			amdgpu_vm_frag_ptes(adev, src, pages_addr, ib,
+					    last_pe_start, last_pe_end,
+					    last_dst, flags);
  
-                        last_pe_start = pe_start;
-                        last_pe_end = pe_start + 8 * nptes;
+			last_pe_start = pe_start;
+			last_pe_end = pe_start + 8 * nptes;
 			last_dst = dst;
 		} else {
-                        last_pe_end += 8 * nptes;
+			last_pe_end += 8 * nptes;
 		}
 
 		addr += nptes;
 		dst += nptes * AMDGPU_GPU_PAGE_SIZE;
 	}
 
-        amdgpu_vm_frag_ptes(adev, src, pages_addr, ib, last_pe_start,
-                           last_pe_end, last_dst, flags);
+	amdgpu_vm_frag_ptes(adev, src, pages_addr, ib, last_pe_start,
+			   last_pe_end, last_dst, flags);
 }
 
 /**
@@ -785,19 +785,19 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 				       uint32_t flags, uint64_t addr,
 				       struct fence **fence)
 {
-        struct amdgpu_ring *ring;
-        void *owner = AMDGPU_FENCE_OWNER_VM;
+	struct amdgpu_ring *ring;
+	void *owner = AMDGPU_FENCE_OWNER_VM;
 	unsigned nptes, ncmds, ndw;
-        struct amdgpu_job *job;
+	struct amdgpu_job *job;
 	struct amdgpu_ib *ib;
 	struct fence *f = NULL;
 	int r;
 
-        ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
+	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
 
-        /* sync to everything on unmapping */
-        if (!(flags & AMDGPU_PTE_VALID))
-                owner = AMDGPU_FENCE_OWNER_UNDEFINED;
+	/* sync to everything on unmapping */
+	if (!(flags & AMDGPU_PTE_VALID))
+		owner = AMDGPU_FENCE_OWNER_UNDEFINED;
  
 	nptes = last - start + 1;
 
@@ -828,29 +828,29 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 		/* two extra commands for begin/end of fragment */
 		ndw += 2 * 10;
 	}
-        r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
-        if (r)
+	r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
+	if (r)
 
 		return r;
 
-        ib = &job->ibs[0];
+	ib = &job->ibs[0];
 
-        r = amdgpu_sync_resv(adev, &job->sync, vm->page_directory->tbo.resv,
-                             owner);
-        if (r)
-                goto error_free;
+	r = amdgpu_sync_resv(adev, &job->sync, vm->page_directory->tbo.resv,
+			     owner);
+	if (r)
+		goto error_free;
 
-        r = reservation_object_reserve_shared(vm->page_directory->tbo.resv);
-        if (r)
-                goto error_free;
+	r = reservation_object_reserve_shared(vm->page_directory->tbo.resv);
+	if (r)
+		goto error_free;
  
-        amdgpu_vm_update_ptes(adev, src, pages_addr, vm, ib, start,
-                              last + 1, addr, flags);
+	amdgpu_vm_update_ptes(adev, src, pages_addr, vm, ib, start,
+			      last + 1, addr, flags);
 
-        amdgpu_ring_pad_ib(ring, ib);
+	amdgpu_ring_pad_ib(ring, ib);
 	WARN_ON(ib->length_dw > ndw);
-        r = amdgpu_job_submit(job, ring, &vm->entity,
-                              AMDGPU_FENCE_OWNER_VM, &f);
+	r = amdgpu_job_submit(job, ring, &vm->entity,
+			      AMDGPU_FENCE_OWNER_VM, &f);
 	if (r)
 		goto error_free;
 
@@ -863,7 +863,7 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 	return 0;
 
 error_free:
-        amdgpu_job_free(job);
+	amdgpu_job_free(job);
 	return r;
 }
 
@@ -1205,11 +1205,11 @@ int amdgpu_vm_bo_map(struct amdgpu_device *adev,
 	/* walk over the address space and allocate the page tables */
 	for (pt_idx = saddr; pt_idx <= eaddr; ++pt_idx) {
 		struct reservation_object *resv = vm->page_directory->tbo.resv;
-                struct amdgpu_bo_list_entry *entry;
+		struct amdgpu_bo_list_entry *entry;
 		struct amdgpu_bo *pt;
 
-                entry = &vm->page_tables[pt_idx].entry;
-                if (entry->robj)
+		entry = &vm->page_tables[pt_idx].entry;
+		if (entry->robj)
 			continue;
 
 		r = amdgpu_bo_create(adev, AMDGPU_VM_PTE_COUNT * 8,
@@ -1225,17 +1225,17 @@ int amdgpu_vm_bo_map(struct amdgpu_device *adev,
 		 */
 		pt->parent = amdgpu_bo_ref(vm->page_directory);
 
-                r = amdgpu_vm_clear_bo(adev, vm, pt);
+		r = amdgpu_vm_clear_bo(adev, vm, pt);
 		if (r) {
 			amdgpu_bo_unref(&pt);
 			goto error_free;
 		}
 
-                entry->robj = pt;
-                entry->priority = 0;
-                entry->tv.bo = &entry->robj->tbo;
-                entry->tv.shared = true;
-                entry->user_pages = NULL;
+		entry->robj = pt;
+		entry->priority = 0;
+		entry->tv.bo = &entry->robj->tbo;
+		entry->tv.shared = true;
+		entry->user_pages = NULL;
 		vm->page_tables[pt_idx].addr = 0;
 	}
 
@@ -1375,16 +1375,15 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 	const unsigned align = min(AMDGPU_VM_PTB_ALIGN_SIZE,
 		AMDGPU_VM_PTE_COUNT * 8);
 	unsigned pd_size, pd_entries;
-        unsigned ring_instance;
-        struct amdgpu_ring *ring;
-
-        struct amd_sched_rq *rq;
+	unsigned ring_instance;
+	struct amdgpu_ring *ring;
+	struct amd_sched_rq *rq;
 	int i, r;
 
-        for (i = 0; i < AMDGPU_MAX_RINGS; ++i)
-               vm->ids[i] = NULL;
+	for (i = 0; i < AMDGPU_MAX_RINGS; ++i)
+		vm->ids[i] = NULL;
 	vm->va = RB_ROOT;
-        vm->client_id = atomic64_inc_return(&adev->vm_manager.client_counter);
+	vm->client_id = atomic64_inc_return(&adev->vm_manager.client_counter);
 	spin_lock_init(&vm->status_lock);
 	INIT_LIST_HEAD(&vm->invalidated);
 	INIT_LIST_HEAD(&vm->cleared);
@@ -1400,16 +1399,16 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 		return -ENOMEM;
 	}
 
-        /* create scheduler entity for page table updates */
+	/* create scheduler entity for page table updates */
 
-        ring_instance = atomic_inc_return(&adev->vm_manager.vm_pte_next_ring);
-        ring_instance %= adev->vm_manager.vm_pte_num_rings;
-        ring = adev->vm_manager.vm_pte_rings[ring_instance];
-        rq = &ring->sched.sched_rq[AMD_SCHED_PRIORITY_KERNEL];
-        r = amd_sched_entity_init(&ring->sched, &vm->entity,
-                                  rq, amdgpu_sched_jobs);
-        if (r)
-                return r;
+	ring_instance = atomic_inc_return(&adev->vm_manager.vm_pte_next_ring);
+	ring_instance %= adev->vm_manager.vm_pte_num_rings;
+	ring = adev->vm_manager.vm_pte_rings[ring_instance];
+	rq = &ring->sched.sched_rq[AMD_SCHED_PRIORITY_KERNEL];
+	r = amd_sched_entity_init(&ring->sched, &vm->entity,
+				  rq, amdgpu_sched_jobs);
+	if (r)
+		return r;
 
 	vm->page_directory_fence = NULL;
 
@@ -1418,27 +1417,27 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 			     AMDGPU_GEM_CREATE_NO_CPU_ACCESS,
 			     NULL, NULL, &vm->page_directory);
 	if (r)
-                goto error_free_sched_entity;
+		goto error_free_sched_entity;
 
 	r = amdgpu_bo_reserve(vm->page_directory, false);
 	if (r)
-                goto error_free_page_directory;
+		goto error_free_page_directory;
 
 	r = amdgpu_vm_clear_bo(adev, vm, vm->page_directory);
 	amdgpu_bo_unreserve(vm->page_directory);
-        if (r)
-                goto error_free_page_directory;
+	if (r)
+		goto error_free_page_directory;
 
 	return 0;
 
 error_free_page_directory:
-        amdgpu_bo_unref(&vm->page_directory);
-        vm->page_directory = NULL;
+	amdgpu_bo_unref(&vm->page_directory);
+	vm->page_directory = NULL;
 
 error_free_sched_entity:
-        amd_sched_entity_fini(&ring->sched, &vm->entity);
+	amd_sched_entity_fini(&ring->sched, &vm->entity);
 
-       return r;
+	return r;
 }
 
 /**
@@ -1455,7 +1454,7 @@ void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 	struct amdgpu_bo_va_mapping *mapping, *tmp;
 	int i;
 
-        amd_sched_entity_fini(vm->entity.sched, &vm->entity);
+	amd_sched_entity_fini(vm->entity.sched, &vm->entity);
 
 	if (!RB_EMPTY_ROOT(&vm->va)) {
 		dev_err(adev->dev, "still active bo inside vm\n");
@@ -1471,12 +1470,11 @@ void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 	}
 
 	for (i = 0; i < amdgpu_vm_num_pdes(adev); i++)
-                amdgpu_bo_unref(&vm->page_tables[i].entry.robj);
+		amdgpu_bo_unref(&vm->page_tables[i].entry.robj);
 	drm_free_large(vm->page_tables);
 
 	amdgpu_bo_unref(&vm->page_directory);
 	fence_put(vm->page_directory_fence);
-
 }
 
 /**
@@ -1493,14 +1491,14 @@ void amdgpu_vm_manager_init(struct amdgpu_device *adev)
 	INIT_LIST_HEAD(&adev->vm_manager.ids_lru);
 
 	/* skip over VMID 0, since it is the system VM */
-        for (i = 1; i < adev->vm_manager.num_ids; ++i) {
-		 amdgpu_vm_reset_id(adev, i);
-                 amdgpu_sync_create(&adev->vm_manager.ids[i].active);
+	for (i = 1; i < adev->vm_manager.num_ids; ++i) {
+		amdgpu_vm_reset_id(adev, i);
+		amdgpu_sync_create(&adev->vm_manager.ids[i].active);
 		list_add_tail(&adev->vm_manager.ids[i].list,
 			      &adev->vm_manager.ids_lru);
-        }
+	}
 
-        atomic64_set(&adev->vm_manager.client_counter, 0);
+	atomic64_set(&adev->vm_manager.client_counter, 0);
 }
 
 /**
diff --git a/drivers/gpu/drm/amd/amdgpu/cik_sdma.c b/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
index 03e89d2..85e78b8 100644
--- a/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
+++ b/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
@@ -636,7 +636,7 @@ static int cik_sdma_ring_test_ib(struct amdgpu_ring *ring)
 	tmp = 0xCAFEDEAD;
 	adev->wb.wb[index] = cpu_to_le32(tmp);
 	memset(&ib, 0, sizeof(ib));
-        r = amdgpu_ib_get(adev, NULL, 256, &ib);
+	r = amdgpu_ib_get(adev, NULL, 256, &ib);
 	if (r) {
 		DRM_ERROR("amdgpu: failed to get ib (%d).\n", r);
 		goto err0;
@@ -648,7 +648,7 @@ static int cik_sdma_ring_test_ib(struct amdgpu_ring *ring)
 	ib.ptr[3] = 1;
 	ib.ptr[4] = 0xDEADBEEF;
 	ib.length_dw = 5;
-        r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
+	r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
 	if (r)
 		goto err1;
 
@@ -1359,14 +1359,14 @@ static const struct amdgpu_vm_pte_funcs cik_sdma_vm_pte_funcs = {
 
 static void cik_sdma_set_vm_pte_funcs(struct amdgpu_device *adev)
 {
-        unsigned i;
+	unsigned i;
 
 	if (adev->vm_manager.vm_pte_funcs == NULL) {
 		adev->vm_manager.vm_pte_funcs = &cik_sdma_vm_pte_funcs;
-                for (i = 0; i < adev->sdma.num_instances; i++)
-                        adev->vm_manager.vm_pte_rings[i] =
-                                &adev->sdma.instance[i].ring;
+		for (i = 0; i < adev->sdma.num_instances; i++)
+			adev->vm_manager.vm_pte_rings[i] =
+				&adev->sdma.instance[i].ring;
  
-                adev->vm_manager.vm_pte_num_rings = adev->sdma.num_instances;
+		adev->vm_manager.vm_pte_num_rings = adev->sdma.num_instances;
 	}
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
index 356a389..0d01c0b 100644
--- a/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
@@ -1675,9 +1675,9 @@ static void gmc_v7_0_init_compute_vmid(struct amdgpu_device *adev)
 
 	/*
 	 * Configure apertures:
-	 * LDS:         0x60000000'00000000 - 0x60000001'00000000 (4GB)
-	 * Scratch:     0x60000001'00000000 - 0x60000002'00000000 (4GB)
-	 * GPUVM:       0x60010000'00000000 - 0x60020000'00000000 (1TB)
+	 * LDS:		0x60000000'00000000 - 0x60000001'00000000 (4GB)
+	 * Scratch:	0x60000001'00000000 - 0x60000002'00000000 (4GB)
+	 * GPUVM:	0x60010000'00000000 - 0x60020000'00000000 (1TB)
 	*/
 	sh_mem_bases = DEFAULT_SH_MEM_BASES | (DEFAULT_SH_MEM_BASES << 16);
 	sh_mem_config = SH_MEM_ALIGNMENT_MODE_UNALIGNED <<
@@ -2125,7 +2125,7 @@ static int gfx_v7_0_ring_test_ib(struct amdgpu_ring *ring)
 	}
 	WREG32(scratch, 0xCAFEDEAD);
 	memset(&ib, 0, sizeof(ib));
-        r = amdgpu_ib_get(adev, NULL, 256, &ib);
+	r = amdgpu_ib_get(adev, NULL, 256, &ib);
 	if (r) {
 		DRM_ERROR("amdgpu: failed to get ib (%d).\n", r);
 		goto err1;
@@ -2135,7 +2135,7 @@ static int gfx_v7_0_ring_test_ib(struct amdgpu_ring *ring)
 	ib.ptr[2] = 0xDEADBEEF;
 	ib.length_dw = 3;
 
-        r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
+	r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
 	if (r)
 		goto err2;
 
@@ -2626,7 +2626,7 @@ static int gfx_v7_0_mec_init(struct amdgpu_device *adev)
 	u32 *hpd;
 
 	/*
-	 * KV:    2 MEC, 4 Pipes/MEC, 8 Queues/Pipe - 64 Queues total
+	 * KV:	  2 MEC, 4 Pipes/MEC, 8 Queues/Pipe - 64 Queues total
 	 * CI/KB: 1 MEC, 4 Pipes/MEC, 8 Queues/Pipe - 32 Queues total
 	 * Nonetheless, we assign only 1 pipe because all other pipes will
 	 * be handled by KFD
diff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
index 3d1a5ee..92f3ee6 100644
--- a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
@@ -66,12 +66,12 @@
 #define MACRO_TILE_ASPECT(x)				((x) << GB_MACROTILE_MODE0__MACRO_TILE_ASPECT__SHIFT)
 #define NUM_BANKS(x)					((x) << GB_MACROTILE_MODE0__NUM_BANKS__SHIFT)
 
-#define RLC_CGTT_MGCG_OVERRIDE__CPF_MASK            0x00000001L
-#define RLC_CGTT_MGCG_OVERRIDE__RLC_MASK            0x00000002L
-#define RLC_CGTT_MGCG_OVERRIDE__MGCG_MASK           0x00000004L
-#define RLC_CGTT_MGCG_OVERRIDE__CGCG_MASK           0x00000008L
-#define RLC_CGTT_MGCG_OVERRIDE__CGLS_MASK           0x00000010L
-#define RLC_CGTT_MGCG_OVERRIDE__GRBM_MASK           0x00000020L
+#define RLC_CGTT_MGCG_OVERRIDE__CPF_MASK	    0x00000001L
+#define RLC_CGTT_MGCG_OVERRIDE__RLC_MASK	    0x00000002L
+#define RLC_CGTT_MGCG_OVERRIDE__MGCG_MASK	    0x00000004L
+#define RLC_CGTT_MGCG_OVERRIDE__CGCG_MASK	    0x00000008L
+#define RLC_CGTT_MGCG_OVERRIDE__CGLS_MASK	    0x00000010L
+#define RLC_CGTT_MGCG_OVERRIDE__GRBM_MASK	    0x00000020L
 
 /* BPM SERDES CMD */
 #define SET_BPM_SERDES_CMD    1
@@ -79,15 +79,15 @@
 
 /* BPM Register Address*/
 enum {
-	BPM_REG_CGLS_EN = 0,        /* Enable/Disable CGLS */
-	BPM_REG_CGLS_ON,            /* ON/OFF CGLS: shall be controlled by RLC FW */
-	BPM_REG_CGCG_OVERRIDE,      /* Set/Clear CGCG Override */
-	BPM_REG_MGCG_OVERRIDE,      /* Set/Clear MGCG Override */
-	BPM_REG_FGCG_OVERRIDE,      /* Set/Clear FGCG Override */
+	BPM_REG_CGLS_EN = 0,	    /* Enable/Disable CGLS */
+	BPM_REG_CGLS_ON,	    /* ON/OFF CGLS: shall be controlled by RLC FW */
+	BPM_REG_CGCG_OVERRIDE,	    /* Set/Clear CGCG Override */
+	BPM_REG_MGCG_OVERRIDE,	    /* Set/Clear MGCG Override */
+	BPM_REG_FGCG_OVERRIDE,	    /* Set/Clear FGCG Override */
 	BPM_REG_FGCG_MAX
 };
 
-#define RLC_FormatDirectRegListLength        14
+#define RLC_FormatDirectRegListLength	     14
 
 MODULE_FIRMWARE("amdgpu/carrizo_ce.bin");
 MODULE_FIRMWARE("amdgpu/carrizo_pfp.bin");
@@ -791,7 +791,7 @@ static int gfx_v8_0_ring_test_ib(struct amdgpu_ring *ring)
 	}
 	WREG32(scratch, 0xCAFEDEAD);
 	memset(&ib, 0, sizeof(ib));
-        r = amdgpu_ib_get(adev, NULL, 256, &ib);
+	r = amdgpu_ib_get(adev, NULL, 256, &ib);
 	if (r) {
 		DRM_ERROR("amdgpu: failed to get ib (%d).\n", r);
 		goto err1;
@@ -801,7 +801,7 @@ static int gfx_v8_0_ring_test_ib(struct amdgpu_ring *ring)
 	ib.ptr[2] = 0xDEADBEEF;
 	ib.length_dw = 3;
 
-        r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
+	r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
 	if (r)
 		goto err2;
 
@@ -1192,7 +1192,7 @@ static void cz_init_cp_jump_table(struct amdgpu_device *adev)
 				 le32_to_cpu(hdr->header.ucode_array_offset_bytes));
 			table_offset = le32_to_cpu(hdr->jt_offset);
 			table_size = le32_to_cpu(hdr->jt_size);
-		} else  if (me == 4) {
+		} else	if (me == 4) {
 			const struct gfx_firmware_header_v1_0 *hdr =
 				(const struct gfx_firmware_header_v1_0 *)adev->gfx.mec2_fw->data;
 			fw_data = (const __le32 *)
@@ -1296,7 +1296,7 @@ static int gfx_v8_0_rlc_init(struct amdgpu_device *adev)
 
 	if ((adev->asic_type == CHIP_CARRIZO) ||
 	    (adev->asic_type == CHIP_STONEY)) {
-                adev->gfx.rlc.cp_table_size = ALIGN(96 * 5 * 4, 2048) + (64 * 1024); /* JT + GDS */
+		adev->gfx.rlc.cp_table_size = ALIGN(96 * 5 * 4, 2048) + (64 * 1024); /* JT + GDS */
 		if (adev->gfx.rlc.cp_table_obj == NULL) {
 			r = amdgpu_bo_create(adev, adev->gfx.rlc.cp_table_size, PAGE_SIZE, true,
 					     AMDGPU_GEM_DOMAIN_VRAM,
@@ -1592,7 +1592,7 @@ static int gfx_v8_0_do_edc_gpr_workarounds(struct amdgpu_device *adev)
 
 	/* allocate an indirect buffer to put the commands in */
 	memset(&ib, 0, sizeof(ib));
-        r = amdgpu_ib_get(adev, NULL, total_size, &ib);
+	r = amdgpu_ib_get(adev, NULL, total_size, &ib);
 	if (r) {
 		DRM_ERROR("amdgpu: failed to get ib (%d).\n", r);
 		return r;
@@ -1687,7 +1687,7 @@ static int gfx_v8_0_do_edc_gpr_workarounds(struct amdgpu_device *adev)
 	ib.ptr[ib.length_dw++] = EVENT_TYPE(7) | EVENT_INDEX(4);
 
 	/* shedule the ib on the ring */
-        r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
+	r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
 	if (r) {
 		DRM_ERROR("amdgpu: ib submit failed (%d).\n", r);
 		goto fail;
@@ -3525,9 +3525,9 @@ static void gfx_v8_0_init_compute_vmid(struct amdgpu_device *adev)
 
 	/*
 	 * Configure apertures:
-	 * LDS:         0x60000000'00000000 - 0x60000001'00000000 (4GB)
-	 * Scratch:     0x60000001'00000000 - 0x60000002'00000000 (4GB)
-	 * GPUVM:       0x60010000'00000000 - 0x60020000'00000000 (1TB)
+	 * LDS:		0x60000000'00000000 - 0x60000001'00000000 (4GB)
+	 * Scratch:	0x60000001'00000000 - 0x60000002'00000000 (4GB)
+	 * GPUVM:	0x60010000'00000000 - 0x60020000'00000000 (1TB)
 	 */
 	sh_mem_bases = DEFAULT_SH_MEM_BASES | (DEFAULT_SH_MEM_BASES << 16);
 
@@ -3796,80 +3796,80 @@ static void gfx_v8_0_enable_save_restore_machine(struct amdgpu_device *adev)
 
 static void gfx_v8_0_init_power_gating(struct amdgpu_device *adev)
 {
-        uint32_t data;
+	uint32_t data;
  
-        if (adev->pg_flags & (AMDGPU_PG_SUPPORT_GFX_PG |
-                              AMDGPU_PG_SUPPORT_GFX_SMG |
-                              AMDGPU_PG_SUPPORT_GFX_DMG)) {
-                data = RREG32(mmCP_RB_WPTR_POLL_CNTL);
-                data &= ~CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT_MASK;
-                data |= (0x60 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
-                WREG32(mmCP_RB_WPTR_POLL_CNTL, data);
+	if (adev->pg_flags & (AMDGPU_PG_SUPPORT_GFX_PG |
+			      AMDGPU_PG_SUPPORT_GFX_SMG |
+			      AMDGPU_PG_SUPPORT_GFX_DMG)) {
+		data = RREG32(mmCP_RB_WPTR_POLL_CNTL);
+		data &= ~CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT_MASK;
+		data |= (0x60 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
+		WREG32(mmCP_RB_WPTR_POLL_CNTL, data);
  
-                data = 0;
-                data |= (0x10 << RLC_PG_DELAY__POWER_UP_DELAY__SHIFT);
-                data |= (0x10 << RLC_PG_DELAY__POWER_DOWN_DELAY__SHIFT);
-                data |= (0x10 << RLC_PG_DELAY__CMD_PROPAGATE_DELAY__SHIFT);
-                data |= (0x10 << RLC_PG_DELAY__MEM_SLEEP_DELAY__SHIFT);
-                WREG32(mmRLC_PG_DELAY, data);
+		data = 0;
+		data |= (0x10 << RLC_PG_DELAY__POWER_UP_DELAY__SHIFT);
+		data |= (0x10 << RLC_PG_DELAY__POWER_DOWN_DELAY__SHIFT);
+		data |= (0x10 << RLC_PG_DELAY__CMD_PROPAGATE_DELAY__SHIFT);
+		data |= (0x10 << RLC_PG_DELAY__MEM_SLEEP_DELAY__SHIFT);
+		WREG32(mmRLC_PG_DELAY, data);
  
-                data = RREG32(mmRLC_PG_DELAY_2);
-                data &= ~RLC_PG_DELAY_2__SERDES_CMD_DELAY_MASK;
-                data |= (0x3 << RLC_PG_DELAY_2__SERDES_CMD_DELAY__SHIFT);
-                WREG32(mmRLC_PG_DELAY_2, data);
+		data = RREG32(mmRLC_PG_DELAY_2);
+		data &= ~RLC_PG_DELAY_2__SERDES_CMD_DELAY_MASK;
+		data |= (0x3 << RLC_PG_DELAY_2__SERDES_CMD_DELAY__SHIFT);
+		WREG32(mmRLC_PG_DELAY_2, data);
  
-                data = RREG32(mmRLC_AUTO_PG_CTRL);
-                data &= ~RLC_AUTO_PG_CTRL__GRBM_REG_SAVE_GFX_IDLE_THRESHOLD_MASK;
-                data |= (0x55f0 << RLC_AUTO_PG_CTRL__GRBM_REG_SAVE_GFX_IDLE_THRESHOLD__SHIFT);
-                WREG32(mmRLC_AUTO_PG_CTRL, data);
-        }
+		data = RREG32(mmRLC_AUTO_PG_CTRL);
+		data &= ~RLC_AUTO_PG_CTRL__GRBM_REG_SAVE_GFX_IDLE_THRESHOLD_MASK;
+		data |= (0x55f0 << RLC_AUTO_PG_CTRL__GRBM_REG_SAVE_GFX_IDLE_THRESHOLD__SHIFT);
+		WREG32(mmRLC_AUTO_PG_CTRL, data);
+	}
  }
 
 static void cz_enable_sck_slow_down_on_power_up(struct amdgpu_device *adev,
-                                               bool enable)
+					       bool enable)
 {
-        u32 data, orig;
+	u32 data, orig;
  
-        orig = data = RREG32(mmRLC_PG_CNTL);
+	orig = data = RREG32(mmRLC_PG_CNTL);
  
-        if (enable)
-                data |= RLC_PG_CNTL__SMU_CLK_SLOWDOWN_ON_PU_ENABLE_MASK;
-        else
-                data &= ~RLC_PG_CNTL__SMU_CLK_SLOWDOWN_ON_PU_ENABLE_MASK;
+	if (enable)
+		data |= RLC_PG_CNTL__SMU_CLK_SLOWDOWN_ON_PU_ENABLE_MASK;
+	else
+		data &= ~RLC_PG_CNTL__SMU_CLK_SLOWDOWN_ON_PU_ENABLE_MASK;
  
-        if (orig != data)
-                WREG32(mmRLC_PG_CNTL, data);
+	if (orig != data)
+		WREG32(mmRLC_PG_CNTL, data);
 }
 
 static void cz_enable_sck_slow_down_on_power_down(struct amdgpu_device *adev,
-                                                 bool enable)
+						 bool enable)
 {
-        u32 data, orig;
+	u32 data, orig;
  
-        orig = data = RREG32(mmRLC_PG_CNTL);
+	orig = data = RREG32(mmRLC_PG_CNTL);
  
-        if (enable)
-                data |= RLC_PG_CNTL__SMU_CLK_SLOWDOWN_ON_PD_ENABLE_MASK;
-        else
-                data &= ~RLC_PG_CNTL__SMU_CLK_SLOWDOWN_ON_PD_ENABLE_MASK;
+	if (enable)
+		data |= RLC_PG_CNTL__SMU_CLK_SLOWDOWN_ON_PD_ENABLE_MASK;
+	else
+		data &= ~RLC_PG_CNTL__SMU_CLK_SLOWDOWN_ON_PD_ENABLE_MASK;
  
-        if (orig != data)
-                WREG32(mmRLC_PG_CNTL, data);
+	if (orig != data)
+		WREG32(mmRLC_PG_CNTL, data);
 }
 
 static void cz_enable_cp_power_gating(struct amdgpu_device *adev, bool enable)
 {
-        u32 data, orig;
+	u32 data, orig;
  
-        orig = data = RREG32(mmRLC_PG_CNTL);
+	orig = data = RREG32(mmRLC_PG_CNTL);
  
-        if (enable)
-                data &= ~RLC_PG_CNTL__CP_PG_DISABLE_MASK;
-        else
-                data |= RLC_PG_CNTL__CP_PG_DISABLE_MASK;
+	if (enable)
+		data &= ~RLC_PG_CNTL__CP_PG_DISABLE_MASK;
+	else
+		data |= RLC_PG_CNTL__CP_PG_DISABLE_MASK;
  
-        if (orig != data)
-                WREG32(mmRLC_PG_CNTL, data);
+	if (orig != data)
+		WREG32(mmRLC_PG_CNTL, data);
  }
 
 static void polaris11_init_power_gating(struct amdgpu_device *adev)
@@ -3877,8 +3877,8 @@ static void polaris11_init_power_gating(struct amdgpu_device *adev)
 	uint32_t data;
 
 	if (adev->pg_flags & (AMD_PG_SUPPORT_GFX_PG |
-			AMD_PG_SUPPORT_GFX_SMG |
-			AMD_PG_SUPPORT_GFX_DMG)) {
+			      AMD_PG_SUPPORT_GFX_SMG |
+			      AMD_PG_SUPPORT_GFX_DMG)) {
 		data = RREG32(mmCP_RB_WPTR_POLL_CNTL);
 		data &= ~CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT_MASK;
 		data |= (0x60 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
@@ -3915,27 +3915,27 @@ static void gfx_v8_0_init_pg(struct amdgpu_device *adev)
 		gfx_v8_0_init_save_restore_list(adev);
 		gfx_v8_0_enable_save_restore_machine(adev);
  
-                if ((adev->asic_type == CHIP_CARRIZO) ||
-                    (adev->asic_type == CHIP_STONEY)) {
-                        struct amdgpu_cu_info cu_info;
+		if ((adev->asic_type == CHIP_CARRIZO) ||
+		    (adev->asic_type == CHIP_STONEY)) {
+			struct amdgpu_cu_info cu_info;
  
-                        gfx_v8_0_get_cu_info(adev, &cu_info);
+			gfx_v8_0_get_cu_info(adev, &cu_info);
  
-                        WREG32(mmRLC_JUMP_TABLE_RESTORE, adev->gfx.rlc.cp_table_gpu_addr >> 8);
-                        gfx_v8_0_init_power_gating(adev);
-                        WREG32(mmRLC_PG_ALWAYS_ON_CU_MASK, cu_info.ao_cu_mask);
- 			if (adev->pg_flags & AMDGPU_PG_SUPPORT_RLC_SMU_HS) {
-                                cz_enable_sck_slow_down_on_power_up(adev, true);
-                                cz_enable_sck_slow_down_on_power_down(adev, true);
-                        } else {
-                                cz_enable_sck_slow_down_on_power_up(adev, false);
-                                cz_enable_sck_slow_down_on_power_down(adev, false);
-                        }
-                        if (adev->pg_flags & AMDGPU_PG_SUPPORT_CP)
-                                cz_enable_cp_power_gating(adev, true);
-                        else
-                                cz_enable_cp_power_gating(adev, false);
-                } else if (adev->asic_type == CHIP_POLARIS11) {
+			WREG32(mmRLC_JUMP_TABLE_RESTORE, adev->gfx.rlc.cp_table_gpu_addr >> 8);
+			gfx_v8_0_init_power_gating(adev);
+			WREG32(mmRLC_PG_ALWAYS_ON_CU_MASK, cu_info.ao_cu_mask);
+			if (adev->pg_flags & AMDGPU_PG_SUPPORT_RLC_SMU_HS) {
+				cz_enable_sck_slow_down_on_power_up(adev, true);
+				cz_enable_sck_slow_down_on_power_down(adev, true);
+			} else {
+				cz_enable_sck_slow_down_on_power_up(adev, false);
+				cz_enable_sck_slow_down_on_power_down(adev, false);
+			}
+			if (adev->pg_flags & AMDGPU_PG_SUPPORT_CP)
+				cz_enable_cp_power_gating(adev, true);
+			else
+				cz_enable_cp_power_gating(adev, false);
+		} else if (adev->asic_type == CHIP_POLARIS11) {
 			polaris11_init_power_gating(adev);
 		}
 	}
@@ -4387,9 +4387,9 @@ struct vi_mqd {
 	uint32_t compute_start_x;  /* ordinal5 */
 	uint32_t compute_start_y;  /* ordinal6 */
 	uint32_t compute_start_z;  /* ordinal7 */
-	uint32_t compute_num_thread_x;  /* ordinal8 */
-	uint32_t compute_num_thread_y;  /* ordinal9 */
-	uint32_t compute_num_thread_z;  /* ordinal10 */
+	uint32_t compute_num_thread_x;	/* ordinal8 */
+	uint32_t compute_num_thread_y;	/* ordinal9 */
+	uint32_t compute_num_thread_z;	/* ordinal10 */
 	uint32_t compute_pipelinestat_enable;  /* ordinal11 */
 	uint32_t compute_perfcount_enable;  /* ordinal12 */
 	uint32_t compute_pgm_lo;  /* ordinal13 */
@@ -4400,11 +4400,11 @@ struct vi_mqd {
 	uint32_t compute_tma_hi;  /* ordinal18 */
 	uint32_t compute_pgm_rsrc1;  /* ordinal19 */
 	uint32_t compute_pgm_rsrc2;  /* ordinal20 */
-	uint32_t compute_vmid;  /* ordinal21 */
+	uint32_t compute_vmid;	/* ordinal21 */
 	uint32_t compute_resource_limits;  /* ordinal22 */
 	uint32_t compute_static_thread_mgmt_se0;  /* ordinal23 */
 	uint32_t compute_static_thread_mgmt_se1;  /* ordinal24 */
-	uint32_t compute_tmpring_size;  /* ordinal25 */
+	uint32_t compute_tmpring_size;	/* ordinal25 */
 	uint32_t compute_static_thread_mgmt_se2;  /* ordinal26 */
 	uint32_t compute_static_thread_mgmt_se3;  /* ordinal27 */
 	uint32_t compute_restart_x;  /* ordinal28 */
@@ -4415,9 +4415,9 @@ struct vi_mqd {
 	uint32_t compute_dispatch_id;  /* ordinal33 */
 	uint32_t compute_threadgroup_id;  /* ordinal34 */
 	uint32_t compute_relaunch;  /* ordinal35 */
-	uint32_t compute_wave_restore_addr_lo;  /* ordinal36 */
-	uint32_t compute_wave_restore_addr_hi;  /* ordinal37 */
-	uint32_t compute_wave_restore_control;  /* ordinal38 */
+	uint32_t compute_wave_restore_addr_lo;	/* ordinal36 */
+	uint32_t compute_wave_restore_addr_hi;	/* ordinal37 */
+	uint32_t compute_wave_restore_control;	/* ordinal38 */
 	uint32_t reserved9;  /* ordinal39 */
 	uint32_t reserved10;  /* ordinal40 */
 	uint32_t reserved11;  /* ordinal41 */
@@ -4454,21 +4454,21 @@ struct vi_mqd {
 	uint32_t compute_user_data_7;  /* ordinal72 */
 	uint32_t compute_user_data_8;  /* ordinal73 */
 	uint32_t compute_user_data_9;  /* ordinal74 */
-	uint32_t compute_user_data_10;  /* ordinal75 */
-	uint32_t compute_user_data_11;  /* ordinal76 */
-	uint32_t compute_user_data_12;  /* ordinal77 */
-	uint32_t compute_user_data_13;  /* ordinal78 */
-	uint32_t compute_user_data_14;  /* ordinal79 */
-	uint32_t compute_user_data_15;  /* ordinal80 */
+	uint32_t compute_user_data_10;	/* ordinal75 */
+	uint32_t compute_user_data_11;	/* ordinal76 */
+	uint32_t compute_user_data_12;	/* ordinal77 */
+	uint32_t compute_user_data_13;	/* ordinal78 */
+	uint32_t compute_user_data_14;	/* ordinal79 */
+	uint32_t compute_user_data_15;	/* ordinal80 */
 	uint32_t cp_compute_csinvoc_count_lo;  /* ordinal81 */
 	uint32_t cp_compute_csinvoc_count_hi;  /* ordinal82 */
 	uint32_t reserved35;  /* ordinal83 */
 	uint32_t reserved36;  /* ordinal84 */
 	uint32_t reserved37;  /* ordinal85 */
-	uint32_t cp_mqd_query_time_lo;  /* ordinal86 */
-	uint32_t cp_mqd_query_time_hi;  /* ordinal87 */
-	uint32_t cp_mqd_connect_start_time_lo;  /* ordinal88 */
-	uint32_t cp_mqd_connect_start_time_hi;  /* ordinal89 */
+	uint32_t cp_mqd_query_time_lo;	/* ordinal86 */
+	uint32_t cp_mqd_query_time_hi;	/* ordinal87 */
+	uint32_t cp_mqd_connect_start_time_lo;	/* ordinal88 */
+	uint32_t cp_mqd_connect_start_time_hi;	/* ordinal89 */
 	uint32_t cp_mqd_connect_end_time_lo;  /* ordinal90 */
 	uint32_t cp_mqd_connect_end_time_hi;  /* ordinal91 */
 	uint32_t cp_mqd_connect_end_wf_count;  /* ordinal92 */
@@ -4481,8 +4481,8 @@ struct vi_mqd {
 	uint32_t cp_mqd_save_start_time_hi;  /* ordinal99 */
 	uint32_t cp_mqd_save_end_time_lo;  /* ordinal100 */
 	uint32_t cp_mqd_save_end_time_hi;  /* ordinal101 */
-	uint32_t cp_mqd_restore_start_time_lo;  /* ordinal102 */
-	uint32_t cp_mqd_restore_start_time_hi;  /* ordinal103 */
+	uint32_t cp_mqd_restore_start_time_lo;	/* ordinal102 */
+	uint32_t cp_mqd_restore_start_time_hi;	/* ordinal103 */
 	uint32_t cp_mqd_restore_end_time_lo;  /* ordinal104 */
 	uint32_t cp_mqd_restore_end_time_hi;  /* ordinal105 */
 	uint32_t reserved40;  /* ordinal106 */
@@ -4512,7 +4512,7 @@ struct vi_mqd {
 	uint32_t cp_hqd_active;  /* ordinal130 */
 	uint32_t cp_hqd_vmid;  /* ordinal131 */
 	uint32_t cp_hqd_persistent_state;  /* ordinal132 */
-	uint32_t cp_hqd_pipe_priority;  /* ordinal133 */
+	uint32_t cp_hqd_pipe_priority;	/* ordinal133 */
 	uint32_t cp_hqd_queue_priority;  /* ordinal134 */
 	uint32_t cp_hqd_quantum;  /* ordinal135 */
 	uint32_t cp_hqd_pq_base_lo;  /* ordinal136 */
@@ -4550,15 +4550,15 @@ struct vi_mqd {
 	uint32_t cp_hqd_eop_rptr;  /* ordinal168 */
 	uint32_t cp_hqd_eop_wptr;  /* ordinal169 */
 	uint32_t cp_hqd_eop_done_events;  /* ordinal170 */
-	uint32_t cp_hqd_ctx_save_base_addr_lo;  /* ordinal171 */
-	uint32_t cp_hqd_ctx_save_base_addr_hi;  /* ordinal172 */
+	uint32_t cp_hqd_ctx_save_base_addr_lo;	/* ordinal171 */
+	uint32_t cp_hqd_ctx_save_base_addr_hi;	/* ordinal172 */
 	uint32_t cp_hqd_ctx_save_control;  /* ordinal173 */
 	uint32_t cp_hqd_cntl_stack_offset;  /* ordinal174 */
 	uint32_t cp_hqd_cntl_stack_size;  /* ordinal175 */
 	uint32_t cp_hqd_wg_state_offset;  /* ordinal176 */
-	uint32_t cp_hqd_ctx_save_size;  /* ordinal177 */
+	uint32_t cp_hqd_ctx_save_size;	/* ordinal177 */
 	uint32_t cp_hqd_gds_resource_state;  /* ordinal178 */
-	uint32_t cp_hqd_error;  /* ordinal179 */
+	uint32_t cp_hqd_error;	/* ordinal179 */
 	uint32_t cp_hqd_eop_wptr_mem;  /* ordinal180 */
 	uint32_t cp_hqd_eop_dones;  /* ordinal181 */
 	uint32_t reserved46;  /* ordinal182 */
@@ -4607,7 +4607,7 @@ struct vi_mqd {
 	uint32_t reserved56;  /* ordinal225 */
 	uint32_t reserved57;  /* ordinal226 */
 	uint32_t reserved58;  /* ordinal227 */
-	uint32_t set_resources_header;  /* ordinal228 */
+	uint32_t set_resources_header;	/* ordinal228 */
 	uint32_t set_resources_dw1;  /* ordinal229 */
 	uint32_t set_resources_dw2;  /* ordinal230 */
 	uint32_t set_resources_dw3;  /* ordinal231 */
@@ -5271,124 +5271,124 @@ static int gfx_v8_0_late_init(void *handle)
 }
 
 static void gfx_v8_0_enable_gfx_static_mg_power_gating(struct amdgpu_device *adev,
-                                                       bool enable)
+						       bool enable)
 {
-         uint32_t data, temp;
+	 uint32_t data, temp;
  
-         if (enable) {
-                 /* Enable static MGPG */
-                 temp = data = RREG32(mmRLC_PG_CNTL);
-                 data |= RLC_PG_CNTL__STATIC_PER_CU_PG_ENABLE_MASK;
+	 if (enable) {
+		 /* Enable static MGPG */
+		 temp = data = RREG32(mmRLC_PG_CNTL);
+		 data |= RLC_PG_CNTL__STATIC_PER_CU_PG_ENABLE_MASK;
  
-                 if (temp != data)
-                         WREG32(mmRLC_PG_CNTL, data);
-         } else {
-                 temp = data = RREG32(mmRLC_PG_CNTL);
-                 data &= ~RLC_PG_CNTL__STATIC_PER_CU_PG_ENABLE_MASK;
+		 if (temp != data)
+			 WREG32(mmRLC_PG_CNTL, data);
+	 } else {
+		 temp = data = RREG32(mmRLC_PG_CNTL);
+		 data &= ~RLC_PG_CNTL__STATIC_PER_CU_PG_ENABLE_MASK;
  
-                 if (temp != data)
-                         WREG32(mmRLC_PG_CNTL, data);
-         }
+		 if (temp != data)
+			 WREG32(mmRLC_PG_CNTL, data);
+	 }
 }
 
 static void gfx_v8_0_enable_gfx_dynamic_mg_power_gating(struct amdgpu_device *adev,
-                                                        bool enable)
+							bool enable)
 {
-         uint32_t data, temp;
+	 uint32_t data, temp;
  
-         if (enable) {
-                 /* Enable dynamic MGPG */
-                 temp = data = RREG32(mmRLC_PG_CNTL);
-                 data |= RLC_PG_CNTL__DYN_PER_CU_PG_ENABLE_MASK;
+	 if (enable) {
+		 /* Enable dynamic MGPG */
+		 temp = data = RREG32(mmRLC_PG_CNTL);
+		 data |= RLC_PG_CNTL__DYN_PER_CU_PG_ENABLE_MASK;
  
-                 if (temp != data)
-                         WREG32(mmRLC_PG_CNTL, data);
-         } else {
-                 temp = data = RREG32(mmRLC_PG_CNTL);
-                 data &= ~RLC_PG_CNTL__DYN_PER_CU_PG_ENABLE_MASK;
+		 if (temp != data)
+			 WREG32(mmRLC_PG_CNTL, data);
+	 } else {
+		 temp = data = RREG32(mmRLC_PG_CNTL);
+		 data &= ~RLC_PG_CNTL__DYN_PER_CU_PG_ENABLE_MASK;
  
-                 if (temp != data)
-                         WREG32(mmRLC_PG_CNTL, data);
-         }
+		 if (temp != data)
+			 WREG32(mmRLC_PG_CNTL, data);
+	 }
 }
 
 static void cz_enable_gfx_cg_power_gating(struct amdgpu_device *adev,
-                                         bool enable)
+					  bool enable)
 {
-       u32 data, orig;
+	u32 data, orig;
  
-        orig = data = RREG32(mmRLC_PG_CNTL);
+	orig = data = RREG32(mmRLC_PG_CNTL);
  
-        if (enable)
-                data |= RLC_PG_CNTL__GFX_POWER_GATING_ENABLE_MASK;
-        else
-                data &= ~RLC_PG_CNTL__GFX_POWER_GATING_ENABLE_MASK;
+	if (enable)
+		data |= RLC_PG_CNTL__GFX_POWER_GATING_ENABLE_MASK;
+	else
+		data &= ~RLC_PG_CNTL__GFX_POWER_GATING_ENABLE_MASK;
  
-        if (orig != data)
-                WREG32(mmRLC_PG_CNTL, data);
+	if (orig != data)
+		WREG32(mmRLC_PG_CNTL, data);
 }
 
 static void cz_enable_gfx_pipeline_power_gating(struct amdgpu_device *adev,
-                                               bool enable)
+						bool enable)
 {
-       u32 data, orig;
+	u32 data, orig;
  
-        orig = data = RREG32(mmRLC_PG_CNTL);
+	orig = data = RREG32(mmRLC_PG_CNTL);
  
-        if (enable)
-                data |= RLC_PG_CNTL__GFX_PIPELINE_PG_ENABLE_MASK;
-        else
-                data &= ~RLC_PG_CNTL__GFX_PIPELINE_PG_ENABLE_MASK;
+	if (enable)
+		data |= RLC_PG_CNTL__GFX_PIPELINE_PG_ENABLE_MASK;
+	else
+		data &= ~RLC_PG_CNTL__GFX_PIPELINE_PG_ENABLE_MASK;
  
-        if (orig != data)
-                WREG32(mmRLC_PG_CNTL, data);
+	if (orig != data)
+		WREG32(mmRLC_PG_CNTL, data);
  
-        /* Read any GFX register to wake up GFX. */
-        if (!enable)
-                data = RREG32(mmDB_RENDER_CONTROL);
+	/* Read any GFX register to wake up GFX. */
+	if (!enable)
+		data = RREG32(mmDB_RENDER_CONTROL);
 }
 
 static void cz_update_gfx_cg_power_gating(struct amdgpu_device *adev,
-                                         bool enable)
+					  bool enable)
 {
        if ((adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_PG) && enable) {
-                cz_enable_gfx_cg_power_gating(adev, true);
-                if (adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_PIPELINE)
-                        cz_enable_gfx_pipeline_power_gating(adev, true);
-        } else {
-                cz_enable_gfx_cg_power_gating(adev, false);
-                cz_enable_gfx_pipeline_power_gating(adev, false);
-        }
+		cz_enable_gfx_cg_power_gating(adev, true);
+		if (adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_PIPELINE)
+			cz_enable_gfx_pipeline_power_gating(adev, true);
+	} else {
+		cz_enable_gfx_cg_power_gating(adev, false);
+		cz_enable_gfx_pipeline_power_gating(adev, false);
+	}
 }
 
 static int gfx_v8_0_set_powergating_state(void *handle,
 					  enum amd_powergating_state state)
 {
-        struct amdgpu_device *adev = (struct amdgpu_device *)handle;
-        bool enable = (state == AMD_PG_STATE_GATE) ? true : false;
+	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
+	bool enable = (state == AMD_PG_STATE_GATE) ? true : false;
  
-        if (!(adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_PG))
-                return 0;
+	if (!(adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_PG))
+		return 0;
  
-        switch (adev->asic_type) {
-        case CHIP_CARRIZO:
-        case CHIP_STONEY:
-                if (adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_PG)
-                        cz_update_gfx_cg_power_gating(adev, enable);
+	switch (adev->asic_type) {
+	case CHIP_CARRIZO:
+	case CHIP_STONEY:
+		if (adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_PG)
+			cz_update_gfx_cg_power_gating(adev, enable);
  
-                if ((adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_SMG) && enable)
-                        gfx_v8_0_enable_gfx_static_mg_power_gating(adev, true);
-                else
-                        gfx_v8_0_enable_gfx_static_mg_power_gating(adev, false);
+		if ((adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_SMG) && enable)
+			gfx_v8_0_enable_gfx_static_mg_power_gating(adev, true);
+		else
+			gfx_v8_0_enable_gfx_static_mg_power_gating(adev, false);
  
-                if ((adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_DMG) && enable)
-                       gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, true);
-                else
-                       gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, false);
-                break;
-        default:
-                break;
-        }
+		if ((adev->pg_flags & AMDGPU_PG_SUPPORT_GFX_DMG) && enable)
+		       gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, true);
+		else
+		       gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, false);
+		break;
+	default:
+		break;
+	}
 
 	return 0;
 }
@@ -5435,11 +5435,11 @@ static void gfx_v8_0_send_serdes_cmd(struct amdgpu_device *adev,
 }
 
 #define MSG_ENTER_RLC_SAFE_MODE     1
-#define MSG_EXIT_RLC_SAFE_MODE      0
+#define MSG_EXIT_RLC_SAFE_MODE	    0
 
-#define RLC_GPR_REG2__REQ_MASK           0x00000001
-#define RLC_GPR_REG2__MESSAGE__SHIFT     0x00000001
-#define RLC_GPR_REG2__MESSAGE_MASK       0x0000001e
+#define RLC_GPR_REG2__REQ_MASK		 0x00000001
+#define RLC_GPR_REG2__MESSAGE__SHIFT	 0x00000001
+#define RLC_GPR_REG2__MESSAGE_MASK	 0x0000001e
 
 static void cz_enter_rlc_safe_mode(struct amdgpu_device *adev)
 {
@@ -5795,13 +5795,13 @@ static int gfx_v8_0_update_gfx_clock_gating(struct amdgpu_device *adev,
 {
 	if (enable) {
 		/* CGCG/CGLS should be enabled after MGCG/MGLS/TS(CG/LS)
-		 * ===  MGCG + MGLS + TS(CG/LS) ===
+		 * ===	MGCG + MGLS + TS(CG/LS) ===
 		 */
 		gfx_v8_0_update_medium_grain_clock_gating(adev, enable);
 		gfx_v8_0_update_coarse_grain_clock_gating(adev, enable);
 	} else {
 		/* CGCG/CGLS should be disabled before MGCG/MGLS/TS(CG/LS)
-		 * ===  CGCG + CGLS ===
+		 * ===	CGCG + CGLS ===
 		 */
 		gfx_v8_0_update_coarse_grain_clock_gating(adev, enable);
 		gfx_v8_0_update_medium_grain_clock_gating(adev, enable);
@@ -5909,7 +5909,7 @@ static void gfx_v8_0_ring_emit_hdp_invalidate(struct amdgpu_ring *ring)
 }
 
 static void gfx_v8_0_ring_emit_ib_gfx(struct amdgpu_ring *ring,
-				  struct amdgpu_ib *ib, bool ctx_switch)
+				      struct amdgpu_ib *ib, bool ctx_switch)
 {
 	u32 header, control = 0;
 	u32 next_rptr = ring->wptr + 5;
@@ -5952,7 +5952,7 @@ static void gfx_v8_0_ring_emit_ib_gfx(struct amdgpu_ring *ring,
 }
 
 static void gfx_v8_0_ring_emit_ib_compute(struct amdgpu_ring *ring,
-				  struct amdgpu_ib *ib, bool ctx_switch)
+					  struct amdgpu_ib *ib, bool ctx_switch)
 {
 	u32 header, control = 0;
 	u32 next_rptr = ring->wptr + 5;
diff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c b/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c
index f462549..0b88711 100644
--- a/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c
+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c
@@ -688,7 +688,7 @@ static int sdma_v2_4_ring_test_ib(struct amdgpu_ring *ring)
 	tmp = 0xCAFEDEAD;
 	adev->wb.wb[index] = cpu_to_le32(tmp);
 	memset(&ib, 0, sizeof(ib));
-        r = amdgpu_ib_get(adev, NULL, 256, &ib);
+	r = amdgpu_ib_get(adev, NULL, 256, &ib);
 	if (r) {
 		DRM_ERROR("amdgpu: failed to get ib (%d).\n", r);
 		goto err0;
@@ -705,7 +705,7 @@ static int sdma_v2_4_ring_test_ib(struct amdgpu_ring *ring)
 	ib.ptr[7] = SDMA_PKT_HEADER_OP(SDMA_OP_NOP);
 	ib.length_dw = 8;
 
-        r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
+	r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
 	if (r)
 		goto err1;
 
@@ -1366,14 +1366,14 @@ static const struct amdgpu_vm_pte_funcs sdma_v2_4_vm_pte_funcs = {
 
 static void sdma_v2_4_set_vm_pte_funcs(struct amdgpu_device *adev)
 {
-        unsigned i;
+	unsigned i;
 
 	if (adev->vm_manager.vm_pte_funcs == NULL) {
 		adev->vm_manager.vm_pte_funcs = &sdma_v2_4_vm_pte_funcs;
-               for (i = 0; i < adev->sdma.num_instances; i++)
-                       adev->vm_manager.vm_pte_rings[i] =
-                               &adev->sdma.instance[i].ring;
+	        for (i = 0; i < adev->sdma.num_instances; i++)
+		        adev->vm_manager.vm_pte_rings[i] =
+			        &adev->sdma.instance[i].ring;
  
-                adev->vm_manager.vm_pte_num_rings = adev->sdma.num_instances;
+		adev->vm_manager.vm_pte_num_rings = adev->sdma.num_instances;
 	}
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c b/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c
index 9c30f9a..5efac1e 100644
--- a/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c
@@ -915,7 +915,7 @@ static int sdma_v3_0_ring_test_ib(struct amdgpu_ring *ring)
 	tmp = 0xCAFEDEAD;
 	adev->wb.wb[index] = cpu_to_le32(tmp);
 	memset(&ib, 0, sizeof(ib));
-        r = amdgpu_ib_get(adev, NULL, 256, &ib);
+	r = amdgpu_ib_get(adev, NULL, 256, &ib);
 	if (r) {
 		DRM_ERROR("amdgpu: failed to get ib (%d).\n", r);
 		goto err0;
@@ -932,7 +932,7 @@ static int sdma_v3_0_ring_test_ib(struct amdgpu_ring *ring)
 	ib.ptr[7] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);
 	ib.length_dw = 8;
 
-        r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
+	r = amdgpu_ib_schedule(ring, 1, &ib, NULL, NULL, &f);
 	if (r)
 		goto err1;
 
@@ -1681,14 +1681,14 @@ static const struct amdgpu_vm_pte_funcs sdma_v3_0_vm_pte_funcs = {
 
 static void sdma_v3_0_set_vm_pte_funcs(struct amdgpu_device *adev)
 {
-        unsigned i;
+	unsigned i;
 
 	if (adev->vm_manager.vm_pte_funcs == NULL) {
 		adev->vm_manager.vm_pte_funcs = &sdma_v3_0_vm_pte_funcs;
-                for (i = 0; i < adev->sdma.num_instances; i++)
-                        adev->vm_manager.vm_pte_rings[i] =
-                                &adev->sdma.instance[i].ring;
+		for (i = 0; i < adev->sdma.num_instances; i++)
+			adev->vm_manager.vm_pte_rings[i] =
+				&adev->sdma.instance[i].ring;
 
-                adev->vm_manager.vm_pte_num_rings = adev->sdma.num_instances;
+		adev->vm_manager.vm_pte_num_rings = adev->sdma.num_instances;
 	}
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c b/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
index 9b27c69..0887ea9 100644
--- a/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
@@ -415,10 +415,14 @@ static int uvd_v6_0_start(struct amdgpu_device *adev)
 	mdelay(1);
 
 	/* put LMI, VCPU, RBC etc... into reset */
-	WREG32(mmUVD_SOFT_RESET, UVD_SOFT_RESET__LMI_SOFT_RESET_MASK |
-		UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK | UVD_SOFT_RESET__LBSI_SOFT_RESET_MASK |
-		UVD_SOFT_RESET__RBC_SOFT_RESET_MASK | UVD_SOFT_RESET__CSM_SOFT_RESET_MASK |
-		UVD_SOFT_RESET__CXW_SOFT_RESET_MASK | UVD_SOFT_RESET__TAP_SOFT_RESET_MASK |
+	WREG32(mmUVD_SOFT_RESET,
+		UVD_SOFT_RESET__LMI_SOFT_RESET_MASK |
+		UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK |
+		UVD_SOFT_RESET__LBSI_SOFT_RESET_MASK |
+		UVD_SOFT_RESET__RBC_SOFT_RESET_MASK |
+		UVD_SOFT_RESET__CSM_SOFT_RESET_MASK |
+		UVD_SOFT_RESET__CXW_SOFT_RESET_MASK |
+		UVD_SOFT_RESET__TAP_SOFT_RESET_MASK |
 		UVD_SOFT_RESET__LMI_UMC_SOFT_RESET_MASK);
 	mdelay(5);
 
diff --git a/drivers/gpu/drm/amd/amdgpu/vi.c b/drivers/gpu/drm/amd/amdgpu/vi.c
index f4027d6..02ba429 100644
--- a/drivers/gpu/drm/amd/amdgpu/vi.c
+++ b/drivers/gpu/drm/amd/amdgpu/vi.c
@@ -135,8 +135,8 @@ static void vi_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)
 }
 
 /* smu_8_0_d.h */
-#define mmMP0PUB_IND_INDEX                                                      0x180
-#define mmMP0PUB_IND_DATA                                                       0x181
+#define mmMP0PUB_IND_INDEX							0x180
+#define mmMP0PUB_IND_DATA							0x181
 
 static u32 cz_smc_rreg(struct amdgpu_device *adev, u32 reg)
 {
@@ -1423,9 +1423,9 @@ int vi_set_ip_blocks(struct amdgpu_device *adev)
 	return 0;
 }
 
-#define ATI_REV_ID_FUSE_MACRO__ADDRESS      0xC0014044
-#define ATI_REV_ID_FUSE_MACRO__SHIFT        9
-#define ATI_REV_ID_FUSE_MACRO__MASK         0x00001E00
+#define ATI_REV_ID_FUSE_MACRO__ADDRESS	    0xC0014044
+#define ATI_REV_ID_FUSE_MACRO__SHIFT	    9
+#define ATI_REV_ID_FUSE_MACRO__MASK	    0x00001E00
 
 static uint32_t vi_get_rev_id(struct amdgpu_device *adev)
 {
@@ -1537,13 +1537,13 @@ static int vi_common_early_init(void *handle)
 			AMD_CG_SUPPORT_HDP_LS |
 			AMD_CG_SUPPORT_SDMA_MGCG |
 			AMD_CG_SUPPORT_SDMA_LS;
-               /* rev0 hardware doesn't support PG */
-               if (adev->rev_id != 0x00)
-                       adev->pg_flags |= AMDGPU_PG_SUPPORT_GFX_PG |
-                                         AMDGPU_PG_SUPPORT_GFX_SMG |
-                                         AMDGPU_PG_SUPPORT_GFX_DMG |
-                                         AMDGPU_PG_SUPPORT_CP |
-                                         AMDGPU_PG_SUPPORT_GFX_PIPELINE;
+	        /* rev0 hardware doesn't support PG */
+	        if (adev->rev_id != 0x00)
+		        adev->pg_flags |= AMDGPU_PG_SUPPORT_GFX_PG |
+				 AMDGPU_PG_SUPPORT_GFX_SMG |
+				 AMDGPU_PG_SUPPORT_GFX_DMG |
+				 AMDGPU_PG_SUPPORT_CP |
+				 AMDGPU_PG_SUPPORT_GFX_PIPELINE;
 
 		adev->external_rev_id = adev->rev_id + 0x1;
 		break;
diff --git a/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c b/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c
index eed16c4..7bbf6fc 100644
--- a/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c
+++ b/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c
@@ -212,8 +212,10 @@ static struct dc_sink *dm_dp_mst_add_mst_sink(
 	/* dc_sink_retain(&core_sink->public); */
 
 	return dc_sink;
+
 fail:
 	dc_link_remove_remote_sink(dc_link, dc_sink);
+
 fail_add_sink:
 	return NULL;
 }
diff --git a/drivers/gpu/drm/amd/dal/dc/core/dc.c b/drivers/gpu/drm/amd/dal/dc/core/dc.c
index e26a099..863a1bd 100644
--- a/drivers/gpu/drm/amd/dal/dc/core/dc.c
+++ b/drivers/gpu/drm/amd/dal/dc/core/dc.c
@@ -399,7 +399,7 @@ static int8_t acquire_first_free_underlay(
  ******************************************************************************/
 
 struct dc *dc_create(const struct dc_init_data *init_params)
- {
+{
 	struct dc_context ctx = {
 		.driver_context = init_params->driver,
 		.cgs_device = init_params->cgs_device
diff --git a/drivers/gpu/drm/amd/dal/dc/core/dc_resource.c b/drivers/gpu/drm/amd/dal/dc/core/dc_resource.c
index 3f62986..84ddabc 100644
--- a/drivers/gpu/drm/amd/dal/dc/core/dc_resource.c
+++ b/drivers/gpu/drm/amd/dal/dc/core/dc_resource.c
@@ -347,14 +347,13 @@ static void calculate_scaling_ratios(
 
 	if (surface->stereo_format == PLANE_STEREO_FORMAT_SIDE_BY_SIDE)
 		pipe_ctx->scl_data.ratios.horz.value *= 2;
-	else if (surface->stereo_format
-					== PLANE_STEREO_FORMAT_TOP_AND_BOTTOM)
+	else if (surface->stereo_format	== PLANE_STEREO_FORMAT_TOP_AND_BOTTOM)
 		pipe_ctx->scl_data.ratios.vert.value *= 2;
 
-	pipe_ctx->scl_data.ratios.vert.value = div64_s64(pipe_ctx->scl_data.ratios.vert.value * in_h,
-			out_h);
-	pipe_ctx->scl_data.ratios.horz.value = div64_s64(pipe_ctx->scl_data.ratios.horz.value * in_w,
-			out_w);
+	pipe_ctx->scl_data.ratios.vert.value = div64_s64(
+		pipe_ctx->scl_data.ratios.vert.value * in_h, out_h);
+	pipe_ctx->scl_data.ratios.horz.value = div64_s64(
+		pipe_ctx->scl_data.ratios.horz.value * in_w, out_w);
 
 	pipe_ctx->scl_data.ratios.horz_c = pipe_ctx->scl_data.ratios.horz;
 	pipe_ctx->scl_data.ratios.vert_c = pipe_ctx->scl_data.ratios.vert;
diff --git a/drivers/gpu/drm/amd/dal/dc/core/dc_stream.c b/drivers/gpu/drm/amd/dal/dc/core/dc_stream.c
index b696401..9a97574 100644
--- a/drivers/gpu/drm/amd/dal/dc/core/dc_stream.c
+++ b/drivers/gpu/drm/amd/dal/dc/core/dc_stream.c
@@ -148,7 +148,8 @@ void dc_stream_release(struct dc_stream *public)
 	}
 }
 
-struct dc_stream *dc_create_stream_for_sink(const struct dc_sink *dc_sink)
+struct dc_stream *dc_create_stream_for_sink(
+		const struct dc_sink *dc_sink)
 {
 	struct core_sink *sink = DC_SINK_TO_CORE(dc_sink);
 	struct stream *stream;
diff --git a/drivers/gpu/drm/amd/dal/dc/inc/core_types.h b/drivers/gpu/drm/amd/dal/dc/inc/core_types.h
index ef6ce30..91041e8 100644
--- a/drivers/gpu/drm/amd/dal/dc/inc/core_types.h
+++ b/drivers/gpu/drm/amd/dal/dc/inc/core_types.h
@@ -299,7 +299,7 @@ struct resource_context {
 	bool is_audio_acquired[MAX_PIPES];
 	uint8_t clock_source_ref_count[MAX_CLOCK_SOURCES];
 	uint8_t dp_clock_source_ref_count;
- };
+};
 
 struct target_flags {
 	bool unchanged;
diff --git a/drivers/gpu/drm/amd/dal/modules/freesync/freesync.c b/drivers/gpu/drm/amd/dal/modules/freesync/freesync.c
index 01cfeb0..002dbad 100644
--- a/drivers/gpu/drm/amd/dal/modules/freesync/freesync.c
+++ b/drivers/gpu/drm/amd/dal/modules/freesync/freesync.c
@@ -30,19 +30,19 @@
 static const MOD_FREESYNC_MAX_CONCURRENT_SINKS = 32;
 
 struct sink_caps {
-    const struct dc_sink *sink;
-    struct mod_freesync_caps caps;
+	const struct dc_sink *sink;
+	struct mod_freesync_caps caps;
 };
 
 struct core_freesync {
-    struct mod_freesync public;
-    struct dc *dc;
-    struct sink_caps *caps;
-    int num_sinks;
+	struct mod_freesync public;
+	struct dc *dc;
+	struct sink_caps *caps;
+	int num_sinks;
 };
 
 #define MOD_FREESYNC_TO_CORE(mod_freesync)\
-    container_of(mod_freesync, struct core_freesync, public)
+		container_of(mod_freesync, struct core_freesync, public)
 
 static bool check_dc_support(const struct dc *dc)
 {
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c
index 1f14c47..019356a 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c
@@ -1644,7 +1644,7 @@ static void cz_hw_print_display_cfg(
 			cc6_settings->cpu_pstate_separation_time);
 }
 
- static int cz_set_cpu_power_state(struct pp_hwmgr *hwmgr)
+static int cz_set_cpu_power_state(struct pp_hwmgr *hwmgr)
 {
 	struct cz_hwmgr *hw_data = (struct cz_hwmgr *)(hwmgr->backend);
 	uint32_t data = 0;
-- 
2.7.4

