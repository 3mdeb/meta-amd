From 3728ee0126d0223c3fe8018b4b092eecdf6af74f Mon Sep 17 00:00:00 2001
From: Kalyan Alle <kalyan.alle@amd.com>
Date: Thu, 22 Dec 2016 12:47:34 +0530
Subject: [PATCH 1479/1722] drm/amd: Code Indentation

Signed-off-by: Kalyan Alle <kalyan.alle@amd.com
---
 drivers/gpu/drm/amd/amdgpu/amdgpu.h                | 156 ++---
 drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c            |  27 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c   |  67 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c           |   4 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_cgs.c            |   0
 drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c             | 621 ++++++++---------
 drivers/gpu/drm/amd/amdgpu/amdgpu_display.c        |  15 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c            |  12 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c            |  14 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c             | 102 +--
 drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c            |   2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_job.c            |  28 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c            | 223 +++---
 drivers/gpu/drm/amd/amdgpu/amdgpu_object.h         |   4 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c           |   6 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h          |  14 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c            | 195 +++---
 drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c            |  79 +--
 drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c            |  16 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c             | 760 +++++++++++----------
 drivers/gpu/drm/amd/amdgpu/atombios_i2c.c          |  25 +-
 drivers/gpu/drm/amd/amdgpu/atombios_i2c.h          |   3 +-
 drivers/gpu/drm/amd/amdgpu/cik_sdma.c              |   8 +-
 drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c              |  23 +-
 drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c              | 237 ++++---
 drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c              |   2 +-
 drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c              |  28 +-
 drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c             |   8 +-
 drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c              |  13 +-
 drivers/gpu/drm/amd/amdgpu/vi.c                    |  42 +-
 drivers/gpu/drm/amd/dal/dc/core/dc.c               |  23 +-
 drivers/gpu/drm/amd/include/amd_shared.h           |  12 +-
 .../gpu/drm/amd/include/asic_reg/dce/dce_11_2_d.h  |   0
 .../amd/include/asic_reg/dce/dce_11_2_sh_mask.h    |   0
 drivers/gpu/drm/amd/include/cgs_common.h           |   0
 drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c     |   2 +-
 drivers/gpu/drm/amd/powerplay/smumgr/fiji_smumgr.c |   0
 drivers/gpu/drm/amd/scheduler/gpu_scheduler.c      | 112 +--
 drivers/gpu/drm/amd/scheduler/gpu_scheduler.h      |  19 +-
 39 files changed, 1460 insertions(+), 1442 deletions(-)
 mode change 100755 => 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_cgs.c
 mode change 100755 => 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
 mode change 100755 => 100644 drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_d.h
 mode change 100755 => 100644 drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_sh_mask.h
 mode change 100755 => 100644 drivers/gpu/drm/amd/include/cgs_common.h
 mode change 100755 => 100644 drivers/gpu/drm/amd/powerplay/smumgr/fiji_smumgr.c

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu.h b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
index d5d57e0..f51085e 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *	    Alex Deucher
- *	    Jerome Glisse
+ *          Alex Deucher
+ *          Jerome Glisse
  */
 #ifndef __AMDGPU_H__
 #define __AMDGPU_H__
@@ -105,7 +105,7 @@ extern char *amdgpu_virtual_display;
 extern unsigned amdgpu_pp_feature_mask;
 extern int amdgpu_vram_page_split;
 
-#define AMDGPU_WAIT_IDLE_TIMEOUT_IN_MS		3000
+#define AMDGPU_WAIT_IDLE_TIMEOUT_IN_MS	        3000
 #define AMDGPU_MAX_USEC_TIMEOUT			100000	/* 100 ms */
 #define AMDGPU_FENCE_JIFFIES_TIMEOUT		(HZ / 2)
 /* AMDGPU_IB_POOL_SIZE must be a power of 2 */
@@ -121,7 +121,7 @@ extern int amdgpu_vram_page_split;
 #define AMDGPU_VA_RESERVED_SIZE			(8 << 20)
 
 /* hard reset data */
-#define AMDGPU_ASIC_RESET_DATA			0x39d5e86b
+#define AMDGPU_ASIC_RESET_DATA                  0x39d5e86b
 
 /* reset flags */
 #define AMDGPU_RESET_GFX			(1 << 0)
@@ -371,7 +371,7 @@ struct amdgpu_bo_va_mapping {
 struct amdgpu_bo_va {
 	/* protected by bo being reserved */
 	struct list_head		bo_list;
-	struct fence			*last_pt_update;
+	struct fence		        *last_pt_update;
 	unsigned			ref_count;
 
 	/* protected by vm mutex and spinlock */
@@ -485,7 +485,7 @@ struct amdgpu_sa_bo {
 	struct amdgpu_sa_manager	*manager;
 	unsigned			soffset;
 	unsigned			eoffset;
-	struct fence			*fence;
+	struct fence		        *fence;
 };
 
 /*
@@ -564,14 +564,14 @@ struct amdgpu_mc {
 	unsigned		vram_width;
 	u64			real_vram_size;
 	int			vram_mtrr;
-	u64			gtt_base_align;
-	u64			mc_mask;
-	const struct firmware	*fw;	/* MC firmware */
-	uint32_t		fw_version;
+	u64                     gtt_base_align;
+	u64                     mc_mask;
+	const struct firmware   *fw;	/* MC firmware */
+	uint32_t                fw_version;
 	struct amdgpu_irq_src	vm_fault;
 	uint32_t		vram_type;
-        uint32_t                srbm_soft_reset;
-        struct amdgpu_mode_mc_save save;
+	uint32_t                srbm_soft_reset;
+	struct amdgpu_mode_mc_save save;
 };
 
 /*
@@ -579,23 +579,23 @@ struct amdgpu_mc {
  */
 typedef enum _AMDGPU_DOORBELL_ASSIGNMENT
 {
-	AMDGPU_DOORBELL_KIQ			= 0x000,
-	AMDGPU_DOORBELL_HIQ			= 0x001,
-	AMDGPU_DOORBELL_DIQ			= 0x002,
-	AMDGPU_DOORBELL_MEC_RING0		= 0x010,
-	AMDGPU_DOORBELL_MEC_RING1		= 0x011,
-	AMDGPU_DOORBELL_MEC_RING2		= 0x012,
-	AMDGPU_DOORBELL_MEC_RING3		= 0x013,
-	AMDGPU_DOORBELL_MEC_RING4		= 0x014,
-	AMDGPU_DOORBELL_MEC_RING5		= 0x015,
-	AMDGPU_DOORBELL_MEC_RING6		= 0x016,
-	AMDGPU_DOORBELL_MEC_RING7		= 0x017,
-	AMDGPU_DOORBELL_GFX_RING0		= 0x020,
-	AMDGPU_DOORBELL_sDMA_ENGINE0		= 0x1E0,
-	AMDGPU_DOORBELL_sDMA_ENGINE1		= 0x1E1,
-	AMDGPU_DOORBELL_IH			= 0x1E8,
-	AMDGPU_DOORBELL_MAX_ASSIGNMENT		= 0x3FF,
-	AMDGPU_DOORBELL_INVALID			= 0xFFFF
+	AMDGPU_DOORBELL_KIQ                     = 0x000,
+	AMDGPU_DOORBELL_HIQ                     = 0x001,
+	AMDGPU_DOORBELL_DIQ                     = 0x002,
+	AMDGPU_DOORBELL_MEC_RING0               = 0x010,
+	AMDGPU_DOORBELL_MEC_RING1               = 0x011,
+	AMDGPU_DOORBELL_MEC_RING2               = 0x012,
+	AMDGPU_DOORBELL_MEC_RING3               = 0x013,
+	AMDGPU_DOORBELL_MEC_RING4               = 0x014,
+	AMDGPU_DOORBELL_MEC_RING5               = 0x015,
+	AMDGPU_DOORBELL_MEC_RING6               = 0x016,
+	AMDGPU_DOORBELL_MEC_RING7               = 0x017,
+	AMDGPU_DOORBELL_GFX_RING0               = 0x020,
+	AMDGPU_DOORBELL_sDMA_ENGINE0            = 0x1E0,
+	AMDGPU_DOORBELL_sDMA_ENGINE1            = 0x1E1,
+	AMDGPU_DOORBELL_IH                      = 0x1E8,
+	AMDGPU_DOORBELL_MAX_ASSIGNMENT          = 0x3FF,
+	AMDGPU_DOORBELL_INVALID                 = 0xFFFF
 } AMDGPU_DOORBELL_ASSIGNMENT;
 
 struct amdgpu_doorbell {
@@ -644,6 +644,7 @@ struct amdgpu_ib {
 };
 
 extern const struct amd_sched_backend_ops amdgpu_sched_ops;
+
 int amdgpu_job_alloc(struct amdgpu_device *adev, unsigned num_ibs,
 		     struct amdgpu_job **job, struct amdgpu_vm *vm);
 int amdgpu_job_alloc_with_ib(struct amdgpu_device *adev, unsigned size,
@@ -669,12 +670,12 @@ struct amdgpu_ctx_ring {
 
 struct amdgpu_ctx {
 	struct kref		refcount;
-	struct amdgpu_device	*adev;
+	struct amdgpu_device    *adev;
 	unsigned		reset_counter;
 	spinlock_t		ring_lock;
-	struct fence		**fences;
+	struct fence            **fences;
 	struct amdgpu_ctx_ring	rings[AMDGPU_MAX_RINGS];
-        bool preamble_presented;
+	bool preamble_presented;
 };
 
 struct amdgpu_ctx_mgr {
@@ -745,19 +746,19 @@ struct amdgpu_rlc {
 	struct amdgpu_bo	*save_restore_obj;
 	uint64_t		save_restore_gpu_addr;
 	volatile uint32_t	*sr_ptr;
-	const u32		*reg_list;
-	u32			reg_list_size;
+	const u32               *reg_list;
+	u32                     reg_list_size;
 	/* for clear state */
 	struct amdgpu_bo	*clear_state_obj;
 	uint64_t		clear_state_gpu_addr;
 	volatile uint32_t	*cs_ptr;
 	const struct cs_section_def   *cs_data;
-	u32			clear_state_size;
+	u32                     clear_state_size;
 	/* for cp tables */
 	struct amdgpu_bo	*cp_table_obj;
 	uint64_t		cp_table_gpu_addr;
 	volatile uint32_t	*cp_table_ptr;
-	u32			cp_table_size;
+	u32                     cp_table_size;
 
 	/* safe mode for updating CG/PG state */
 	bool in_safe_mode;
@@ -791,7 +792,7 @@ struct amdgpu_mec {
  */
 struct amdgpu_scratch {
 	unsigned		num_reg;
-	uint32_t		reg_base;
+	uint32_t                reg_base;
 	bool			free[32];
 	uint32_t		reg[32];
 };
@@ -837,6 +838,7 @@ struct amdgpu_gca_config {
 
 	uint32_t tile_mode_array[32];
 	uint32_t macrotile_mode_array[16];
+
 	struct amdgpu_rb_config rb_config[AMDGPU_GFX_MAX_SE][AMDGPU_GFX_MAX_SH_PER_SE];
 };
 
@@ -847,10 +849,10 @@ struct amdgpu_cu_info {
 };
 
 struct amdgpu_gfx_funcs {
-        /* get the gpu clock counter */
-        uint64_t (*get_gpu_clock_counter)(struct amdgpu_device *adev);
-        void (*select_se_sh)(struct amdgpu_device *adev, u32 se_num, u32 sh_num, u32 instance);
-        void (*read_wave_data)(struct amdgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t *dst, int *no_fields);
+	/* get the gpu clock counter */
+	uint64_t (*get_gpu_clock_counter)(struct amdgpu_device *adev);
+	void (*select_se_sh)(struct amdgpu_device *adev, u32 se_num, u32 sh_num, u32 instance);
+	void (*read_wave_data)(struct amdgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t *dst, int *no_fields);
 };
 
 struct amdgpu_gfx {
@@ -888,12 +890,12 @@ struct amdgpu_gfx {
 	uint32_t			gfx_current_status;
 	/* ce ram size*/
 	unsigned			ce_ram_size;
-        struct amdgpu_cu_info           cu_info;
-        const struct amdgpu_gfx_funcs   *funcs;
- 
-        /* reset mask */
-        uint32_t                        grbm_soft_reset;
-        uint32_t                        srbm_soft_reset;
+	struct amdgpu_cu_info		cu_info;
+	const struct amdgpu_gfx_funcs	*funcs;
+
+	/* reset mask */
+	uint32_t                        grbm_soft_reset;
+	uint32_t                        srbm_soft_reset;
 };
 
 int amdgpu_ib_get(struct amdgpu_device *adev, struct amdgpu_vm *vm,
@@ -920,7 +922,7 @@ struct amdgpu_cs_parser {
 	struct amdgpu_device	*adev;
 	struct drm_file		*filp;
 	struct amdgpu_ctx	*ctx;
-	
+
 	/* chunks */
 	unsigned		nchunks;
 	struct amdgpu_cs_chunk	*chunks;
@@ -947,27 +949,27 @@ struct amdgpu_cs_parser {
 #define AMDGPU_HAVE_CTX_SWITCH              (1 << 2) /* bit set means context switch occured */
 
 struct amdgpu_job {
-	struct amd_sched_job	base;
+	struct amd_sched_job    base;
 	struct amdgpu_device	*adev;
 	struct amdgpu_vm	*vm;
 	struct amdgpu_ring	*ring;
 	struct amdgpu_sync	sync;
 	struct amdgpu_ib	*ibs;
 	struct fence		*fence; /* the hw fence */
-        uint32_t                preamble_status;
+	uint32_t		preamble_status;
 	uint32_t		num_ibs;
 	void			*owner;
-        uint64_t                fence_ctx; /* the fence_context this job uses */
-        bool                    vm_needs_flush;
-        unsigned                vm_id;
-        uint64_t                vm_pd_addr;
-        uint32_t                gds_base, gds_size;
-        uint32_t                gws_base, gws_size;
-        uint32_t                oa_base, oa_size;
+	uint64_t		fence_ctx; /* the fence_context this job uses */
+	bool                    vm_needs_flush;
+	unsigned		vm_id;
+	uint64_t		vm_pd_addr;
+	uint32_t		gds_base, gds_size;
+	uint32_t		gws_base, gws_size;
+	uint32_t		oa_base, oa_size;
 	uint64_t		fence_context;
-        /* user fence handling */
-        uint64_t                uf_addr;
-        uint64_t                uf_sequence;
+	/* user fence handling */
+	uint64_t		uf_addr;
+	uint64_t		uf_sequence;
 
 };
 #define to_amdgpu_job(sched_job)		\
@@ -1007,12 +1009,12 @@ void amdgpu_get_pcie_info(struct amdgpu_device *adev);
 /*
  * UVD
  */
-#define AMDGPU_DEFAULT_UVD_HANDLES     10
-#define AMDGPU_MAX_UVD_HANDLES	       40
-#define AMDGPU_UVD_STACK_SIZE	       (200*1024)
-#define AMDGPU_UVD_HEAP_SIZE	       (256*1024)
-#define AMDGPU_UVD_SESSION_SIZE		       (50*1024)
-#define AMDGPU_UVD_FIRMWARE_OFFSET     256
+#define AMDGPU_DEFAULT_UVD_HANDLES	10
+#define AMDGPU_MAX_UVD_HANDLES		40
+#define AMDGPU_UVD_STACK_SIZE		(200*1024)
+#define AMDGPU_UVD_HEAP_SIZE		(256*1024)
+#define AMDGPU_UVD_SESSION_SIZE		(50*1024)
+#define AMDGPU_UVD_FIRMWARE_OFFSET	256
 
 struct amdgpu_uvd {
 	struct amdgpu_bo	*vcpu_bo;
@@ -1336,7 +1338,7 @@ struct amdgpu_device {
 	struct amdgpu_atcs		atcs;
 	struct mutex			srbm_mutex;
 	/* GRBM index mutex. Protects concurrent access to GRBM index */
-	struct mutex			grbm_idx_mutex;
+	struct mutex                    grbm_idx_mutex;
 	struct dev_pm_domain		vga_pm_domain;
 	bool				have_disp_power_ref;
 
@@ -1378,12 +1380,12 @@ struct amdgpu_device {
 	spinlock_t audio_endpt_idx_lock;
 	amdgpu_block_rreg_t		audio_endpt_rreg;
 	amdgpu_block_wreg_t		audio_endpt_wreg;
-	void __iomem			*rio_mem;
+	void __iomem                    *rio_mem;
 	resource_size_t			rio_mem_size;
 	struct amdgpu_doorbell		doorbell;
 
 	/* clock/pll info */
-	struct amdgpu_clock	       clock;
+	struct amdgpu_clock            clock;
 
 	/* MC */
 	struct amdgpu_mc		mc;
@@ -1474,16 +1476,16 @@ struct amdgpu_device {
 	u64 gart_pin_size;
 
 	/* amdkfd interface */
-	struct kfd_dev		*kfd;
+	struct kfd_dev          *kfd;
 
 	struct amdgpu_virtualization virtualization;
- 
-        /* link all shadow bo */
-        struct list_head                shadow_list;
-        struct mutex                    shadow_list_lock;
-        /* link all gtt */
-        spinlock_t                      gtt_list_lock;
-        struct list_head                gtt_list;
+
+	/* link all shadow bo */
+	struct list_head                shadow_list;
+	struct mutex                    shadow_list_lock;
+	/* link all gtt */
+	spinlock_t			gtt_list_lock;
+	struct list_head                gtt_list;
 
 };
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c
index b250278..11c8303 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c
@@ -35,17 +35,17 @@
 
 #include "acp_gfx_if.h"
 
-#define ACP_TILE_ON_MASK		0x03
-#define ACP_TILE_OFF_MASK		0x02
-#define ACP_TILE_ON_RETAIN_REG_MASK	0x1f
-#define ACP_TILE_OFF_RETAIN_REG_MASK	0x20
+#define ACP_TILE_ON_MASK                0x03
+#define ACP_TILE_OFF_MASK               0x02
+#define ACP_TILE_ON_RETAIN_REG_MASK     0x1f
+#define ACP_TILE_OFF_RETAIN_REG_MASK    0x20
 
-#define ACP_TILE_P1_MASK		0x3e
-#define ACP_TILE_P2_MASK		0x3d
-#define ACP_TILE_DSP0_MASK		0x3b
-#define ACP_TILE_DSP1_MASK		0x37
+#define ACP_TILE_P1_MASK                0x3e
+#define ACP_TILE_P2_MASK                0x3d
+#define ACP_TILE_DSP0_MASK              0x3b
+#define ACP_TILE_DSP1_MASK              0x37
 
-#define ACP_TILE_DSP2_MASK		0x2f
+#define ACP_TILE_DSP2_MASK              0x2f
 
 #define ACP_DMA_REGS_END		0x146c0
 #define ACP_I2S_PLAY_REGS_START		0x14840
@@ -397,12 +397,11 @@ static int acp_hw_fini(void *handle)
 {
 	int i, ret;
 	struct device *dev;
-
 	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
- 
-        /* return early if no ACP */
-        if (!adev->acp.acp_genpd)
-                return 0;
+
+	/* return early if no ACP */
+	if (!adev->acp.acp_genpd)
+		return 0;
 
 	for (i = 0; i < ACP_DEVS ; i++) {
 		dev = get_mfd_cell_dev(adev->acp.acp_cell[i].name, i);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
index 172ee13..d42a976 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
@@ -65,6 +65,7 @@ struct atpx_mux {
 bool amdgpu_has_atpx(void) {
 	return amdgpu_atpx_priv.atpx_detected;
 }
+
 bool amdgpu_has_atpx_dgpu_power_cntl(void) {
 	return amdgpu_atpx_priv.atpx.functions.power_cntl;
 }
@@ -156,7 +157,7 @@ static void amdgpu_atpx_parse_functions(struct amdgpu_atpx_functions *f, u32 mas
  */
 static int amdgpu_atpx_validate(struct amdgpu_atpx *atpx)
 {
-        u32 valid_bits = 0;
+	u32 valid_bits = 0;
 
 	if (atpx->functions.px_params) {
 		union acpi_object *info;
@@ -183,43 +184,43 @@ static int amdgpu_atpx_validate(struct amdgpu_atpx *atpx)
 
 		kfree(info);
 	}
- 
-        /* if separate mux flag is set, mux controls are required */
-        if (valid_bits & ATPX_SEPARATE_MUX_FOR_I2C) {
-                atpx->functions.i2c_mux_cntl = true;
-                atpx->functions.disp_mux_cntl = true;
-        }
-        /* if any outputs are muxed, mux controls are required */
-        if (valid_bits & (ATPX_CRT1_RGB_SIGNAL_MUXED |
-                          ATPX_TV_SIGNAL_MUXED |
-                          ATPX_DFP_SIGNAL_MUXED))
-                atpx->functions.disp_mux_cntl = true;
- 
- 
-        /* some bioses set these bits rather than flagging power_cntl as supported */
-        if (valid_bits & (ATPX_DYNAMIC_PX_SUPPORTED |
-                          ATPX_DYNAMIC_DGPU_POWER_OFF_SUPPORTED))
-                atpx->functions.power_cntl = true;
-	
-	atpx->is_hybrid = false; 
-        if (valid_bits & ATPX_MS_HYBRID_GFX_SUPPORTED) {
+
+	/* if separate mux flag is set, mux controls are required */
+	if (valid_bits & ATPX_SEPARATE_MUX_FOR_I2C) {
+		atpx->functions.i2c_mux_cntl = true;
+		atpx->functions.disp_mux_cntl = true;
+	}
+	/* if any outputs are muxed, mux controls are required */
+	if (valid_bits & (ATPX_CRT1_RGB_SIGNAL_MUXED |
+			  ATPX_TV_SIGNAL_MUXED |
+			  ATPX_DFP_SIGNAL_MUXED))
+		atpx->functions.disp_mux_cntl = true;
+
+
+	/* some bioses set these bits rather than flagging power_cntl as supported */
+	if (valid_bits & (ATPX_DYNAMIC_PX_SUPPORTED |
+			  ATPX_DYNAMIC_DGPU_POWER_OFF_SUPPORTED))
+		atpx->functions.power_cntl = true;
+
+	atpx->is_hybrid = false;
+	if (valid_bits & ATPX_MS_HYBRID_GFX_SUPPORTED) {
                 printk("Hybrid Graphics, ATPX dGPU power cntl disabled\n");
 #if 1
-                /* This is a temporary hack until the D3 cold support
-                 * makes it upstream.  The ATPX power_control method seems
-                 * to still work on even if the system should be using
-                 * the new standardized hybrid D3 cold ACPI interface.
-                 */
-                atpx->functions.power_cntl = true;
+		/* This is a temporary hack until the D3 cold support
+		 * makes it upstream.  The ATPX power_control method seems
+		 * to still work on even if the system should be using
+		 * the new standardized hybrid D3 cold ACPI interface.
+		 */
+		atpx->functions.power_cntl = true;
 #else
-                atpx->functions.power_cntl = false;
+		atpx->functions.power_cntl = false;
 #endif
 		atpx->is_hybrid = true;
-        }
- 
-        atpx->dgpu_req_power_for_displays = false;
-        if (valid_bits & ATPX_DGPU_REQ_POWER_FOR_DISPLAYS)
-                atpx->dgpu_req_power_for_displays = true;
+	}
+
+	atpx->dgpu_req_power_for_displays = false;
+	if (valid_bits & ATPX_DGPU_REQ_POWER_FOR_DISPLAYS)
+		atpx->dgpu_req_power_for_displays = true;
 
 	return 0;
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c
index 5c4d2bd..b7e2762 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *	    Alex Deucher
- *	    Jerome Glisse
+ *          Alex Deucher
+ *          Jerome Glisse
  */
 #include <drm/drmP.h>
 #include "amdgpu.h"
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cgs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cgs.c
old mode 100755
new mode 100644
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
index 44a05ef..64d2d33 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
@@ -184,7 +184,7 @@ int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p, void *data)
 		ret = -EINVAL;
 		goto free_chunk;
 	}
- 
+
 	/* get chunks */
 	chunk_array_user = (uint64_t __user *)(unsigned long)(cs->in.chunks);
 	if (copy_from_user(chunk_array, chunk_array_user,
@@ -285,109 +285,112 @@ free_chunk:
 /* Convert microseconds to bytes. */
 static u64 us_to_bytes(struct amdgpu_device *adev, s64 us)
 {
-        if (us <= 0 || !adev->mm_stats.log2_max_MBps)
-                 return 0;
-  
-        /* Since accum_us is incremented by a million per second, just
-         * multiply it by the number of MB/s to get the number of bytes.
-         */
-        return us << adev->mm_stats.log2_max_MBps;
+	if (us <= 0 || !adev->mm_stats.log2_max_MBps)
+		return 0;
+
+	/* Since accum_us is incremented by a million per second, just
+	 * multiply it by the number of MB/s to get the number of bytes.
+	 */
+	return us << adev->mm_stats.log2_max_MBps;
 }
 
 static s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)
 {
-        if (!adev->mm_stats.log2_max_MBps)
-                return 0;
- 
-        return bytes >> adev->mm_stats.log2_max_MBps;
+	if (!adev->mm_stats.log2_max_MBps)
+		return 0;
+
+	return bytes >> adev->mm_stats.log2_max_MBps;
 }
- 
- /* Returns how many bytes TTM can move right now. If no bytes can be moved,
-  * it returns 0. If it returns non-zero, it's OK to move at least one buffer,
-  * which means it can go over the threshold once. If that happens, the driver
-  * will be in debt and no other buffer migrations can be done until that debt
-  * is repaid.
-  *
-  * This approach allows moving a buffer of any size (it's important to allow
-  * that).
-  *
-  * The currency is simply time in microseconds and it increases as the clock
-  * ticks. The accumulated microseconds (us) are converted to bytes and
-  * returned.
-*/
+
+/* Returns how many bytes TTM can move right now. If no bytes can be moved,
+ * it returns 0. If it returns non-zero, it's OK to move at least one buffer,
+ * which means it can go over the threshold once. If that happens, the driver
+ * will be in debt and no other buffer migrations can be done until that debt
+ * is repaid.
+ *
+ * This approach allows moving a buffer of any size (it's important to allow
+ * that).
+ *
+ * The currency is simply time in microseconds and it increases as the clock
+ * ticks. The accumulated microseconds (us) are converted to bytes and
+ * returned.
+ */
 static u64 amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev)
 {
-        s64 time_us, increment_us;
-        u64 max_bytes;
-        u64 free_vram, total_vram, used_vram;
- 
-        /* Allow a maximum of 200 accumulated ms. This is basically per-IB
-         * throttling.	
+	s64 time_us, increment_us;
+	u64 max_bytes;
+	u64 free_vram, total_vram, used_vram;
+
+	/* Allow a maximum of 200 accumulated ms. This is basically per-IB
+	 * throttling.
 	 *
-         * It means that in order to get full max MBps, at least 5 IBs per
-         * second must be submitted and not more than 200ms apart from each
-         * other.
-         */
-        const s64 us_upper_bound = 200000;
-
-        if(!adev->mm_stats.log2_max_MBps)
-                return 0;
- 
-        total_vram = adev->mc.real_vram_size - adev->vram_pin_size;
-        used_vram = atomic64_read(&adev->vram_usage);
-        free_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;
- 
-        spin_lock(&adev->mm_stats.lock);
- 
-        /* Increase the amount of accumulated us. */
-        time_us = ktime_to_us(ktime_get());
-        increment_us = time_us - adev->mm_stats.last_update_us;
-        adev->mm_stats.last_update_us = time_us;
-        adev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,
-                                       us_upper_bound);
- 
-        /* This prevents the short period of low performance when the VRAM
-         * usage is low and the driver is in debt or doesn't have enough
-         * accumulated us to fill VRAM quickly.
-         * The situation can occur in these cases:
-         * - a lot of VRAM is freed by userspace
-         * - the presence of a big buffer causes a lot of evictions
-         *   (solution: split buffers into smaller ones)
-         * If 128 MB or 1/8th of VRAM is free, start filling it now by setting
-         * accum_us to a positive number.
-         */
-        if (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {
-                s64 min_us;
- 
-                /* Be more aggresive on dGPUs. Try to fill a portion of free
-                 * VRAM now.
-                 */
-                if (!(adev->flags & AMD_IS_APU))
-                        min_us = bytes_to_us(adev, free_vram / 4);
-                else
-                        min_us = 0; /* Reset accum_us on APUs. */
- 
-                adev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);
-         }
-        /* This returns 0 if the driver is in debt to disallow (optional)
-         * buffer moves.
-         */
-        max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);
- 
-        spin_unlock(&adev->mm_stats.lock);
-        return max_bytes;
+	 * It means that in order to get full max MBps, at least 5 IBs per
+	 * second must be submitted and not more than 200ms apart from each
+	 * other.
+	 */
+	const s64 us_upper_bound = 200000;
+
+	if (!adev->mm_stats.log2_max_MBps)
+		return 0;
+
+	total_vram = adev->mc.real_vram_size - adev->vram_pin_size;
+	used_vram = atomic64_read(&adev->vram_usage);
+	free_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;
+
+	spin_lock(&adev->mm_stats.lock);
+
+	/* Increase the amount of accumulated us. */
+	time_us = ktime_to_us(ktime_get());
+	increment_us = time_us - adev->mm_stats.last_update_us;
+	adev->mm_stats.last_update_us = time_us;
+	adev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,
+                                      us_upper_bound);
+
+	/* This prevents the short period of low performance when the VRAM
+	 * usage is low and the driver is in debt or doesn't have enough
+	 * accumulated us to fill VRAM quickly.
+	 *
+	 * The situation can occur in these cases:
+	 * - a lot of VRAM is freed by userspace
+	 * - the presence of a big buffer causes a lot of evictions
+	 *   (solution: split buffers into smaller ones)
+	 *
+	 * If 128 MB or 1/8th of VRAM is free, start filling it now by setting
+	 * accum_us to a positive number.
+	 */
+	if (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {
+		s64 min_us;
+
+		/* Be more aggresive on dGPUs. Try to fill a portion of free
+		 * VRAM now.
+		 */
+		if (!(adev->flags & AMD_IS_APU))
+			min_us = bytes_to_us(adev, free_vram / 4);
+		else
+			min_us = 0; /* Reset accum_us on APUs. */
+
+		adev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);
+	}
+
+	/* This returns 0 if the driver is in debt to disallow (optional)
+	 * buffer moves.
+	 */
+	max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);
+
+	spin_unlock(&adev->mm_stats.lock);
+	return max_bytes;
 }
- 
- /* Report how many bytes have really been moved for the last command
-  * submission. This can result in a debt that can stop buffer migrations
-  * temporarily.
-  */
+
+/* Report how many bytes have really been moved for the last command
+ * submission. This can result in a debt that can stop buffer migrations
+ * temporarily.
+ */
 static void amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev,
-                                         u64 num_bytes)
+					 u64 num_bytes)
 {
-        spin_lock(&adev->mm_stats.lock);
-        adev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);
-        spin_unlock(&adev->mm_stats.lock);
+	spin_lock(&adev->mm_stats.lock);
+	adev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);
+	spin_unlock(&adev->mm_stats.lock);
 }
 
 static int amdgpu_cs_bo_validate(struct amdgpu_cs_parser *p,
@@ -517,12 +520,12 @@ static int amdgpu_cs_list_validate(struct amdgpu_cs_parser *p,
 			memcpy(bo->tbo.ttm->pages, lobj->user_pages, size);
 			binding_userptr = true;
 		}
-                
+
 		if (p->evictable == lobj)
-                        p->evictable = NULL;
- 
-                r = amdgpu_cs_validate(p, bo);
-                if (r)
+			p->evictable = NULL;
+
+		r = amdgpu_cs_validate(p, bo);
+		if (r)
 			return r;
 
 		if (binding_userptr) {
@@ -534,206 +537,206 @@ static int amdgpu_cs_list_validate(struct amdgpu_cs_parser *p,
 }
 
 static int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,
-                                union drm_amdgpu_cs *cs)
+				union drm_amdgpu_cs *cs)
 {
-        struct amdgpu_fpriv *fpriv = p->filp->driver_priv;
-        struct amdgpu_bo_list_entry *e;
-        struct list_head duplicates;
-        bool need_mmap_lock = false;
-        unsigned i, tries = 10;
-        int r;
-
-        INIT_LIST_HEAD(&p->validated);
-
-        p->bo_list = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle);
-        if (p->bo_list) {
-                need_mmap_lock = p->bo_list->first_userptr !=
-                        p->bo_list->num_entries;
-                amdgpu_bo_list_get_list(p->bo_list, &p->validated);
-        }
-
-        INIT_LIST_HEAD(&duplicates);
-        amdgpu_vm_get_pd_bo(&fpriv->vm, &p->validated, &p->vm_pd);
-
-        if (p->uf_entry.robj)
-                list_add(&p->uf_entry.tv.head, &p->validated);
-
-        if (need_mmap_lock)
-                down_read(&current->mm->mmap_sem);
-
-        while (1) {
-                struct list_head need_pages;
-                unsigned i;
-
-                r = ttm_eu_reserve_buffers(&p->ticket, &p->validated, true,
-                                           &duplicates);
-                if (unlikely(r != 0)) {
-                       if (r != -ERESTARTSYS)
-                               DRM_ERROR("ttm_eu_reserve_buffers failed.\n");
-                        goto error_free_pages;
-                }
-
-                /* Without a BO list we don't have userptr BOs */
-                if (!p->bo_list)
-                        break;
-
-                INIT_LIST_HEAD(&need_pages);
-                for (i = p->bo_list->first_userptr;
-                     i < p->bo_list->num_entries; ++i) {
-
-                        e = &p->bo_list->array[i];
-
-                        if (amdgpu_ttm_tt_userptr_invalidated(e->robj->tbo.ttm,
-                                 &e->user_invalidated) && e->user_pages) {
-
-                                /* We acquired a page array, but somebody
-                                 * invalidated it. Free it an try again
-                                 */
-                                release_pages(e->user_pages,
-                                              e->robj->tbo.ttm->num_pages,
-                                              false);
-                                drm_free_large(e->user_pages);
-                                e->user_pages = NULL;
-                        }
-
-                        if (e->robj->tbo.ttm->state != tt_bound &&
-                            !e->user_pages) {
-                                list_del(&e->tv.head);
-                                list_add(&e->tv.head, &need_pages);
-
-                                amdgpu_bo_unreserve(e->robj);
-                        }
-                }
-
-                if (list_empty(&need_pages))
-                        break;
-
-                /* Unreserve everything again. */
-                ttm_eu_backoff_reservation(&p->ticket, &p->validated);
-
-                /* We tried too many times, just abort */
-                if (!--tries) {
-                        r = -EDEADLK;
-                        DRM_ERROR("deadlock in %s\n", __func__);
-                        goto error_free_pages;
-                }
-
-                /* Fill the page arrays for all useptrs. */
-                list_for_each_entry(e, &need_pages, tv.head) {
-                        struct ttm_tt *ttm = e->robj->tbo.ttm;
-
-                        e->user_pages = drm_calloc_large(ttm->num_pages,
-                                                         sizeof(struct page*));
-                        if (!e->user_pages) {
-                                r = -ENOMEM;
-                                DRM_ERROR("calloc failure in %s\n", __func__);
-                                goto error_free_pages;
-                        }
-
-                        r = amdgpu_ttm_tt_get_user_pages(ttm, e->user_pages);
-                        if (r) {
-                                DRM_ERROR("amdgpu_ttm_tt_get_user_pages failed.\n");
-                                drm_free_large(e->user_pages);
-                                e->user_pages = NULL;
-                                goto error_free_pages;
-                        }
-                }
-
-                /* And try again. */
-                list_splice(&need_pages, &p->validated);
-        }
-
-        p->bytes_moved_threshold = amdgpu_cs_get_threshold_for_moves(p->adev);
-        p->bytes_moved = 0;
-        p->evictable = list_last_entry(&p->validated,
-                                       struct amdgpu_bo_list_entry,
-                                       tv.head);
-        r = amdgpu_vm_validate_pt_bos(p->adev, &fpriv->vm,
-                                      amdgpu_cs_validate, p);
- 
-        if (r) {
-                DRM_ERROR("amdgpu_vm_validate_pt_bos() failed.\n");
-                goto error_validate;
-        }
-
-        r = amdgpu_cs_list_validate(p, &duplicates);
-        if (r) {
-                DRM_ERROR("amdgpu_cs_list_validate(duplicates) failed.\n");
-                goto error_validate;
-        }
-
-        r = amdgpu_cs_list_validate(p, &p->validated);
-        if (r) {
-                DRM_ERROR("amdgpu_cs_list_validate(validated) failed.\n");
-                goto error_validate;
-        }
-
-        amdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved);
-
-        fpriv->vm.last_eviction_counter =
-                atomic64_read(&p->adev->num_evictions);
-
-        if (p->bo_list) {
-                struct amdgpu_bo *gds = p->bo_list->gds_obj;
-                struct amdgpu_bo *gws = p->bo_list->gws_obj;
-                struct amdgpu_bo *oa = p->bo_list->oa_obj;
-                struct amdgpu_vm *vm = &fpriv->vm;
-                unsigned i;
-
-                for (i = 0; i < p->bo_list->num_entries; i++) {
-                        struct amdgpu_bo *bo = p->bo_list->array[i].robj;
-
-                        p->bo_list->array[i].bo_va = amdgpu_vm_bo_find(vm, bo);
-                }
-
-                if (gds) {
-                        p->job->gds_base = amdgpu_bo_gpu_offset(gds);
-                        p->job->gds_size = amdgpu_bo_size(gds);
-                }
-                if (gws) {
-                        p->job->gws_base = amdgpu_bo_gpu_offset(gws);
-                        p->job->gws_size = amdgpu_bo_size(gws);
-                }
-                if (oa) {
-                        p->job->oa_base = amdgpu_bo_gpu_offset(oa);
-                        p->job->oa_size = amdgpu_bo_size(oa);
-                }
-        }
-
-        if (!r && p->uf_entry.robj) {
-                struct amdgpu_bo *uf = p->uf_entry.robj;
-
-                r = amdgpu_ttm_bind(&uf->tbo, &uf->tbo.mem);
-                p->job->uf_addr += amdgpu_bo_gpu_offset(uf);
-        }
+	struct amdgpu_fpriv *fpriv = p->filp->driver_priv;
+	struct amdgpu_bo_list_entry *e;
+	struct list_head duplicates;
+	bool need_mmap_lock = false;
+	unsigned i, tries = 10;
+	int r;
+
+	INIT_LIST_HEAD(&p->validated);
+
+	p->bo_list = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle);
+	if (p->bo_list) {
+		need_mmap_lock = p->bo_list->first_userptr !=
+			p->bo_list->num_entries;
+		amdgpu_bo_list_get_list(p->bo_list, &p->validated);
+	}
+
+	INIT_LIST_HEAD(&duplicates);
+	amdgpu_vm_get_pd_bo(&fpriv->vm, &p->validated, &p->vm_pd);
+
+	if (p->uf_entry.robj)
+		list_add(&p->uf_entry.tv.head, &p->validated);
+
+	if (need_mmap_lock)
+		down_read(&current->mm->mmap_sem);
+
+	while (1) {
+		struct list_head need_pages;
+		unsigned i;
+
+		r = ttm_eu_reserve_buffers(&p->ticket, &p->validated, true,
+					   &duplicates);
+		if (unlikely(r != 0)) {
+			if (r != -ERESTARTSYS)
+				DRM_ERROR("ttm_eu_reserve_buffers failed.\n");
+			goto error_free_pages;
+		}
+
+		/* Without a BO list we don't have userptr BOs */
+		if (!p->bo_list)
+			break;
+
+		INIT_LIST_HEAD(&need_pages);
+		for (i = p->bo_list->first_userptr;
+		     i < p->bo_list->num_entries; ++i) {
+
+			e = &p->bo_list->array[i];
+
+			if (amdgpu_ttm_tt_userptr_invalidated(e->robj->tbo.ttm,
+				 &e->user_invalidated) && e->user_pages) {
+
+				/* We acquired a page array, but somebody
+				 * invalidated it. Free it an try again
+				 */
+				release_pages(e->user_pages,
+					      e->robj->tbo.ttm->num_pages,
+					      false);
+				drm_free_large(e->user_pages);
+				e->user_pages = NULL;
+			}
+
+			if (e->robj->tbo.ttm->state != tt_bound &&
+			    !e->user_pages) {
+				list_del(&e->tv.head);
+				list_add(&e->tv.head, &need_pages);
+
+				amdgpu_bo_unreserve(e->robj);
+			}
+		}
+
+		if (list_empty(&need_pages))
+			break;
+
+		/* Unreserve everything again. */
+		ttm_eu_backoff_reservation(&p->ticket, &p->validated);
+
+		/* We tried too many times, just abort */
+		if (!--tries) {
+			r = -EDEADLK;
+			DRM_ERROR("deadlock in %s\n", __func__);
+			goto error_free_pages;
+		}
+
+		/* Fill the page arrays for all useptrs. */
+		list_for_each_entry(e, &need_pages, tv.head) {
+			struct ttm_tt *ttm = e->robj->tbo.ttm;
+
+			e->user_pages = drm_calloc_large(ttm->num_pages,
+							 sizeof(struct page*));
+			if (!e->user_pages) {
+				r = -ENOMEM;
+				DRM_ERROR("calloc failure in %s\n", __func__);
+				goto error_free_pages;
+			}
+
+			r = amdgpu_ttm_tt_get_user_pages(ttm, e->user_pages);
+			if (r) {
+				DRM_ERROR("amdgpu_ttm_tt_get_user_pages failed.\n");
+				drm_free_large(e->user_pages);
+				e->user_pages = NULL;
+				goto error_free_pages;
+			}
+		}
+
+		/* And try again. */
+		list_splice(&need_pages, &p->validated);
+	}
+
+	p->bytes_moved_threshold = amdgpu_cs_get_threshold_for_moves(p->adev);
+	p->bytes_moved = 0;
+	p->evictable = list_last_entry(&p->validated,
+				       struct amdgpu_bo_list_entry,
+				       tv.head);
+
+	r = amdgpu_vm_validate_pt_bos(p->adev, &fpriv->vm,
+				      amdgpu_cs_validate, p);
+	if (r) {
+		DRM_ERROR("amdgpu_vm_validate_pt_bos() failed.\n");
+		goto error_validate;
+	}
+
+	r = amdgpu_cs_list_validate(p, &duplicates);
+	if (r) {
+		DRM_ERROR("amdgpu_cs_list_validate(duplicates) failed.\n");
+		goto error_validate;
+	}
+
+	r = amdgpu_cs_list_validate(p, &p->validated);
+	if (r) {
+		DRM_ERROR("amdgpu_cs_list_validate(validated) failed.\n");
+		goto error_validate;
+	}
+
+	amdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved);
+
+	fpriv->vm.last_eviction_counter =
+		atomic64_read(&p->adev->num_evictions);
+
+	if (p->bo_list) {
+		struct amdgpu_bo *gds = p->bo_list->gds_obj;
+		struct amdgpu_bo *gws = p->bo_list->gws_obj;
+		struct amdgpu_bo *oa = p->bo_list->oa_obj;
+		struct amdgpu_vm *vm = &fpriv->vm;
+		unsigned i;
+
+		for (i = 0; i < p->bo_list->num_entries; i++) {
+			struct amdgpu_bo *bo = p->bo_list->array[i].robj;
+
+			p->bo_list->array[i].bo_va = amdgpu_vm_bo_find(vm, bo);
+		}
+
+		if (gds) {
+			p->job->gds_base = amdgpu_bo_gpu_offset(gds);
+			p->job->gds_size = amdgpu_bo_size(gds);
+		}
+		if (gws) {
+			p->job->gws_base = amdgpu_bo_gpu_offset(gws);
+			p->job->gws_size = amdgpu_bo_size(gws);
+		}
+		if (oa) {
+			p->job->oa_base = amdgpu_bo_gpu_offset(oa);
+			p->job->oa_size = amdgpu_bo_size(oa);
+		}
+	}
+
+	if (!r && p->uf_entry.robj) {
+		struct amdgpu_bo *uf = p->uf_entry.robj;
+
+		r = amdgpu_ttm_bind(&uf->tbo, &uf->tbo.mem);
+		p->job->uf_addr += amdgpu_bo_gpu_offset(uf);
+	}
 
 error_validate:
-        if (r) {
-                amdgpu_vm_move_pt_bos_in_lru(p->adev, &fpriv->vm);
-                ttm_eu_backoff_reservation(&p->ticket, &p->validated);
-        }
+	if (r) {
+		amdgpu_vm_move_pt_bos_in_lru(p->adev, &fpriv->vm);
+		ttm_eu_backoff_reservation(&p->ticket, &p->validated);
+	}
 
 error_free_pages:
 
-        if (need_mmap_lock)
-                up_read(&current->mm->mmap_sem);
+	if (need_mmap_lock)
+		up_read(&current->mm->mmap_sem);
 
-        if (p->bo_list) {
-                for (i = p->bo_list->first_userptr;
-                     i < p->bo_list->num_entries; ++i) {
-                        e = &p->bo_list->array[i];
+	if (p->bo_list) {
+		for (i = p->bo_list->first_userptr;
+		     i < p->bo_list->num_entries; ++i) {
+			e = &p->bo_list->array[i];
 
-                        if (!e->user_pages)
-                                continue;
+			if (!e->user_pages)
+				continue;
 
-                        release_pages(e->user_pages,
-                                      e->robj->tbo.ttm->num_pages,
-                                      false);
-                        drm_free_large(e->user_pages);
-                }
-        }
+			release_pages(e->user_pages,
+				      e->robj->tbo.ttm->num_pages,
+				      false);
+			drm_free_large(e->user_pages);
+		}
+	}
 
-        return r;
+	return r;
 }
 
 static int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)
@@ -905,18 +908,18 @@ static int amdgpu_cs_ib_fill(struct amdgpu_device *adev,
 				       &ring);
 		if (r)
 			return r;
- 
-                if (ib->flags & AMDGPU_IB_FLAG_PREAMBLE) {
-                        parser->job->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;
-                        if (!parser->ctx->preamble_presented) {
-                                parser->job->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;
-                                parser->ctx->preamble_presented = true;
-                        }
-                }
- 
+
+		if (ib->flags & AMDGPU_IB_FLAG_PREAMBLE) {
+			parser->job->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;
+			if (!parser->ctx->preamble_presented) {
+				parser->job->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;
+				parser->ctx->preamble_presented = true;
+			}
+		}
+
 		if (parser->job->ring && parser->job->ring != ring)
 			return -EINVAL;
- 
+
 		parser->job->ring = ring;
 
 		if (ring->funcs->parse_cs) {
@@ -970,11 +973,11 @@ static int amdgpu_cs_ib_fill(struct amdgpu_device *adev,
 		j++;
 	}
 
-        /* UVD & VCE fw doesn't support user fences */
-        if (parser->job->uf_addr && (
-            parser->job->ring->funcs->type == AMDGPU_RING_TYPE_UVD ||
-            parser->job->ring->funcs->type == AMDGPU_RING_TYPE_VCE))
-                return -EINVAL;
+	/* UVD & VCE fw doesn't support user fences */
+	if (parser->job->uf_addr && (
+	    parser->job->ring->funcs->type == AMDGPU_RING_TYPE_UVD ||
+	    parser->job->ring->funcs->type == AMDGPU_RING_TYPE_VCE))
+		return -EINVAL;
 
 	return 0;
 }
@@ -1046,18 +1049,18 @@ static int amdgpu_cs_submit(struct amdgpu_cs_parser *p,
 	job = p->job;
 	p->job = NULL;
 
-        r = amd_sched_job_init(&job->base, &ring->sched, entity, p->filp);
+	r = amd_sched_job_init(&job->base, &ring->sched, entity, p->filp);
 	if (r) {
 		amdgpu_job_free(job);
 		return r;
 	}
 
 	job->owner = p->filp;
-        job->fence_ctx = entity->fence_context;
-        p->fence = fence_get(&job->base.s_fence->finished);
-        cs->out.handle = amdgpu_ctx_add_fence(p->ctx, ring, p->fence);
-        job->uf_sequence = cs->out.handle;
-        amdgpu_job_free_resources(job);	
+	job->fence_ctx = entity->fence_context;
+	p->fence = fence_get(&job->base.s_fence->finished);
+	cs->out.handle = amdgpu_ctx_add_fence(p->ctx, ring, p->fence);
+	job->uf_sequence = cs->out.handle;
+	amdgpu_job_free_resources(job);
 
 	trace_amdgpu_cs_ioctl(job);
 	amd_sched_entity_push_job(&job->base);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
index 8ed22fe..3573ffb 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
@@ -132,15 +132,16 @@ static void amdgpu_flip_work_func(struct work_struct *__work)
 				 vblank->linedur_ns / 1000, stat, vpos, hpos);
 
 	/* Do the flip (mmio) */
-        adev->mode_info.funcs->page_flip(adev, work->crtc_id, work->base, work->async);
+	adev->mode_info.funcs->page_flip(adev, work->crtc_id, work->base, work->async);
 
-	/* set the flip status */
+	/* Set the flip status */
 	amdgpuCrtc->pflip_status = AMDGPU_FLIP_SUBMITTED;
-
 	spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
- 
-        DRM_DEBUG_DRIVER("crtc:%d[%p], pflip_stat:AMDGPU_FLIP_SUBMITTED, work: %p,\n",
-                                         amdgpuCrtc->crtc_id, amdgpuCrtc, work);
+
+
+	DRM_DEBUG_DRIVER("crtc:%d[%p], pflip_stat:AMDGPU_FLIP_SUBMITTED, work: %p,\n",
+					 amdgpuCrtc->crtc_id, amdgpuCrtc, work);
+
 }
 
 /*
@@ -196,7 +197,7 @@ int amdgpu_crtc_page_flip(struct drm_crtc *crtc,
 	work->event = event;
 	work->adev = adev;
 	work->crtc_id = amdgpu_crtc->crtc_id;
-        work->async = (page_flip_flags & DRM_MODE_PAGE_FLIP_ASYNC) != 0;
+	work->async = (page_flip_flags & DRM_MODE_PAGE_FLIP_ASYNC) != 0;
 
 	/* schedule unpin of the old buffer */
 	old_amdgpu_fb = to_amdgpu_framebuffer(crtc->primary->fb);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
old mode 100755
new mode 100644
index a351b85..3f1bf2d
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
@@ -671,7 +671,7 @@ static struct drm_driver kms_driver = {
 	.driver_features =
 	    DRIVER_USE_AGP |
 	    DRIVER_HAVE_IRQ | DRIVER_IRQ_SHARED | DRIVER_GEM |
-            DRIVER_PRIME | DRIVER_RENDER | DRIVER_MODESET,
+	    DRIVER_PRIME | DRIVER_RENDER | DRIVER_MODESET,
 	.dev_priv_size = 0,
 	.load = amdgpu_driver_load_kms,
 	.open = amdgpu_driver_open_kms,
@@ -767,13 +767,13 @@ static int __init amdgpu_init(void)
 	return drm_pci_init(driver, pdriver);
 
 error_sched:
-        amdgpu_fence_slab_fini();
+	amdgpu_fence_slab_fini();
 
 error_fence:
-        amdgpu_sync_fini();
+	amdgpu_sync_fini();
 
 error_sync:
-        return r;
+	return r;
 }
 
 static void __exit amdgpu_exit(void)
@@ -782,8 +782,8 @@ static void __exit amdgpu_exit(void)
 	drm_pci_exit(driver, pdriver);
 	amdgpu_unregister_atpx_handler();
 	amdgpu_sync_fini();
-        amd_sched_fence_slab_fini();
-        amdgpu_fence_slab_fini();
+	amd_sched_fence_slab_fini();
+	amdgpu_fence_slab_fini();
 }
 
 module_init(amdgpu_init);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
index 3c5a7a9..a26f471 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *	    Alex Deucher
- *	    Jerome Glisse
+ *          Alex Deucher
+ *          Jerome Glisse
  */
 #include <linux/ktime.h>
 #include <linux/pagemap.h>
@@ -259,9 +259,9 @@ int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
 	    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
 	    AMDGPU_GEM_USERPTR_REGISTER))
 		return -EINVAL;
- 
-        if (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) && (
-             !(args->flags & AMDGPU_GEM_USERPTR_REGISTER))) {
+
+	if (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&
+	     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
 
 		/* if we want to write to it we must install a MMU notifier */
 		return -EACCES;
@@ -514,7 +514,7 @@ static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
 	r = ttm_eu_reserve_buffers(&ticket, &list, true, &duplicates);
 	if (r)
 		goto error_print;
- 
+
 	list_for_each_entry(entry, &list, head) {
 		domain = amdgpu_mem_type_to_domain(entry->bo->mem.mem_type);
 		/* if anything is swapped out don't swap it in here,
@@ -687,7 +687,7 @@ int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
 		robj->allowed_domains = robj->prefered_domains;
 		if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
 			robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
- 
+
 		amdgpu_bo_unreserve(robj);
 		break;
 	default:
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c
index 4c180bf..acf48de 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c
@@ -22,9 +22,9 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *	    Alex Deucher
- *	    Jerome Glisse
- *	    Christian König
+ *          Alex Deucher
+ *          Jerome Glisse
+ *          Christian König
  */
 #include <linux/seq_file.h>
 #include <linux/slab.h>
@@ -100,6 +100,7 @@ void amdgpu_ib_free(struct amdgpu_device *adev, struct amdgpu_ib *ib,
  * @adev: amdgpu_device pointer
  * @num_ibs: number of IBs to schedule
  * @ibs: IB objects to schedule
+ * @f: fence created during this submission
  *
  * Schedule an IB on the associated ring (all asics).
  * Returns 0 on success, error on failure.
@@ -111,7 +112,7 @@ void amdgpu_ib_free(struct amdgpu_device *adev, struct amdgpu_ib *ib,
  * the resource descriptors will be already in cache when the draw is
  * processed.  To accomplish this, the userspace driver submits two
  * IBs, one for the CE and one for the DE.  If there is a CE IB (called
- * a CONST_IB), it will be put on the ring prior to the DE IB.	Prior
+ * a CONST_IB), it will be put on the ring prior to the DE IB.  Prior
  * to SI there was just a DE IB.
  */
 int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
@@ -120,41 +121,40 @@ int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib *ib = &ibs[0];
-        bool skip_preamble, need_ctx_switch;
-        unsigned patch_offset = ~0;
-        struct amdgpu_vm *vm;
+	bool skip_preamble, need_ctx_switch;
+	unsigned patch_offset = ~0;
+	struct amdgpu_vm *vm;
 	uint64_t fence_ctx;
-        uint32_t status = 0, alloc_size;
-
-        unsigned i;
+	uint32_t status = 0, alloc_size;
 
+	unsigned i;
 	int r = 0;
 
 	if (num_ibs == 0)
 		return -EINVAL;
 
-        /* ring tests don't use a job */
-        if (job) {
-                vm = job->vm;
-                fence_ctx = job->fence_ctx;
-        } else {
-                vm = NULL;
-                fence_ctx = 0;
-        }
+	/* ring tests don't use a job */
+	if (job) {
+		vm = job->vm;
+		fence_ctx = job->fence_ctx;
+	} else {
+		vm = NULL;
+		fence_ctx = 0;
+	}
 
 	if (!ring->ready) {
-                dev_err(adev->dev, "couldn't schedule ib on ring <%s>\n", ring->name);
+		dev_err(adev->dev, "couldn't schedule ib on ring <%s>\n", ring->name);
 		return -EINVAL;
 	}
 
-        if (vm && !job->vm_id) {
+	if (vm && !job->vm_id) {
 		dev_err(adev->dev, "VM IB without ID\n");
 		return -EINVAL;
 	}
 
-        alloc_size = ring->funcs->emit_frame_size + num_ibs *
-                ring->funcs->emit_ib_size; 
-        
+	alloc_size = ring->funcs->emit_frame_size + num_ibs *
+		ring->funcs->emit_ib_size;
+
 	r = amdgpu_ring_alloc(ring, alloc_size);
 	if (r) {
 		dev_err(adev->dev, "scheduling IB failed (%d).\n", r);
@@ -165,50 +165,50 @@ int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
 		patch_offset = amdgpu_ring_init_cond_exec(ring);
 
 	if (vm) {
-                r = amdgpu_vm_flush(ring, job);
+		r = amdgpu_vm_flush(ring, job);
 		if (r) {
 			amdgpu_ring_undo(ring);
 			return r;
-		    }
 		}
+	}
 
 	if (ring->funcs->emit_hdp_flush)
 		amdgpu_ring_emit_hdp_flush(ring);
 
 	/* always set cond_exec_polling to CONTINUE */
 	*ring->cond_exe_cpu_addr = 1;
- 
-        skip_preamble = ring->current_ctx == fence_ctx;
-        need_ctx_switch = ring->current_ctx != fence_ctx; 
-        if (job && ring->funcs->emit_cntxcntl) {
-                if (need_ctx_switch)
-                        status |= AMDGPU_HAVE_CTX_SWITCH;
-                status |= job->preamble_status;
-                amdgpu_ring_emit_cntxcntl(ring, status);
-        }
+
+	skip_preamble = ring->current_ctx == fence_ctx;
+	need_ctx_switch = ring->current_ctx != fence_ctx;
+	if (job && ring->funcs->emit_cntxcntl) {
+		if (need_ctx_switch)
+			status |= AMDGPU_HAVE_CTX_SWITCH;
+		status |= job->preamble_status;
+		amdgpu_ring_emit_cntxcntl(ring, status);
+	}
 
 	for (i = 0; i < num_ibs; ++i) {
-                ib = &ibs[i]; 
-                
+		ib = &ibs[i];
+
 		/* drop preamble IBs if we don't have a context switch */
-                if ((ib->flags & AMDGPU_IB_FLAG_PREAMBLE) &&
-                        skip_preamble &&
-                        !(status & AMDGPU_PREAMBLE_IB_PRESENT_FIRST))
-                        continue;
-
-                amdgpu_ring_emit_ib(ring, ib, job ? job->vm_id : 0,
-                                    need_ctx_switch);
-                need_ctx_switch = false;
+		if ((ib->flags & AMDGPU_IB_FLAG_PREAMBLE) &&
+			skip_preamble &&
+			!(status & AMDGPU_PREAMBLE_IB_PRESENT_FIRST))
+			continue;
+
+		amdgpu_ring_emit_ib(ring, ib, job ? job->vm_id : 0,
+				    need_ctx_switch);
+		need_ctx_switch = false;
 	}
 
 	if (ring->funcs->emit_hdp_invalidate)
 		amdgpu_ring_emit_hdp_invalidate(ring);
 
-        r = amdgpu_fence_emit(ring, f);
+	r = amdgpu_fence_emit(ring, f);
 	if (r) {
 		dev_err(adev->dev, "failed to emit fence (%d)\n", r);
-                if (job && job->vm_id)
-                        amdgpu_vm_reset_id(adev, job->vm_id);
+		if (job && job->vm_id)
+			amdgpu_vm_reset_id(adev, job->vm_id);
 		amdgpu_ring_undo(ring);
 		return r;
 	}
@@ -221,10 +221,10 @@ int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
 
 	if (patch_offset != ~0 && ring->funcs->patch_cond_exec)
 		amdgpu_ring_patch_cond_exec(ring, patch_offset);
- 
-        ring->current_ctx = fence_ctx;
-        if (ring->funcs->emit_switch_buffer)
-                amdgpu_ring_emit_switch_buffer(ring);
+
+	ring->current_ctx = fence_ctx;
+	if (ring->funcs->emit_switch_buffer)
+		amdgpu_ring_emit_switch_buffer(ring);
 	amdgpu_ring_commit(ring);
 	return 0;
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c
index 8e17e5e..3cf3a19 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c
@@ -240,7 +240,7 @@ int amdgpu_irq_init(struct amdgpu_device *adev)
 		INIT_WORK(&adev->hotplug_work,
 				amdgpu_hotplug_work_func);
 	}
-	
+
 	INIT_WORK(&adev->reset_work, amdgpu_irq_reset_work_func);
 
 	adev->irq.installed = true;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c
index 65ffb79..8c58079 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c
@@ -30,13 +30,13 @@
 
 static void amdgpu_job_timedout(struct amd_sched_job *s_job)
 {
-        struct amdgpu_job *job = container_of(s_job, struct amdgpu_job, base);
+	struct amdgpu_job *job = container_of(s_job, struct amdgpu_job, base);
 
 	DRM_ERROR("ring %s timeout, last signaled seq=%u, last emitted seq=%u\n",
-                  job->base.sched->name,
-                  atomic_read(&job->ring->fence_drv.last_seq),
-                  job->ring->fence_drv.sync_seq);
-        amdgpu_gpu_reset(job->adev);
+		  job->base.sched->name,
+		  atomic_read(&job->ring->fence_drv.last_seq),
+		  job->ring->fence_drv.sync_seq);
+	amdgpu_gpu_reset(job->adev);
 }
 
 int amdgpu_job_alloc(struct amdgpu_device *adev, unsigned num_ibs,
@@ -115,17 +115,18 @@ int amdgpu_job_submit(struct amdgpu_job *job, struct amdgpu_ring *ring,
 {
 	int r;
 	job->ring = ring;
+
 	if (!f)
 		return -EINVAL;
 
-        r = amd_sched_job_init(&job->base, &ring->sched, entity, owner);
+	r = amd_sched_job_init(&job->base, &ring->sched, entity, owner);
 	if (r)
 		return r;
 
 	job->owner = owner;
-        job->fence_ctx = entity->fence_context;
-        *f = fence_get(&job->base.s_fence->finished);
-        amdgpu_job_free_resources(job);
+	job->fence_ctx = entity->fence_context;
+	*f = fence_get(&job->base.s_fence->finished);
+	amdgpu_job_free_resources(job);
 	amd_sched_entity_push_job(&job->base);
 
 	return 0;
@@ -135,15 +136,16 @@ static struct fence *amdgpu_job_dependency(struct amd_sched_job *sched_job)
 {
 	struct amdgpu_job *job = to_amdgpu_job(sched_job);
 	struct amdgpu_vm *vm = job->vm;
+
 	struct fence *fence = amdgpu_sync_get_fence(&job->sync);
 
-        if (fence == NULL && vm && !job->vm_id) {
+	if (fence == NULL && vm && !job->vm_id) {
 		struct amdgpu_ring *ring = job->ring;
 		int r;
 
 		r = amdgpu_vm_grab_id(vm, ring, &job->sync,
-                                      &job->base.s_fence->finished,
-                                      job);
+				      &job->base.s_fence->finished,
+				      job);
 		if (r)
 			DRM_ERROR("Error getting VM ID (%d)\n", r);
 
@@ -186,5 +188,3 @@ const struct amd_sched_backend_ops amdgpu_sched_ops = {
 	.timedout_job = amdgpu_job_timedout,
 	.free_job = amdgpu_job_free_cb
 };
-
-
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
index de8e49b..b7b96b5 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *	    Alex Deucher
- *	    Jerome Glisse
+ *          Alex Deucher
+ *          Jerome Glisse
  */
 #include <drm/drmP.h>
 #include "amdgpu.h"
@@ -498,8 +498,8 @@ static int amdgpu_info_ioctl(struct drm_device *dev, void *data, struct drm_file
 			dev_info.max_memory_clock = adev->pm.default_mclk * 10;
 		}
 		dev_info.enabled_rb_pipes_mask = adev->gfx.config.backend_enable_mask;
-                dev_info.num_rb_pipes = adev->gfx.config.max_backends_per_se *
-                        adev->gfx.config.max_shader_engines;
+		dev_info.num_rb_pipes = adev->gfx.config.max_backends_per_se *
+			adev->gfx.config.max_shader_engines;
 		dev_info.num_hw_gfx_contexts = adev->gfx.config.max_hw_contexts;
 		dev_info._pad = 0;
 		dev_info.ids_flags = 0;
@@ -841,123 +841,122 @@ const int amdgpu_max_kms_ioctl = ARRAY_SIZE(amdgpu_ioctls_kms);
 
 static int amdgpu_debugfs_firmware_info(struct seq_file *m, void *data)
 {
-        struct drm_info_node *node = (struct drm_info_node *) m->private;
-        struct drm_device *dev = node->minor->dev;
-        struct amdgpu_device *adev = dev->dev_private;
-        struct drm_amdgpu_info_firmware fw_info;
-        struct drm_amdgpu_query_fw query_fw;
-        int ret, i;
- 
-        /* VCE */
-        query_fw.fw_type = AMDGPU_INFO_FW_VCE;
-        ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-        if (ret)
-                return ret;
-        seq_printf(m, "VCE feature version: %u, firmware version: 0x%08x\n",
-                   fw_info.feature, fw_info.ver);
- 
-        /* UVD */
-        query_fw.fw_type = AMDGPU_INFO_FW_UVD;
-        ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-        if (ret)
-                return ret;
-        seq_printf(m, "UVD feature version: %u, firmware version: 0x%08x\n",
-                   fw_info.feature, fw_info.ver);
- 
-        /* GMC */
-        query_fw.fw_type = AMDGPU_INFO_FW_GMC;
-        ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-        if (ret)
-                return ret;
-        seq_printf(m, "MC feature version: %u, firmware version: 0x%08x\n",
-                   fw_info.feature, fw_info.ver);
- 
-        /* ME */
-        query_fw.fw_type = AMDGPU_INFO_FW_GFX_ME;
-        ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-        if (ret)
-                return ret;
-        seq_printf(m, "ME feature version: %u, firmware version: 0x%08x\n",
-                   fw_info.feature, fw_info.ver);
- 
-        /* PFP */
-        query_fw.fw_type = AMDGPU_INFO_FW_GFX_PFP;
-        ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-        if (ret)
-                return ret;
-        seq_printf(m, "PFP feature version: %u, firmware version: 0x%08x\n",
-                   fw_info.feature, fw_info.ver);
- 
-        /* CE */
-        query_fw.fw_type = AMDGPU_INFO_FW_GFX_CE;
-        ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-        if (ret)
-                return ret;
-        seq_printf(m, "CE feature version: %u, firmware version: 0x%08x\n",
-                   fw_info.feature, fw_info.ver);
- 
-        /* RLC */
-        query_fw.fw_type = AMDGPU_INFO_FW_GFX_RLC;
-        ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-        if (ret)
-                return ret;
-        seq_printf(m, "RLC feature version: %u, firmware version: 0x%08x\n",
-                   fw_info.feature, fw_info.ver);
- 
-        /* MEC */
-        query_fw.fw_type = AMDGPU_INFO_FW_GFX_MEC;
-        query_fw.index = 0;
-        ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-        if (ret)
-                return ret;
-        seq_printf(m, "MEC feature version: %u, firmware version: 0x%08x\n",
-                   fw_info.feature, fw_info.ver);
- 
-        /* MEC2 */
-        if (adev->asic_type == CHIP_KAVERI ||
-            (adev->asic_type > CHIP_TOPAZ && adev->asic_type != CHIP_STONEY)) {
-                query_fw.index = 1;
-               ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-                if (ret)
-                        return ret;
-                seq_printf(m, "MEC2 feature version: %u, firmware version: 0x%08x\n",
-                           fw_info.feature, fw_info.ver);
-        }
- 
-        /* SMC */
-        query_fw.fw_type = AMDGPU_INFO_FW_SMC;
-        ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-        if (ret)
-                return ret;
-        seq_printf(m, "SMC feature version: %u, firmware version: 0x%08x\n",
-                   fw_info.feature, fw_info.ver);
- 
-        /* SDMA */
-        query_fw.fw_type = AMDGPU_INFO_FW_SDMA;
-        for (i = 0; i < adev->sdma.num_instances; i++) {
-                query_fw.index = i;
-                ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
-                if (ret)
-                        return ret;
-                seq_printf(m, "SDMA%d feature version: %u, firmware version: 0x%08x\n",
-                           i, fw_info.feature, fw_info.ver);
-        }
- 
-        return 0;
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct amdgpu_device *adev = dev->dev_private;
+	struct drm_amdgpu_info_firmware fw_info;
+	struct drm_amdgpu_query_fw query_fw;
+	int ret, i;
+
+	/* VCE */
+	query_fw.fw_type = AMDGPU_INFO_FW_VCE;
+	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "VCE feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* UVD */
+	query_fw.fw_type = AMDGPU_INFO_FW_UVD;
+	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "UVD feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* GMC */
+	query_fw.fw_type = AMDGPU_INFO_FW_GMC;
+	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "MC feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* ME */
+	query_fw.fw_type = AMDGPU_INFO_FW_GFX_ME;
+	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "ME feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* PFP */
+	query_fw.fw_type = AMDGPU_INFO_FW_GFX_PFP;
+	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "PFP feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* CE */
+	query_fw.fw_type = AMDGPU_INFO_FW_GFX_CE;
+	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "CE feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* RLC */
+	query_fw.fw_type = AMDGPU_INFO_FW_GFX_RLC;
+	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "RLC feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* MEC */
+	query_fw.fw_type = AMDGPU_INFO_FW_GFX_MEC;
+	query_fw.index = 0;
+	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "MEC feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* MEC2 */
+	if (adev->asic_type == CHIP_KAVERI ||
+	    (adev->asic_type > CHIP_TOPAZ && adev->asic_type != CHIP_STONEY)) {
+		query_fw.index = 1;
+		ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+		if (ret)
+			return ret;
+		seq_printf(m, "MEC2 feature version: %u, firmware version: 0x%08x\n",
+			   fw_info.feature, fw_info.ver);
+	}
+
+	/* SMC */
+	query_fw.fw_type = AMDGPU_INFO_FW_SMC;
+	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "SMC feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* SDMA */
+	query_fw.fw_type = AMDGPU_INFO_FW_SDMA;
+	for (i = 0; i < adev->sdma.num_instances; i++) {
+		query_fw.index = i;
+		ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
+		if (ret)
+			return ret;
+		seq_printf(m, "SDMA%d feature version: %u, firmware version: 0x%08x\n",
+			   i, fw_info.feature, fw_info.ver);
+	}
+
+	return 0;
 }
 
 static const struct drm_info_list amdgpu_firmware_info_list[] = {
-        {"amdgpu_firmware_info", amdgpu_debugfs_firmware_info, 0, NULL},
+	{"amdgpu_firmware_info", amdgpu_debugfs_firmware_info, 0, NULL},
 };
 #endif
 
 int amdgpu_debugfs_firmware_init(struct amdgpu_device *adev)
 {
 #if defined(CONFIG_DEBUG_FS)
-        return amdgpu_debugfs_add_files(adev, amdgpu_firmware_info_list,
-                                        ARRAY_SIZE(amdgpu_firmware_info_list));
+	return amdgpu_debugfs_add_files(adev, amdgpu_firmware_info_list,
+					ARRAY_SIZE(amdgpu_firmware_info_list));
 #else
-        return 0;
+	return 0;
 #endif
 }
-
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h
index 0dcd25d..0b9d4df 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h
@@ -71,13 +71,13 @@ static inline unsigned amdgpu_mem_type_to_domain(u32 mem_type)
  */
 static inline int amdgpu_bo_reserve(struct amdgpu_bo *bo, bool no_intr)
 {
-        struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
+	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
 	int r;
 
 	r = ttm_bo_reserve(&bo->tbo, !no_intr, false, false, NULL);
 	if (unlikely(r != 0)) {
 		if (r != -ERESTARTSYS)
-                        dev_err(adev->dev, "%p reserve failed\n", bo);
+			dev_err(adev->dev, "%p reserve failed\n", bo);
 		return r;
 	}
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c
index 659e24e..ad7c851 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c
@@ -22,9 +22,9 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *	    Alex Deucher
- *	    Jerome Glisse
- *	    Christian König
+ *          Alex Deucher
+ *          Jerome Glisse
+ *          Christian König
  */
 #include <linux/seq_file.h>
 #include <linux/slab.h>
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h
index 6a81164..067e5e6 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h
@@ -87,10 +87,10 @@ TRACE_EVENT(amdgpu_cs,
 
 	    TP_fast_assign(
 			   __entry->bo_list = p->bo_list;
-                           __entry->ring = p->job->ring->idx;
-                           __entry->dw = p->job->ibs[i].length_dw;
+			   __entry->ring = p->job->ring->idx;
+			   __entry->dw = p->job->ibs[i].length_dw;
 			   __entry->fences = amdgpu_fence_count_emitted(
-                                p->job->ring);
+				p->job->ring);
 			   ),
 	    TP_printk("bo_list=%p, ring=%u, dw=%u, fences=%u",
 		      __entry->bo_list, __entry->ring, __entry->dw,
@@ -113,8 +113,8 @@ TRACE_EVENT(amdgpu_cs_ioctl,
 			   __entry->adev = job->adev;
 			   __entry->sched_job = &job->base;
 			   __entry->ib = job->ibs;
-                           __entry->fence = &job->base.s_fence->finished;
-                           __entry->ring_name = job->ring->name;
+			   __entry->fence = &job->base.s_fence->finished;
+			   __entry->ring_name = job->ring->name;
 			   __entry->num_ibs = job->num_ibs;
 			   ),
 	    TP_printk("adev=%p, sched_job=%p, first ib=%p, sched fence=%p, ring name:%s, num_ibs:%u",
@@ -138,8 +138,8 @@ TRACE_EVENT(amdgpu_sched_run_job,
 			   __entry->adev = job->adev;
 			   __entry->sched_job = &job->base;
 			   __entry->ib = job->ibs;
-                           __entry->fence = &job->base.s_fence->finished;
-                           __entry->ring_name = job->ring->name;
+			   __entry->fence = &job->base.s_fence->finished;
+			   __entry->ring_name = job->ring->name;
 			   __entry->num_ibs = job->num_ibs;
 			   ),
 	    TP_printk("adev=%p, sched_job=%p, first ib=%p, sched fence=%p, ring name:%s, num_ibs:%u",
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
index f820e01..7fb9fed 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
@@ -275,91 +275,91 @@ static int amdgpu_mm_node_addr(struct ttm_buffer_object *bo,
 }
 
 static int amdgpu_move_blit(struct ttm_buffer_object *bo,
-                             bool evict, bool no_wait_gpu,
-                             struct ttm_mem_reg *new_mem,
-                             struct ttm_mem_reg *old_mem)
+			    bool evict, bool no_wait_gpu,
+			    struct ttm_mem_reg *new_mem,
+			    struct ttm_mem_reg *old_mem)
 {
-         struct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);
-         struct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;
- 
-         struct drm_mm_node *old_mm, *new_mm;
-         uint64_t old_start, old_size, new_start, new_size;
-         unsigned long num_pages;
-         struct fence *fence = NULL;
-         int r;
- 
-         BUILD_BUG_ON((PAGE_SIZE % AMDGPU_GPU_PAGE_SIZE) != 0);
- 
-         if (!ring->ready) {
-                 DRM_ERROR("Trying to move memory with ring turned off.\n");
-                 return -EINVAL;
-         }
- 
-         old_mm = old_mem->mm_node;
-         r = amdgpu_mm_node_addr(bo, old_mm, old_mem, &old_start);
-         if (r)
-                 return r;
-         old_size = old_mm->size;
- 
- 
-         new_mm = new_mem->mm_node;
-         r = amdgpu_mm_node_addr(bo, new_mm, new_mem, &new_start);
-         if (r)
-                 return r;
-         new_size = new_mm->size;
- 
-         num_pages = new_mem->num_pages;
-         while (num_pages) {
-                 unsigned long cur_pages = min(old_size, new_size);
-                 struct fence *next;
- 
-                 r = amdgpu_copy_buffer(ring, old_start, new_start,
-                                        cur_pages * PAGE_SIZE,
-                                        bo->resv, &next, false);
-                 if (r)
-                         goto error;
- 
-                 fence_put(fence);
-                 fence = next;
- 
-                 num_pages -= cur_pages;
-                 if (!num_pages)
-                         break;
- 
-                 old_size -= cur_pages;
-                 if (!old_size) {
-                         r = amdgpu_mm_node_addr(bo, ++old_mm, old_mem,
-                                                 &old_start);
-                         if (r)
-                                 goto error;
-                         old_size = old_mm->size;
-                 } else {
-                         old_start += cur_pages * PAGE_SIZE;
-                 }
- 
-                 new_size -= cur_pages;
-                 if (!new_size) {
-                         r = amdgpu_mm_node_addr(bo, ++new_mm, new_mem,
-                                                 &new_start);
-                         if (r)
-                                 goto error;
- 
-                         new_size = new_mm->size;
-                 } else {
-                         new_start += cur_pages * PAGE_SIZE;
-                 }
-         }
+	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);
+	struct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;
+
+	struct drm_mm_node *old_mm, *new_mm;
+	uint64_t old_start, old_size, new_start, new_size;
+	unsigned long num_pages;
+	struct fence *fence = NULL;
+	int r;
+
+	BUILD_BUG_ON((PAGE_SIZE % AMDGPU_GPU_PAGE_SIZE) != 0);
+
+	if (!ring->ready) {
+		DRM_ERROR("Trying to move memory with ring turned off.\n");
+		return -EINVAL;
+	}
+
+	old_mm = old_mem->mm_node;
+	r = amdgpu_mm_node_addr(bo, old_mm, old_mem, &old_start);
+	if (r)
+		return r;
+	old_size = old_mm->size;
+
+
+	new_mm = new_mem->mm_node;
+	r = amdgpu_mm_node_addr(bo, new_mm, new_mem, &new_start);
+	if (r)
+		return r;
+	new_size = new_mm->size;
+
+	num_pages = new_mem->num_pages;
+	while (num_pages) {
+		unsigned long cur_pages = min(old_size, new_size);
+		struct fence *next;
+
+		r = amdgpu_copy_buffer(ring, old_start, new_start,
+				       cur_pages * PAGE_SIZE,
+				       bo->resv, &next, false);
+		if (r)
+			goto error;
+
+		fence_put(fence);
+		fence = next;
+
+		num_pages -= cur_pages;
+		if (!num_pages)
+			break;
+
+		old_size -= cur_pages;
+		if (!old_size) {
+			r = amdgpu_mm_node_addr(bo, ++old_mm, old_mem,
+						&old_start);
+			if (r)
+				goto error;
+			old_size = old_mm->size;
+		} else {
+			old_start += cur_pages * PAGE_SIZE;
+		}
+
+		new_size -= cur_pages;
+		if (!new_size) {
+			r = amdgpu_mm_node_addr(bo, ++new_mm, new_mem,
+						&new_start);
+			if (r)
+				goto error;
+
+			new_size = new_mm->size;
+		} else {
+			new_start += cur_pages * PAGE_SIZE;
+		}
+	}
  
-         r = ttm_bo_move_accel_cleanup(bo, fence,
+    r = ttm_bo_move_accel_cleanup(bo, fence,
                                       evict, no_wait_gpu, new_mem);
-         fence_put(fence);
-         return r;
+	fence_put(fence);
+	return r;
 
 error:
-         if (fence)
-                 fence_wait(fence, false);
-         fence_put(fence);
-         return r;
+	if (fence)
+		fence_wait(fence, false);
+	fence_put(fence);
+	return r;
 }
 
 static int amdgpu_move_vram_ram(struct ttm_buffer_object *bo,
@@ -589,10 +589,10 @@ struct amdgpu_ttm_tt {
 	uint64_t		userptr;
 	struct mm_struct	*usermm;
 	uint32_t		userflags;
-	spinlock_t		guptasklock;
-	struct list_head	guptasks;
+	spinlock_t              guptasklock;
+	struct list_head        guptasks;
 	atomic_t		mmu_invalidations;
-        struct list_head        list;
+	struct list_head        list;
 };
 
 int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)
@@ -1148,7 +1148,7 @@ int amdgpu_ttm_init(struct amdgpu_device *adev)
 {
 	unsigned i, j;
 	int r;
-	
+
 	/* No others user of address space so set it to 0 */
 	r = ttm_bo_device_init(&adev->mman.bdev,
 			       adev->mman.bo_global_ref.ref.object,
@@ -1333,7 +1333,7 @@ int amdgpu_copy_buffer(struct amdgpu_ring *ring,
 		num_dw++;
 
 	r = amdgpu_job_alloc_with_ib(adev, num_dw * 4, &job);
-	if (r) 
+	if (r)
 		return r;
 
 	if (resv) {
@@ -1355,21 +1355,22 @@ int amdgpu_copy_buffer(struct amdgpu_ring *ring,
 		dst_offset += cur_size_in_bytes;
 		byte_count -= cur_size_in_bytes;
 	}
+
 	amdgpu_ring_pad_ib(ring, &job->ibs[0]);
 	WARN_ON(job->ibs[0].length_dw > num_dw);
 	if (direct_submit) {
-                r = amdgpu_ib_schedule(ring, job->num_ibs, job->ibs,
-                                       NULL, NULL, fence);
-                job->fence = fence_get(*fence);
-                if (r)
-                        DRM_ERROR("Error scheduling IBs (%d)\n", r);
-                amdgpu_job_free(job);
-        } else {
-                r = amdgpu_job_submit(job, ring, &adev->mman.entity,
-                                      AMDGPU_FENCE_OWNER_UNDEFINED, fence);
-                if (r)
-                        goto error_free;
-        }
+		r = amdgpu_ib_schedule(ring, job->num_ibs, job->ibs,
+				       NULL, NULL, fence);
+		job->fence = fence_get(*fence);
+		if (r)
+			DRM_ERROR("Error scheduling IBs (%d)\n", r);
+		amdgpu_job_free(job);
+	} else {
+		r = amdgpu_job_submit(job, ring, &adev->mman.entity,
+				      AMDGPU_FENCE_OWNER_UNDEFINED, fence);
+		if (r)
+			goto error_free;
+	}
 
 	return r;
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c
index ad1d46b1..1b54cc2 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c
@@ -178,6 +178,7 @@ int amdgpu_uvd_sw_init(struct amdgpu_device *adev)
 	version_minor = (le32_to_cpu(hdr->ucode_version) >> 8) & 0xff;
 	DRM_INFO("Found UVD firmware Version: %hu.%hu Family ID: %hu\n",
 		version_major, version_minor, family_id);
+
 	/*
 	 * Limit the number of UVD handles depending on microcode major
 	 * and minor versions. The firmware version which has 40 UVD
@@ -187,22 +188,22 @@ int amdgpu_uvd_sw_init(struct amdgpu_device *adev)
 	if ((version_major > 0x01) ||
 	    ((version_major == 0x01) && (version_minor >= 0x50)))
 		adev->uvd.max_handles = AMDGPU_MAX_UVD_HANDLES;
-	
+
 	adev->uvd.fw_version = ((version_major << 24) | (version_minor << 16) |
 				(family_id << 8));
 
-        if ((adev->asic_type == CHIP_POLARIS10 ||
-             adev->asic_type == CHIP_POLARIS11) &&
-            (adev->uvd.fw_version < FW_1_66_16))
-                DRM_ERROR("POLARIS10/11 UVD firmware version %hu.%hu is too old.\n",
-                          version_major, version_minor);
- 
+	if ((adev->asic_type == CHIP_POLARIS10 ||
+	     adev->asic_type == CHIP_POLARIS11) &&
+	    (adev->uvd.fw_version < FW_1_66_16))
+		DRM_ERROR("POLARIS10/11 UVD firmware version %hu.%hu is too old.\n",
+			  version_major, version_minor);
+
 	bo_size = AMDGPU_GPU_PAGE_ALIGN(le32_to_cpu(hdr->ucode_size_bytes) + 8)
 		  +  AMDGPU_UVD_STACK_SIZE + AMDGPU_UVD_HEAP_SIZE
 		  +  AMDGPU_UVD_SESSION_SIZE * adev->uvd.max_handles;
-        r = amdgpu_bo_create_kernel(adev, bo_size, PAGE_SIZE,
-                                    AMDGPU_GEM_DOMAIN_VRAM, &adev->uvd.vcpu_bo,
-                                    &adev->uvd.gpu_addr, &adev->uvd.cpu_addr);
+	r = amdgpu_bo_create_kernel(adev, bo_size, PAGE_SIZE,
+				    AMDGPU_GEM_DOMAIN_VRAM, &adev->uvd.vcpu_bo,
+				    &adev->uvd.gpu_addr, &adev->uvd.cpu_addr);
 	if (r) {
 		dev_err(adev->dev, "(%d) failed to allocate UVD bo\n", r);
 		return r;
@@ -251,10 +252,10 @@ int amdgpu_uvd_sw_fini(struct amdgpu_device *adev)
 	kfree(adev->uvd.saved_bo);
 
 	amd_sched_entity_fini(&adev->uvd.ring.sched, &adev->uvd.entity);
-	
-        amdgpu_bo_free_kernel(&adev->uvd.vcpu_bo,
-                              &adev->uvd.gpu_addr,
-                              (void **)&adev->uvd.cpu_addr);
+
+	amdgpu_bo_free_kernel(&adev->uvd.vcpu_bo,
+			      &adev->uvd.gpu_addr,
+			      (void **)&adev->uvd.cpu_addr);
 
 	amdgpu_ring_fini(&adev->uvd.ring);
 
@@ -275,10 +276,10 @@ int amdgpu_uvd_suspend(struct amdgpu_device *adev)
 	for (i = 0; i < adev->uvd.max_handles; ++i)
 		if (atomic_read(&adev->uvd.handles[i]))
 			break;
- 
+
 	if (i == AMDGPU_MAX_UVD_HANDLES)
 		return 0;
-	
+
 	cancel_delayed_work_sync(&adev->uvd.idle_work);
 
 	size = amdgpu_bo_size(adev->uvd.vcpu_bo);
@@ -287,8 +288,8 @@ int amdgpu_uvd_suspend(struct amdgpu_device *adev)
 	adev->uvd.saved_bo = kmalloc(size, GFP_KERNEL);
 	if (!adev->uvd.saved_bo)
 		return -ENOMEM;
- 
-        memcpy_fromio(adev->uvd.saved_bo, ptr, size);
+
+	memcpy_fromio(adev->uvd.saved_bo, ptr, size);
 
 	return 0;
 }
@@ -311,14 +312,14 @@ int amdgpu_uvd_resume(struct amdgpu_device *adev)
 	} else {
 		const struct common_firmware_header *hdr;
 		unsigned offset;
- 
+
 		hdr = (const struct common_firmware_header *)adev->uvd.fw->data;
 		offset = le32_to_cpu(hdr->ucode_array_offset_bytes);
-                memcpy_toio(adev->uvd.cpu_addr, adev->uvd.fw->data + offset,
-                            le32_to_cpu(hdr->ucode_size_bytes));
+		memcpy_toio(adev->uvd.cpu_addr, adev->uvd.fw->data + offset,
+			    le32_to_cpu(hdr->ucode_size_bytes));
 		size -= le32_to_cpu(hdr->ucode_size_bytes);
 		ptr += le32_to_cpu(hdr->ucode_size_bytes);
-                memset_io(ptr, 0, size);
+		memset_io(ptr, 0, size);
 	}
 
 	return 0;
@@ -954,25 +955,25 @@ static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,
 	ib->ptr[3] = addr >> 32;
 	ib->ptr[4] = PACKET0(mmUVD_GPCOM_VCPU_CMD, 0);
 	ib->ptr[5] = 0;
-        for (i = 6; i < 16; i += 2) {
-                ib->ptr[i] = PACKET0(mmUVD_NO_OP, 0);
-                ib->ptr[i+1] = 0;
-        }
+	for (i = 6; i < 16; i += 2) {
+		ib->ptr[i] = PACKET0(mmUVD_NO_OP, 0);
+		ib->ptr[i+1] = 0;
+	}
 	ib->length_dw = 16;
 
-        if (direct) {
-                r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
-                job->fence = fence_get(f);
-                if (r)
-                        goto err_free;
- 
-                amdgpu_job_free(job);
-        } else {
-                r = amdgpu_job_submit(job, ring, &adev->uvd.entity,
-                                      AMDGPU_FENCE_OWNER_UNDEFINED, &f);
-                if (r)
-                        goto err_free;
-        }
+	if (direct) {
+		r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
+		job->fence = fence_get(f);
+		if (r)
+			goto err_free;
+
+		amdgpu_job_free(job);
+	} else {
+		r = amdgpu_job_submit(job, ring, &adev->uvd.entity,
+				      AMDGPU_FENCE_OWNER_UNDEFINED, &f);
+		if (r)
+			goto err_free;
+	}
 
 	ttm_eu_fence_buffer_objects(&ticket, &head, f);
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c
index 06f1e90..56babb8 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c
@@ -51,7 +51,7 @@
 #define FIRMWARE_FIJI		"amdgpu/fiji_vce.bin"
 #define FIRMWARE_STONEY		"amdgpu/stoney_vce.bin"
 #define FIRMWARE_POLARIS10	"amdgpu/polaris10_vce.bin"
-#define FIRMWARE_POLARIS11	   "amdgpu/polaris11_vce.bin"
+#define FIRMWARE_POLARIS11         "amdgpu/polaris11_vce.bin"
 
 #ifdef CONFIG_DRM_AMDGPU_CIK
 MODULE_FIRMWARE(FIRMWARE_BONAIRE);
@@ -406,10 +406,10 @@ int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 	int i, r;
 
 	r = amdgpu_job_alloc_with_ib(ring->adev, ib_size_dw * 4, &job);
-	if (r) 
+	if (r)
 		return r;
 
-	ib = &job->ibs[0];	
+	ib = &job->ibs[0];
 
 	dummy = ib->gpu_addr + 1024;
 
@@ -451,10 +451,10 @@ int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 		ib->ptr[i] = 0x0;
 
 	r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
-        job->fence = fence_get(f);
+	job->fence = fence_get(f);
 	if (r)
 		goto err;
-       
+
 	amdgpu_job_free(job);
 	if (fence)
 		*fence = fence_get(f);
@@ -485,10 +485,10 @@ int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
 	struct fence *f = NULL;
 	int i, r;
 
-	r = amdgpu_job_alloc_with_ib(ring->adev, ib_size_dw * 4, &job);	
-	if (r) 
+	r = amdgpu_job_alloc_with_ib(ring->adev, ib_size_dw * 4, &job);
+	if (r)
 		return r;
-	
+
 	ib = &job->ibs[0];
 
 	/* stitch together an VCE destroy msg */
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index 7027620..d05546e 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *	    Alex Deucher
- *	    Jerome Glisse
+ *          Alex Deucher
+ *          Jerome Glisse
  */
 #include <linux/fence-array.h>
 #include <drm/drmP.h>
@@ -95,6 +95,7 @@ static unsigned amdgpu_vm_directory_size(struct amdgpu_device *adev)
 
 /**
  * amdgpu_vm_get_pd_bo - add the VM PD to a validation list
+ *
  * @vm: vm providing the BOs
  * @validated: head of validation list
  * @entry: entry to add
@@ -128,16 +129,16 @@ int amdgpu_vm_validate_pt_bos(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 			      int (*validate)(void *p, struct amdgpu_bo *bo),
 			      void *param)
 {
-        uint64_t num_evictions;
+	uint64_t num_evictions;
 	unsigned i;
 	int r;
- 
-        /* We only need to validate the page tables
-         * if they aren't already valid.
-         */
-        num_evictions = atomic64_read(&adev->num_evictions);
-        if (num_evictions == vm->last_eviction_counter)
-                return 0;
+
+	/* We only need to validate the page tables
+	 * if they aren't already valid.
+	 */
+	num_evictions = atomic64_read(&adev->num_evictions);
+	if (num_evictions == vm->last_eviction_counter)
+		return 0;
 
 	/* add the vm page table to the list */
 	for (i = 0; i <= vm->max_pde_used; ++i) {
@@ -149,8 +150,8 @@ int amdgpu_vm_validate_pt_bos(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 		r = validate(param, bo);
 		if (r)
 			return r;
-	} 
-	
+	}
+
 	return 0;
 }
 
@@ -167,15 +168,15 @@ void amdgpu_vm_move_pt_bos_in_lru(struct amdgpu_device *adev,
 {
 	struct ttm_bo_global *glob = adev->mman.bdev.glob;
 	unsigned i;
- 
+
 	spin_lock(&glob->lru_lock);
 	for (i = 0; i <= vm->max_pde_used; ++i) {
-                struct amdgpu_bo *bo = vm->page_tables[i].bo; 
-		
-                if (!bo)
+		struct amdgpu_bo *bo = vm->page_tables[i].bo;
+
+		if (!bo)
 			continue;
- 
-                ttm_bo_move_to_lru_tail(&bo->tbo);
+
+		ttm_bo_move_to_lru_tail(&bo->tbo);
 	}
 	spin_unlock(&glob->lru_lock);
 }
@@ -199,10 +200,10 @@ static bool amdgpu_vm_is_gpu_reset(struct amdgpu_device *adev,
  */
 int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 		      struct amdgpu_sync *sync, struct fence *fence,
-                      struct amdgpu_job *job)
+		      struct amdgpu_job *job)
 {
 	struct amdgpu_device *adev = ring->adev;
-        uint64_t fence_context = adev->fence_context + ring->idx;
+	uint64_t fence_context = adev->fence_context + ring->idx;
 	struct fence *updates = sync->last_vm_update;
 	struct amdgpu_vm_id *id, *idle;
 	struct fence **fences;
@@ -256,7 +257,8 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 
 	}
 	kfree(fences);
-        job->vm_needs_flush = true;
+
+	job->vm_needs_flush = true;
 	/* Check if we can use a VMID already assigned to this VM */
 	i = ring->idx;
 	do {
@@ -275,14 +277,14 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 		if (atomic64_read(&id->owner) != vm->client_id)
 			continue;
 
-                if (job->vm_pd_addr != id->pd_gpu_addr)
+		if (job->vm_pd_addr != id->pd_gpu_addr)
+			continue;
+
+		if (!id->last_flush)
 			continue;
 
-                if (!id->last_flush)
-                        continue;
- 
-                if (id->last_flush->context != fence_context &&
-                    !fence_is_signaled(id->last_flush))
+		if (id->last_flush->context != fence_context &&
+		    !fence_is_signaled(id->last_flush))
 			continue;
 
 		flushed  = id->flushed_updates;
@@ -301,9 +303,9 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 		list_move_tail(&id->list, &adev->vm_manager.ids_lru);
 		vm->ids[ring->idx] = id;
 
-                job->vm_id = id - adev->vm_manager.ids;
-                job->vm_needs_flush = false;
-                trace_amdgpu_vm_grab_id(vm, ring->idx, job);
+		job->vm_id = id - adev->vm_manager.ids;
+		job->vm_needs_flush = false;
+		trace_amdgpu_vm_grab_id(vm, ring->idx, job);
 
 		mutex_unlock(&adev->vm_manager.lock);
 		return 0;
@@ -327,14 +329,14 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 	fence_put(id->flushed_updates);
 	id->flushed_updates = fence_get(updates);
 
-        id->pd_gpu_addr = job->vm_pd_addr;
+	id->pd_gpu_addr = job->vm_pd_addr;
 	id->current_gpu_reset_count = atomic_read(&adev->gpu_reset_counter);
 	list_move_tail(&id->list, &adev->vm_manager.ids_lru);
 	atomic64_set(&id->owner, vm->client_id);
 	vm->ids[ring->idx] = id;
 
-        job->vm_id = id - adev->vm_manager.ids;
-        trace_amdgpu_vm_grab_id(vm, ring->idx, job);
+	job->vm_id = id - adev->vm_manager.ids;
+	trace_amdgpu_vm_grab_id(vm, ring->idx, job);
 
 error:
 	mutex_unlock(&adev->vm_manager.lock);
@@ -379,51 +381,49 @@ static bool amdgpu_vm_ring_has_compute_vm_bug(struct amdgpu_ring *ring)
 int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job)
 {
 	struct amdgpu_device *adev = ring->adev;
-        struct amdgpu_vm_id *id = &adev->vm_manager.ids[job->vm_id];
+	struct amdgpu_vm_id *id = &adev->vm_manager.ids[job->vm_id];
 	bool gds_switch_needed = ring->funcs->emit_gds_switch && (
-                id->gds_base != job->gds_base ||
-                id->gds_size != job->gds_size ||
-                id->gws_base != job->gws_base ||
-                id->gws_size != job->gws_size ||
-                id->oa_base != job->oa_base ||
-                id->oa_size != job->oa_size);
-
+		id->gds_base != job->gds_base ||
+		id->gds_size != job->gds_size ||
+		id->gws_base != job->gws_base ||
+		id->gws_size != job->gws_size ||
+		id->oa_base != job->oa_base ||
+		id->oa_size != job->oa_size);
 	int r;
 
 	if (ring->funcs->emit_pipeline_sync && (
-            job->vm_needs_flush || gds_switch_needed ||
-            amdgpu_vm_ring_has_compute_vm_bug(ring)))
-	       amdgpu_ring_emit_pipeline_sync(ring);
-
-        if (ring->funcs->emit_vm_flush && (job->vm_needs_flush ||
-            amdgpu_vm_is_gpu_reset(adev, id))) {
+	    job->vm_needs_flush || gds_switch_needed ||
+	    amdgpu_vm_ring_has_compute_vm_bug(ring)))
+		amdgpu_ring_emit_pipeline_sync(ring);
 
+	if (ring->funcs->emit_vm_flush && (job->vm_needs_flush ||
+	    amdgpu_vm_is_gpu_reset(adev, id))) {
 		struct fence *fence;
 
-                trace_amdgpu_vm_flush(job->vm_pd_addr, ring->idx, job->vm_id);
-                amdgpu_ring_emit_vm_flush(ring, job->vm_id, job->vm_pd_addr); 
-                
+		trace_amdgpu_vm_flush(job->vm_pd_addr, ring->idx, job->vm_id);
+		amdgpu_ring_emit_vm_flush(ring, job->vm_id, job->vm_pd_addr);
+
 		r = amdgpu_fence_emit(ring, &fence);
-                if (r)
-                        return r;
-  
+		if (r)
+			return r;
+
 		mutex_lock(&adev->vm_manager.lock);
-                fence_put(id->last_flush);
-                id->last_flush = fence;
+		fence_put(id->last_flush);
+		id->last_flush = fence;
 		mutex_unlock(&adev->vm_manager.lock);
 	}
 
 	if (gds_switch_needed) {
-                id->gds_base = job->gds_base;
-                id->gds_size = job->gds_size;
-                id->gws_base = job->gws_base;
-                id->gws_size = job->gws_size;
-                id->oa_base = job->oa_base;
-                id->oa_size = job->oa_size;
-                amdgpu_ring_emit_gds_switch(ring, job->vm_id,
-                                            job->gds_base, job->gds_size,
-                                            job->gws_base, job->gws_size,
-                                            job->oa_base, job->oa_size);
+		id->gds_base = job->gds_base;
+		id->gds_size = job->gds_size;
+		id->gws_base = job->gws_base;
+		id->gws_size = job->gws_size;
+		id->oa_base = job->oa_base;
+		id->oa_size = job->oa_size;
+		amdgpu_ring_emit_gds_switch(ring, job->vm_id,
+					    job->gds_base, job->gds_size,
+					    job->gws_base, job->gws_size,
+					    job->oa_base, job->oa_size);
 	}
 
 	return 0;
@@ -553,134 +553,148 @@ static uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
 	return result;
 }
 
+/*
+ * amdgpu_vm_update_pdes - make sure that page directory is valid
+ *
+ * @adev: amdgpu_device pointer
+ * @vm: requested vm
+ * @start: start of GPU address range
+ * @end: end of GPU address range
+ *
+ * Allocates new page tables if necessary
+ * and updates the page directory.
+ * Returns 0 for success, error for failure.
+ */
 int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 				    struct amdgpu_vm *vm)
 {
 	struct amdgpu_bo *shadow;
-        struct amdgpu_ring *ring;
-        uint64_t pd_addr, shadow_addr;
-        uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
-        uint64_t last_pde = ~0, last_pt = ~0 ,last_shadow = ~0;
-        unsigned count = 0, pt_idx, ndw;
-        struct amdgpu_job *job;
-        struct amdgpu_pte_update_params params;
-        struct fence *fence = NULL;
-
-        int r;
-
-        ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
-        shadow = vm->page_directory->shadow;
-
-        /* padding, etc. */
-        ndw = 64;
-
-        /* assume the worst case */
-        ndw += vm->max_pde_used * 6;
- 
-        pd_addr = amdgpu_bo_gpu_offset(vm->page_directory);
-        if (shadow) {
-                r = amdgpu_ttm_bind(&shadow->tbo, &shadow->tbo.mem);
-                if (r)
-                        return r;
-                shadow_addr = amdgpu_bo_gpu_offset(shadow);
-                ndw *= 2;
-        } else {
-                shadow_addr = 0;
-        }
- 
-        r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
-        if (r)
-                return r;
-
-        memset(&params, 0, sizeof(params));
-        params.adev = adev;
-        params.ib = &job->ibs[0];
-
-        /* walk over the address space and update the page directory */
-        for (pt_idx = 0; pt_idx <= vm->max_pde_used; ++pt_idx) {
-                struct amdgpu_bo *bo = vm->page_tables[pt_idx].bo;
-                uint64_t pde, pt;
-
-                if (bo == NULL)
-                        continue;
-
-                if (bo->shadow) {
-                        struct amdgpu_bo *pt_shadow = bo->shadow;
-
-                        r = amdgpu_ttm_bind(&pt_shadow->tbo,
-                                            &pt_shadow->tbo.mem);
-                        if (r)
-                                return r;
-                }
-	
+	struct amdgpu_ring *ring;
+	uint64_t pd_addr, shadow_addr;
+	uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
+	uint64_t last_pde = ~0, last_pt = ~0, last_shadow = ~0;
+	unsigned count = 0, pt_idx, ndw;
+	struct amdgpu_job *job;
+	struct amdgpu_pte_update_params params;
+	struct fence *fence = NULL;
+
+	int r;
+
+	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
+	shadow = vm->page_directory->shadow;
+
+	/* padding, etc. */
+	ndw = 64;
+
+	/* assume the worst case */
+	ndw += vm->max_pde_used * 6;
+
+	pd_addr = amdgpu_bo_gpu_offset(vm->page_directory);
+	if (shadow) {
+		r = amdgpu_ttm_bind(&shadow->tbo, &shadow->tbo.mem);
+		if (r)
+			return r;
+		shadow_addr = amdgpu_bo_gpu_offset(shadow);
+		ndw *= 2;
+	} else {
+		shadow_addr = 0;
+	}
+
+	r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
+	if (r)
+		return r;
+
+	memset(&params, 0, sizeof(params));
+	params.adev = adev;
+	params.ib = &job->ibs[0];
+
+	/* walk over the address space and update the page directory */
+	for (pt_idx = 0; pt_idx <= vm->max_pde_used; ++pt_idx) {
+		struct amdgpu_bo *bo = vm->page_tables[pt_idx].bo;
+		uint64_t pde, pt;
+
+		if (bo == NULL)
+			continue;
+
+		if (bo->shadow) {
+			struct amdgpu_bo *pt_shadow = bo->shadow;
+
+			r = amdgpu_ttm_bind(&pt_shadow->tbo,
+					    &pt_shadow->tbo.mem);
+			if (r)
+				return r;
+		}
+
 		pt = amdgpu_bo_gpu_offset(bo);
-                if (vm->page_tables[pt_idx].addr == pt)
-                        continue;
- 
-                vm->page_tables[pt_idx].addr = pt;
-
-                pde = pd_addr + pt_idx * 8;
-                if (((last_pde + 8 * count) != pde) ||
-                    ((last_pt + incr * count) != pt) ||
-                    (count == AMDGPU_VM_MAX_UPDATE_SIZE)) {
-
-                        if (count) {
-                                if (shadow)
-                                        amdgpu_vm_do_set_ptes(&params,
-                                                              last_shadow,
-                                                              last_pt, count,
-                                                              incr,
-                                                              AMDGPU_PTE_VALID);
-                                amdgpu_vm_do_set_ptes(&params, last_pde,
-                                                      last_pt, count, incr,
-                                                      AMDGPU_PTE_VALID);
-                        }
-
-                        count = 1;
-                        last_pde = pde;
-                        last_shadow = shadow_addr + pt_idx * 8;
-                        last_pt = pt;
-                } else {
-                        ++count;
-                }
-        }
-
-        if (count) {
-                if (vm->page_directory->shadow)
-                        amdgpu_vm_do_set_ptes(&params, last_shadow, last_pt,
-                                              count, incr, AMDGPU_PTE_VALID);
-
-                amdgpu_vm_do_set_ptes(&params, last_pde, last_pt,
-                                      count, incr, AMDGPU_PTE_VALID);
-        }
- 
-        if (params.ib->length_dw == 0) {
-                amdgpu_job_free(job);
-                return 0;
-        }
- 
-        amdgpu_ring_pad_ib(ring, params.ib);
-        amdgpu_sync_resv(adev, &job->sync, vm->page_directory->tbo.resv,
-                                 AMDGPU_FENCE_OWNER_VM);
-        if (shadow)
-                amdgpu_sync_resv(adev, &job->sync, shadow->tbo.resv,
-                                AMDGPU_FENCE_OWNER_VM);
-        WARN_ON(params.ib->length_dw > ndw);
-        r = amdgpu_job_submit(job, ring, &vm->entity,
-                              AMDGPU_FENCE_OWNER_VM, &fence);
-        if (r)
-                goto error_free;
-
-        amdgpu_bo_fence(vm->page_directory, fence, true);
-        fence_put(vm->page_directory_fence);
-        vm->page_directory_fence = fence_get(fence);
-        fence_put(fence);
-
-        return 0;
+		if (vm->page_tables[pt_idx].addr == pt)
+			continue;
+
+		vm->page_tables[pt_idx].addr = pt;
+
+		pde = pd_addr + pt_idx * 8;
+		if (((last_pde + 8 * count) != pde) ||
+		    ((last_pt + incr * count) != pt) ||
+		    (count == AMDGPU_VM_MAX_UPDATE_SIZE)) {
+
+			if (count) {
+				if (shadow)
+					amdgpu_vm_do_set_ptes(&params,
+							      last_shadow,
+							      last_pt, count,
+							      incr,
+							      AMDGPU_PTE_VALID);
+
+				amdgpu_vm_do_set_ptes(&params, last_pde,
+						      last_pt, count, incr,
+						      AMDGPU_PTE_VALID);
+			}
+
+			count = 1;
+			last_pde = pde;
+			last_shadow = shadow_addr + pt_idx * 8;
+			last_pt = pt;
+		} else {
+			++count;
+		}
+	}
+
+	if (count) {
+		if (vm->page_directory->shadow)
+			amdgpu_vm_do_set_ptes(&params, last_shadow, last_pt,
+					      count, incr, AMDGPU_PTE_VALID);
+
+		amdgpu_vm_do_set_ptes(&params, last_pde, last_pt,
+				      count, incr, AMDGPU_PTE_VALID);
+	}
+
+	if (params.ib->length_dw == 0) {
+		amdgpu_job_free(job);
+		return 0;
+	}
+
+	amdgpu_ring_pad_ib(ring, params.ib);
+	amdgpu_sync_resv(adev, &job->sync, vm->page_directory->tbo.resv,
+			 AMDGPU_FENCE_OWNER_VM);
+	if (shadow)
+		amdgpu_sync_resv(adev, &job->sync, shadow->tbo.resv,
+				 AMDGPU_FENCE_OWNER_VM);
+
+	WARN_ON(params.ib->length_dw > ndw);
+	r = amdgpu_job_submit(job, ring, &vm->entity,
+			      AMDGPU_FENCE_OWNER_VM, &fence);
+	if (r)
+		goto error_free;
+
+	amdgpu_bo_fence(vm->page_directory, fence, true);
+	fence_put(vm->page_directory_fence);
+	vm->page_directory_fence = fence_get(fence);
+	fence_put(fence);
+
+	return 0;
 
 error_free:
-        amdgpu_job_free(job);
-        return r;
+	amdgpu_job_free(job);
+	return r;
 }
 
 /**
@@ -701,143 +715,145 @@ static void amdgpu_vm_update_ptes(struct amdgpu_pte_update_params *params,
 				  uint64_t dst, uint32_t flags)
 {
 	const uint64_t mask = AMDGPU_VM_PTE_COUNT - 1;
- 
-        uint64_t cur_pe_start, cur_nptes, cur_dst;
-        uint64_t addr; /* next GPU address to be updated */
-        uint64_t pt_idx;
-        struct amdgpu_bo *pt;
-        unsigned nptes; /* next number of ptes to be updated */
-        uint64_t next_pe_start;
- 
-        /* initialize the variables */
-        addr = start;
-        pt_idx = addr >> amdgpu_vm_block_size;
-        pt = vm->page_tables[pt_idx].bo;
-        if (params->shadow) {
-                if (!pt->shadow)
-                        return;
-                pt = pt->shadow;
-        } 
-        if ((addr & ~mask) == (end & ~mask))
-                nptes = end - addr;
-        else
-                nptes = AMDGPU_VM_PTE_COUNT - (addr & mask);
- 
-        cur_pe_start = amdgpu_bo_gpu_offset(pt);
-        cur_pe_start += (addr & mask) * 8;
-        cur_nptes = nptes;
-        cur_dst = dst;
- 
-        /* for next ptb*/
-        addr += nptes;
-        dst += nptes * AMDGPU_GPU_PAGE_SIZE;
+
+	uint64_t cur_pe_start, cur_nptes, cur_dst;
+	uint64_t addr; /* next GPU address to be updated */
+	uint64_t pt_idx;
+	struct amdgpu_bo *pt;
+	unsigned nptes; /* next number of ptes to be updated */
+	uint64_t next_pe_start;
+
+	/* initialize the variables */
+	addr = start;
+	pt_idx = addr >> amdgpu_vm_block_size;
+	pt = vm->page_tables[pt_idx].bo;
+	if (params->shadow) {
+		if (!pt->shadow)
+			return;
+		pt = pt->shadow;
+	}
+	if ((addr & ~mask) == (end & ~mask))
+		nptes = end - addr;
+	else
+		nptes = AMDGPU_VM_PTE_COUNT - (addr & mask);
+
+	cur_pe_start = amdgpu_bo_gpu_offset(pt);
+	cur_pe_start += (addr & mask) * 8;
+	cur_nptes = nptes;
+	cur_dst = dst;
+
+	/* for next ptb*/
+	addr += nptes;
+	dst += nptes * AMDGPU_GPU_PAGE_SIZE;
 
 	/* walk over the address space and update the page tables */
-        while (addr < end) {
-                pt_idx = addr >> amdgpu_vm_block_size;
-                pt = vm->page_tables[pt_idx].bo;
-                if (params->shadow) {
-                        if (!pt->shadow)
-                                return;
-                        pt = pt->shadow;
-                }
+	while (addr < end) {
+		pt_idx = addr >> amdgpu_vm_block_size;
+		pt = vm->page_tables[pt_idx].bo;
+		if (params->shadow) {
+			if (!pt->shadow)
+				return;
+			pt = pt->shadow;
+		}
 
 		if ((addr & ~mask) == (end & ~mask))
 			nptes = end - addr;
 		else
 			nptes = AMDGPU_VM_PTE_COUNT - (addr & mask);
 
-                next_pe_start = amdgpu_bo_gpu_offset(pt);
-                next_pe_start += (addr & mask) * 8;	
- 
-                if ((cur_pe_start + 8 * cur_nptes) == next_pe_start &&
-                    ((cur_nptes + nptes) <= AMDGPU_VM_MAX_UPDATE_SIZE)) {
-                        /* The next ptb is consecutive to current ptb.
-                         * Don't call the update function now.
-                         * Will update two ptbs together in future.
-                        */
-                        cur_nptes += nptes;
-                } else {
-                        params->func(params, cur_pe_start, cur_dst, cur_nptes,
-                                     AMDGPU_GPU_PAGE_SIZE, flags);
-                        cur_pe_start = next_pe_start;
-                        cur_nptes = nptes;
-                        cur_dst = dst;
+		next_pe_start = amdgpu_bo_gpu_offset(pt);
+		next_pe_start += (addr & mask) * 8;
+
+		if ((cur_pe_start + 8 * cur_nptes) == next_pe_start &&
+		    ((cur_nptes + nptes) <= AMDGPU_VM_MAX_UPDATE_SIZE)) {
+			/* The next ptb is consecutive to current ptb.
+			 * Don't call the update function now.
+			 * Will update two ptbs together in future.
+			*/
+			cur_nptes += nptes;
+		} else {
+			params->func(params, cur_pe_start, cur_dst, cur_nptes,
+				     AMDGPU_GPU_PAGE_SIZE, flags);
+
+			cur_pe_start = next_pe_start;
+			cur_nptes = nptes;
+			cur_dst = dst;
 		}
-                /* for next ptb*/
+
+		/* for next ptb*/
 		addr += nptes;
 		dst += nptes * AMDGPU_GPU_PAGE_SIZE;
 	}
 
-        params->func(params, cur_pe_start, cur_dst, cur_nptes,
-                     AMDGPU_GPU_PAGE_SIZE, flags);
+	params->func(params, cur_pe_start, cur_dst, cur_nptes,
+		     AMDGPU_GPU_PAGE_SIZE, flags);
 }
- 
- /*
-  * amdgpu_vm_frag_ptes - add fragment information to PTEs
-  *
-  * @params: see amdgpu_pte_update_params definition
-  * @vm: requested vm
-  * @start: first PTE to handle
-  * @end: last PTE to handle
-  * @dst: addr those PTEs should point to
-  * @flags: hw mapping flags
-  */
-static void amdgpu_vm_frag_ptes(struct amdgpu_pte_update_params        *params,
-                                struct amdgpu_vm *vm,
-                                uint64_t start, uint64_t end,
-                                uint64_t dst, uint32_t flags)
+
+/*
+ * amdgpu_vm_frag_ptes - add fragment information to PTEs
+ *
+ * @params: see amdgpu_pte_update_params definition
+ * @vm: requested vm
+ * @start: first PTE to handle
+ * @end: last PTE to handle
+ * @dst: addr those PTEs should point to
+ * @flags: hw mapping flags
+ */
+static void amdgpu_vm_frag_ptes(struct amdgpu_pte_update_params	*params,
+				struct amdgpu_vm *vm,
+				uint64_t start, uint64_t end,
+				uint64_t dst, uint32_t flags)
 {
-        /**
-         * The MC L1 TLB supports variable sized pages, based on a fragment
-         * field in the PTE. When this field is set to a non-zero value, page
-         * granularity is increased from 4KB to (1 << (12 + frag)). The PTE
-         * flags are considered valid for all PTEs within the fragment range
-         * and corresponding mappings are assumed to be physically contiguous.
-         *
-         * The L1 TLB can store a single PTE for the whole fragment,
-         * significantly increasing the space available for translation
-         * caching. This leads to large improvements in throughput when the
-         * TLB is under pressure.
-         *
-         * The L2 TLB distributes small and large fragments into two
-         * asymmetric partitions. The large fragment cache is significantly
-         * larger. Thus, we try to use large fragments wherever possible.
-         * Userspace can support this by aligning virtual base address and
-         * allocation size to the fragment size.
-         */
- 
-        /* SI and newer are optimized for 64KB */
-        uint64_t frag_flags = AMDGPU_PTE_FRAG(AMDGPU_LOG2_PAGES_PER_FRAG);
-        uint64_t frag_align = 1 << AMDGPU_LOG2_PAGES_PER_FRAG;
-       
+	/**
+	 * The MC L1 TLB supports variable sized pages, based on a fragment
+	 * field in the PTE. When this field is set to a non-zero value, page
+	 * granularity is increased from 4KB to (1 << (12 + frag)). The PTE
+	 * flags are considered valid for all PTEs within the fragment range
+	 * and corresponding mappings are assumed to be physically contiguous.
+	 *
+	 * The L1 TLB can store a single PTE for the whole fragment,
+	 * significantly increasing the space available for translation
+	 * caching. This leads to large improvements in throughput when the
+	 * TLB is under pressure.
+	 *
+	 * The L2 TLB distributes small and large fragments into two
+	 * asymmetric partitions. The large fragment cache is significantly
+	 * larger. Thus, we try to use large fragments wherever possible.
+	 * Userspace can support this by aligning virtual base address and
+	 * allocation size to the fragment size.
+	 */
+
+	/* SI and newer are optimized for 64KB */
+	uint64_t frag_flags = AMDGPU_PTE_FRAG(AMDGPU_LOG2_PAGES_PER_FRAG);
+	uint64_t frag_align = 1 << AMDGPU_LOG2_PAGES_PER_FRAG;
+
 	uint64_t frag_start = ALIGN(start, frag_align);
 	uint64_t frag_end = end & ~(frag_align - 1);
- 
-        /* system pages are non continuously */
-        if (params->src || !(flags & AMDGPU_PTE_VALID) ||
-            (frag_start >= frag_end)) {
- 
-                amdgpu_vm_update_ptes(params, vm, start, end, dst, flags);
-                return;
-        }
- 
-        /* handle the 4K area at the beginning */
-        if (start != frag_start) {
-                amdgpu_vm_update_ptes(params, vm, start, frag_start,
-                                      dst, flags);
-                dst += (frag_start - start) * AMDGPU_GPU_PAGE_SIZE;
-        }
- 
-        /* handle the area in the middle */
-        amdgpu_vm_update_ptes(params, vm, frag_start, frag_end, dst,
-                              flags | frag_flags);
- 
-        /* handle the 4K area at the end */
-        if (frag_end != end) {
-                dst += (frag_end - frag_start) * AMDGPU_GPU_PAGE_SIZE;
-                amdgpu_vm_update_ptes(params, vm, frag_end, end, dst, flags);
-        }
+
+	/* system pages are non continuously */
+	if (params->src || !(flags & AMDGPU_PTE_VALID) ||
+	    (frag_start >= frag_end)) {
+
+		amdgpu_vm_update_ptes(params, vm, start, end, dst, flags);
+		return;
+	}
+
+	/* handle the 4K area at the beginning */
+	if (start != frag_start) {
+		amdgpu_vm_update_ptes(params, vm, start, frag_start,
+				      dst, flags);
+		dst += (frag_start - start) * AMDGPU_GPU_PAGE_SIZE;
+	}
+
+	/* handle the area in the middle */
+	amdgpu_vm_update_ptes(params, vm, frag_start, frag_end, dst,
+			      flags | frag_flags);
+
+	/* handle the 4K area at the end */
+	if (frag_end != end) {
+		dst += (frag_end - frag_start) * AMDGPU_GPU_PAGE_SIZE;
+		amdgpu_vm_update_ptes(params, vm, frag_end, end, dst, flags);
+	}
 }
 
 /**
@@ -870,24 +886,24 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 	void *owner = AMDGPU_FENCE_OWNER_VM;
 	unsigned nptes, ncmds, ndw;
 	struct amdgpu_job *job;
-        struct amdgpu_pte_update_params params;
+	struct amdgpu_pte_update_params params;
 	struct fence *f = NULL;
 	int r;
- 
-        memset(&params, 0, sizeof(params));
-        params.adev = adev;
-        params.src = src;
+
+	memset(&params, 0, sizeof(params));
+	params.adev = adev;
+	params.src = src;
 
 	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
 
-        memset(&params, 0, sizeof(params));
-        params.adev = adev;
-        params.src = src;
+	memset(&params, 0, sizeof(params));
+	params.adev = adev;
+	params.src = src;
 
 	/* sync to everything on unmapping */
 	if (!(flags & AMDGPU_PTE_VALID))
 		owner = AMDGPU_FENCE_OWNER_UNDEFINED;
- 
+
 	nptes = last - start + 1;
 
 	/*
@@ -899,20 +915,20 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 	/* padding, etc. */
 	ndw = 64;
 
-        if (src) {
+	if (src) {
 		/* only copy commands needed */
 		ndw += ncmds * 7;
- 
-                params.func = amdgpu_vm_do_copy_ptes;
 
-        } else if (pages_addr) {
-                /* copy commands needed */
-                ndw += ncmds * 7;
-                
+		params.func = amdgpu_vm_do_copy_ptes;
+
+	} else if (pages_addr) {
+		/* copy commands needed */
+		ndw += ncmds * 7;
+
 		/* and also PTEs */
 		ndw += nptes * 2;
- 
-                params.func = amdgpu_vm_do_copy_ptes;
+
+		params.func = amdgpu_vm_do_copy_ptes;
 
 	} else {
 		/* set page commands needed */
@@ -920,37 +936,37 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 
 		/* two extra commands for begin/end of fragment */
 		ndw += 2 * 10;
- 
-                params.func = amdgpu_vm_do_set_ptes;
+
+		params.func = amdgpu_vm_do_set_ptes;
 	}
+
 	r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
 	if (r)
-
 		return r;
 
-        params.ib = &job->ibs[0];
- 
-        if (!src && pages_addr) {
-                uint64_t *pte;
-                unsigned i;
- 
-                /* Put the PTEs at the end of the IB. */
-                i = ndw - nptes * 2;
-                pte= (uint64_t *)&(job->ibs->ptr[i]);
-                params.src = job->ibs->gpu_addr + i * 4;
- 
-                for (i = 0; i < nptes; ++i) {
-                        pte[i] = amdgpu_vm_map_gart(pages_addr, addr + i *
-                                                    AMDGPU_GPU_PAGE_SIZE);
-                        pte[i] |= flags;
-                }
+	params.ib = &job->ibs[0];
+
+	if (!src && pages_addr) {
+		uint64_t *pte;
+		unsigned i;
+
+		/* Put the PTEs at the end of the IB. */
+		i = ndw - nptes * 2;
+		pte= (uint64_t *)&(job->ibs->ptr[i]);
+		params.src = job->ibs->gpu_addr + i * 4;
+
+		for (i = 0; i < nptes; ++i) {
+			pte[i] = amdgpu_vm_map_gart(pages_addr, addr + i *
+						    AMDGPU_GPU_PAGE_SIZE);
+			pte[i] |= flags;
+		}
 		addr = 0;
-        }
- 
-        r = amdgpu_sync_fence(adev, &job->sync, exclusive);
-        if (r)
-                goto error_free;
- 
+	}
+
+	r = amdgpu_sync_fence(adev, &job->sync, exclusive);
+	if (r)
+		goto error_free;
+
 	r = amdgpu_sync_resv(adev, &job->sync, vm->page_directory->tbo.resv,
 			     owner);
 	if (r)
@@ -959,14 +975,14 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 	r = reservation_object_reserve_shared(vm->page_directory->tbo.resv);
 	if (r)
 		goto error_free;
- 
-        params.shadow = true;
-        amdgpu_vm_frag_ptes(&params, vm, start, last + 1, addr, flags);
-        params.shadow = false; 
-        amdgpu_vm_frag_ptes(&params, vm, start, last + 1, addr, flags);
-        
+
+	params.shadow = true;
+	amdgpu_vm_frag_ptes(&params, vm, start, last + 1, addr, flags);
+	params.shadow = false;
+	amdgpu_vm_frag_ptes(&params, vm, start, last + 1, addr, flags);
+
 	amdgpu_ring_pad_ib(ring, params.ib);
-        WARN_ON(params.ib->length_dw > ndw);
+	WARN_ON(params.ib->length_dw > ndw);
 	r = amdgpu_job_submit(job, ring, &vm->entity,
 			      AMDGPU_FENCE_OWNER_VM, &f);
 	if (r)
@@ -1170,20 +1186,21 @@ int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
 {
 	struct amdgpu_bo_va_mapping *mapping;
 	int r;
- 
+
 	while (!list_empty(&vm->freed)) {
 		mapping = list_first_entry(&vm->freed,
 			struct amdgpu_bo_va_mapping, list);
 		list_del(&mapping->list);
-                r = amdgpu_vm_bo_split_mapping(adev, NULL, 0, NULL, vm, mapping,
+
+		r = amdgpu_vm_bo_split_mapping(adev, NULL, 0, NULL, vm, mapping,
 					       0, 0, NULL);
 		kfree(mapping);
 		if (r)
 			return r;
- 
-	}
 
+	}
 	return 0;
+
 }
 
 /**
@@ -1345,8 +1362,8 @@ int amdgpu_vm_bo_map(struct amdgpu_device *adev,
 	for (pt_idx = saddr; pt_idx <= eaddr; ++pt_idx) {
 		struct reservation_object *resv = vm->page_directory->tbo.resv;
 		struct amdgpu_bo *pt;
- 
-                if (vm->page_tables[pt_idx].bo)
+
+		if (vm->page_tables[pt_idx].bo)
 			continue;
 
 		r = amdgpu_bo_create(adev, AMDGPU_VM_PTE_COUNT * 8,
@@ -1365,7 +1382,7 @@ int amdgpu_vm_bo_map(struct amdgpu_device *adev,
 		 */
 		pt->parent = amdgpu_bo_ref(vm->page_directory);
 
-                vm->page_tables[pt_idx].bo = pt;
+		vm->page_tables[pt_idx].bo = pt;
 		vm->page_tables[pt_idx].addr = 0;
 	}
 
@@ -1424,11 +1441,11 @@ int amdgpu_vm_bo_unmap(struct amdgpu_device *adev,
 	interval_tree_remove(&mapping->it, &vm->va);
 	trace_amdgpu_vm_bo_unmap(bo_va, mapping);
 
-	if (valid) 
+	if (valid)
 		list_add(&mapping->list, &vm->freed);
-        else 
+	else
 		kfree(mapping);
-	
+
 	return 0;
 }
 
@@ -1605,7 +1622,7 @@ void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 	}
 
 	for (i = 0; i < amdgpu_vm_num_pdes(adev); i++) {
-                struct amdgpu_bo *pt = vm->page_tables[i].bo;
+		struct amdgpu_bo *pt = vm->page_tables[i].bo;
 
 		if (!pt)
 			continue;
@@ -1640,11 +1657,12 @@ void amdgpu_vm_manager_init(struct amdgpu_device *adev)
 		list_add_tail(&adev->vm_manager.ids[i].list,
 			      &adev->vm_manager.ids_lru);
 	}
+
 	adev->vm_manager.fence_context = fence_context_alloc(AMDGPU_MAX_RINGS);
-        for (i = 0; i < AMDGPU_MAX_RINGS; ++i)
-                adev->vm_manager.seqno[i] = 0;
-	
-	atomic_set(&adev->vm_manager.vm_pte_next_ring, 0);	
+	for (i = 0; i < AMDGPU_MAX_RINGS; ++i)
+		adev->vm_manager.seqno[i] = 0;
+
+	atomic_set(&adev->vm_manager.vm_pte_next_ring, 0);
 	atomic64_set(&adev->vm_manager.client_counter, 0);
 }
 
diff --git a/drivers/gpu/drm/amd/amdgpu/atombios_i2c.c b/drivers/gpu/drm/amd/amdgpu/atombios_i2c.c
index 745797e..b374653 100644
--- a/drivers/gpu/drm/amd/amdgpu/atombios_i2c.c
+++ b/drivers/gpu/drm/amd/amdgpu/atombios_i2c.c
@@ -159,17 +159,16 @@ u32 amdgpu_atombios_i2c_func(struct i2c_adapter *adap)
 
 void amdgpu_atombios_i2c_channel_trans(struct amdgpu_device* adev, u8 slave_addr, u8 line_number, u8 offset, u8 data)
 {
-        PROCESS_I2C_CHANNEL_TRANSACTION_PS_ALLOCATION args;
-        int index = GetIndexIntoMasterTable(COMMAND, ProcessI2cChannelTransaction);
- 
-        args.ucRegIndex = offset;
-        args.lpI2CDataOut = data;
-        args.ucFlag = 1;
-        args.ucI2CSpeed = TARGET_HW_I2C_CLOCK;
-        args.ucTransBytes = 1;
-        args.ucSlaveAddr = slave_addr;
-        args.ucLineNumber = line_number;
- 
-        amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args);
-}
+	PROCESS_I2C_CHANNEL_TRANSACTION_PS_ALLOCATION args;
+	int index = GetIndexIntoMasterTable(COMMAND, ProcessI2cChannelTransaction);
 
+	args.ucRegIndex = offset;
+	args.lpI2CDataOut = data;
+	args.ucFlag = 1;
+	args.ucI2CSpeed = TARGET_HW_I2C_CLOCK;
+	args.ucTransBytes = 1;
+	args.ucSlaveAddr = slave_addr;
+	args.ucLineNumber = line_number;
+
+	amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args);
+}
diff --git a/drivers/gpu/drm/amd/amdgpu/atombios_i2c.h b/drivers/gpu/drm/amd/amdgpu/atombios_i2c.h
index 5e951f6..251aaf4 100644
--- a/drivers/gpu/drm/amd/amdgpu/atombios_i2c.h
+++ b/drivers/gpu/drm/amd/amdgpu/atombios_i2c.h
@@ -28,5 +28,6 @@ int amdgpu_atombios_i2c_xfer(struct i2c_adapter *i2c_adap,
 		      struct i2c_msg *msgs, int num);
 u32 amdgpu_atombios_i2c_func(struct i2c_adapter *adap);
 void amdgpu_atombios_i2c_channel_trans(struct amdgpu_device* adev,
-                u8 slave_addr, u8 line_number, u8 offset, u8 data);
+		u8 slave_addr, u8 line_number, u8 offset, u8 data);
+
 #endif
diff --git a/drivers/gpu/drm/amd/amdgpu/cik_sdma.c b/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
index 1f929bf..914058f 100644
--- a/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
+++ b/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
@@ -220,10 +220,10 @@ static void cik_sdma_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)
  * Schedule an IB in the DMA ring (CIK).
  */
 static void cik_sdma_ring_emit_ib(struct amdgpu_ring *ring,
-                                  struct amdgpu_ib *ib,
-                                  unsigned vm_id, bool ctx_switch)
+				  struct amdgpu_ib *ib,
+				  unsigned vm_id, bool ctx_switch)
 {
-        u32 extra_bits = vm_id & 0xf;
+	u32 extra_bits = vm_id & 0xf;
 
 	/* IB packet must end on a 8 DW boundary */
 	cik_sdma_ring_insert_nop(ring, (12 - (ring->wptr & 7)) % 8);
@@ -1335,7 +1335,7 @@ static void cik_sdma_set_vm_pte_funcs(struct amdgpu_device *adev)
 		for (i = 0; i < adev->sdma.num_instances; i++)
 			adev->vm_manager.vm_pte_rings[i] =
 				&adev->sdma.instance[i].ring;
- 
+
 		adev->vm_manager.vm_pte_num_rings = adev->sdma.num_instances;
 	}
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
index 40c5c93..bbef2e8 100644
--- a/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
@@ -1855,9 +1855,9 @@ static void gmc_v7_0_init_compute_vmid(struct amdgpu_device *adev)
 
 	/*
 	 * Configure apertures:
-	 * LDS:		0x60000000'00000000 - 0x60000001'00000000 (4GB)
-	 * Scratch:	0x60000001'00000000 - 0x60000002'00000000 (4GB)
-	 * GPUVM:	0x60010000'00000000 - 0x60020000'00000000 (1TB)
+	 * LDS:         0x60000000'00000000 - 0x60000001'00000000 (4GB)
+	 * Scratch:     0x60000001'00000000 - 0x60000002'00000000 (4GB)
+	 * GPUVM:       0x60010000'00000000 - 0x60020000'00000000 (1TB)
 	*/
 	sh_mem_bases = DEFAULT_SH_MEM_BASES | (DEFAULT_SH_MEM_BASES << 16);
 	sh_mem_config = SH_MEM_ALIGNMENT_MODE_UNALIGNED <<
@@ -2222,9 +2222,8 @@ static void gfx_v7_0_ring_emit_fence_compute(struct amdgpu_ring *ring,
  * on the gfx ring for execution by the GPU.
  */
 static void gfx_v7_0_ring_emit_ib_gfx(struct amdgpu_ring *ring,
-                                      struct amdgpu_ib *ib,
-                                      unsigned vm_id, bool ctx_switch)
-
+				      struct amdgpu_ib *ib,
+				      unsigned vm_id, bool ctx_switch)
 {
 	u32 header, control = 0;
 
@@ -2252,12 +2251,12 @@ static void gfx_v7_0_ring_emit_ib_gfx(struct amdgpu_ring *ring,
 }
 
 static void gfx_v7_0_ring_emit_ib_compute(struct amdgpu_ring *ring,
-                                          struct amdgpu_ib *ib,
-                                          unsigned vm_id, bool ctx_switch)
+					  struct amdgpu_ib *ib,
+					  unsigned vm_id, bool ctx_switch)
 {
-        u32 control = INDIRECT_BUFFER_VALID | ib->length_dw | (vm_id << 24);
- 
-        amdgpu_ring_write(ring, PACKET3(PACKET3_INDIRECT_BUFFER, 2));
+	u32 control = INDIRECT_BUFFER_VALID | ib->length_dw | (vm_id << 24);
+
+	amdgpu_ring_write(ring, PACKET3(PACKET3_INDIRECT_BUFFER, 2));
 	amdgpu_ring_write(ring,
 #ifdef __BIG_ENDIAN
 					  (2 << 0) |
@@ -2805,7 +2804,7 @@ static int gfx_v7_0_mec_init(struct amdgpu_device *adev)
 	u32 *hpd;
 
 	/*
-	 * KV:	  2 MEC, 4 Pipes/MEC, 8 Queues/Pipe - 64 Queues total
+	 * KV:    2 MEC, 4 Pipes/MEC, 8 Queues/Pipe - 64 Queues total
 	 * CI/KB: 1 MEC, 4 Pipes/MEC, 8 Queues/Pipe - 32 Queues total
 	 * Nonetheless, we assign only 1 pipe because all other pipes will
 	 * be handled by KFD
diff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
index 893244e..eb7f349 100644
--- a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
@@ -69,12 +69,12 @@
 #define MACRO_TILE_ASPECT(x)				((x) << GB_MACROTILE_MODE0__MACRO_TILE_ASPECT__SHIFT)
 #define NUM_BANKS(x)					((x) << GB_MACROTILE_MODE0__NUM_BANKS__SHIFT)
 
-#define RLC_CGTT_MGCG_OVERRIDE__CPF_MASK	    0x00000001L
-#define RLC_CGTT_MGCG_OVERRIDE__RLC_MASK	    0x00000002L
-#define RLC_CGTT_MGCG_OVERRIDE__MGCG_MASK	    0x00000004L
-#define RLC_CGTT_MGCG_OVERRIDE__CGCG_MASK	    0x00000008L
-#define RLC_CGTT_MGCG_OVERRIDE__CGLS_MASK	    0x00000010L
-#define RLC_CGTT_MGCG_OVERRIDE__GRBM_MASK	    0x00000020L
+#define RLC_CGTT_MGCG_OVERRIDE__CPF_MASK            0x00000001L
+#define RLC_CGTT_MGCG_OVERRIDE__RLC_MASK            0x00000002L
+#define RLC_CGTT_MGCG_OVERRIDE__MGCG_MASK           0x00000004L
+#define RLC_CGTT_MGCG_OVERRIDE__CGCG_MASK           0x00000008L
+#define RLC_CGTT_MGCG_OVERRIDE__CGLS_MASK           0x00000010L
+#define RLC_CGTT_MGCG_OVERRIDE__GRBM_MASK           0x00000020L
 
 /* BPM SERDES CMD */
 #define SET_BPM_SERDES_CMD    1
@@ -82,15 +82,15 @@
 
 /* BPM Register Address*/
 enum {
-	BPM_REG_CGLS_EN = 0,	    /* Enable/Disable CGLS */
-	BPM_REG_CGLS_ON,	    /* ON/OFF CGLS: shall be controlled by RLC FW */
-	BPM_REG_CGCG_OVERRIDE,	    /* Set/Clear CGCG Override */
-	BPM_REG_MGCG_OVERRIDE,	    /* Set/Clear MGCG Override */
-	BPM_REG_FGCG_OVERRIDE,	    /* Set/Clear FGCG Override */
+	BPM_REG_CGLS_EN = 0,        /* Enable/Disable CGLS */
+	BPM_REG_CGLS_ON,            /* ON/OFF CGLS: shall be controlled by RLC FW */
+	BPM_REG_CGCG_OVERRIDE,      /* Set/Clear CGCG Override */
+	BPM_REG_MGCG_OVERRIDE,      /* Set/Clear MGCG Override */
+	BPM_REG_FGCG_OVERRIDE,      /* Set/Clear FGCG Override */
 	BPM_REG_FGCG_MAX
 };
 
-#define RLC_FormatDirectRegListLength	     14
+#define RLC_FormatDirectRegListLength        14
 
 MODULE_FIRMWARE("amdgpu/carrizo_ce.bin");
 MODULE_FIRMWARE("amdgpu/carrizo_pfp.bin");
@@ -704,13 +704,13 @@ static void gfx_v8_0_init_golden_registers(struct amdgpu_device *adev)
 						 polaris10_golden_common_all,
 						 (const u32)ARRAY_SIZE(polaris10_golden_common_all));
 		WREG32_SMC(ixCG_ACLK_CNTL, 0x0000001C);
-                if (adev->pdev->revision == 0xc7 &&
-                    ((adev->pdev->subsystem_device == 0xb37 && adev->pdev->subsystem_vendor == 0x1002) ||
-                     (adev->pdev->subsystem_device == 0x4a8 && adev->pdev->subsystem_vendor == 0x1043) ||
-                     (adev->pdev->subsystem_device == 0x9480 && adev->pdev->subsystem_vendor == 0x1682))) {
-                        amdgpu_atombios_i2c_channel_trans(adev, 0x10, 0x96, 0x1E, 0xDD);
-                        amdgpu_atombios_i2c_channel_trans(adev, 0x10, 0x96, 0x1F, 0xD0);
-                }
+		if (adev->pdev->revision == 0xc7 &&
+		    ((adev->pdev->subsystem_device == 0xb37 && adev->pdev->subsystem_vendor == 0x1002) ||
+		     (adev->pdev->subsystem_device == 0x4a8 && adev->pdev->subsystem_vendor == 0x1043) ||
+		     (adev->pdev->subsystem_device == 0x9480 && adev->pdev->subsystem_vendor == 0x1682))) {
+			amdgpu_atombios_i2c_channel_trans(adev, 0x10, 0x96, 0x1E, 0xDD);
+			amdgpu_atombios_i2c_channel_trans(adev, 0x10, 0x96, 0x1F, 0xD0);
+		}
 		break;
 	case CHIP_CARRIZO:
 		amdgpu_program_register_sequence(adev,
@@ -851,23 +851,24 @@ err1:
 	return r;
 }
 
+
 static void gfx_v8_0_free_microcode(struct amdgpu_device *adev) {
-        release_firmware(adev->gfx.pfp_fw);
-        adev->gfx.pfp_fw = NULL;
-        release_firmware(adev->gfx.me_fw);
-        adev->gfx.me_fw = NULL;
-        release_firmware(adev->gfx.ce_fw);
-        adev->gfx.ce_fw = NULL;
-        release_firmware(adev->gfx.rlc_fw);
-        adev->gfx.rlc_fw = NULL;
-        release_firmware(adev->gfx.mec_fw);
-        adev->gfx.mec_fw = NULL;
-        if ((adev->asic_type != CHIP_STONEY) &&
-            (adev->asic_type != CHIP_TOPAZ))
-                release_firmware(adev->gfx.mec2_fw);
-        adev->gfx.mec2_fw = NULL;
- 
-        kfree(adev->gfx.rlc.register_list_format);
+	release_firmware(adev->gfx.pfp_fw);
+	adev->gfx.pfp_fw = NULL;
+	release_firmware(adev->gfx.me_fw);
+	adev->gfx.me_fw = NULL;
+	release_firmware(adev->gfx.ce_fw);
+	adev->gfx.ce_fw = NULL;
+	release_firmware(adev->gfx.rlc_fw);
+	adev->gfx.rlc_fw = NULL;
+	release_firmware(adev->gfx.mec_fw);
+	adev->gfx.mec_fw = NULL;
+	if ((adev->asic_type != CHIP_STONEY) &&
+	    (adev->asic_type != CHIP_TOPAZ))
+		release_firmware(adev->gfx.mec2_fw);
+	adev->gfx.mec2_fw = NULL;
+
+	kfree(adev->gfx.rlc.register_list_format);
 }
 
 static int gfx_v8_0_init_microcode(struct amdgpu_device *adev)
@@ -1196,7 +1197,7 @@ static void cz_init_cp_jump_table(struct amdgpu_device *adev)
 				 le32_to_cpu(hdr->header.ucode_array_offset_bytes));
 			table_offset = le32_to_cpu(hdr->jt_offset);
 			table_size = le32_to_cpu(hdr->jt_size);
-		} else	if (me == 4) {
+		} else  if (me == 4) {
 			const struct gfx_firmware_header_v1_0 *hdr =
 				(const struct gfx_firmware_header_v1_0 *)adev->gfx.mec2_fw->data;
 			fw_data = (const __le32 *)
@@ -3701,9 +3702,9 @@ static void gfx_v8_0_init_compute_vmid(struct amdgpu_device *adev)
 
 	/*
 	 * Configure apertures:
-	 * LDS:		0x60000000'00000000 - 0x60000001'00000000 (4GB)
-	 * Scratch:	0x60000001'00000000 - 0x60000002'00000000 (4GB)
-	 * GPUVM:	0x60010000'00000000 - 0x60020000'00000000 (1TB)
+	 * LDS:         0x60000000'00000000 - 0x60000001'00000000 (4GB)
+	 * Scratch:     0x60000001'00000000 - 0x60000002'00000000 (4GB)
+	 * GPUVM:       0x60010000'00000000 - 0x60020000'00000000 (1TB)
 	 */
 	sh_mem_bases = DEFAULT_SH_MEM_BASES | (DEFAULT_SH_MEM_BASES << 16);
 
@@ -3979,23 +3980,23 @@ static void gfx_v8_0_init_power_gating(struct amdgpu_device *adev)
 		WREG32_FIELD(RLC_PG_DELAY_2, SERDES_CMD_DELAY, 0x3);
 		WREG32_FIELD(RLC_AUTO_PG_CTRL, GRBM_REG_SAVE_GFX_IDLE_THRESHOLD, 0x55f0);
 	}
- }
+}
 
 static void cz_enable_sck_slow_down_on_power_up(struct amdgpu_device *adev,
-					       bool enable)
+						bool enable)
 {
-        WREG32_FIELD(RLC_PG_CNTL, SMU_CLK_SLOWDOWN_ON_PU_ENABLE, enable ? 1 : 0);
+	WREG32_FIELD(RLC_PG_CNTL, SMU_CLK_SLOWDOWN_ON_PU_ENABLE, enable ? 1 : 0);
 }
 
 static void cz_enable_sck_slow_down_on_power_down(struct amdgpu_device *adev,
-						 bool enable)
+						  bool enable)
 {
-        WREG32_FIELD(RLC_PG_CNTL, SMU_CLK_SLOWDOWN_ON_PD_ENABLE, enable ? 1 : 0);
+	WREG32_FIELD(RLC_PG_CNTL, SMU_CLK_SLOWDOWN_ON_PD_ENABLE, enable ? 1 : 0);
 }
 
 static void cz_enable_cp_power_gating(struct amdgpu_device *adev, bool enable)
 {
-        WREG32_FIELD(RLC_PG_CNTL, CP_PG_DISABLE, enable ? 1 : 0);
+	WREG32_FIELD(RLC_PG_CNTL, CP_PG_DISABLE, enable ? 1 : 0);
 }
 
 static void gfx_v8_0_init_pg(struct amdgpu_device *adev)
@@ -4009,10 +4010,9 @@ static void gfx_v8_0_init_pg(struct amdgpu_device *adev)
 		gfx_v8_0_init_csb(adev);
 		gfx_v8_0_init_save_restore_list(adev);
 		gfx_v8_0_enable_save_restore_machine(adev);
- 
+
 		if ((adev->asic_type == CHIP_CARRIZO) ||
 		    (adev->asic_type == CHIP_STONEY)) {
- 
 			WREG32(mmRLC_JUMP_TABLE_RESTORE, adev->gfx.rlc.cp_table_gpu_addr >> 8);
 			gfx_v8_0_init_power_gating(adev);
 			WREG32(mmRLC_PG_ALWAYS_ON_CU_MASK, adev->gfx.cu_info.ao_cu_mask);
@@ -4480,9 +4480,9 @@ struct vi_mqd {
 	uint32_t compute_start_x;  /* ordinal5 */
 	uint32_t compute_start_y;  /* ordinal6 */
 	uint32_t compute_start_z;  /* ordinal7 */
-	uint32_t compute_num_thread_x;	/* ordinal8 */
-	uint32_t compute_num_thread_y;	/* ordinal9 */
-	uint32_t compute_num_thread_z;	/* ordinal10 */
+	uint32_t compute_num_thread_x;  /* ordinal8 */
+	uint32_t compute_num_thread_y;  /* ordinal9 */
+	uint32_t compute_num_thread_z;  /* ordinal10 */
 	uint32_t compute_pipelinestat_enable;  /* ordinal11 */
 	uint32_t compute_perfcount_enable;  /* ordinal12 */
 	uint32_t compute_pgm_lo;  /* ordinal13 */
@@ -4493,11 +4493,11 @@ struct vi_mqd {
 	uint32_t compute_tma_hi;  /* ordinal18 */
 	uint32_t compute_pgm_rsrc1;  /* ordinal19 */
 	uint32_t compute_pgm_rsrc2;  /* ordinal20 */
-	uint32_t compute_vmid;	/* ordinal21 */
+	uint32_t compute_vmid;  /* ordinal21 */
 	uint32_t compute_resource_limits;  /* ordinal22 */
 	uint32_t compute_static_thread_mgmt_se0;  /* ordinal23 */
 	uint32_t compute_static_thread_mgmt_se1;  /* ordinal24 */
-	uint32_t compute_tmpring_size;	/* ordinal25 */
+	uint32_t compute_tmpring_size;  /* ordinal25 */
 	uint32_t compute_static_thread_mgmt_se2;  /* ordinal26 */
 	uint32_t compute_static_thread_mgmt_se3;  /* ordinal27 */
 	uint32_t compute_restart_x;  /* ordinal28 */
@@ -4508,9 +4508,9 @@ struct vi_mqd {
 	uint32_t compute_dispatch_id;  /* ordinal33 */
 	uint32_t compute_threadgroup_id;  /* ordinal34 */
 	uint32_t compute_relaunch;  /* ordinal35 */
-	uint32_t compute_wave_restore_addr_lo;	/* ordinal36 */
-	uint32_t compute_wave_restore_addr_hi;	/* ordinal37 */
-	uint32_t compute_wave_restore_control;	/* ordinal38 */
+	uint32_t compute_wave_restore_addr_lo;  /* ordinal36 */
+	uint32_t compute_wave_restore_addr_hi;  /* ordinal37 */
+	uint32_t compute_wave_restore_control;  /* ordinal38 */
 	uint32_t reserved9;  /* ordinal39 */
 	uint32_t reserved10;  /* ordinal40 */
 	uint32_t reserved11;  /* ordinal41 */
@@ -4547,21 +4547,21 @@ struct vi_mqd {
 	uint32_t compute_user_data_7;  /* ordinal72 */
 	uint32_t compute_user_data_8;  /* ordinal73 */
 	uint32_t compute_user_data_9;  /* ordinal74 */
-	uint32_t compute_user_data_10;	/* ordinal75 */
-	uint32_t compute_user_data_11;	/* ordinal76 */
-	uint32_t compute_user_data_12;	/* ordinal77 */
-	uint32_t compute_user_data_13;	/* ordinal78 */
-	uint32_t compute_user_data_14;	/* ordinal79 */
-	uint32_t compute_user_data_15;	/* ordinal80 */
+	uint32_t compute_user_data_10;  /* ordinal75 */
+	uint32_t compute_user_data_11;  /* ordinal76 */
+	uint32_t compute_user_data_12;  /* ordinal77 */
+	uint32_t compute_user_data_13;  /* ordinal78 */
+	uint32_t compute_user_data_14;  /* ordinal79 */
+	uint32_t compute_user_data_15;  /* ordinal80 */
 	uint32_t cp_compute_csinvoc_count_lo;  /* ordinal81 */
 	uint32_t cp_compute_csinvoc_count_hi;  /* ordinal82 */
 	uint32_t reserved35;  /* ordinal83 */
 	uint32_t reserved36;  /* ordinal84 */
 	uint32_t reserved37;  /* ordinal85 */
-	uint32_t cp_mqd_query_time_lo;	/* ordinal86 */
-	uint32_t cp_mqd_query_time_hi;	/* ordinal87 */
-	uint32_t cp_mqd_connect_start_time_lo;	/* ordinal88 */
-	uint32_t cp_mqd_connect_start_time_hi;	/* ordinal89 */
+	uint32_t cp_mqd_query_time_lo;  /* ordinal86 */
+	uint32_t cp_mqd_query_time_hi;  /* ordinal87 */
+	uint32_t cp_mqd_connect_start_time_lo;  /* ordinal88 */
+	uint32_t cp_mqd_connect_start_time_hi;  /* ordinal89 */
 	uint32_t cp_mqd_connect_end_time_lo;  /* ordinal90 */
 	uint32_t cp_mqd_connect_end_time_hi;  /* ordinal91 */
 	uint32_t cp_mqd_connect_end_wf_count;  /* ordinal92 */
@@ -4574,8 +4574,8 @@ struct vi_mqd {
 	uint32_t cp_mqd_save_start_time_hi;  /* ordinal99 */
 	uint32_t cp_mqd_save_end_time_lo;  /* ordinal100 */
 	uint32_t cp_mqd_save_end_time_hi;  /* ordinal101 */
-	uint32_t cp_mqd_restore_start_time_lo;	/* ordinal102 */
-	uint32_t cp_mqd_restore_start_time_hi;	/* ordinal103 */
+	uint32_t cp_mqd_restore_start_time_lo;  /* ordinal102 */
+	uint32_t cp_mqd_restore_start_time_hi;  /* ordinal103 */
 	uint32_t cp_mqd_restore_end_time_lo;  /* ordinal104 */
 	uint32_t cp_mqd_restore_end_time_hi;  /* ordinal105 */
 	uint32_t reserved40;  /* ordinal106 */
@@ -4605,7 +4605,7 @@ struct vi_mqd {
 	uint32_t cp_hqd_active;  /* ordinal130 */
 	uint32_t cp_hqd_vmid;  /* ordinal131 */
 	uint32_t cp_hqd_persistent_state;  /* ordinal132 */
-	uint32_t cp_hqd_pipe_priority;	/* ordinal133 */
+	uint32_t cp_hqd_pipe_priority;  /* ordinal133 */
 	uint32_t cp_hqd_queue_priority;  /* ordinal134 */
 	uint32_t cp_hqd_quantum;  /* ordinal135 */
 	uint32_t cp_hqd_pq_base_lo;  /* ordinal136 */
@@ -4643,15 +4643,15 @@ struct vi_mqd {
 	uint32_t cp_hqd_eop_rptr;  /* ordinal168 */
 	uint32_t cp_hqd_eop_wptr;  /* ordinal169 */
 	uint32_t cp_hqd_eop_done_events;  /* ordinal170 */
-	uint32_t cp_hqd_ctx_save_base_addr_lo;	/* ordinal171 */
-	uint32_t cp_hqd_ctx_save_base_addr_hi;	/* ordinal172 */
+	uint32_t cp_hqd_ctx_save_base_addr_lo;  /* ordinal171 */
+	uint32_t cp_hqd_ctx_save_base_addr_hi;  /* ordinal172 */
 	uint32_t cp_hqd_ctx_save_control;  /* ordinal173 */
 	uint32_t cp_hqd_cntl_stack_offset;  /* ordinal174 */
 	uint32_t cp_hqd_cntl_stack_size;  /* ordinal175 */
 	uint32_t cp_hqd_wg_state_offset;  /* ordinal176 */
-	uint32_t cp_hqd_ctx_save_size;	/* ordinal177 */
+	uint32_t cp_hqd_ctx_save_size;  /* ordinal177 */
 	uint32_t cp_hqd_gds_resource_state;  /* ordinal178 */
-	uint32_t cp_hqd_error;	/* ordinal179 */
+	uint32_t cp_hqd_error;  /* ordinal179 */
 	uint32_t cp_hqd_eop_wptr_mem;  /* ordinal180 */
 	uint32_t cp_hqd_eop_dones;  /* ordinal181 */
 	uint32_t reserved46;  /* ordinal182 */
@@ -4700,7 +4700,7 @@ struct vi_mqd {
 	uint32_t reserved56;  /* ordinal225 */
 	uint32_t reserved57;  /* ordinal226 */
 	uint32_t reserved58;  /* ordinal227 */
-	uint32_t set_resources_header;	/* ordinal228 */
+	uint32_t set_resources_header;  /* ordinal228 */
 	uint32_t set_resources_dw1;  /* ordinal229 */
 	uint32_t set_resources_dw2;  /* ordinal230 */
 	uint32_t set_resources_dw3;  /* ordinal231 */
@@ -5525,48 +5525,48 @@ static int gfx_v8_0_late_init(void *handle)
 static void gfx_v8_0_enable_gfx_static_mg_power_gating(struct amdgpu_device *adev,
 						       bool enable)
 {
-         if (adev->asic_type == CHIP_POLARIS11)
-                /* Send msg to SMU via Powerplay */
-                amdgpu_set_powergating_state(adev,
-                                             AMD_IP_BLOCK_TYPE_SMC,
-                                             enable ?
-                                             AMD_PG_STATE_GATE : AMD_PG_STATE_UNGATE);
- 
-        WREG32_FIELD(RLC_PG_CNTL, STATIC_PER_CU_PG_ENABLE, enable ? 1 : 0);
+	if (adev->asic_type == CHIP_POLARIS11)
+		/* Send msg to SMU via Powerplay */
+		amdgpu_set_powergating_state(adev,
+					     AMD_IP_BLOCK_TYPE_SMC,
+					     enable ?
+					     AMD_PG_STATE_GATE : AMD_PG_STATE_UNGATE);
+
+	WREG32_FIELD(RLC_PG_CNTL, STATIC_PER_CU_PG_ENABLE, enable ? 1 : 0);
 }
 
 static void gfx_v8_0_enable_gfx_dynamic_mg_power_gating(struct amdgpu_device *adev,
 							bool enable)
 {
-        WREG32_FIELD(RLC_PG_CNTL, DYN_PER_CU_PG_ENABLE, enable ? 1 : 0);
+	WREG32_FIELD(RLC_PG_CNTL, DYN_PER_CU_PG_ENABLE, enable ? 1 : 0);
 }
 
 static void polaris11_enable_gfx_quick_mg_power_gating(struct amdgpu_device *adev,
-                bool enable)
+		bool enable)
 {
-        WREG32_FIELD(RLC_PG_CNTL, QUICK_PG_ENABLE, enable ? 1 : 0);
+	WREG32_FIELD(RLC_PG_CNTL, QUICK_PG_ENABLE, enable ? 1 : 0);
 }
 
 static void cz_enable_gfx_cg_power_gating(struct amdgpu_device *adev,
 					  bool enable)
 {
-        WREG32_FIELD(RLC_PG_CNTL, GFX_POWER_GATING_ENABLE, enable ? 1 : 0);
+	WREG32_FIELD(RLC_PG_CNTL, GFX_POWER_GATING_ENABLE, enable ? 1 : 0);
 }
 
 static void cz_enable_gfx_pipeline_power_gating(struct amdgpu_device *adev,
 						bool enable)
 {
-        WREG32_FIELD(RLC_PG_CNTL, GFX_PIPELINE_PG_ENABLE, enable ? 1 : 0); 
-	
+	WREG32_FIELD(RLC_PG_CNTL, GFX_PIPELINE_PG_ENABLE, enable ? 1 : 0);
+
 	/* Read any GFX register to wake up GFX. */
 	if (!enable)
-                RREG32(mmDB_RENDER_CONTROL);
+		RREG32(mmDB_RENDER_CONTROL);
 }
 
 static void cz_update_gfx_cg_power_gating(struct amdgpu_device *adev,
 					  bool enable)
 {
-       if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_PG) && enable) {
+	if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_PG) && enable) {
 		cz_enable_gfx_cg_power_gating(adev, true);
 		if (adev->pg_flags & AMD_PG_SUPPORT_GFX_PIPELINE)
 			cz_enable_gfx_pipeline_power_gating(adev, true);
@@ -5581,42 +5581,42 @@ static int gfx_v8_0_set_powergating_state(void *handle,
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
 	bool enable = (state == AMD_PG_STATE_GATE) ? true : false;
- 
+
 	if (!(adev->pg_flags & AMD_PG_SUPPORT_GFX_PG))
 		return 0;
- 
+
 	switch (adev->asic_type) {
 	case CHIP_CARRIZO:
 	case CHIP_STONEY:
 		if (adev->pg_flags & AMD_PG_SUPPORT_GFX_PG)
 			cz_update_gfx_cg_power_gating(adev, enable);
- 
+
 		if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_SMG) && enable)
 			gfx_v8_0_enable_gfx_static_mg_power_gating(adev, true);
 		else
 			gfx_v8_0_enable_gfx_static_mg_power_gating(adev, false);
- 
+
 		if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_DMG) && enable)
-		       gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, true);
+			gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, true);
 		else
-		       gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, false);
+			gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, false);
+		break;
+	case CHIP_POLARIS11:
+		if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_SMG) && enable)
+			gfx_v8_0_enable_gfx_static_mg_power_gating(adev, true);
+		else
+			gfx_v8_0_enable_gfx_static_mg_power_gating(adev, false);
+
+		if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_DMG) && enable)
+			gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, true);
+		else
+			gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, false);
+
+		if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_QUICK_MG) && enable)
+			polaris11_enable_gfx_quick_mg_power_gating(adev, true);
+		else
+			polaris11_enable_gfx_quick_mg_power_gating(adev, false);
 		break;
-        case CHIP_POLARIS11:
-                if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_SMG) && enable)
-                        gfx_v8_0_enable_gfx_static_mg_power_gating(adev, true);
-                else
-                        gfx_v8_0_enable_gfx_static_mg_power_gating(adev, false);
-
-                if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_DMG) && enable)
-                        gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, true);
-                else
-                        gfx_v8_0_enable_gfx_dynamic_mg_power_gating(adev, false);
-
-                if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_QUICK_MG) && enable)
-                        polaris11_enable_gfx_quick_mg_power_gating(adev, true);
-                else
-                        polaris11_enable_gfx_quick_mg_power_gating(adev, false);
-                break;
 	default:
 		break;
 	}
@@ -5666,8 +5666,7 @@ static void gfx_v8_0_send_serdes_cmd(struct amdgpu_device *adev,
 }
 
 #define MSG_ENTER_RLC_SAFE_MODE     1
-#define MSG_EXIT_RLC_SAFE_MODE	    0
-
+#define MSG_EXIT_RLC_SAFE_MODE      0
 #define RLC_GPR_REG2__REQ_MASK 0x00000001
 #define RLC_GPR_REG2__REQ__SHIFT 0
 #define RLC_GPR_REG2__MESSAGE__SHIFT 0x00000001
@@ -5938,19 +5937,19 @@ static void gfx_v8_0_update_coarse_grain_clock_gating(struct amdgpu_device *adev
 		if (temp1 != data1)
 			WREG32(mmRLC_CGTT_MGCG_OVERRIDE, data1);
 
-		/* : wait for RLC_SERDES_CU_MASTER & RLC_SERDES_NONCU_MASTER idle */
+		/* 2 wait for RLC_SERDES_CU_MASTER & RLC_SERDES_NONCU_MASTER idle */
 		gfx_v8_0_wait_for_rlc_serdes(adev);
 
-		/* 2 - clear cgcg override */
+		/* 3 - clear cgcg override */
 		gfx_v8_0_send_serdes_cmd(adev, BPM_REG_CGCG_OVERRIDE, CLE_BPM_SERDES_CMD);
 
 		/* wait for RLC_SERDES_CU_MASTER & RLC_SERDES_NONCU_MASTER idle */
 		gfx_v8_0_wait_for_rlc_serdes(adev);
 
-		/* 3 - write cmd to set CGLS */
+		/* 4 - write cmd to set CGLS */
 		gfx_v8_0_send_serdes_cmd(adev, BPM_REG_CGLS_EN, SET_BPM_SERDES_CMD);
 
-		/* 4 - enable cgcg */
+		/* 5 - enable cgcg */
 		data |= RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
 
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGLS) {
@@ -6018,13 +6017,13 @@ static int gfx_v8_0_update_gfx_clock_gating(struct amdgpu_device *adev,
 {
 	if (enable) {
 		/* CGCG/CGLS should be enabled after MGCG/MGLS/TS(CG/LS)
-		 * ===	MGCG + MGLS + TS(CG/LS) ===
+		 * ===  MGCG + MGLS + TS(CG/LS) ===
 		 */
 		gfx_v8_0_update_medium_grain_clock_gating(adev, enable);
 		gfx_v8_0_update_coarse_grain_clock_gating(adev, enable);
 	} else {
 		/* CGCG/CGLS should be disabled before MGCG/MGLS/TS(CG/LS)
-		 * ===	CGCG + CGLS ===
+		 * ===  CGCG + CGLS ===
 		 */
 		gfx_v8_0_update_coarse_grain_clock_gating(adev, enable);
 		gfx_v8_0_update_medium_grain_clock_gating(adev, enable);
diff --git a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c b/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
index 8d56bd6..3a25f72 100644
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
@@ -985,7 +985,7 @@ static int gmc_v7_0_sw_fini(void *handle)
 		adev->vm_manager.enabled = false;
 	}
 	gmc_v7_0_gart_fini(adev);
-        amdgpu_gem_force_release(adev);
+	amdgpu_gem_force_release(adev);
 	amdgpu_bo_fini(adev);
 
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c b/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
index a45c610..f7372d3 100644
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
@@ -893,19 +893,19 @@ static int gmc_v8_0_sw_init(void *handle)
 	int r;
 	int dma_bits;
 	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
- 
-        if (adev->flags & AMD_IS_APU) {
-                adev->mc.vram_type = AMDGPU_VRAM_TYPE_UNKNOWN;
-        } else {
-                u32 tmp;
- 
-                if (adev->asic_type == CHIP_FIJI)
-                        tmp = RREG32(mmMC_SEQ_MISC0_FIJI);
-                else
-                        tmp = RREG32(mmMC_SEQ_MISC0);
-                tmp &= MC_SEQ_MISC0__MT__MASK;
-                adev->mc.vram_type = gmc_v8_0_convert_vram_type(tmp);
-        }
+
+	if (adev->flags & AMD_IS_APU) {
+		adev->mc.vram_type = AMDGPU_VRAM_TYPE_UNKNOWN;
+	} else {
+		u32 tmp;
+
+		if (adev->asic_type == CHIP_FIJI)
+			tmp = RREG32(mmMC_SEQ_MISC0_FIJI);
+		else
+			tmp = RREG32(mmMC_SEQ_MISC0);
+		tmp &= MC_SEQ_MISC0__MT__MASK;
+		adev->mc.vram_type = gmc_v8_0_convert_vram_type(tmp);
+	}
 
 	r = amdgpu_irq_add_id(adev, 146, &adev->mc.vm_fault);
 	if (r)
@@ -992,7 +992,7 @@ static int gmc_v8_0_sw_fini(void *handle)
 		adev->vm_manager.enabled = false;
 	}
 	gmc_v8_0_gart_fini(adev);
-        amdgpu_gem_force_release(adev);
+	amdgpu_gem_force_release(adev);
 	amdgpu_bo_fini(adev);
 
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c b/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c
index 1a85c31..0575d16 100644
--- a/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c
+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c
@@ -1337,10 +1337,10 @@ static void sdma_v2_4_set_vm_pte_funcs(struct amdgpu_device *adev)
 
 	if (adev->vm_manager.vm_pte_funcs == NULL) {
 		adev->vm_manager.vm_pte_funcs = &sdma_v2_4_vm_pte_funcs;
-	        for (i = 0; i < adev->sdma.num_instances; i++)
-		        adev->vm_manager.vm_pte_rings[i] =
-			        &adev->sdma.instance[i].ring;
- 
+		for (i = 0; i < adev->sdma.num_instances; i++)
+			adev->vm_manager.vm_pte_rings[i] =
+				&adev->sdma.instance[i].ring;
+
 		adev->vm_manager.vm_pte_num_rings = adev->sdma.num_instances;
 	}
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c b/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
index 4b66f74..17ed6c6 100644
--- a/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
@@ -399,18 +399,17 @@ static int uvd_v6_0_start(struct amdgpu_device *adev)
 	/* disable byte swapping */
 	lmi_swap_cntl = 0;
 	mp_swap_cntl = 0;
- 
-        amdgpu_asic_set_uvd_clocks(adev, 10000, 10000);
-        uvd_v6_0_set_clockgating_state(adev, AMD_CG_STATE_UNGATE);
-        uvd_v6_0_enable_mgcg(adev, true);
 
+	amdgpu_asic_set_uvd_clocks(adev, 10000, 10000);
+	uvd_v6_0_set_clockgating_state(adev, AMD_CG_STATE_UNGATE);
+	uvd_v6_0_enable_mgcg(adev, true);
 	uvd_v6_0_mc_resume(adev);
- 
+
 	/* disable interupt */
-        WREG32_FIELD(UVD_MASTINT_EN, VCPU_EN, 0);
+	WREG32_FIELD(UVD_MASTINT_EN, VCPU_EN, 0);
 
 	/* stall UMC and register bus before resetting VCPU */
-        WREG32_FIELD(UVD_LMI_CTRL2, STALL_ARB_UMC, 1);
+	WREG32_FIELD(UVD_LMI_CTRL2, STALL_ARB_UMC, 1);
 	mdelay(1);
 
 	/* put LMI, VCPU, RBC etc... into reset */
diff --git a/drivers/gpu/drm/amd/amdgpu/vi.c b/drivers/gpu/drm/amd/amdgpu/vi.c
index 8f47d6e..d09c25a 100644
--- a/drivers/gpu/drm/amd/amdgpu/vi.c
+++ b/drivers/gpu/drm/amd/amdgpu/vi.c
@@ -141,8 +141,8 @@ static void vi_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)
 }
 
 /* smu_8_0_d.h */
-#define mmMP0PUB_IND_INDEX							0x180
-#define mmMP0PUB_IND_DATA							0x181
+#define mmMP0PUB_IND_INDEX                                                      0x180
+#define mmMP0PUB_IND_DATA                                                       0x181
 
 static u32 cz_smc_rreg(struct amdgpu_device *adev, u32 reg)
 {
@@ -849,9 +849,9 @@ static void vi_enable_doorbell_aperture(struct amdgpu_device *adev,
 	WREG32(mmBIF_DOORBELL_APER_EN, tmp);
 }
 
-#define ATI_REV_ID_FUSE_MACRO__ADDRESS	    0xC0014044
-#define ATI_REV_ID_FUSE_MACRO__SHIFT	    9
-#define ATI_REV_ID_FUSE_MACRO__MASK	    0x00001E00
+#define ATI_REV_ID_FUSE_MACRO__ADDRESS      0xC0014044
+#define ATI_REV_ID_FUSE_MACRO__SHIFT        9
+#define ATI_REV_ID_FUSE_MACRO__MASK         0x00001E00
 
 static uint32_t vi_get_rev_id(struct amdgpu_device *adev)
 {
@@ -962,17 +962,17 @@ static int vi_common_early_init(void *handle)
 			AMD_CG_SUPPORT_HDP_MGCG |
 			AMD_CG_SUPPORT_HDP_LS |
 			AMD_CG_SUPPORT_SDMA_MGCG |
-                        AMD_CG_SUPPORT_SDMA_LS |
-                        AMD_CG_SUPPORT_VCE_MGCG;
-                /* rev0 hardware requires workarounds to support PG */
+			AMD_CG_SUPPORT_SDMA_LS |
+			AMD_CG_SUPPORT_VCE_MGCG;
+		/* rev0 hardware requires workarounds to support PG */
 		adev->pg_flags = 0;
-                if (adev->rev_id != 0x00) {
-                        adev->pg_flags |= AMD_PG_SUPPORT_GFX_PG |
-                                AMD_PG_SUPPORT_GFX_SMG |
-                                AMD_PG_SUPPORT_GFX_PIPELINE |
-                                AMD_PG_SUPPORT_UVD |
-                                AMD_PG_SUPPORT_VCE;
-                }
+		if (adev->rev_id != 0x00) {
+			adev->pg_flags |= AMD_PG_SUPPORT_GFX_PG |
+				AMD_PG_SUPPORT_GFX_SMG |
+				AMD_PG_SUPPORT_GFX_PIPELINE |
+				AMD_PG_SUPPORT_UVD |
+				AMD_PG_SUPPORT_VCE;
+		}
 		adev->external_rev_id = adev->rev_id + 0x1;
 		break;
 	case CHIP_STONEY:
@@ -1268,16 +1268,15 @@ static const struct amd_ip_funcs vi_common_ip_funcs = {
 
 static const struct amdgpu_ip_block_version vi_common_ip_block =
 {
-        .type = AMD_IP_BLOCK_TYPE_COMMON,
-        .major = 1,
-        .minor = 0,
-        .rev = 0,
-        .funcs = &vi_common_ip_funcs,
+	.type = AMD_IP_BLOCK_TYPE_COMMON,
+	.major = 1,
+	.minor = 0,
+	.rev = 0,
+	.funcs = &vi_common_ip_funcs,
 };
 
 int vi_set_ip_blocks(struct amdgpu_device *adev)
 {
-	
 	switch (adev->asic_type) {
 	case CHIP_TOPAZ:
 		/* topaz has no DCE, UVD, VCE */
@@ -1394,4 +1393,3 @@ int vi_set_ip_blocks(struct amdgpu_device *adev)
 
 	return 0;
 }
-
diff --git a/drivers/gpu/drm/amd/dal/dc/core/dc.c b/drivers/gpu/drm/amd/dal/dc/core/dc.c
index 21e0ead..99239ad 100644
--- a/drivers/gpu/drm/amd/dal/dc/core/dc.c
+++ b/drivers/gpu/drm/amd/dal/dc/core/dc.c
@@ -589,8 +589,7 @@ static void destruct(struct core_dc *dc)
 	resource_validate_ctx_destruct(dc->current_context);
 
 	dm_free(dc->temp_flip_context);
-        dc->current_context = NULL;
-
+	dc->current_context = NULL;
 	destroy_links(dc);
 
 	if (dc->res_pool)
@@ -602,8 +601,8 @@ static void destruct(struct core_dc *dc)
         if (dc->ctx->i2caux)
                 dal_i2caux_destroy(&dc->ctx->i2caux);
 
-        if (dc->ctx->created_bios)
-                dal_bios_parser_destroy(&dc->ctx->dc_bios);
+	if (dc->ctx->created_bios)
+		dal_bios_parser_destroy(&dc->ctx->dc_bios);
 
         if (dc->ctx->logger)
                 dal_logger_destroy(&dc->ctx->logger);
@@ -1387,17 +1386,17 @@ void dc_update_surfaces_for_target(struct dc *dc, struct dc_surface_update *upda
 	for (j = 0; j < MAX_SURFACES; j++)
 		is_new_pipe_surface[j] = true;
 
-	for (i = 0 ; i < surface_count; i++) {
-		struct core_surface *surface = DC_SURFACE_TO_CORE(updates[i].surface);
+		for (i = 0 ; i < surface_count; i++) {
+			struct core_surface *surface = DC_SURFACE_TO_CORE(updates[i].surface);
 
-		new_surfaces[i] = updates[i].surface;
-		for (j = 0; j < context->res_ctx.pool->pipe_count; j++) {
-			struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j];
+			new_surfaces[i] = updates[i].surface;
+			for (j = 0; j < context->res_ctx.pool->pipe_count; j++) {
+				struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j];
 
-			if (surface == pipe_ctx->surface)
-				is_new_pipe_surface[i] = false;
+				if (surface == pipe_ctx->surface)
+					is_new_pipe_surface[i] = false;
+			}
 		}
-	}
 
 	if (dc_target) {
 		struct core_target *target = DC_TARGET_TO_CORE(dc_target);
diff --git a/drivers/gpu/drm/amd/include/amd_shared.h b/drivers/gpu/drm/amd/include/amd_shared.h
index 6fdfc70..959585a 100644
--- a/drivers/gpu/drm/amd/include/amd_shared.h
+++ b/drivers/gpu/drm/amd/include/amd_shared.h
@@ -187,14 +187,14 @@ struct amd_ip_funcs {
 	bool (*is_idle)(void *handle);
 	/* poll for idle */
 	int (*wait_for_idle)(void *handle);
-        /* check soft reset the IP block */
-        bool (*check_soft_reset)(void *handle);
-        /* pre soft reset the IP block */
-        int (*pre_soft_reset)(void *handle);
+	/* check soft reset the IP block */
+	bool (*check_soft_reset)(void *handle);
+	/* pre soft reset the IP block */
+	int (*pre_soft_reset)(void *handle);
 	/* soft reset the IP block */
 	int (*soft_reset)(void *handle);
-        /* post soft reset the IP block */
-        int (*post_soft_reset)(void *handle);
+	/* post soft reset the IP block */
+	int (*post_soft_reset)(void *handle);
  	/* dump the IP block status registers */
 	void (*print_status)(void *handle);
 	/* enable/disable cg for the IP block */
diff --git a/drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_d.h b/drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_d.h
old mode 100755
new mode 100644
diff --git a/drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_sh_mask.h b/drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_sh_mask.h
old mode 100755
new mode 100644
diff --git a/drivers/gpu/drm/amd/include/cgs_common.h b/drivers/gpu/drm/amd/include/cgs_common.h
old mode 100755
new mode 100644
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c
index b3f628e..4b14f25 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/cz_hwmgr.c
@@ -1553,7 +1553,7 @@ static void cz_hw_print_display_cfg(
 			cc6_settings->cpu_pstate_separation_time);
 }
 
-static int cz_set_cpu_power_state(struct pp_hwmgr *hwmgr)
+ static int cz_set_cpu_power_state(struct pp_hwmgr *hwmgr)
 {
 	struct cz_hwmgr *hw_data = (struct cz_hwmgr *)(hwmgr->backend);
 	uint32_t data = 0;
diff --git a/drivers/gpu/drm/amd/powerplay/smumgr/fiji_smumgr.c b/drivers/gpu/drm/amd/powerplay/smumgr/fiji_smumgr.c
old mode 100755
new mode 100644
diff --git a/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c b/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c
index 602d77b..ffe1f85 100644
--- a/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c
+++ b/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c
@@ -381,54 +381,54 @@ static void amd_sched_job_timedout(struct work_struct *work)
 
 void amd_sched_hw_job_reset(struct amd_gpu_scheduler *sched)
 {
-        struct amd_sched_job *s_job;
- 
-        spin_lock(&sched->job_list_lock);
-        list_for_each_entry_reverse(s_job, &sched->ring_mirror_list, node) {
-                if (fence_remove_callback(s_job->s_fence->parent, &s_job->s_fence->cb)) {
-                        fence_put(s_job->s_fence->parent);
-                        s_job->s_fence->parent = NULL;
-                }
-        }
-        atomic_set(&sched->hw_rq_count, 0);
-        spin_unlock(&sched->job_list_lock);
+	struct amd_sched_job *s_job;
+
+	spin_lock(&sched->job_list_lock);
+	list_for_each_entry_reverse(s_job, &sched->ring_mirror_list, node) {
+		if (fence_remove_callback(s_job->s_fence->parent, &s_job->s_fence->cb)) {
+			fence_put(s_job->s_fence->parent);
+			s_job->s_fence->parent = NULL;
+		}
+	}
+	atomic_set(&sched->hw_rq_count, 0);
+	spin_unlock(&sched->job_list_lock);
 }
 
 void amd_sched_job_recovery(struct amd_gpu_scheduler *sched)
 {
-        struct amd_sched_job *s_job, *tmp;
-        int r;
- 
-        spin_lock(&sched->job_list_lock);
-        s_job = list_first_entry_or_null(&sched->ring_mirror_list,
-                                         struct amd_sched_job, node);
-        if (s_job && sched->timeout != MAX_SCHEDULE_TIMEOUT)
-                schedule_delayed_work(&s_job->work_tdr, sched->timeout);
- 
-        list_for_each_entry_safe(s_job, tmp, &sched->ring_mirror_list, node) {
-                struct amd_sched_fence *s_fence = s_job->s_fence;
-                struct fence *fence;
- 
-                spin_unlock(&sched->job_list_lock);
-                fence = sched->ops->run_job(s_job);
-                atomic_inc(&sched->hw_rq_count);
-                if (fence) {
-                        s_fence->parent = fence_get(fence);
-                        r = fence_add_callback(fence, &s_fence->cb,
-                                               amd_sched_process_job);
-                        if (r == -ENOENT)
-                                amd_sched_process_job(fence, &s_fence->cb);
-                        else if (r)
-                                DRM_ERROR("fence add callback failed (%d)\n",
-                                          r);
-                        fence_put(fence);
-                } else {
-                        DRM_ERROR("Failed to run job!\n");
-                        amd_sched_process_job(NULL, &s_fence->cb);
-                }
-                spin_lock(&sched->job_list_lock);
-        }
-        spin_unlock(&sched->job_list_lock);
+	struct amd_sched_job *s_job, *tmp;
+	int r;
+
+	spin_lock(&sched->job_list_lock);
+	s_job = list_first_entry_or_null(&sched->ring_mirror_list,
+					 struct amd_sched_job, node);
+	if (s_job && sched->timeout != MAX_SCHEDULE_TIMEOUT)
+		schedule_delayed_work(&s_job->work_tdr, sched->timeout);
+
+	list_for_each_entry_safe(s_job, tmp, &sched->ring_mirror_list, node) {
+		struct amd_sched_fence *s_fence = s_job->s_fence;
+		struct fence *fence;
+
+		spin_unlock(&sched->job_list_lock);
+		fence = sched->ops->run_job(s_job);
+		atomic_inc(&sched->hw_rq_count);
+		if (fence) {
+			s_fence->parent = fence_get(fence);
+			r = fence_add_callback(fence, &s_fence->cb,
+					       amd_sched_process_job);
+			if (r == -ENOENT)
+				amd_sched_process_job(fence, &s_fence->cb);
+			else if (r)
+				DRM_ERROR("fence add callback failed (%d)\n",
+					  r);
+			fence_put(fence);
+		} else {
+			DRM_ERROR("Failed to run job!\n");
+			amd_sched_process_job(NULL, &s_fence->cb);
+		}
+		spin_lock(&sched->job_list_lock);
+	}
+	spin_unlock(&sched->job_list_lock);
 }
 
 /**
@@ -451,21 +451,21 @@ void amd_sched_entity_push_job(struct amd_sched_job *sched_job)
 
 /* init a sched_job with basic field */
 int amd_sched_job_init(struct amd_sched_job *job,
-                       struct amd_gpu_scheduler *sched,
-                       struct amd_sched_entity *entity,
-                       void *owner)
+		       struct amd_gpu_scheduler *sched,
+		       struct amd_sched_entity *entity,
+		       void *owner)
 {
-        job->sched = sched;
-        job->s_entity = entity;
-        job->s_fence = amd_sched_fence_create(entity, owner);
-        if (!job->s_fence)
-                return -ENOMEM;
+	job->sched = sched;
+	job->s_entity = entity;
+	job->s_fence = amd_sched_fence_create(entity, owner);
+	if (!job->s_fence)
+		return -ENOMEM;
 
-        INIT_WORK(&job->finish_work, amd_sched_job_finish);
-        INIT_LIST_HEAD(&job->node);
-        INIT_DELAYED_WORK(&job->work_tdr, amd_sched_job_timedout);
+	INIT_WORK(&job->finish_work, amd_sched_job_finish);
+	INIT_LIST_HEAD(&job->node);
+	INIT_DELAYED_WORK(&job->work_tdr, amd_sched_job_timedout);
 
-        return 0;
+	return 0;
 }
 
 /**
diff --git a/drivers/gpu/drm/amd/scheduler/gpu_scheduler.h b/drivers/gpu/drm/amd/scheduler/gpu_scheduler.h
index d97d0a9..51068e6 100644
--- a/drivers/gpu/drm/amd/scheduler/gpu_scheduler.h
+++ b/drivers/gpu/drm/amd/scheduler/gpu_scheduler.h
@@ -76,22 +76,21 @@ struct amd_sched_job {
 	struct amd_gpu_scheduler        *sched;
 	struct amd_sched_entity         *s_entity;
 	struct amd_sched_fence          *s_fence;
-        struct fence_cb                 finish_cb;
-        struct work_struct              finish_work;
-        struct list_head                node;
-        struct delayed_work             work_tdr;
+	struct fence_cb			finish_cb;
+	struct work_struct		finish_work;
+	struct list_head		node;
+	struct delayed_work		work_tdr;
 };
 
 extern const struct fence_ops amd_sched_fence_ops_scheduled;
 extern const struct fence_ops amd_sched_fence_ops_finished;
 static inline struct amd_sched_fence *to_amd_sched_fence(struct fence *f)
 {
-        if (f->ops == &amd_sched_fence_ops_scheduled)
-                return container_of(f, struct amd_sched_fence, scheduled);
+	if (f->ops == &amd_sched_fence_ops_scheduled)
+		return container_of(f, struct amd_sched_fence, scheduled);
 
-
-        if (f->ops == &amd_sched_fence_ops_finished)
-                return container_of(f, struct amd_sched_fence, finished);
+	if (f->ops == &amd_sched_fence_ops_finished)
+		return container_of(f, struct amd_sched_fence, finished);
 
 	return NULL;
 }
@@ -153,7 +152,7 @@ void amd_sched_fence_finished(struct amd_sched_fence *fence);
 int amd_sched_job_init(struct amd_sched_job *job,
 		       struct amd_gpu_scheduler *sched,
 		       struct amd_sched_entity *entity,
-                       void *owner);
+		       void *owner);
 void amd_sched_hw_job_reset(struct amd_gpu_scheduler *sched);
 void amd_sched_job_recovery(struct amd_gpu_scheduler *sched);
 #endif
-- 
2.7.4

