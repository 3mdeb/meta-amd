From d94c775f54f5d0285e97f6702e5ae61c4aae5485 Mon Sep 17 00:00:00 2001
From: Kalyan Alle <kalyan.alle@amd.com>
Date: Wed, 16 Nov 2016 20:33:45 +0530
Subject: [PATCH 1275/1722] drm/amdgpu : resolves the kernel compiling issues

after applying the patches from 1 to 1259.

Signed-off-by: Kalyan Alle <kalyan.alle@amd.com>
---
 arch/alpha/include/asm/rwsem.h                     |   9 ++
 arch/x86/include/asm/frame.h                       |  39 ++++++
 arch/x86/include/asm/rwsem.h                       |  29 +++++
 arch/x86/lib/rwsem.S                               |  11 ++
 drivers/dma-buf/Makefile                           |   2 +-
 drivers/dma-buf/fence-array.c                      | 144 +++++++++++++++++++++
 drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c   |   3 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c        |   2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c             |   4 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_display.c        |   2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c            |  10 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c            |   7 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h            |  12 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c             |   2 +-
 drivers/gpu/drm/amd/amdgpu/dce_v10_0.c             |   2 +-
 drivers/gpu/drm/amd/amdgpu/dce_v11_0.c             |   4 +-
 drivers/gpu/drm/amd/amdgpu/dce_virtual.c           |   2 +-
 drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c              |   4 +-
 .../drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c    |   3 +-
 .../gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_types.c    |  11 +-
 .../dal/dc/bios/dce112/bios_parser_helper_dce112.c |   1 -
 drivers/gpu/drm/amd/scheduler/gpu_scheduler.c      |   1 -
 drivers/gpu/drm/drm_drv.c                          |   1 +
 drivers/gpu/drm/ttm/ttm_memory.c                   |   8 ++
 include/drm/drmP.h                                 |  12 +-
 include/drm/ttm/ttm_bo_api.h                       |   4 +
 include/drm/ttm/ttm_memory.h                       |   1 +
 include/linux/fence-array.h                        |  73 +++++++++++
 include/linux/lockdep.h                            |  15 +++
 include/linux/rwsem.h                              |   2 +-
 include/uapi/drm/amdgpu_drm.h                      |  86 ++++++++++++
 kernel/locking/rwsem-xadd.c                        |  92 +++++++++++++
 kernel/locking/rwsem.c                             |  21 +++
 33 files changed, 579 insertions(+), 40 deletions(-)
 create mode 100644 drivers/dma-buf/fence-array.c
 create mode 100644 include/linux/fence-array.h

diff --git a/arch/alpha/include/asm/rwsem.h b/arch/alpha/include/asm/rwsem.h
index a83bbea6..2d03d10 100644
--- a/arch/alpha/include/asm/rwsem.h
+++ b/arch/alpha/include/asm/rwsem.h
@@ -87,6 +87,15 @@ static inline void __down_write(struct rw_semaphore *sem)
 		rwsem_down_write_failed(sem);
 }
 
+static inline int __down_write_killable(struct rw_semaphore *sem)
+{
+        if (unlikely(___down_write(sem)))
+                if (IS_ERR(rwsem_down_write_failed_killable(sem)))
+                        return -EINTR;
+
+        return 0;
+}
+
 /*
  * trylock for writing -- returns 1 if successful, 0 if contention
  */
diff --git a/arch/x86/include/asm/frame.h b/arch/x86/include/asm/frame.h
index 793179c..5ea8827 100644
--- a/arch/x86/include/asm/frame.h
+++ b/arch/x86/include/asm/frame.h
@@ -21,3 +21,42 @@
 #endif
 
 #endif  /*  __ASSEMBLY__  */
+
+/*
+ * These are stack frame creation macros.  They should be used by every
+ * callable non-leaf asm function to make kernel stack traces more reliable.
+ */
+
+#ifdef CONFIG_FRAME_POINTER
+
+#ifdef __ASSEMBLY__
+
+.macro FRAME_BEGIN
+        push %_ASM_BP
+        _ASM_MOV %_ASM_SP, %_ASM_BP
+.endm
+
+.macro FRAME_END
+        pop %_ASM_BP
+.endm
+
+#else /* !__ASSEMBLY__ */
+
+#define FRAME_BEGIN                             \
+        "push %" _ASM_BP "\n"                   \
+        _ASM_MOV "%" _ASM_SP ", %" _ASM_BP "\n"
+
+#define FRAME_END "pop %" _ASM_BP "\n"
+
+#endif /* __ASSEMBLY__ */
+
+#define FRAME_OFFSET __ASM_SEL(4, 8)
+
+#else /* !CONFIG_FRAME_POINTER */
+
+#define FRAME_BEGIN
+#define FRAME_END
+#define FRAME_OFFSET 0
+
+#endif /* CONFIG_FRAME_POINTER */
+
diff --git a/arch/x86/include/asm/rwsem.h b/arch/x86/include/asm/rwsem.h
index cad82c9..4122673 100644
--- a/arch/x86/include/asm/rwsem.h
+++ b/arch/x86/include/asm/rwsem.h
@@ -116,11 +116,40 @@ static inline void __down_write_nested(struct rw_semaphore *sem, int subclass)
 		     : "memory", "cc");
 }
 
+/*
+ * lock for writing
+ */
+#define ____down_write(sem, slow_path)                  \
+({                                                      \
+        long tmp;                                       \
+        struct rw_semaphore* ret;                       \
+        asm volatile("# beginning down_write\n\t"       \
+                     LOCK_PREFIX "  xadd      %1,(%3)\n\t"      \
+                     /* adds 0xffff0001, returns the old value */ \
+                     "  test " __ASM_SEL(%w1,%k1) "," __ASM_SEL(%w1,%k1) "\n\t" \
+                     /* was the active mask 0 before? */\
+                     "  jz        1f\n"                 \
+                     "  call " slow_path "\n"           \
+                     "1:\n"                             \
+                     "# ending down_write"              \
+                     : "+m" (sem->count), "=d" (tmp), "=a" (ret)        \
+                     : "a" (sem), "1" (RWSEM_ACTIVE_WRITE_BIAS) \
+                     : "memory", "cc");                 \
+        ret;                                            \
+})
+
 static inline void __down_write(struct rw_semaphore *sem)
 {
 	__down_write_nested(sem, 0);
 }
 
+static inline int __down_write_killable(struct rw_semaphore *sem)
+{
+        if (IS_ERR(____down_write(sem, "call_rwsem_down_write_failed_killable")))
+                return -EINTR;
+
+        return 0;
+}
 /*
  * trylock for writing -- returns 1 if successful, 0 if contention
  */
diff --git a/arch/x86/lib/rwsem.S b/arch/x86/lib/rwsem.S
index 40027db..6fd664c 100644
--- a/arch/x86/lib/rwsem.S
+++ b/arch/x86/lib/rwsem.S
@@ -15,6 +15,7 @@
 
 #include <linux/linkage.h>
 #include <asm/alternative-asm.h>
+#include <asm/frame.h>
 
 #define __ASM_HALF_REG(reg)	__ASM_SEL(reg, e##reg)
 #define __ASM_HALF_SIZE(inst)	__ASM_SEL(inst##w, inst##l)
@@ -101,6 +102,16 @@ ENTRY(call_rwsem_down_write_failed)
 	ret
 ENDPROC(call_rwsem_down_write_failed)
 
+ENTRY(call_rwsem_down_write_failed_killable)
+        FRAME_BEGIN
+        save_common_regs
+        movq %rax,%rdi
+        call rwsem_down_write_failed_killable
+        restore_common_regs
+        FRAME_END
+        ret
+ENDPROC(call_rwsem_down_write_failed_killable)
+
 ENTRY(call_rwsem_wake)
 	/* do nothing if still outstanding active readers */
 	__ASM_HALF_SIZE(dec) %__ASM_HALF_REG(dx)
diff --git a/drivers/dma-buf/Makefile b/drivers/dma-buf/Makefile
index 57a675f..85f6928 100644
--- a/drivers/dma-buf/Makefile
+++ b/drivers/dma-buf/Makefile
@@ -1 +1 @@
-obj-y := dma-buf.o fence.o reservation.o seqno-fence.o
+obj-y := dma-buf.o fence.o reservation.o seqno-fence.o fence-array.o
diff --git a/drivers/dma-buf/fence-array.c b/drivers/dma-buf/fence-array.c
new file mode 100644
index 0000000..a8731c8
--- /dev/null
+++ b/drivers/dma-buf/fence-array.c
@@ -0,0 +1,144 @@
+/*
+ * fence-array: aggregate fences to be waited together
+ *
+ * Copyright (C) 2016 Collabora Ltd
+ * Copyright (C) 2016 Advanced Micro Devices, Inc.
+ * Authors:
+ *	Gustavo Padovan <gustavo@padovan.org>
+ *	Christian König <christian.koenig@amd.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published by
+ * the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/export.h>
+#include <linux/slab.h>
+#include <linux/fence-array.h>
+
+static void fence_array_cb_func(struct fence *f, struct fence_cb *cb);
+
+static const char *fence_array_get_driver_name(struct fence *fence)
+{
+	return "fence_array";
+}
+
+static const char *fence_array_get_timeline_name(struct fence *fence)
+{
+	return "unbound";
+}
+
+static void fence_array_cb_func(struct fence *f, struct fence_cb *cb)
+{
+	struct fence_array_cb *array_cb =
+		container_of(cb, struct fence_array_cb, cb);
+	struct fence_array *array = array_cb->array;
+
+	if (atomic_dec_and_test(&array->num_pending))
+		fence_signal(&array->base);
+	fence_put(&array->base);
+}
+
+static bool fence_array_enable_signaling(struct fence *fence)
+{
+	struct fence_array *array = to_fence_array(fence);
+	struct fence_array_cb *cb = (void *)(&array[1]);
+	unsigned i;
+
+	for (i = 0; i < array->num_fences; ++i) {
+		cb[i].array = array;
+		/*
+		 * As we may report that the fence is signaled before all
+		 * callbacks are complete, we need to take an additional
+		 * reference count on the array so that we do not free it too
+		 * early. The core fence handling will only hold the reference
+		 * until we signal the array as complete (but that is now
+		 * insufficient).
+		 */
+		fence_get(&array->base);
+		if (fence_add_callback(array->fences[i], &cb[i].cb,
+				       fence_array_cb_func)) {
+			fence_put(&array->base);
+			if (atomic_dec_and_test(&array->num_pending))
+				return false;
+		}
+	}
+
+	return true;
+}
+
+static bool fence_array_signaled(struct fence *fence)
+{
+	struct fence_array *array = to_fence_array(fence);
+
+	return atomic_read(&array->num_pending) <= 0;
+}
+
+static void fence_array_release(struct fence *fence)
+{
+	struct fence_array *array = to_fence_array(fence);
+	unsigned i;
+
+	for (i = 0; i < array->num_fences; ++i)
+		fence_put(array->fences[i]);
+
+	kfree(array->fences);
+	fence_free(fence);
+}
+
+const struct fence_ops fence_array_ops = {
+	.get_driver_name = fence_array_get_driver_name,
+	.get_timeline_name = fence_array_get_timeline_name,
+	.enable_signaling = fence_array_enable_signaling,
+	.signaled = fence_array_signaled,
+	.wait = fence_default_wait,
+	.release = fence_array_release,
+};
+
+/**
+ * fence_array_create - Create a custom fence array
+ * @num_fences:		[in]	number of fences to add in the array
+ * @fences:		[in]	array containing the fences
+ * @context:		[in]	fence context to use
+ * @seqno:		[in]	sequence number to use
+ * @signal_on_any	[in]	signal on any fence in the array
+ *
+ * Allocate a fence_array object and initialize the base fence with fence_init().
+ * In case of error it returns NULL.
+ *
+ * The caller should allocte the fences array with num_fences size
+ * and fill it with the fences it wants to add to the object. Ownership of this
+ * array is take and fence_put() is used on each fence on release.
+ *
+ * If @signal_on_any is true the fence array signals if any fence in the array
+ * signals, otherwise it signals when all fences in the array signal.
+ */
+struct fence_array *fence_array_create(int num_fences, struct fence **fences,
+				       u64 context, unsigned seqno,
+				       bool signal_on_any)
+{
+	struct fence_array *array;
+	size_t size = sizeof(*array);
+
+	/* Allocate the callback structures behind the array. */
+	size += num_fences * sizeof(struct fence_array_cb);
+	array = kzalloc(size, GFP_KERNEL);
+	if (!array)
+		return NULL;
+
+	spin_lock_init(&array->lock);
+	fence_init(&array->base, &fence_array_ops, &array->lock,
+		   context, seqno);
+
+	array->num_fences = num_fences;
+	atomic_set(&array->num_pending, signal_on_any ? 1 : num_fences);
+	array->fences = fences;
+
+	return array;
+}
+EXPORT_SYMBOL(fence_array_create);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
index f407964..172ee13 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
@@ -595,14 +595,13 @@ static bool amdgpu_atpx_detect(void)
 void amdgpu_register_atpx_handler(void)
 {
 	bool r;
-        enum vga_switcheroo_handler_flags_t handler_flags = 0;
 
 	/* detect if we have any ATPX + 2 VGA in the system */
 	r = amdgpu_atpx_detect();
 	if (!r)
 		return;
 
-        vga_switcheroo_register_handler(&amdgpu_atpx_handler, handler_flags);
+        vga_switcheroo_register_handler(&amdgpu_atpx_handler);
 }
 
 /**
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c
index 651115d..844214d 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c
@@ -107,7 +107,7 @@ static int amdgpu_bo_list_set(struct amdgpu_device *adev,
 		struct amdgpu_bo *bo;
 		struct mm_struct *usermm;
 
-		gobj = drm_gem_object_lookup(filp, info[i].bo_handle);
+		gobj = drm_gem_object_lookup(adev->ddev, filp, info[i].bo_handle);
 		if (!gobj) {
 			r = -ENOENT;
 			goto error_free;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
index 2b21c5c..fa4dc28 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
@@ -134,7 +134,7 @@ static int amdgpu_cs_user_fence_chunk(struct amdgpu_cs_parser *p,
 	struct drm_gem_object *gobj;
 	unsigned long size;
 
-	gobj = drm_gem_object_lookup(p->filp, data->handle);
+	gobj = drm_gem_object_lookup(p->adev->ddev, p->filp, data->handle);
 	if (gobj == NULL)
 		return -EINVAL;
 
@@ -1095,7 +1095,7 @@ int amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)
 		r = amdgpu_cs_handle_lockup(adev, r);
 		return r;
 	}
-	r = amdgpu_cs_parser_bos(&parser);
+	r = amdgpu_cs_parser_bos(&parser, data);
 	if (r == -ENOMEM)
 		DRM_ERROR("Not enough memory for command submission!\n");
 	else if (r && r != -ERESTARTSYS)
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
index befc8b8..8ed22fe 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
@@ -563,7 +563,7 @@ amdgpu_user_framebuffer_create(struct drm_device *dev,
 	struct amdgpu_framebuffer *amdgpu_fb;
 	int ret;
 
-	obj = drm_gem_object_lookup(file_priv, mode_cmd->handles[0]);
+	obj = drm_gem_object_lookup(dev, file_priv, mode_cmd->handles[0]);
 	if (obj ==  NULL) {
 		dev_err(&dev->pdev->dev, "No GEM object associated to handle 0x%08X, "
 			"can't create framebuffer\n", mode_cmd->handles[0]);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
index e25c237..3c5a7a9 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
@@ -339,7 +339,7 @@ int amdgpu_mode_dumb_mmap(struct drm_file *filp,
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *robj;
 
-	gobj = drm_gem_object_lookup(filp, handle);
+	gobj = drm_gem_object_lookup(dev, filp, handle);
 	if (gobj == NULL) {
 		return -ENOENT;
 	}
@@ -403,7 +403,7 @@ int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
 	int r = 0;
 	long ret;
 
-	gobj = drm_gem_object_lookup(filp, handle);
+	gobj = drm_gem_object_lookup(dev, filp, handle);
 	if (gobj == NULL) {
 		return -ENOENT;
 	}
@@ -437,7 +437,7 @@ int amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,
 	int r = -1;
 
 	DRM_DEBUG("%d \n", args->handle);
-	gobj = drm_gem_object_lookup(filp, args->handle);
+	gobj = drm_gem_object_lookup(dev, filp, args->handle);
 	if (gobj == NULL)
 		return -ENOENT;
 	robj = gem_to_amdgpu_bo(gobj);
@@ -591,7 +591,7 @@ int amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,
 		return -EINVAL;
 	}
 
-	gobj = drm_gem_object_lookup(filp, args->handle);
+	gobj = drm_gem_object_lookup(dev, filp, args->handle);
 	if (gobj == NULL)
 		return -ENOENT;
 	abo = gem_to_amdgpu_bo(gobj);
@@ -651,7 +651,7 @@ int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
 	struct amdgpu_bo *robj;
 	int r;
 
-	gobj = drm_gem_object_lookup(filp, args->handle);
+	gobj = drm_gem_object_lookup(dev, filp, args->handle);
 	if (gobj == NULL) {
 		return -ENOENT;
 	}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
index 09d0813..ae585e6 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
@@ -34,7 +34,6 @@
 #include <ttm/ttm_placement.h>
 #include <ttm/ttm_module.h>
 #include <ttm/ttm_page_alloc.h>
-#include <ttm/ttm_memory.h>
 #include <drm/drmP.h>
 #include <drm/amdgpu_drm.h>
 #include <linux/seq_file.h>
@@ -371,7 +370,7 @@ static int amdgpu_move_vram_ram(struct ttm_buffer_object *bo,
 	if (unlikely(r)) {
 		goto out_cleanup;
 	}
-	r = ttm_bo_move_ttm(bo, true, interruptible, no_wait_gpu, new_mem);
+	r = ttm_bo_move_ttm(bo, true, no_wait_gpu, new_mem);
 out_cleanup:
 	ttm_bo_mem_put(bo, &tmp_mem);
 	return r;
@@ -404,7 +403,7 @@ static int amdgpu_move_ram_vram(struct ttm_buffer_object *bo,
 	if (unlikely(r)) {
 		return r;
 	}
-	r = ttm_bo_move_ttm(bo, true, interruptible, no_wait_gpu, &tmp_mem);
+	r = ttm_bo_move_ttm(bo, true, no_wait_gpu, &tmp_mem);
 	if (unlikely(r)) {
 		goto out_cleanup;
 	}
@@ -471,7 +470,7 @@ static int amdgpu_bo_move(struct ttm_buffer_object *bo,
 
 	if (r) {
 memcpy:
-		r = ttm_bo_move_memcpy(bo, evict, interruptible,
+		r = ttm_bo_move_memcpy(bo, evict,
 				       no_wait_gpu, new_mem);
 		if (r) {
 			return r;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h
index d1c00c0..6115a6a 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h
@@ -26,13 +26,13 @@
 
 #include "gpu_scheduler.h"
 
-#define AMDGPU_PL_GDS		(TTM_PL_PRIV + 0)
-#define AMDGPU_PL_GWS		(TTM_PL_PRIV + 1)
-#define AMDGPU_PL_OA		(TTM_PL_PRIV + 2)
+#define AMDGPU_PL_GDS		(TTM_PL_PRIV0 + 0)
+#define AMDGPU_PL_GWS		(TTM_PL_PRIV0 + 1)
+#define AMDGPU_PL_OA		(TTM_PL_PRIV0 + 2)
 
-#define AMDGPU_PL_FLAG_GDS		(TTM_PL_FLAG_PRIV << 0)
-#define AMDGPU_PL_FLAG_GWS		(TTM_PL_FLAG_PRIV << 1)
-#define AMDGPU_PL_FLAG_OA		(TTM_PL_FLAG_PRIV << 2)
+#define AMDGPU_PL_FLAG_GDS		(TTM_PL_FLAG_PRIV0 << 0)
+#define AMDGPU_PL_FLAG_GWS		(TTM_PL_FLAG_PRIV0 << 1)
+#define AMDGPU_PL_FLAG_OA		(TTM_PL_FLAG_PRIV0 << 2)
 
 #define AMDGPU_TTM_LRU_SIZE	20
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index cf769e6..23908ce 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -1501,7 +1501,7 @@ int amdgpu_vm_bo_unmap(struct amdgpu_device *adev,
 	interval_tree_remove(&mapping->it, &vm->va);
 	trace_amdgpu_vm_bo_unmap(bo_va, mapping);
 
-	if (valid) {
+	if (valid) 
 		list_add(&mapping->list, &vm->freed);
         else 
 		kfree(mapping);
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_v10_0.c b/drivers/gpu/drm/amd/amdgpu/dce_v10_0.c
index 4012e1b..444a30a 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_v10_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/dce_v10_0.c
@@ -2554,7 +2554,7 @@ static int dce_v10_0_crtc_cursor_set2(struct drm_crtc *crtc,
 		return -EINVAL;
 	}
 
-	obj = drm_gem_object_lookup(file_priv, handle);
+	obj = drm_gem_object_lookup(crtc->dev, file_priv, handle);
 	if (!obj) {
 		DRM_ERROR("Cannot find cursor object %x for crtc %d\n", handle, amdgpu_crtc->crtc_id);
 		return -ENOENT;
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_v11_0.c b/drivers/gpu/drm/amd/amdgpu/dce_v11_0.c
index 5a06b78..009ad64 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_v11_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/dce_v11_0.c
@@ -2570,7 +2570,7 @@ static int dce_v11_0_crtc_cursor_set2(struct drm_crtc *crtc,
 		return -EINVAL;
 	}
 
-	obj = drm_gem_object_lookup(file_priv, handle);
+	obj = drm_gem_object_lookup(crtc->dev, file_priv, handle);
 	if (!obj) {
 		DRM_ERROR("Cannot find cursor object %x for crtc %d\n", handle, amdgpu_crtc->crtc_id);
 		return -ENOENT;
@@ -2982,6 +2982,7 @@ static int dce_v11_0_early_init(void *handle)
 		adev->mode_info.num_hpd = 5;
 		adev->mode_info.num_dig = 5;
 		break;
+#if 0
         case CHIP_ELLESMERE:
                 adev->mode_info.num_crtc = 6;
                 adev->mode_info.num_hpd = 6;
@@ -2992,6 +2993,7 @@ static int dce_v11_0_early_init(void *handle)
                 adev->mode_info.num_hpd = 5;
                 adev->mode_info.num_dig = 5;
                 break;
+#endif
 	default:
 		/* FIXME: not supported yet */
 		return -EINVAL;
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_virtual.c b/drivers/gpu/drm/amd/amdgpu/dce_virtual.c
index 0124e24..c0950d4 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_virtual.c
+++ b/drivers/gpu/drm/amd/amdgpu/dce_virtual.c
@@ -664,7 +664,7 @@ static int dce_virtual_connector_encoder_init(struct amdgpu_device *adev,
 		return -ENOMEM;
 	encoder->possible_crtcs = 1 << index;
 	drm_encoder_init(adev->ddev, encoder, &dce_virtual_encoder_funcs,
-			 DRM_MODE_ENCODER_VIRTUAL, NULL);
+			 DRM_MODE_ENCODER_VIRTUAL);
 	drm_encoder_helper_add(encoder, &dce_virtual_encoder_helper_funcs);
 
 	connector = kzalloc(sizeof(struct drm_connector), GFP_KERNEL);
diff --git a/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c b/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
index aff8368..772db95 100644
--- a/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
@@ -248,8 +248,8 @@ static int uvd_v6_0_resume(void *handle)
 	r = uvd_v6_0_hw_init(adev);
 	if (r)
 		return r;
-	
-	amdgpu_dpm_enable_uvd(adev, true);
+
+	amdgpu_dpm_enable_uvd(adev, true);	
 
 	return r;
 }
diff --git a/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c b/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c
index 7521efd..bc3f72e 100644
--- a/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c
+++ b/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_mst_types.c
@@ -211,8 +211,7 @@ dm_dp_create_fake_mst_encoder(struct amdgpu_connector *connector)
 		dev,
 		&amdgpu_encoder->base,
 		NULL,
-		DRM_MODE_ENCODER_DPMST,
-		NULL);
+		DRM_MODE_ENCODER_DPMST);
 
 	drm_encoder_helper_add(encoder, &amdgpu_dm_encoder_helper_funcs);
 
diff --git a/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_types.c b/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_types.c
index 1f3e95f..de5b233 100644
--- a/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_types.c
+++ b/drivers/gpu/drm/amd/dal/amdgpu_dm/amdgpu_dm_types.c
@@ -156,7 +156,7 @@ static int dm_crtc_pin_cursor_bo_new(
 
 		amdgpu_crtc = to_amdgpu_crtc(crtc);
 
-		obj = drm_gem_object_lookup(file_priv, handle);
+		obj = drm_gem_object_lookup(crtc->dev, file_priv, handle);
 
 		if (!obj) {
 			DRM_ERROR(
@@ -1088,7 +1088,7 @@ retry:
 		goto fail;
 	}
 	acrtc->flip_flags = flags;
-	ret = drm_atomic_nonblocking_commit(state);
+	ret = drm_atomic_async_commit(state);
 	if (ret != 0)
 		goto fail;
 
@@ -1818,7 +1818,7 @@ int amdgpu_dm_crtc_init(struct amdgpu_display_manager *dm,
 		&dm_plane_funcs,
 		rgb_formats,
 		ARRAY_SIZE(rgb_formats),
-		DRM_PLANE_TYPE_PRIMARY, NULL);
+		DRM_PLANE_TYPE_PRIMARY);
 
 	primary_plane->crtc = &acrtc->base;
 
@@ -1829,7 +1829,7 @@ int amdgpu_dm_crtc_init(struct amdgpu_display_manager *dm,
 			&acrtc->base,
 			primary_plane,
 			NULL,
-			&amdgpu_dm_crtc_funcs, NULL);
+			&amdgpu_dm_crtc_funcs);
 
 	if (res)
 		goto fail;
@@ -2256,8 +2256,7 @@ int amdgpu_dm_encoder_init(
 	int res = drm_encoder_init(dev,
 				   &aencoder->base,
 				   &amdgpu_dm_encoder_funcs,
-				   DRM_MODE_ENCODER_TMDS,
-				   NULL);
+				   DRM_MODE_ENCODER_TMDS);
 
 	aencoder->base.possible_crtcs = amdgpu_dm_get_encoder_crtc_mask(adev);
 
diff --git a/drivers/gpu/drm/amd/dal/dc/bios/dce112/bios_parser_helper_dce112.c b/drivers/gpu/drm/amd/dal/dc/bios/dce112/bios_parser_helper_dce112.c
index b03c8f9..f9e72d2 100644
--- a/drivers/gpu/drm/amd/dal/dc/bios/dce112/bios_parser_helper_dce112.c
+++ b/drivers/gpu/drm/amd/dal/dc/bios/dce112/bios_parser_helper_dce112.c
@@ -155,7 +155,6 @@ static enum signal_type detect_sink(
 /* function table */
 static const struct bios_parser_helper bios_parser_helper_funcs = {
 	.is_accelerated_mode = is_accelerated_mode,
-	.set_scratch_acc_mode_change = set_scratch_acc_mode_change
 };
 
 /*
diff --git a/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c b/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c
index e7c7dbc..d6c19d3 100644
--- a/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c
+++ b/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c
@@ -460,7 +460,6 @@ int amd_sched_job_init(struct amd_sched_job *job,
 {
         job->sched = sched;
         job->s_entity = entity;
-        job->fence_context = entity->fence_context;
         job->s_fence = amd_sched_fence_create(entity, owner);
         if (!job->s_fence)
                 return -ENOMEM;
diff --git a/drivers/gpu/drm/drm_drv.c b/drivers/gpu/drm/drm_drv.c
index 7dd6728..b29f380f 100644
--- a/drivers/gpu/drm/drm_drv.c
+++ b/drivers/gpu/drm/drm_drv.c
@@ -592,6 +592,7 @@ struct drm_device *drm_dev_alloc(struct drm_driver *driver,
 	spin_lock_init(&dev->buf_lock);
 	spin_lock_init(&dev->event_lock);
 	mutex_init(&dev->struct_mutex);
+	mutex_init(&dev->filelist_mutex);
 	mutex_init(&dev->ctxlist_mutex);
 	mutex_init(&dev->master_mutex);
 
diff --git a/drivers/gpu/drm/ttm/ttm_memory.c b/drivers/gpu/drm/ttm/ttm_memory.c
index a1803fb..8fb778e 100644
--- a/drivers/gpu/drm/ttm/ttm_memory.c
+++ b/drivers/gpu/drm/ttm/ttm_memory.c
@@ -600,3 +600,11 @@ size_t ttm_round_pot(size_t size)
 	return 0;
 }
 EXPORT_SYMBOL(ttm_round_pot);
+
+uint64_t ttm_get_kernel_zone_memory_size(struct ttm_mem_global *glob)
+{
+         return glob->zone_kernel->max_mem;
+}
+EXPORT_SYMBOL(ttm_get_kernel_zone_memory_size);
+
+
diff --git a/include/drm/drmP.h b/include/drm/drmP.h
index 1664f2b..2c7ce7b 100644
--- a/include/drm/drmP.h
+++ b/include/drm/drmP.h
@@ -582,6 +582,15 @@ struct drm_driver {
 	 * Returns 0 on success.
 	 */
 	void (*gem_free_object) (struct drm_gem_object *obj);
+ 
+        /**
+          * @gem_free_object_unlocked: deconstructor for drm_gem_objects
+          *
+          * This is for drivers which are not encumbered with dev->struct_mutex
+          * legacy locking schemes. Use this hook instead of @gem_free_object.
+          */
+        void (*gem_free_object_unlocked) (struct drm_gem_object *obj);
+
 	int (*gem_open_object) (struct drm_gem_object *, struct drm_file *);
 	void (*gem_close_object) (struct drm_gem_object *, struct drm_file *);
 
@@ -757,7 +766,8 @@ struct drm_device {
 	int buf_use;			/**< Buffers in use -- cannot alloc */
 	atomic_t buf_alloc;		/**< Buffer allocation in progress */
 	/*@} */
-
+ 
+        struct mutex filelist_mutex;
 	struct list_head filelist;
 
 	/** \name Memory management */
diff --git a/include/drm/ttm/ttm_bo_api.h b/include/drm/ttm/ttm_bo_api.h
index afae231..e80a016 100644
--- a/include/drm/ttm/ttm_bo_api.h
+++ b/include/drm/ttm/ttm_bo_api.h
@@ -240,6 +240,10 @@ struct ttm_buffer_object {
 	 */
 
 	unsigned long priv_flags;
+        /**
+         * Members protected by a bo reservation.
+         */
+        struct fence *moving;
 
 	struct drm_vma_offset_node vma_node;
 
diff --git a/include/drm/ttm/ttm_memory.h b/include/drm/ttm/ttm_memory.h
index 72dcbe8..c452089 100644
--- a/include/drm/ttm/ttm_memory.h
+++ b/include/drm/ttm/ttm_memory.h
@@ -155,4 +155,5 @@ extern int ttm_mem_global_alloc_page(struct ttm_mem_global *glob,
 extern void ttm_mem_global_free_page(struct ttm_mem_global *glob,
 				     struct page *page);
 extern size_t ttm_round_pot(size_t size);
+extern uint64_t ttm_get_kernel_zone_memory_size(struct ttm_mem_global *glob);
 #endif
diff --git a/include/linux/fence-array.h b/include/linux/fence-array.h
new file mode 100644
index 0000000..86baaa4
--- /dev/null
+++ b/include/linux/fence-array.h
@@ -0,0 +1,73 @@
+/*
+ * fence-array: aggregates fence to be waited together
+ *
+ * Copyright (C) 2016 Collabora Ltd
+ * Copyright (C) 2016 Advanced Micro Devices, Inc.
+ * Authors:
+ *	Gustavo Padovan <gustavo@padovan.org>
+ *	Christian König <christian.koenig@amd.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published by
+ * the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __LINUX_FENCE_ARRAY_H
+#define __LINUX_FENCE_ARRAY_H
+
+#include <linux/fence.h>
+
+/**
+ * struct fence_array_cb - callback helper for fence array
+ * @cb: fence callback structure for signaling
+ * @array: reference to the parent fence array object
+ */
+struct fence_array_cb {
+	struct fence_cb cb;
+	struct fence_array *array;
+};
+
+/**
+ * struct fence_array - fence to represent an array of fences
+ * @base: fence base class
+ * @lock: spinlock for fence handling
+ * @num_fences: number of fences in the array
+ * @num_pending: fences in the array still pending
+ * @fences: array of the fences
+ */
+struct fence_array {
+	struct fence base;
+
+	spinlock_t lock;
+	unsigned num_fences;
+	atomic_t num_pending;
+	struct fence **fences;
+};
+
+extern const struct fence_ops fence_array_ops;
+
+/**
+ * to_fence_array - cast a fence to a fence_array
+ * @fence: fence to cast to a fence_array
+ *
+ * Returns NULL if the fence is not a fence_array,
+ * or the fence_array otherwise.
+ */
+static inline struct fence_array *to_fence_array(struct fence *fence)
+{
+	if (fence->ops != &fence_array_ops)
+		return NULL;
+
+	return container_of(fence, struct fence_array, base);
+}
+
+struct fence_array *fence_array_create(int num_fences, struct fence **fences,
+				       u64 context, unsigned seqno,
+				       bool signal_on_any);
+
+#endif /* __LINUX_FENCE_ARRAY_H */
diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index c57e424..9ac7b62 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -446,6 +446,18 @@ do {								\
 	lock_acquired(&(_lock)->dep_map, _RET_IP_);			\
 } while (0)
 
+#define LOCK_CONTENDED_RETURN(_lock, try, lock)                 \
+({                                                              \
+        int ____err = 0;                                        \
+        if (!try(_lock)) {                                      \
+                lock_contended(&(_lock)->dep_map, _RET_IP_);    \
+                ____err = lock(_lock);                          \
+        }                                                       \
+        if (!____err)                                           \
+                lock_acquired(&(_lock)->dep_map, _RET_IP_);     \
+        ____err;                                                \
+})
+
 #else /* CONFIG_LOCK_STAT */
 
 #define lock_contended(lockdep_map, ip) do {} while (0)
@@ -454,6 +466,9 @@ do {								\
 #define LOCK_CONTENDED(_lock, try, lock) \
 	lock(_lock)
 
+#define LOCK_CONTENDED_RETURN(_lock, try, lock) \
+        lock(_lock)
+
 #endif /* CONFIG_LOCK_STAT */
 
 #ifdef CONFIG_LOCKDEP
diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 8f498cd..bd1f7c9 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -116,7 +116,7 @@ extern int down_read_trylock(struct rw_semaphore *sem);
  * lock for writing
  */
 extern void down_write(struct rw_semaphore *sem);
-
+extern int __must_check down_write_killable(struct rw_semaphore *sem);
 /*
  * trylock for writing -- returns 1 if successful, 0 if contention
  */
diff --git a/include/uapi/drm/amdgpu_drm.h b/include/uapi/drm/amdgpu_drm.h
index 7039d22..5b2a297 100644
--- a/include/uapi/drm/amdgpu_drm.h
+++ b/include/uapi/drm/amdgpu_drm.h
@@ -75,6 +75,12 @@
 #define AMDGPU_GEM_CREATE_NO_CPU_ACCESS		(1 << 1)
 /* Flag that USWC attributes should be used for GTT */
 #define AMDGPU_GEM_CREATE_CPU_GTT_USWC		(1 << 2)
+/* Flag that the memory should be in VRAM and cleared */
+#define AMDGPU_GEM_CREATE_VRAM_CLEARED          (1 << 3)
+/* Flag that create shadow bo(GTT) while allocating vram bo */
+#define AMDGPU_GEM_CREATE_SHADOW                (1 << 4)
+/* Flag that allocating the BO should use linear VRAM */
+#define AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS       (1 << 5)
 
 struct drm_amdgpu_gem_create_in  {
 	/** the requested memory size */
@@ -479,12 +485,34 @@ struct drm_amdgpu_cs_chunk_data {
 #define AMDGPU_INFO_DEV_INFO			0x16
 /* visible vram usage */
 #define AMDGPU_INFO_VIS_VRAM_USAGE		0x17
+/* number of TTM buffer evictions */
+#define AMDGPU_INFO_NUM_EVICTIONS               0x18
+/* Query memory about VRAM and GTT domains */
+#define AMDGPU_INFO_MEMORY                      0x19
+/* Query vce clock table */
+#define AMDGPU_INFO_VCE_CLOCK_TABLE             0x1A
 
 #define AMDGPU_INFO_MMR_SE_INDEX_SHIFT	0
 #define AMDGPU_INFO_MMR_SE_INDEX_MASK	0xff
 #define AMDGPU_INFO_MMR_SH_INDEX_SHIFT	8
 #define AMDGPU_INFO_MMR_SH_INDEX_MASK	0xff
 
+struct drm_amdgpu_query_fw {
+        /** AMDGPU_INFO_FW_* */
+        __u32 fw_type;
+        /**
+         * Index of the IP if there are more IPs of
+         * the same type.
+         */
+        __u32 ip_instance;
+        /**
+         * Index of the engine. Whether this is used depends
+         * on the firmware type. (e.g. MEC, SDMA)
+         */
+        __u32 index;
+        __u32 _pad;
+};
+
 /* Input structure for the INFO ioctl */
 struct drm_amdgpu_info {
 	/* Where the return value will be stored */
@@ -562,6 +590,35 @@ struct drm_amdgpu_info_vram_gtt {
 	uint64_t gtt_size;
 };
 
+struct drm_amdgpu_heap_info {
+        /** max. physical memory */
+        __u64 total_heap_size;
+
+        /** Theoretical max. available memory in the given heap */
+        __u64 usable_heap_size;
+
+        /**
+         * Number of bytes allocated in the heap. This includes all processes
+         * and private allocations in the kernel. It changes when new buffers
+         * are allocated, freed, and moved. It cannot be larger than
+         * heap_size.
+         */
+        __u64 heap_usage;
+
+        /**
+         * Theoretical possible max. size of buffer which
+         * could be allocated in the given heap
+         */
+        __u64 max_allocation;
+};
+
+struct drm_amdgpu_memory_info {
+        struct drm_amdgpu_heap_info vram;
+        struct drm_amdgpu_heap_info cpu_accessible_vram;
+        struct drm_amdgpu_heap_info gtt;
+};
+
+
 struct drm_amdgpu_info_firmware {
 	uint32_t ver;
 	uint32_t feature;
@@ -635,6 +692,24 @@ struct drm_amdgpu_info_hw_ip {
 	uint32_t  _pad;
 };
 
+#define AMDGPU_VCE_CLOCK_TABLE_ENTRIES          6
+
+struct drm_amdgpu_info_vce_clock_table_entry {
+        /** System clock */
+        __u32 sclk;
+        /** Memory clock */
+        __u32 mclk;
+        /** VCE clock */
+        __u32 eclk;
+        __u32 pad;
+};
+
+struct drm_amdgpu_info_vce_clock_table {
+        struct drm_amdgpu_info_vce_clock_table_entry entries[AMDGPU_VCE_CLOCK_TABLE_ENTRIES];
+        __u32 num_valid_entries;
+        __u32 pad;
+};
+
 /*
  * Supported GPU families
  */
@@ -644,10 +719,21 @@ struct drm_amdgpu_info_hw_ip {
 #define AMDGPU_FAMILY_VI			130 /* Iceland, Tonga */
 #define AMDGPU_FAMILY_CZ			135 /* Carrizo, Stoney */
 
+/*
+ * Definition of free sync enter and exit signals
+ * We may have more options in the future
+ */
+#define AMDGPU_FREESYNC_FULLSCREEN_ENTER                1
+#define AMDGPU_FREESYNC_FULLSCREEN_EXIT                 2
+
 struct drm_amdgpu_freesync {
          __u32 op;                       /* AMDGPU_FREESYNC_FULLSCREEN_ENTER or */
                                          /* AMDGPU_FREESYNC_FULLSCREEN_ENTER */
          __u32 spare[7];
 };
 
+#if defined(__cplusplus)
+}
+#endif
+
 #endif
diff --git a/kernel/locking/rwsem-xadd.c b/kernel/locking/rwsem-xadd.c
index a4d4de0..10fd859 100644
--- a/kernel/locking/rwsem-xadd.c
+++ b/kernel/locking/rwsem-xadd.c
@@ -418,6 +418,91 @@ static inline bool rwsem_has_spinner(struct rw_semaphore *sem)
 	return osq_is_locked(&sem->osq);
 }
 
+/*
+ * Wait until we successfully acquire the write lock
+ */
+static inline struct rw_semaphore *
+__rwsem_down_write_failed_common(struct rw_semaphore *sem, int state)
+{
+        long count;
+        bool waiting = true; /* any queued threads before us */
+        struct rwsem_waiter waiter;
+        struct rw_semaphore *ret = sem;
+
+        /* undo write bias from down_write operation, stop active locking */
+        count = rwsem_atomic_update(-RWSEM_ACTIVE_WRITE_BIAS, sem);
+
+        /* do optimistic spinning and steal lock if possible */
+        if (rwsem_optimistic_spin(sem))
+                return sem;
+
+        /*
+         * Optimistic spinning failed, proceed to the slowpath
+         * and block until we can acquire the sem.
+         */
+        waiter.task = current;
+        waiter.type = RWSEM_WAITING_FOR_WRITE;
+
+        raw_spin_lock_irq(&sem->wait_lock);
+
+        /* account for this before adding a new element to the list */
+        if (list_empty(&sem->wait_list))
+                waiting = false;
+
+        list_add_tail(&waiter.list, &sem->wait_list);
+
+        /* we're now waiting on the lock, but no longer actively locking */
+        if (waiting) {
+                count = READ_ONCE(sem->count);
+
+                /*
+                 * If there were already threads queued before us and there are
+                 * no active writers, the lock must be read owned; so we try to
+                 * wake any read locks that were queued ahead of us.
+                 */
+                if (count > RWSEM_WAITING_BIAS)
+                        sem = __rwsem_do_wake(sem, RWSEM_WAKE_READERS);
+
+        } else
+                count = rwsem_atomic_update(RWSEM_WAITING_BIAS, sem);
+
+        /* wait until we successfully acquire the lock */
+        set_current_state(state);
+        while (true) {
+                if (rwsem_try_write_lock(count, sem))
+                        break;
+                raw_spin_unlock_irq(&sem->wait_lock);
+
+                /* Block until there are no active lockers. */
+                do {
+                        if (signal_pending_state(state, current))
+                                goto out_nolock;
+
+                        schedule();
+                        set_current_state(state);
+                } while ((count = sem->count) & RWSEM_ACTIVE_MASK);
+
+                raw_spin_lock_irq(&sem->wait_lock);
+        }
+        __set_current_state(TASK_RUNNING);
+        list_del(&waiter.list);
+        raw_spin_unlock_irq(&sem->wait_lock);
+
+        return ret;
+
+out_nolock:
+        __set_current_state(TASK_RUNNING);
+        raw_spin_lock_irq(&sem->wait_lock);
+        list_del(&waiter.list);
+        if (list_empty(&sem->wait_list))
+                rwsem_atomic_update(-RWSEM_WAITING_BIAS, sem);
+        else
+                __rwsem_do_wake(sem, RWSEM_WAKE_ANY);
+        raw_spin_unlock_irq(&sem->wait_lock);
+
+        return ERR_PTR(-EINTR);
+}
+
 #else
 static bool rwsem_optimistic_spin(struct rw_semaphore *sem)
 {
@@ -501,6 +586,13 @@ struct rw_semaphore __sched *rwsem_down_write_failed(struct rw_semaphore *sem)
 }
 EXPORT_SYMBOL(rwsem_down_write_failed);
 
+__visible struct rw_semaphore * __sched
+rwsem_down_write_failed_killable(struct rw_semaphore *sem)
+{
+        return __rwsem_down_write_failed_common(sem, TASK_KILLABLE);
+}
+EXPORT_SYMBOL(rwsem_down_write_failed_killable);
+
 /*
  * handle waking up a waiter on the semaphore
  * - up_read/up_write has decremented the active part of count if we come here
diff --git a/kernel/locking/rwsem.c b/kernel/locking/rwsem.c
index 205be0c..7ff94e2 100644
--- a/kernel/locking/rwsem.c
+++ b/kernel/locking/rwsem.c
@@ -55,6 +55,27 @@ void __sched down_write(struct rw_semaphore *sem)
 EXPORT_SYMBOL(down_write);
 
 /*
+ * lock for writing
+ */
+int __sched down_write_killable(struct rw_semaphore *sem)
+{
+        might_sleep();
+        rwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);
+
+        if (LOCK_CONTENDED_RETURN(sem, __down_write_trylock, __down_write_killable)) {
+                rwsem_release(&sem->dep_map, 1, _RET_IP_);
+                return -EINTR;
+        }
+
+        rwsem_set_owner(sem);
+        return 0;
+}
+
+EXPORT_SYMBOL(down_write_killable);
+
+/*
+
+/*
  * trylock for writing -- returns 1 if successful, 0 if contention
  */
 int down_write_trylock(struct rw_semaphore *sem)
-- 
2.7.4

