From 0a033a6a135a516ed9c1936c1e1de7fe4f7ad004 Mon Sep 17 00:00:00 2001
From: Sanjay R Mehta <Sanju.Mehta@amd.com>
Date: Sat, 20 May 2017 14:55:52 +0530
Subject: [PATCH] drm/amd/amdgpu: disabling rocm and taking the delta of

amdgpu driver from the raven-hybrd branch

This patch disables rocm and takes the diff between two branches
at a commit: the commit is the branch point of amd-temp-raven-4.9
on the 4.9 staging branch.

Signed-off-by: Kalyan Alle <kalyan.alle@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/Makefile                |   18 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu.h                |   82 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c            |   14 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_acp.h            |    2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c         |  361 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h         |  147 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c   |  197 -
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c  |  507 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c  |  547 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h  |   62 -
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c   | 2251 -------
 drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c   |   28 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c        |    4 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_connectors.c     |   19 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c             |   58 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_device.c         |   69 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_display.c        |  114 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h            |   30 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c            |   47 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_fb.c             |    1 -
 drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c          |    2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c            |   53 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c             |    2 -
 drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c            |   12 -
 drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c             |  165 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h           |   17 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_object.c         |   21 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_object.h         |    2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c             |  202 -
 drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c          |  106 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c           |    3 -
 drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h           |    2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_sa.c             |    3 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_sched.c          |    0
 drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c           |   59 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h           |    3 -
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c            |  271 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h            |   17 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c            |    2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c             |  436 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h             |   30 -
 drivers/gpu/drm/amd/amdgpu/ci_dpm.c                |  260 -
 drivers/gpu/drm/amd/amdgpu/ci_dpm.h                |   12 -
 drivers/gpu/drm/amd/amdgpu/cik.c                   |   10 +-
 drivers/gpu/drm/amd/amdgpu/cik_sdma.c              |   66 -
 drivers/gpu/drm/amd/amdgpu/cikd.h                  |    2 +-
 drivers/gpu/drm/amd/amdgpu/dce_v10_0.c             |   48 +-
 drivers/gpu/drm/amd/amdgpu/dce_v10_0.h             |    4 +-
 drivers/gpu/drm/amd/amdgpu/dce_v11_0.c             |   56 +-
 drivers/gpu/drm/amd/amdgpu/dce_v11_0.h             |    4 +-
 drivers/gpu/drm/amd/amdgpu/dce_v6_0.c              |   16 +-
 drivers/gpu/drm/amd/amdgpu/dce_v8_0.c              |   44 +-
 drivers/gpu/drm/amd/amdgpu/dce_v8_0.h              |    4 +-
 drivers/gpu/drm/amd/amdgpu/dce_virtual.c           |   22 +-
 drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c              |   30 +-
 drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c              |   51 +-
 drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c              |   66 -
 drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c              |   79 -
 drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c             |    6 +-
 drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c             |   55 +-
 drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c              |    3 -
 drivers/gpu/drm/amd/amdgpu/vi.c                    |    8 +-
 drivers/gpu/drm/amd/amdgpu/vid.h                   |    2 -
 drivers/gpu/drm/amd/amdkfd/Kconfig                 |    1 -
 drivers/gpu/drm/amd/amdkfd/Makefile                |   12 +-
 drivers/gpu/drm/amd/amdkfd/cik_event_interrupt.c   |   72 +-
 drivers/gpu/drm/amd/amdkfd/cik_int.h               |   23 +-
 drivers/gpu/drm/amd/amdkfd/cik_regs.h              |  175 +-
 .../gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h | 1356 ----
 drivers/gpu/drm/amd/amdkfd/kfd_chardev.c           | 1489 +----
 drivers/gpu/drm/amd/amdkfd/kfd_crat.c              | 1184 ----
 drivers/gpu/drm/amd/amdkfd/kfd_crat.h              |   40 +-
 drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c            |  987 +--
 drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h            |   66 +-
 drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c            |  250 +-
 drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.h            |  313 +-
 drivers/gpu/drm/amd/amdkfd/kfd_debugfs.c           |   76 -
 drivers/gpu/drm/amd/amdkfd/kfd_device.c            |  578 +-
 .../gpu/drm/amd/amdkfd/kfd_device_queue_manager.c  |  711 +--
 .../gpu/drm/amd/amdkfd/kfd_device_queue_manager.h  |   45 +-
 .../drm/amd/amdkfd/kfd_device_queue_manager_cik.c  |   74 +-
 .../drm/amd/amdkfd/kfd_device_queue_manager_vi.c   |  117 +-
 drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c          |   31 +-
 drivers/gpu/drm/amd/amdkfd/kfd_events.c            |  550 +-
 drivers/gpu/drm/amd/amdkfd/kfd_events.h            |    3 +-
 drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c       |   81 +-
 drivers/gpu/drm/amd/amdkfd/kfd_interrupt.c         |   92 +-
 drivers/gpu/drm/amd/amdkfd/kfd_ipc.c               |  337 -
 drivers/gpu/drm/amd/amdkfd/kfd_ipc.h               |   52 -
 drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c      |   69 +-
 drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h      |   10 -
 drivers/gpu/drm/amd/amdkfd/kfd_module.c            |   40 +-
 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c       |    7 -
 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h       |    7 +-
 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c   |  162 +-
 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c    |  274 +-
 drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c    |  274 +-
 drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c        |  513 --
 drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers.h       |  215 +-
 drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h   |   97 +
 drivers/gpu/drm/amd/amdkfd/kfd_priv.h              |  392 +-
 drivers/gpu/drm/amd/amdkfd/kfd_process.c           |  793 +--
 .../gpu/drm/amd/amdkfd/kfd_process_queue_manager.c |  190 +-
 drivers/gpu/drm/amd/amdkfd/kfd_rdma.c              |  298 -
 drivers/gpu/drm/amd/amdkfd/kfd_topology.c          |  936 ++-
 drivers/gpu/drm/amd/amdkfd/kfd_topology.h          |   23 +-
 drivers/gpu/drm/amd/include/amd_shared.h           |    2 -
 .../gpu/drm/amd/include/asic_reg/dce/dce_11_2_d.h  |    0
 .../amd/include/asic_reg/dce/dce_11_2_sh_mask.h    |    0
 drivers/gpu/drm/amd/include/cgs_common.h           |    0
 drivers/gpu/drm/amd/include/kgd_kfd_interface.h    |  203 +-
 drivers/gpu/drm/amd/include/vi_structs.h           |    2 -
 drivers/gpu/drm/amd/powerplay/amd_powerplay.c      |  143 -
 .../gpu/drm/amd/powerplay/eventmgr/eventtasks.c    |    9 +-
 .../amd/powerplay/hwmgr/fiji_clockpowergating.c    |  121 -
 .../amd/powerplay/hwmgr/fiji_clockpowergating.h    |   35 -
 .../drm/amd/powerplay/hwmgr/fiji_dyn_defaults.h    |  105 -
 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c   | 5744 -----------------
 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.h   |  350 --
 .../gpu/drm/amd/powerplay/hwmgr/fiji_powertune.c   |  610 --
 .../gpu/drm/amd/powerplay/hwmgr/fiji_powertune.h   |   81 -
 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.c |  687 ---
 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.h |   62 -
 .../gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c  |   25 -
 drivers/gpu/drm/amd/powerplay/hwmgr/hwmgr.c        |    6 +-
 .../amd/powerplay/hwmgr/iceland_clockpowergating.c |  119 -
 .../amd/powerplay/hwmgr/iceland_clockpowergating.h |   38 -
 .../drm/amd/powerplay/hwmgr/iceland_dyn_defaults.h |   41 -
 .../gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.c    | 5666 -----------------
 .../gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.h    |  424 --
 .../drm/amd/powerplay/hwmgr/iceland_powertune.c    |  490 --
 .../drm/amd/powerplay/hwmgr/iceland_powertune.h    |   60 -
 .../gpu/drm/amd/powerplay/hwmgr/iceland_thermal.c  |  595 --
 .../gpu/drm/amd/powerplay/hwmgr/iceland_thermal.h  |   58 -
 .../powerplay/hwmgr/polaris10_clockpowergating.c   |  444 --
 .../powerplay/hwmgr/polaris10_clockpowergating.h   |   40 -
 .../amd/powerplay/hwmgr/polaris10_dyn_defaults.h   |   62 -
 .../gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c  | 5439 ----------------
 .../gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.h  |  354 --
 .../drm/amd/powerplay/hwmgr/polaris10_powertune.c  |  988 ---
 .../drm/amd/powerplay/hwmgr/polaris10_powertune.h  |   81 -
 .../drm/amd/powerplay/hwmgr/polaris10_thermal.c    |  716 ---
 .../drm/amd/powerplay/hwmgr/polaris10_thermal.h    |   62 -
 drivers/gpu/drm/amd/powerplay/hwmgr/ppatomctrl.c   |    1 -
 .../amd/powerplay/hwmgr/process_pptables_v1_0.c    |    1 -
 drivers/gpu/drm/amd/powerplay/hwmgr/smu7_hwmgr.c   |   15 +-
 .../amd/powerplay/hwmgr/tonga_clockpowergating.c   |  350 --
 .../amd/powerplay/hwmgr/tonga_clockpowergating.h   |   36 -
 .../drm/amd/powerplay/hwmgr/tonga_dyn_defaults.h   |  107 -
 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c  | 6504 --------------------
 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.h  |  402 --
 .../gpu/drm/amd/powerplay/hwmgr/tonga_powertune.c  |  495 --
 .../gpu/drm/amd/powerplay/hwmgr/tonga_powertune.h  |   80 -
 .../gpu/drm/amd/powerplay/hwmgr/tonga_thermal.c    |  590 --
 .../gpu/drm/amd/powerplay/hwmgr/tonga_thermal.h    |   61 -
 drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h  |   25 +-
 drivers/gpu/drm/amd/powerplay/inc/hwmgr.h          |   16 +-
 drivers/gpu/drm/amd/powerplay/smumgr/fiji_smumgr.c |    0
 158 files changed, 2955 insertions(+), 49419 deletions(-)
 delete mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c
 delete mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h
 delete mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
 delete mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_sched.c
 delete mode 100644 drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h
 delete mode 100644 drivers/gpu/drm/amd/amdkfd/kfd_crat.c
 delete mode 100644 drivers/gpu/drm/amd/amdkfd/kfd_debugfs.c
 delete mode 100644 drivers/gpu/drm/amd/amdkfd/kfd_ipc.c
 delete mode 100644 drivers/gpu/drm/amd/amdkfd/kfd_ipc.h
 delete mode 100644 drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c
 delete mode 100644 drivers/gpu/drm/amd/amdkfd/kfd_rdma.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_d.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_sh_mask.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/include/cgs_common.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_clockpowergating.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_clockpowergating.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_dyn_defaults.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_powertune.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_powertune.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/iceland_clockpowergating.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/iceland_clockpowergating.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/iceland_dyn_defaults.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/iceland_powertune.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/iceland_powertune.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/iceland_thermal.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/iceland_thermal.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_clockpowergating.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_clockpowergating.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_dyn_defaults.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_powertune.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_powertune.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_thermal.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_thermal.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_clockpowergating.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_clockpowergating.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_dyn_defaults.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_powertune.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_powertune.h
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_thermal.c
 delete mode 100644 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_thermal.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/powerplay/smumgr/fiji_smumgr.c

diff --git a/drivers/gpu/drm/amd/amdgpu/Makefile b/drivers/gpu/drm/amd/amdgpu/Makefile
index d8f8cce..00588e1 100644
--- a/drivers/gpu/drm/amd/amdgpu/Makefile
+++ b/drivers/gpu/drm/amd/amdgpu/Makefile
@@ -34,7 +34,8 @@ amdgpu-y += amdgpu_device.o amdgpu_kms.o \
 
 # add asic specific block
 amdgpu-$(CONFIG_DRM_AMDGPU_CIK)+= cik.o cik_ih.o kv_smc.o kv_dpm.o \
-	ci_smc.o ci_dpm.o dce_v8_0.o gfx_v7_0.o cik_sdma.o uvd_v4_2.o vce_v2_0.o
+	ci_smc.o ci_dpm.o dce_v8_0.o gfx_v7_0.o cik_sdma.o uvd_v4_2.o vce_v2_0.o \
+	amdgpu_amdkfd_gfx_v7.o
 
 amdgpu-$(CONFIG_DRM_AMDGPU_SI)+= si.o gmc_v6_0.o gfx_v6_0.o si_ih.o si_dma.o dce_v6_0.o si_dpm.o si_smc.o
 
@@ -87,14 +88,9 @@ amdgpu-y += \
 	vce_v3_0.o
 
 # add amdkfd interfaces
-ifneq ($(CONFIG_HSA_AMD),)
 amdgpu-y += \
 	 amdgpu_amdkfd.o \
-	 amdgpu_amdkfd_gfx_v7.o \
-	 amdgpu_amdkfd_gfx_v8.o \
-	 amdgpu_amdkfd_gpuvm.o \
-	 amdgpu_amdkfd_fence.o
-endif
+	 amdgpu_amdkfd_gfx_v8.o
 
 # add cgs
 amdgpu-y += amdgpu_cgs.o
@@ -120,6 +116,10 @@ amdgpu-$(CONFIG_VGA_SWITCHEROO) += amdgpu_atpx_handler.o
 amdgpu-$(CONFIG_ACPI) += amdgpu_acpi.o
 amdgpu-$(CONFIG_MMU_NOTIFIER) += amdgpu_mn.o
 
+include $(FULL_AMD_PATH)/powerplay/Makefile
+
+amdgpu-y += $(AMD_POWERPLAY_FILES)
+
 ifneq ($(CONFIG_DRM_AMD_DC),)
 
 RELATIVE_AMD_DISPLAY_PATH = ../$(DISPLAY_FOLDER_NAME)
@@ -129,10 +129,6 @@ amdgpu-y += $(AMD_DISPLAY_FILES)
 
 endif
 
-include $(FULL_AMD_PATH)/powerplay/Makefile
-
-amdgpu-y += $(AMD_POWERPLAY_FILES)
-
 obj-$(CONFIG_DRM_AMDGPU)+= amdgpu.o
 
 CFLAGS_amdgpu_trace_points.o := -I$(src)
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu.h b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
index 6d188d5..ec74894 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
@@ -53,13 +53,13 @@
 #include "amdgpu_ucode.h"
 #include "amdgpu_ttm.h"
 #include "amdgpu_gds.h"
-#include "amdgpu_acp.h"
-#include "amdgpu_dm.h"
 #include "amdgpu_sync.h"
 #include "amdgpu_ring.h"
 #include "amdgpu_vm.h"
 #include "amd_powerplay.h"
 #include "amdgpu_dpm.h"
+#include "amdgpu_acp.h"
+#include "amdgpu_dm.h"
 
 #include "gpu_scheduler.h"
 #include "amdgpu_virt.h"
@@ -90,9 +90,6 @@ extern int amdgpu_vm_size;
 extern int amdgpu_vm_block_size;
 extern int amdgpu_vm_fault_stop;
 extern int amdgpu_vm_debug;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-extern int amdgpu_vm_update_context;
-#endif
 extern int amdgpu_dc;
 extern int amdgpu_sched_jobs;
 extern int amdgpu_sched_hw_submission;
@@ -107,9 +104,7 @@ extern char *amdgpu_disable_cu;
 extern char *amdgpu_virtual_display;
 extern unsigned amdgpu_pp_feature_mask;
 extern int amdgpu_vram_page_split;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-extern unsigned amdgpu_sdma_phase_quantum;
-#endif
+
 #define AMDGPU_WAIT_IDLE_TIMEOUT_IN_MS	        3000
 #define AMDGPU_MAX_USEC_TIMEOUT			100000	/* 100 ms */
 #define AMDGPU_FENCE_JIFFIES_TIMEOUT		(HZ / 2)
@@ -145,15 +140,6 @@ extern unsigned amdgpu_sdma_phase_quantum;
 #define AMDGPU_RESET_VCE			(1 << 13)
 #define AMDGPU_RESET_VCE1			(1 << 14)
 
-#define AMDGPU_PG_SUPPORT_GFX_QUICK_MG		(1 << 11)
-#define AMDGPU_PG_SUPPORT_GFX_PIPELINE		(1 << 12)
-
-#define AMDGPU_PG_SUPPORT_GFX_PG		(1 << 0)
-#define AMDGPU_PG_SUPPORT_GFX_SMG		(1 << 1)
-#define AMDGPU_PG_SUPPORT_GFX_DMG		(1 << 2)
-#define AMDGPU_PG_SUPPORT_CP			(1 << 5)
-#define AMDGPU_PG_SUPPORT_RLC_SMU_HS		(1 << 7)
-
 /* GFX current status */
 #define AMDGPU_GFX_NORMAL_MODE			0x00000000L
 #define AMDGPU_GFX_SAFE_MODE			0x00000001L
@@ -171,9 +157,6 @@ struct amdgpu_cs_parser;
 struct amdgpu_job;
 struct amdgpu_irq_src;
 struct amdgpu_fpriv;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-struct kfd_vm_fault_info;
-#endif
 
 enum amdgpu_cp_irq {
 	AMDGPU_CP_IRQ_GFX_EOP = 0,
@@ -358,7 +341,6 @@ struct amdgpu_clock {
 /*
  * BO.
  */
-
 struct amdgpu_bo_list_entry {
 	struct amdgpu_bo		*robj;
 	struct ttm_validate_buffer	tv;
@@ -396,16 +378,6 @@ struct amdgpu_bo_va {
 
 #define AMDGPU_GEM_DOMAIN_MAX		0x3
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-struct amdgpu_gem_object {
-	struct drm_gem_object		base;
-	struct list_head		list;
-	struct amdgpu_bo		*bo;
-};
-
-struct kgd_mem;
-#endif
-
 struct amdgpu_bo {
 	/* Protected by tbo.reserved */
 	u32				prefered_domains;
@@ -421,19 +393,12 @@ struct amdgpu_bo {
 	u64				metadata_flags;
 	void				*metadata;
 	u32				metadata_size;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* GEM objects refereing to this BO */
-	struct list_head	gem_objects;
-#endif
+        unsigned                        prime_shared_count;
 	/* list of all virtual address to which this bo
 	 * is associated to
 	 */
 	struct list_head		va;
 	/* Constant after initialization */
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
-	struct drm_gem_object           gem_base;
-#endif
-	struct drm_gem_object           gem_base;
 	struct amdgpu_bo		*parent;
 	struct amdgpu_bo		*shadow;
 
@@ -441,15 +406,8 @@ struct amdgpu_bo {
 	struct amdgpu_mn		*mn;
 	struct list_head		mn_list;
 	struct list_head		shadow_list;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
-	struct kgd_mem			*kfd_bo;
-#endif	
 };
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#define gem_to_amdgpu_bo(gobj) container_of((gobj), struct amdgpu_gem_object, base)->bo
-#else
 #define gem_to_amdgpu_bo(gobj) container_of((gobj), struct amdgpu_bo, gem_base)
-#endif
 
 void amdgpu_gem_object_free(struct drm_gem_object *obj);
 int amdgpu_gem_object_open(struct drm_gem_object *obj,
@@ -458,18 +416,13 @@ void amdgpu_gem_object_close(struct drm_gem_object *obj,
 				struct drm_file *file_priv);
 unsigned long amdgpu_gem_timeout(uint64_t timeout_ns);
 struct sg_table *amdgpu_gem_prime_get_sg_table(struct drm_gem_object *obj);
-struct drm_gem_object *amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
+struct drm_gem_object *
+amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
 				 struct dma_buf_attachment *attach,
 				 struct sg_table *sg);
 struct dma_buf *amdgpu_gem_prime_export(struct drm_device *dev,
 					struct drm_gem_object *gobj,
 					int flags);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-struct drm_gem_object *
-amdgpu_gem_prime_foreign_bo(struct amdgpu_device *adev, struct amdgpu_bo *bo);
-struct drm_gem_object *amdgpu_gem_prime_import(struct drm_device *dev,
-					       struct dma_buf *dma_buf);
-#endif
 int amdgpu_gem_prime_pin(struct drm_gem_object *obj);
 void amdgpu_gem_prime_unpin(struct drm_gem_object *obj);
 struct reservation_object *amdgpu_gem_prime_res_obj(struct drm_gem_object *);
@@ -610,10 +563,6 @@ struct amdgpu_mc {
 	uint32_t		vram_type;
 	uint32_t                srbm_soft_reset;
 	struct amdgpu_mode_mc_save save;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	struct kfd_vm_fault_info *vm_fault_info;
-	atomic_t		vm_fault_info_updated;
-#endif
 };
 
 /*
@@ -658,10 +607,11 @@ void amdgpu_doorbell_get_kfd_info(struct amdgpu_device *adev,
  */
 
 struct amdgpu_flip_work {
-	struct work_struct		flip_work;
+	struct delayed_work		flip_work;
 	struct work_struct		unpin_work;
 	struct amdgpu_device		*adev;
 	int				crtc_id;
+	u32				target_vblank;
 	uint64_t			base;
 	struct drm_pending_vblank_event *event;
 	struct amdgpu_bo		*old_abo;
@@ -706,8 +656,6 @@ struct amdgpu_ctx_ring {
 	uint64_t		sequence;
 	struct fence		**fences;
 	struct amd_sched_entity	entity;
-	/* client id */
-	u64			client_id;
 };
 
 struct amdgpu_ctx {
@@ -887,13 +835,6 @@ struct amdgpu_gca_config {
 struct amdgpu_cu_info {
 	uint32_t number; /* total active CU number */
 	uint32_t ao_cu_mask;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	uint32_t simd_per_cu;
-	uint32_t max_waves_per_simd;
-	uint32_t wave_front_size;
-	uint32_t max_scratch_slots_per_cu;
-	uint32_t lds_size;
-#endif
 	uint32_t bitmap[4][4];
 };
 
@@ -980,7 +921,7 @@ struct amdgpu_cs_parser {
 
 	/* scheduler job object */
 	struct amdgpu_job	*job;
- 
+
 	/* buffer objects */
 	struct ww_acquire_ctx		ticket;
 	struct amdgpu_bo_list		*bo_list;
@@ -1017,7 +958,7 @@ struct amdgpu_job {
 	uint32_t		gds_base, gds_size;
 	uint32_t		gws_base, gws_size;
 	uint32_t		oa_base, oa_size;
-	uint64_t		fence_context;
+
 	/* user fence handling */
 	uint64_t		uf_addr;
 	uint64_t		uf_sequence;
@@ -1737,6 +1678,7 @@ bool amdgpu_ttm_bo_is_amdgpu_bo(struct ttm_buffer_object *bo);
 int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages);
 int amdgpu_ttm_tt_set_userptr(struct ttm_tt *ttm, uint64_t addr,
 				     uint32_t flags);
+bool amdgpu_ttm_tt_has_userptr(struct ttm_tt *ttm);
 struct mm_struct *amdgpu_ttm_tt_get_usermm(struct ttm_tt *ttm);
 bool amdgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
 				  unsigned long end);
@@ -1784,6 +1726,7 @@ void amdgpu_driver_postclose_kms(struct drm_device *dev,
 				 struct drm_file *file_priv);
 void amdgpu_driver_preclose_kms(struct drm_device *dev,
 				struct drm_file *file_priv);
+int amdgpu_suspend(struct amdgpu_device *adev);
 int amdgpu_device_suspend(struct drm_device *dev, bool suspend, bool fbcon);
 int amdgpu_device_resume(struct drm_device *dev, bool resume, bool fbcon);
 u32 amdgpu_get_vblank_counter_kms(struct drm_device *dev, unsigned int pipe);
@@ -1841,4 +1784,3 @@ static inline int amdgpu_dm_display_resume(struct amdgpu_device *adev) { return
 
 #include "amdgpu_object.h"
 #endif
-
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c
index 3c76048..06879d1 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.c
@@ -240,12 +240,10 @@ static int acp_poweron(struct generic_pm_domain *genpd)
 static struct device *get_mfd_cell_dev(const char *device_name, int r)
 {
 	char auto_dev_name[25];
-	char buf[8];
 	struct device *dev;
 
-	sprintf(buf, ".%d.auto", r);
-	strcpy(auto_dev_name, device_name);
-	strcat(auto_dev_name, buf);
+	snprintf(auto_dev_name, sizeof(auto_dev_name),
+		 "%s.%d.auto", device_name, r);
 	dev = bus_find_device_by_name(&platform_bus_type, NULL, auto_dev_name);
 	dev_info(dev, "device %s added to pm domain\n", auto_dev_name);
 
@@ -449,13 +447,6 @@ static int acp_soft_reset(void *handle)
 	return 0;
 }
 
-static void acp_print_status(void *handle)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
-
-	dev_info(adev->dev, "ACP STATUS\n");
-}
-
 static int acp_set_clockgating_state(void *handle,
 				     enum amd_clockgating_state state)
 {
@@ -481,7 +472,6 @@ static const struct amd_ip_funcs acp_ip_funcs = {
 	.is_idle = acp_is_idle,
 	.wait_for_idle = acp_wait_for_idle,
 	.soft_reset = acp_soft_reset,
-	.print_status = acp_print_status,
 	.set_clockgating_state = acp_set_clockgating_state,
 	.set_powergating_state = acp_set_powergating_state,
 };
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.h
index d4df419..a288ce2 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acp.h
@@ -30,7 +30,7 @@
 
 struct amdgpu_acp {
 	struct device *parent;
-	void *cgs_device;
+	struct cgs_device *cgs_device;
 	struct amd_acp_private *private;
 	struct mfd_cell *acp_cell;
 	struct resource *acp_res;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
index 1d07bf3..dba8a5b 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
@@ -21,13 +21,10 @@
  */
 
 #include "amdgpu_amdkfd.h"
-#include <linux/dma-buf.h>
+#include "amd_shared.h"
 #include <drm/drmP.h>
 #include "amdgpu.h"
 #include <linux/module.h>
-#include <linux/mmu_context.h>
-
-#define AMDKFD_SKIP_UNCOMPILED_CODE 1
 
 const struct kfd2kgd_calls *kfd2kgd;
 const struct kgd2kfd_calls *kgd2kfd;
@@ -59,22 +56,19 @@ int amdgpu_amdkfd_init(void)
 #else
 	ret = -ENOENT;
 #endif
-	amdgpu_amdkfd_gpuvm_init_mem_limits();
+
 	return ret;
 }
 
 bool amdgpu_amdkfd_load_interface(struct amdgpu_device *rdev)
 {
 	switch (rdev->asic_type) {
+#ifdef CONFIG_DRM_AMDGPU_CIK
 	case CHIP_KAVERI:
-	case CHIP_HAWAII:
 		kfd2kgd = amdgpu_amdkfd_gfx_7_get_functions();
 		break;
+#endif
 	case CHIP_CARRIZO:
-	case CHIP_TONGA:
-	case CHIP_FIJI:
-	case CHIP_POLARIS10:
-	case CHIP_POLARIS11:
 		kfd2kgd = amdgpu_amdkfd_gfx_8_0_get_functions();
 		break;
 	default:
@@ -107,7 +101,6 @@ void amdgpu_amdkfd_device_init(struct amdgpu_device *rdev)
 
 			.first_compute_pipe = 1,
 			.compute_pipe_count = 4 - 1,
-			.gpuvm_size = (uint64_t)amdgpu_vm_size << 30
 		};
 
 		amdgpu_doorbell_get_kfd_info(rdev,
@@ -150,186 +143,24 @@ int amdgpu_amdkfd_resume(struct amdgpu_device *rdev)
 	return r;
 }
 
-int amdgpu_amdkfd_evict_mem(struct amdgpu_device *adev, struct kgd_mem *mem,
-			    struct mm_struct *mm)
-{
-	int r;
-
-	if (!adev->kfd)
-		return -ENODEV;
-
-	mutex_lock(&mem->lock);
-
-	if (mem->evicted == 1 && delayed_work_pending(&mem->work))
-		/* Cancelling a scheduled restoration */
-		cancel_delayed_work(&mem->work);
-
-	if (++mem->evicted > 1) {
-		mutex_unlock(&mem->lock);
-		return 0;
-	}
-
-	r = amdgpu_amdkfd_gpuvm_evict_mem(mem, mm);
-
-	if (r != 0)
-		/* First eviction failed, setting count back to 0 will
-		 * make the corresponding restore fail gracefully */
-		mem->evicted = 0;
-	else
-		/* First eviction counts as 2. Eviction counter == 1
-		 * means that restoration is scheduled. */
-		mem->evicted = 2;
-
-	mutex_unlock(&mem->lock);
-
-	return r;
-}
-
-static void amdgdu_amdkfd_restore_mem_worker(struct work_struct *work)
-{
-	struct delayed_work *dwork = to_delayed_work(work);
-	struct kgd_mem *mem = container_of(dwork, struct kgd_mem, work);
-	struct amdgpu_device *adev;
-	struct mm_struct *mm;
-
-	mutex_lock(&mem->lock);
-
-	adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);
-	mm = mem->mm;
-
-	/* Restoration may have been canceled by another eviction or
-	 * could already be done by a restore scheduled earlier */
-	if (mem->evicted == 1) {
-		amdgpu_amdkfd_gpuvm_restore_mem(mem, mm);
-		mem->evicted = 0;
-	}
-
-	mutex_unlock(&mem->lock);
-}
-
-int amdgpu_amdkfd_schedule_restore_mem(struct amdgpu_device *adev,
-				       struct kgd_mem *mem,
-				       struct mm_struct *mm,
-				       unsigned long delay)
-{
-	int r = 0;
-
-	if (!adev->kfd)
-		return -ENODEV;
-
-	mutex_lock(&mem->lock);
-
-	if (mem->evicted <= 1) {
-		/* Buffer is not evicted (== 0) or its restoration is
-		 * already scheduled (== 1) */
-		pr_err("Unbalanced restore of evicted buffer %p\n", mem);
-		mutex_unlock(&mem->lock);
-		return -EFAULT;
-	} else if (--mem->evicted > 1) {
-		mutex_unlock(&mem->lock);
-		return 0;
-	}
-
-	/* mem->evicted is 1 after decrememting. Schedule
-	 * restoration. */
-	if (delayed_work_pending(&mem->work))
-		cancel_delayed_work(&mem->work);
-	mem->mm = mm;
-	INIT_DELAYED_WORK(&mem->work,
-			  amdgdu_amdkfd_restore_mem_worker);
-	schedule_delayed_work(&mem->work, delay);
-
-	mutex_unlock(&mem->lock);
-
-	return r;
-}
-
-void amdgpu_amdkfd_cancel_restore_mem(struct amdgpu_device *adev,
-				      struct kgd_mem *mem)
-{
-	if (delayed_work_pending(&mem->work))
-		cancel_delayed_work_sync(&mem->work);
-}
-
-int amdgpu_amdkfd_submit_ib(struct kgd_dev *kgd, enum kgd_engine_type engine,
-				uint32_t vmid, uint64_t gpu_addr,
-				uint32_t *ib_cmd, uint32_t ib_len)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
-	struct amdgpu_job *job;
-	struct amdgpu_ib *ib;
-	struct amdgpu_ring *ring;
-	struct fence *f = NULL;
-	int ret;
-
-	switch (engine) {
-	case KGD_ENGINE_MEC1:
-		ring = &adev->gfx.compute_ring[0];
-		break;
-	case KGD_ENGINE_SDMA1:
-		ring = &adev->sdma.instance[0].ring;
-		break;
-	case KGD_ENGINE_SDMA2:
-		ring = &adev->sdma.instance[1].ring;
-		break;
-	default:
-		pr_err("Invalid engine in IB submission: %d\n", engine);
-		ret = -EINVAL;
-		goto err;
-	}
-
-	ret = amdgpu_job_alloc(adev, 1, &job, NULL);
-	if (ret)
-		goto err;
-
-	ib = &job->ibs[0];
-	memset(ib, 0, sizeof(struct amdgpu_ib));
-
-	ib->gpu_addr = gpu_addr;
-	ib->ptr = ib_cmd;
-	ib->length_dw = ib_len;
-	/* This works for NO_HWS. TODO: need to handle without knowing VMID */
-	job->vm_id = vmid;
-
-	ret = amdgpu_ib_schedule(ring, 1, ib, NULL, job, &f);
-	if (ret) {
-		DRM_ERROR("amdgpu: failed to schedule IB.\n");
-		goto err_ib_sched;
-	}
-
-	ret = fence_wait(f, false);
-
-err_ib_sched:
-	fence_put(f);
-	amdgpu_job_free(job);
-err:
-	return ret;
-}
-
-u32 pool_to_domain(enum kgd_memory_pool p)
-{
-	switch (p) {
-	case KGD_POOL_FRAMEBUFFER: return AMDGPU_GEM_DOMAIN_VRAM;
-	default: return AMDGPU_GEM_DOMAIN_GTT;
-	}
-}
-
 int alloc_gtt_mem(struct kgd_dev *kgd, size_t size,
 			void **mem_obj, uint64_t *gpu_addr,
 			void **cpu_ptr)
 {
 	struct amdgpu_device *rdev = (struct amdgpu_device *)kgd;
-	struct amdgpu_bo *bo = NULL;
+	struct kgd_mem **mem = (struct kgd_mem **) mem_obj;
 	int r;
-	uint64_t gpu_addr_tmp = 0;
-	void *cpu_ptr_tmp = NULL;
 
 	BUG_ON(kgd == NULL);
 	BUG_ON(gpu_addr == NULL);
 	BUG_ON(cpu_ptr == NULL);
 
+	*mem = kmalloc(sizeof(struct kgd_mem), GFP_KERNEL);
+	if ((*mem) == NULL)
+		return -ENOMEM;
+
 	r = amdgpu_bo_create(rdev, size, PAGE_SIZE, true, AMDGPU_GEM_DOMAIN_GTT,
-			AMDGPU_GEM_CREATE_CPU_GTT_USWC, NULL, NULL, &bo);
+			     AMDGPU_GEM_CREATE_CPU_GTT_USWC, NULL, NULL, &(*mem)->bo);
 	if (r) {
 		dev_err(rdev->dev,
 			"failed to allocate BO for amdkfd (%d)\n", r);
@@ -337,87 +168,64 @@ int alloc_gtt_mem(struct kgd_dev *kgd, size_t size,
 	}
 
 	/* map the buffer */
-	r = amdgpu_bo_reserve(bo, true);
+	r = amdgpu_bo_reserve((*mem)->bo, true);
 	if (r) {
 		dev_err(rdev->dev, "(%d) failed to reserve bo for amdkfd\n", r);
 		goto allocate_mem_reserve_bo_failed;
 	}
 
-	r = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT,
-				&gpu_addr_tmp);
+	r = amdgpu_bo_pin((*mem)->bo, AMDGPU_GEM_DOMAIN_GTT,
+				&(*mem)->gpu_addr);
 	if (r) {
 		dev_err(rdev->dev, "(%d) failed to pin bo for amdkfd\n", r);
 		goto allocate_mem_pin_bo_failed;
 	}
+	*gpu_addr = (*mem)->gpu_addr;
 
-	r = amdgpu_bo_kmap(bo, &cpu_ptr_tmp);
+	r = amdgpu_bo_kmap((*mem)->bo, &(*mem)->cpu_ptr);
 	if (r) {
 		dev_err(rdev->dev,
 			"(%d) failed to map bo to kernel for amdkfd\n", r);
 		goto allocate_mem_kmap_bo_failed;
 	}
+	*cpu_ptr = (*mem)->cpu_ptr;
 
-	*mem_obj = bo;
-	*gpu_addr = gpu_addr_tmp;
-	*cpu_ptr = cpu_ptr_tmp;
-
-	amdgpu_bo_unreserve(bo);
+	amdgpu_bo_unreserve((*mem)->bo);
 
 	return 0;
 
 allocate_mem_kmap_bo_failed:
-	amdgpu_bo_unpin(bo);
+	amdgpu_bo_unpin((*mem)->bo);
 allocate_mem_pin_bo_failed:
-	amdgpu_bo_unreserve(bo);
+	amdgpu_bo_unreserve((*mem)->bo);
 allocate_mem_reserve_bo_failed:
-	amdgpu_bo_unref(&bo);
+	amdgpu_bo_unref(&(*mem)->bo);
 
 	return r;
 }
 
 void free_gtt_mem(struct kgd_dev *kgd, void *mem_obj)
 {
-	struct amdgpu_bo *bo = (struct amdgpu_bo *) mem_obj;
+	struct kgd_mem *mem = (struct kgd_mem *) mem_obj;
 
-	BUG_ON(bo == NULL);
+	BUG_ON(mem == NULL);
 
-	amdgpu_bo_reserve(bo, true);
-	amdgpu_bo_kunmap(bo);
-	amdgpu_bo_unpin(bo);
-	amdgpu_bo_unreserve(bo);
-	amdgpu_bo_unref(&(bo));
+	amdgpu_bo_reserve(mem->bo, true);
+	amdgpu_bo_kunmap(mem->bo);
+	amdgpu_bo_unpin(mem->bo);
+	amdgpu_bo_unreserve(mem->bo);
+	amdgpu_bo_unref(&(mem->bo));
+	kfree(mem);
 }
 
-void get_local_mem_info(struct kgd_dev *kgd,
-				struct kfd_local_mem_info *mem_info)
+uint64_t get_vmem_size(struct kgd_dev *kgd)
 {
-	uint64_t address_mask;
-	resource_size_t aper_limit;
-	struct amdgpu_device *rdev = (struct amdgpu_device *)kgd;
+	struct amdgpu_device *rdev =
+		(struct amdgpu_device *)kgd;
 
 	BUG_ON(kgd == NULL);
 
-	address_mask = ~((1UL << 40) - 1);
-	aper_limit = rdev->mc.aper_base + rdev->mc.aper_size;
-	memset(mem_info, 0, sizeof(*mem_info));
-	if (!(rdev->mc.aper_base & address_mask ||
-			aper_limit & address_mask)) {
-		mem_info->local_mem_size_public = rdev->mc.visible_vram_size;
-		mem_info->local_mem_size_private = rdev->mc.real_vram_size -
-				rdev->mc.visible_vram_size;
-	} else {
-		mem_info->local_mem_size_public = 0;
-		mem_info->local_mem_size_private = rdev->mc.real_vram_size;
-	}
-	mem_info->vram_width = rdev->mc.vram_width;
-
-	pr_debug("amdgpu: address base: 0x%llx limit 0x%llx public 0x%llx private 0x%llx\n",
-			rdev->mc.aper_base, aper_limit,
-			mem_info->local_mem_size_public,
-			mem_info->local_mem_size_private);
-
-	if (amdgpu_powerplay || rdev->pm.funcs->get_mclk)
-		mem_info->mem_clk_max = amdgpu_dpm_get_mclk(rdev, false) / 100;
+	return rdev->mc.real_vram_size;
 }
 
 uint64_t get_gpu_clock_counter(struct kgd_dev *kgd)
@@ -434,108 +242,5 @@ uint32_t get_max_engine_clock_in_mhz(struct kgd_dev *kgd)
 	struct amdgpu_device *rdev = (struct amdgpu_device *)kgd;
 
 	/* The sclk is in quantas of 10kHz */
-	if (amdgpu_powerplay)
-		return amdgpu_dpm_get_sclk(rdev, false) / 100;
-	else
-		return rdev->pm.dpm.dyn_state.max_clock_voltage_on_ac.sclk / 100;
-}
-
-void get_cu_info(struct kgd_dev *kgd, struct kfd_cu_info *cu_info)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
-	struct amdgpu_cu_info acu_info = adev->gfx.cu_info;
-
-	memset(cu_info, 0, sizeof(*cu_info));
-	if (sizeof(cu_info->cu_bitmap) != sizeof(acu_info.bitmap))
-		return;
-
-	cu_info->cu_active_number = acu_info.number;
-	cu_info->cu_ao_mask = acu_info.ao_cu_mask;
-	memcpy(&cu_info->cu_bitmap[0], &acu_info.bitmap[0], sizeof(acu_info.bitmap));
-	cu_info->num_shader_engines = adev->gfx.config.max_shader_engines;
-	cu_info->num_shader_arrays_per_engine = adev->gfx.config.max_sh_per_se;
-	cu_info->num_cu_per_sh = adev->gfx.config.max_cu_per_sh;
-	cu_info->simd_per_cu = acu_info.simd_per_cu;
-	cu_info->max_waves_per_simd = acu_info.max_waves_per_simd;
-	cu_info->wave_front_size = acu_info.wave_front_size;
-	cu_info->max_scratch_slots_per_cu = acu_info.max_scratch_slots_per_cu;
-	cu_info->lds_size = acu_info.lds_size;
-}
-
-int amdgpu_amdkfd_get_dmabuf_info(struct kgd_dev *kgd, int dma_buf_fd,
-				  struct kgd_dev **dma_buf_kgd,
-				  uint64_t *bo_size, void *metadata_buffer,
-				  size_t buffer_size, uint32_t *metadata_size,
-				  uint32_t *flags)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
-	struct dma_buf *dma_buf;
-	struct drm_gem_object *obj;
-	struct amdgpu_bo *bo;
-	uint64_t metadata_flags;
-	int r = -EINVAL;
-
-	dma_buf = dma_buf_get(dma_buf_fd);
-	if (IS_ERR(dma_buf))
-		return PTR_ERR(dma_buf);
-
-	if (dma_buf->ops != &drm_gem_prime_dmabuf_ops)
-		/* Can't handle non-graphics buffers */
-		goto out_put;
-
-	obj = dma_buf->priv;
-	if (obj->dev->driver != adev->ddev->driver)
-		/* Can't handle buffers from different drivers */
-		goto out_put;
-
-	adev = obj->dev->dev_private;
-	bo = gem_to_amdgpu_bo(obj);
-	if (!(bo->prefered_domains & (AMDGPU_GEM_DOMAIN_VRAM |
-				    AMDGPU_GEM_DOMAIN_GTT)))
-		/* Only VRAM and GTT BOs are supported */
-		goto out_put;
-
-	r = 0;
-	if (dma_buf_kgd)
-		*dma_buf_kgd = (struct kgd_dev *)adev;
-	if (bo_size)
-		*bo_size = amdgpu_bo_size(bo);
-	if (metadata_size)
-		*metadata_size = bo->metadata_size;
-	if (metadata_buffer)
-		r = amdgpu_bo_get_metadata(bo, metadata_buffer, buffer_size,
-					   metadata_size, &metadata_flags);
-	if (flags) {
-		*flags = (bo->prefered_domains & AMDGPU_GEM_DOMAIN_VRAM) ?
-			ALLOC_MEM_FLAGS_VRAM : ALLOC_MEM_FLAGS_GTT;
-
-		if (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)
-			*flags |= ALLOC_MEM_FLAGS_PUBLIC;
-	}
-
-out_put:
-	dma_buf_put(dma_buf);
-	return r;
-}
-
-bool read_user_wptr(struct mm_struct *mm, uint32_t __user *wptr,
-		    uint32_t *wptr_val)
-{
-	bool wptr_valid = false;
-
-	if (mm && wptr) {
-		if (mm == current->mm) {
-			/* Running in the correct user process context */
-			wptr_valid = !get_user(*wptr_val, wptr);
-		} else if (current->mm == NULL) {
-			/* A kernel thread can temporarily use a user
-			 * process context for AIO
-			 */
-			use_mm(mm);
-			wptr_valid = !get_user(*wptr_val, wptr);
-			unuse_mm(mm);
-		}
-	}
-
-	return wptr_valid;
+	return rdev->pm.dpm.dyn_state.max_clock_voltage_on_ac.sclk / 100;
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
index ac389fd..de530f68d 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
@@ -26,84 +26,14 @@
 #define AMDGPU_AMDKFD_H_INCLUDED
 
 #include <linux/types.h>
-#include <linux/workqueue.h>
 #include <kgd_kfd_interface.h>
-#include "amdgpu.h"
-
-extern const struct kgd2kfd_calls *kgd2kfd;
 
 struct amdgpu_device;
 
-struct kfd_bo_va_list {
-	struct list_head bo_list;
-	struct amdgpu_bo_va *bo_va;
-	void *kgd_dev;
-	bool is_mapped;
-	bool map_fail;
-	uint64_t va;
-};
-
 struct kgd_mem {
-	struct mutex lock;
 	struct amdgpu_bo *bo;
-	struct list_head bo_va_list;
-	struct amdgpu_bo_list_entry bo_list_entry;
-	uint32_t domain;
-	unsigned int mapped_to_gpu_memory;
-	void *kptr;
-	uint64_t va;
-	unsigned int evicted; /* eviction counter */
-	struct delayed_work work; /* for restore evicted mem */
-	struct mm_struct *mm; /* for restore */
-
-	uint32_t pte_flags;
-
-	/* flags bitfield */
-	bool no_substitute : 1;
-	bool aql_queue     : 1;
-};
-
-/* KFD Memory Eviction */
-struct amdgpu_amdkfd_fence {
-	struct fence base;
-	void *mm;
-	spinlock_t lock;
-	char timeline_name[TASK_COMM_LEN];
-};
-
-struct amdgpu_amdkfd_fence *amdgpu_amdkfd_fence_create(u64 context,
-						       void *mm);
-bool amd_kfd_fence_check_mm(struct fence *f, void *mm);
-struct amdgpu_amdkfd_fence *to_amdgpu_amdkfd_fence(struct fence *f);
-
-/* struct amdkfd_vm -
- *  For Memory Eviction KGD requires a mechanism to keep track of all KFD BOs
- * belonging to a KFD process. All the VMs belonging to the same process point
- * to the same master VM. The master VM points to itself.
- * For master VM kfd_bo_list will contain the list of all KFD BOs and it will
- * be empty for all the other VMs. The master VM is decided by KFD and it will
- * pass it on KGD via create_process_vm interface
- */
-struct amdkfd_vm {
-	/* Keep base as the first parameter for pointer compatibility between
-	 * amdkfd_vm and amdgpu_vm.
-	 */
-	struct amdgpu_vm base;
-	/* Points to master VM of the KFD process */
-	struct amdkfd_vm *master;
-	/* List Head for all KFD BOs that belong to a KFD process. Non-empty
-	 * only for Master VM.
-	 */
-	struct list_head kfd_bo_list;
-	/* Lock to protect kfd_bo_list */
-	struct mutex lock;
-	/* List of VMs that belong to a KFD process */
-	struct list_head kfd_vm_list;
-	/* Number of VMs including master VM */
-	unsigned n_vms;
-	struct amdgpu_device *adev;
-	/* Eviction Fence. Initialized only for master_vm */
-	struct amdgpu_amdkfd_fence *eviction_fence;
+	uint64_t gpu_addr;
+	void *cpu_ptr;
 };
 
 int amdgpu_amdkfd_init(void);
@@ -119,88 +49,17 @@ void amdgpu_amdkfd_device_probe(struct amdgpu_device *rdev);
 void amdgpu_amdkfd_device_init(struct amdgpu_device *rdev);
 void amdgpu_amdkfd_device_fini(struct amdgpu_device *rdev);
 
-int amdgpu_amdkfd_evict_mem(struct amdgpu_device *adev, struct kgd_mem *mem,
-			    struct mm_struct *mm);
-int amdgpu_amdkfd_schedule_restore_mem(struct amdgpu_device *adev,
-				       struct kgd_mem *mem,
-				       struct mm_struct *mm,
-				       unsigned long delay);
-void amdgpu_amdkfd_cancel_restore_mem(struct amdgpu_device *adev,
-				      struct kgd_mem *mem);
-int amdgpu_amdkfd_submit_ib(struct kgd_dev *kgd, enum kgd_engine_type engine,
-				uint32_t vmid, uint64_t gpu_addr,
-				uint32_t *ib_cmd, uint32_t ib_len);
-int amdgpu_amdkfd_gpuvm_restore_process_bos(void *master_vm);
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_7_get_functions(void);
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_8_0_get_functions(void);
 
 /* Shared API */
-int map_bo(struct amdgpu_device *rdev, uint64_t va, void *vm,
-		struct amdgpu_bo *bo, struct amdgpu_bo_va **bo_va);
 int alloc_gtt_mem(struct kgd_dev *kgd, size_t size,
 			void **mem_obj, uint64_t *gpu_addr,
 			void **cpu_ptr);
 void free_gtt_mem(struct kgd_dev *kgd, void *mem_obj);
-void get_local_mem_info(struct kgd_dev *kgd,
-			struct kfd_local_mem_info *mem_info);
+uint64_t get_vmem_size(struct kgd_dev *kgd);
 uint64_t get_gpu_clock_counter(struct kgd_dev *kgd);
 
 uint32_t get_max_engine_clock_in_mhz(struct kgd_dev *kgd);
-void get_cu_info(struct kgd_dev *kgd, struct kfd_cu_info *cu_info);
-int amdgpu_amdkfd_get_dmabuf_info(struct kgd_dev *kgd, int dma_buf_fd,
-				  struct kgd_dev **dmabuf_kgd,
-				  uint64_t *bo_size, void *metadata_buffer,
-				  size_t buffer_size, uint32_t *metadata_size,
-				  uint32_t *flags);
-
-bool read_user_wptr(struct mm_struct *mm, uint32_t __user *wptr,
-		    uint32_t *wptr_val);
-
-/* GPUVM API */
-int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
-		struct kgd_dev *kgd, uint64_t va, uint64_t size,
-		void *vm, struct kgd_mem **mem,
-		uint64_t *offset, void **kptr,
-		uint32_t flags);
-int amdgpu_amdkfd_gpuvm_free_memory_of_gpu(
-		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm);
-int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
-		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm);
-int amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(
-		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm);
-
-int amdgpu_amdkfd_gpuvm_create_process_vm(struct kgd_dev *kgd, void **vm,
-					  void *master_vm);
-void amdgpu_amdkfd_gpuvm_destroy_process_vm(struct kgd_dev *kgd, void *vm);
-
-uint32_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *vm);
 
-int amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct kgd_dev *kgd,
-					      struct kfd_vm_fault_info *info);
-
-int amdgpu_amdkfd_gpuvm_mmap_bo(
-		struct kgd_dev *kgd, struct vm_area_struct *vma);
-
-int amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_dev *kgd,
-		struct kgd_mem *mem, void **kptr);
-
-int amdgpu_amdkfd_gpuvm_pin_get_sg_table(struct kgd_dev *kgd,
-		struct kgd_mem *mem, uint64_t offset,
-		uint64_t size, struct sg_table **ret_sg);
-void amdgpu_amdkfd_gpuvm_unpin_put_sg_table(
-		struct kgd_mem *mem, struct sg_table *sg);
-int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
-				      struct dma_buf *dmabuf,
-				      uint64_t va, void *vm,
-				      struct kgd_mem **mem, uint64_t *size,
-				      uint64_t *mmap_offset);
-int amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_dev *kgd, void *vm,
-				      struct kgd_mem *mem,
-				      struct dma_buf **dmabuf);
-int amdgpu_amdkfd_gpuvm_evict_mem(struct kgd_mem *mem, struct mm_struct *mm);
-int amdgpu_amdkfd_gpuvm_restore_mem(struct kgd_mem *mem, struct mm_struct *mm);
-
-void amdgpu_amdkfd_gpuvm_init_mem_limits(void);
-void amdgpu_amdkfd_unreserve_system_memory_limit(struct amdgpu_bo *bo);
 #endif /* AMDGPU_AMDKFD_H_INCLUDED */
-
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c
deleted file mode 100644
index ac167c8..0000000
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c
+++ /dev/null
@@ -1,197 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- */
-
-#include <linux/fence.h>
-#include <linux/spinlock.h>
-#include <linux/atomic.h>
-#include <linux/stacktrace.h>
-#include <linux/sched.h>
-#include <linux/slab.h>
-#include "amdgpu_amdkfd.h"
-
-const struct fence_ops amd_kfd_fence_ops;
-static atomic_t fence_seq = ATOMIC_INIT(0);
-
-static int amd_kfd_fence_signal(struct fence *f);
-
-/* Eviction Fence
- * Fence helper functions to deal with KFD memory eviction.
- * Big Idea - Since KFD submissions are done by user queues, a BO cannot be
- *  evicted unless all the user queues for that process are evicted.
- *
- * All the BOs in a process share an eviction fence. When process X wants
- * to map VRAM memory but TTM can't find enough space, TTM will attempt to
- * evict BOs from its LRU list. TTM checks if the BO is valuable to evict
- * by calling ttm_bo_driver->eviction_valuable().
- *
- * ttm_bo_driver->eviction_valuable() - will return false if the BO belongs
- *  to process X. Otherwise, it will return true to indicate BO can be
- *  evicted by TTM.
- *
- * If ttm_bo_driver->eviction_valuable returns true, then TTM will continue
- * the evcition process for that BO by calling ttm_bo_evict --> amdgpu_bo_move
- * --> amdgpu_copy_buffer(). This sets up job in GPU scheduler.
- *
- * GPU Scheduler (amd_sched_main) - sets up a cb (fence_add_callback) to
- *  nofity when the BO is free to move. fence_add_callback --> enable_signaling
- *  --> amdgpu_amdkfd_fence.enable_signaling
- *
- * amdgpu_amdkfd_fence.enable_signaling - Start a work item that will quiesce
- * user queues and signal fence. The work item will also start another delayed
- * work item to restore BOs
- */
-
-struct amdgpu_amdkfd_fence *amdgpu_amdkfd_fence_create(u64 context,
-						       void *mm)
-{
-	struct amdgpu_amdkfd_fence *fence = NULL;
-
-	fence = kzalloc(sizeof(struct amdgpu_amdkfd_fence), GFP_KERNEL);
-	if (fence == NULL)
-		return NULL;
-
-	/* mm_struct mm is used as void pointer to identify the parent
-	 * KFD process. Don't dereference it. Fence and any threads using
-	 * mm is guranteed to be released before process termination.
-	 */
-	fence->mm = mm;
-	get_task_comm(fence->timeline_name, current);
-	spin_lock_init(&fence->lock);
-
-	fence_init(&fence->base, &amd_kfd_fence_ops, &fence->lock,
-		   context, atomic_inc_return(&fence_seq));
-
-	return fence;
-}
-
-struct amdgpu_amdkfd_fence *to_amdgpu_amdkfd_fence(struct fence *f)
-{
-	struct amdgpu_amdkfd_fence *fence;
-
-	if (!f)
-		return NULL;
-
-	fence = container_of(f, struct amdgpu_amdkfd_fence, base);
-	if (fence && f->ops == &amd_kfd_fence_ops)
-		return fence;
-
-	return NULL;
-}
-
-static const char *amd_kfd_fence_get_driver_name(struct fence *f)
-{
-	return "amdgpu_amdkfd_fence";
-}
-
-static const char *amd_kfd_fence_get_timeline_name(struct fence *f)
-{
-	struct amdgpu_amdkfd_fence *fence = to_amdgpu_amdkfd_fence(f);
-
-	return fence->timeline_name;
-}
-
-/**
- * amd_kfd_fence_enable_signaling - This gets called when TTM wants to evict
- *  a KFD BO and schedules a job to move the BO.
- *  If fence is already signaled return true.
- *  If fence is not signaled schedule a evict KFD process work item.
- */
-static bool amd_kfd_fence_enable_signaling(struct fence *f)
-{
-	struct amdgpu_amdkfd_fence *fence = to_amdgpu_amdkfd_fence(f);
-
-	if (!fence)
-		return false;
-
-	if (fence_is_signaled(f))
-		return true;
-
-	if (!kgd2kfd->schedule_evict_and_restore_process(
-				(struct mm_struct *)fence->mm, f))
-		return true;
-
-	return false;
-}
-
-static int amd_kfd_fence_signal(struct fence *f)
-{
-	unsigned long flags;
-	int ret;
-
-	spin_lock_irqsave(f->lock, flags);
-	/* Set enabled bit so cb will called */
-	set_bit(FENCE_FLAG_ENABLE_SIGNAL_BIT, &f->flags);
-	ret = fence_signal_locked(f);
-	spin_unlock_irqrestore(f->lock, flags);
-
-	return ret;
-}
-
-/**
- * amd_kfd_fence_release - callback that fence can be freed
- *
- * @fence: fence
- *
- * This function is called when the reference count becomes zero.
- * It just RCU schedules freeing up the fence.
-*/
-static void amd_kfd_fence_release(struct fence *f)
-{
-	struct amdgpu_amdkfd_fence *fence = to_amdgpu_amdkfd_fence(f);
-	/* Unconditionally signal the fence. The process is getting
-	 * terminated.
-	 */
-	if (WARN_ON(!fence))
-		return; /* Not an amdgpu_amdkfd_fence */
-
-	amd_kfd_fence_signal(f);
-	kfree_rcu(f, rcu);
-}
-
-/**
- * amd_kfd_fence_check_mm - Check if @mm is same as that of the fence @f
- *  if same return TRUE else return FALSE.
- *
- * @f: [IN] fence
- * @mm: [IN] mm that needs to be verified
-*/
-bool amd_kfd_fence_check_mm(struct fence *f, void *mm)
-{
-	struct amdgpu_amdkfd_fence *fence = to_amdgpu_amdkfd_fence(f);
-
-	if (!fence)
-		return false;
-	else if (fence->mm == mm)
-		return true;
-
-	return false;
-}
-
-const struct fence_ops amd_kfd_fence_ops = {
-	.get_driver_name = amd_kfd_fence_get_driver_name,
-	.get_timeline_name = amd_kfd_fence_get_timeline_name,
-	.enable_signaling = amd_kfd_fence_enable_signaling,
-	.signaled = NULL,
-	.wait = fence_default_wait,
-	.release = amd_kfd_fence_release,
-};
-
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
index c49c852..1a0a5f7 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
@@ -39,15 +39,6 @@
 #include "cik_structs.h"
 
 #define CIK_PIPE_PER_MEC	(4)
-#define CIK_QUEUES_PER_PIPE_MEC	(8)
-
-#define AMDKFD_SKIP_UNCOMPILED_CODE 1
-
-enum hqd_dequeue_request_type {
-	NO_ACTION = 0,
-	DRAIN_PIPE,
-	RESET_WAVES
-};
 
 enum {
 	MAX_TRAPID = 8,		/* 3 bits in the bitfield. */
@@ -65,8 +56,8 @@ enum {
 enum {
 	ADDRESS_WATCH_REG_CNTL_ATC_BIT = 0x10000000UL,
 	ADDRESS_WATCH_REG_CNTL_DEFAULT_MASK = 0x00FFFFFF,
-	ADDRESS_WATCH_REG_ADDLOW_MASK_EXTENTION = 0x03000000,
-	/* extend the mask to 26 bits in order to match the low address field. */
+	ADDRESS_WATCH_REG_ADDLOW_MASK_EXTENSION = 0x03000000,
+	/* extend the mask to 26 bits to match the low address field */
 	ADDRESS_WATCH_REG_ADDLOW_SHIFT = 6,
 	ADDRESS_WATCH_REG_ADDHIGH_MASK = 0xFFFF
 };
@@ -91,40 +82,32 @@ union TCP_WATCH_CNTL_BITS {
 	float f32All;
 };
 
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm, int fd, uint32_t handle, struct kgd_mem **mem);
-
-static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type);
-
 /*
  * Register access functions
  */
 
-static void kgd_program_sh_mem_settings(struct kgd_dev *kgd, uint32_t vmid, uint32_t sh_mem_config,
-		uint32_t sh_mem_ape1_base, uint32_t sh_mem_ape1_limit, uint32_t sh_mem_bases);
-static int kgd_set_pasid_vmid_mapping(struct kgd_dev *kgd, unsigned int pasid, unsigned int vmid);
-static int kgd_init_pipeline(struct kgd_dev *kgd, uint32_t pipe_id, uint32_t hpd_size, uint64_t hpd_gpu_addr);
+static void kgd_program_sh_mem_settings(struct kgd_dev *kgd, uint32_t vmid,
+		uint32_t sh_mem_config,	uint32_t sh_mem_ape1_base,
+		uint32_t sh_mem_ape1_limit, uint32_t sh_mem_bases);
+
+static int kgd_set_pasid_vmid_mapping(struct kgd_dev *kgd, unsigned int pasid,
+					unsigned int vmid);
+
+static int kgd_init_pipeline(struct kgd_dev *kgd, uint32_t pipe_id,
+				uint32_t hpd_size, uint64_t hpd_gpu_addr);
 static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id);
 static int kgd_hqd_load(struct kgd_dev *kgd, void *mqd, uint32_t pipe_id,
-			uint32_t queue_id, uint32_t __user *wptr,
-			uint32_t wptr_shift, uint32_t wptr_mask,
-			struct mm_struct *mm);
-static int kgd_hqd_dump(struct kgd_dev *kgd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs);
-static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm);
-static int kgd_hqd_sdma_dump(struct kgd_dev *kgd,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs);
+			uint32_t queue_id, uint32_t __user *wptr);
+static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd);
 static bool kgd_hqd_is_occupied(struct kgd_dev *kgd, uint64_t queue_address,
-		uint32_t pipe_id, uint32_t queue_id);
-static bool kgd_hqd_sdma_is_occupied(struct kgd_dev *kgd, void *mqd);
-static int kgd_hqd_destroy(struct kgd_dev *kgd,
-				enum kfd_preempt_type reset_type,
-				unsigned int timeout, uint32_t pipe_id,
+				uint32_t pipe_id, uint32_t queue_id);
+
+static int kgd_hqd_destroy(struct kgd_dev *kgd, uint32_t reset_type,
+				unsigned int utimeout, uint32_t pipe_id,
 				uint32_t queue_id);
+static bool kgd_hqd_sdma_is_occupied(struct kgd_dev *kgd, void *mqd);
 static int kgd_hqd_sdma_destroy(struct kgd_dev *kgd, void *mqd,
-				unsigned int timeout);
+				unsigned int utimeout);
 static int kgd_address_watch_disable(struct kgd_dev *kgd);
 static int kgd_address_watch_execute(struct kgd_dev *kgd,
 					unsigned int watch_point_id,
@@ -142,59 +125,21 @@ static bool get_atc_vmid_pasid_mapping_valid(struct kgd_dev *kgd, uint8_t vmid);
 static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 							uint8_t vmid);
 static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid);
-static void set_num_of_requests(struct kgd_dev *dev, uint8_t num_of_req);
-static int alloc_memory_of_scratch(struct kgd_dev *kgd,
-					 uint64_t va, uint32_t vmid);
-static int write_config_static_mem(struct kgd_dev *kgd, bool swizzle_enable,
-		uint8_t element_size, uint8_t index_stride, uint8_t mtype);
-static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
-		uint32_t page_table_base);
-static uint32_t read_vmid_from_vmfault_reg(struct kgd_dev *kgd);
-
-/* Because of REG_GET_FIELD() being used, we put this function in the
- * asic specific file.
- */
-static int amdgpu_amdkfd_get_tile_config(struct kgd_dev *kgd,
-		struct tile_config *config)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
-
-	config->gb_addr_config = adev->gfx.config.gb_addr_config;
-	config->num_banks = REG_GET_FIELD(adev->gfx.config.mc_arb_ramcfg,
-				MC_ARB_RAMCFG, NOOFBANK);
-	config->num_ranks = REG_GET_FIELD(adev->gfx.config.mc_arb_ramcfg,
-				MC_ARB_RAMCFG, NOOFRANKS);
-
-	config->tile_config_ptr = adev->gfx.config.tile_mode_array;
-	config->num_tile_configs =
-			ARRAY_SIZE(adev->gfx.config.tile_mode_array);
-	config->macro_tile_config_ptr =
-			adev->gfx.config.macrotile_mode_array;
-	config->num_macro_tile_configs =
-			ARRAY_SIZE(adev->gfx.config.macrotile_mode_array);
 
-
-	return 0;
-}
+static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type);
 
 static const struct kfd2kgd_calls kfd2kgd = {
 	.init_gtt_mem_allocation = alloc_gtt_mem,
 	.free_gtt_mem = free_gtt_mem,
-	.get_local_mem_info = get_local_mem_info,
+	.get_vmem_size = get_vmem_size,
 	.get_gpu_clock_counter = get_gpu_clock_counter,
 	.get_max_engine_clock_in_mhz = get_max_engine_clock_in_mhz,
-	.create_process_vm = amdgpu_amdkfd_gpuvm_create_process_vm,
-	.destroy_process_vm = amdgpu_amdkfd_gpuvm_destroy_process_vm,
-	.get_process_page_dir = amdgpu_amdkfd_gpuvm_get_process_page_dir,
-	.open_graphic_handle = open_graphic_handle,
 	.program_sh_mem_settings = kgd_program_sh_mem_settings,
 	.set_pasid_vmid_mapping = kgd_set_pasid_vmid_mapping,
 	.init_pipeline = kgd_init_pipeline,
 	.init_interrupts = kgd_init_interrupts,
 	.hqd_load = kgd_hqd_load,
 	.hqd_sdma_load = kgd_hqd_sdma_load,
-	.hqd_dump = kgd_hqd_dump,
-	.hqd_sdma_dump = kgd_hqd_sdma_dump,
 	.hqd_is_occupied = kgd_hqd_is_occupied,
 	.hqd_sdma_is_occupied = kgd_hqd_sdma_is_occupied,
 	.hqd_destroy = kgd_hqd_destroy,
@@ -203,46 +148,17 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.address_watch_execute = kgd_address_watch_execute,
 	.wave_control_execute = kgd_wave_control_execute,
 	.address_watch_get_offset = kgd_address_watch_get_offset,
-	.get_atc_vmid_pasid_mapping_pasid =
-			get_atc_vmid_pasid_mapping_pasid,
-	.get_atc_vmid_pasid_mapping_valid =
-			get_atc_vmid_pasid_mapping_valid,
-	.read_vmid_from_vmfault_reg = read_vmid_from_vmfault_reg,
+	.get_atc_vmid_pasid_mapping_pasid = get_atc_vmid_pasid_mapping_pasid,
+	.get_atc_vmid_pasid_mapping_valid = get_atc_vmid_pasid_mapping_valid,
 	.write_vmid_invalidate_request = write_vmid_invalidate_request,
-	.alloc_memory_of_gpu = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu,
-	.free_memory_of_gpu = amdgpu_amdkfd_gpuvm_free_memory_of_gpu,
-	.map_memory_to_gpu = amdgpu_amdkfd_gpuvm_map_memory_to_gpu,
-	.unmap_memory_to_gpu = amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu,
-	.get_fw_version = get_fw_version,
-	.set_num_of_requests = set_num_of_requests,
-	.get_cu_info = get_cu_info,
-	.alloc_memory_of_scratch = alloc_memory_of_scratch,
-	.write_config_static_mem = write_config_static_mem,
-	.mmap_bo = amdgpu_amdkfd_gpuvm_mmap_bo,
-	.map_gtt_bo_to_kernel = amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel,
-	.set_vm_context_page_table_base = set_vm_context_page_table_base,
-	.pin_get_sg_table_bo = amdgpu_amdkfd_gpuvm_pin_get_sg_table,
-	.unpin_put_sg_table_bo = amdgpu_amdkfd_gpuvm_unpin_put_sg_table,
-	.get_dmabuf_info = amdgpu_amdkfd_get_dmabuf_info,
-	.import_dmabuf = amdgpu_amdkfd_gpuvm_import_dmabuf,
-	.export_dmabuf = amdgpu_amdkfd_gpuvm_export_dmabuf,
-	.get_vm_fault_info = amdgpu_amdkfd_gpuvm_get_vm_fault_info,
-	.submit_ib = amdgpu_amdkfd_submit_ib,
-	.get_tile_config = amdgpu_amdkfd_get_tile_config,
-	.restore_process_bos = amdgpu_amdkfd_gpuvm_restore_process_bos
+	.get_fw_version = get_fw_version
 };
 
-struct kfd2kgd_calls *amdgpu_amdkfd_gfx_7_get_functions()
+struct kfd2kgd_calls *amdgpu_amdkfd_gfx_7_get_functions(void)
 {
 	return (struct kfd2kgd_calls *)&kfd2kgd;
 }
 
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm,
-				int fd, uint32_t handle, struct kgd_mem **mem)
-{
-	return 0;
-}
-
 static inline struct amdgpu_device *get_amdgpu_device(struct kgd_dev *kgd)
 {
 	return (struct amdgpu_device *)kgd;
@@ -305,11 +221,12 @@ static int kgd_set_pasid_vmid_mapping(struct kgd_dev *kgd, unsigned int pasid,
 
 	/*
 	 * We have to assume that there is no outstanding mapping.
-	 * The ATC_VMID_PASID_MAPPING_UPDATE_STATUS bit could be 0 because a mapping
-	 * is in progress or because a mapping finished and the SW cleared it.
-	 * So the protocol is to always wait & clear.
+	 * The ATC_VMID_PASID_MAPPING_UPDATE_STATUS bit could be 0 because
+	 * a mapping is in progress or because a mapping finished and the
+	 * SW cleared it. So the protocol is to always wait & clear.
 	 */
-	uint32_t pasid_mapping = (pasid == 0) ? 0 : (uint32_t)pasid | ATC_VMID0_PASID_MAPPING__VALID_MASK;
+	uint32_t pasid_mapping = (pasid == 0) ? 0 : (uint32_t)pasid |
+			ATC_VMID0_PASID_MAPPING__VALID_MASK;
 
 	WREG32(mmATC_VMID0_PASID_MAPPING + vmid, pasid_mapping);
 
@@ -347,7 +264,7 @@ static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id)
 	uint32_t mec;
 	uint32_t pipe;
 
-	mec = (++pipe_id / CIK_PIPE_PER_MEC) + 1;
+	mec = (pipe_id / CIK_PIPE_PER_MEC) + 1;
 	pipe = (pipe_id % CIK_PIPE_PER_MEC);
 
 	lock_srbm(kgd, mec, pipe, 0, 0);
@@ -366,6 +283,7 @@ static inline uint32_t get_sdma_base_addr(struct cik_sdma_rlc_registers *m)
 
 	retval = m->sdma_engine_id * SDMA1_REGISTER_OFFSET +
 			m->sdma_queue_id * KFD_CIK_SDMA_QUEUE_OFFSET;
+
 	pr_debug("kfd: sdma base address: 0x%x\n", retval);
 
 	return retval;
@@ -382,154 +300,98 @@ static inline struct cik_sdma_rlc_registers *get_sdma_mqd(void *mqd)
 }
 
 static int kgd_hqd_load(struct kgd_dev *kgd, void *mqd, uint32_t pipe_id,
-			uint32_t queue_id, uint32_t __user *wptr,
-			uint32_t wptr_shift, uint32_t wptr_mask,
-			struct mm_struct *mm)
+			uint32_t queue_id, uint32_t __user *wptr)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	uint32_t wptr_shadow, is_wptr_shadow_valid;
 	struct cik_mqd *m;
-	uint32_t *mqd_hqd;
-	uint32_t reg, wptr_val;
 
 	m = get_mqd(mqd);
 
+	is_wptr_shadow_valid = !get_user(wptr_shadow, wptr);
+
 	acquire_queue(kgd, pipe_id, queue_id);
+	WREG32(mmCP_MQD_BASE_ADDR, m->cp_mqd_base_addr_lo);
+	WREG32(mmCP_MQD_BASE_ADDR_HI, m->cp_mqd_base_addr_hi);
+	WREG32(mmCP_MQD_CONTROL, m->cp_mqd_control);
 
-	/* HQD registers extend from CP_MQD_BASE_ADDR to CP_MQD_CONTROL. */
-	mqd_hqd = &m->cp_mqd_base_addr_lo;
+	WREG32(mmCP_HQD_PQ_BASE, m->cp_hqd_pq_base_lo);
+	WREG32(mmCP_HQD_PQ_BASE_HI, m->cp_hqd_pq_base_hi);
+	WREG32(mmCP_HQD_PQ_CONTROL, m->cp_hqd_pq_control);
 
-	for (reg = mmCP_HQD_VMID; reg <= mmCP_MQD_CONTROL; reg++)
-		WREG32(reg, mqd_hqd[reg - mmCP_MQD_BASE_ADDR]);
+	WREG32(mmCP_HQD_IB_CONTROL, m->cp_hqd_ib_control);
+	WREG32(mmCP_HQD_IB_BASE_ADDR, m->cp_hqd_ib_base_addr_lo);
+	WREG32(mmCP_HQD_IB_BASE_ADDR_HI, m->cp_hqd_ib_base_addr_hi);
 
-	/* Copy userspace write pointer value to register.
-	 * Doorbell logic is active and will monitor subsequent changes.
-	 */
-	if (read_user_wptr(mm, wptr, &wptr_val))
-		WREG32(mmCP_HQD_PQ_WPTR, (wptr_val << wptr_shift) & wptr_mask);
+	WREG32(mmCP_HQD_IB_RPTR, m->cp_hqd_ib_rptr);
 
-	/* Write CP_HQD_ACTIVE last. */
-	for (reg = mmCP_MQD_BASE_ADDR; reg <= mmCP_HQD_ACTIVE; reg++)
-		WREG32(reg, mqd_hqd[reg - mmCP_MQD_BASE_ADDR]);
+	WREG32(mmCP_HQD_PERSISTENT_STATE, m->cp_hqd_persistent_state);
+	WREG32(mmCP_HQD_SEMA_CMD, m->cp_hqd_sema_cmd);
+	WREG32(mmCP_HQD_MSG_TYPE, m->cp_hqd_msg_type);
 
-	release_queue(kgd);
+	WREG32(mmCP_HQD_ATOMIC0_PREOP_LO, m->cp_hqd_atomic0_preop_lo);
+	WREG32(mmCP_HQD_ATOMIC0_PREOP_HI, m->cp_hqd_atomic0_preop_hi);
+	WREG32(mmCP_HQD_ATOMIC1_PREOP_LO, m->cp_hqd_atomic1_preop_lo);
+	WREG32(mmCP_HQD_ATOMIC1_PREOP_HI, m->cp_hqd_atomic1_preop_hi);
 
-	return 0;
-}
+	WREG32(mmCP_HQD_PQ_RPTR_REPORT_ADDR, m->cp_hqd_pq_rptr_report_addr_lo);
+	WREG32(mmCP_HQD_PQ_RPTR_REPORT_ADDR_HI,
+			m->cp_hqd_pq_rptr_report_addr_hi);
 
-static int kgd_hqd_dump(struct kgd_dev *kgd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs)
-{
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	uint32_t i = 0, reg;
-#define HQD_N_REGS (35+4)
-#define DUMP_REG(addr) do {				\
-		if (WARN_ON_ONCE(i >= HQD_N_REGS))	\
-			break;				\
-		(*dump)[i][0] = (addr) << 2;		\
-		(*dump)[i++][1] = RREG32(addr);		\
-	} while (0)
-
-	*dump = kmalloc(HQD_N_REGS*2*sizeof(uint32_t), GFP_KERNEL);
-	if (*dump == NULL)
-		return -ENOMEM;
+	WREG32(mmCP_HQD_PQ_RPTR, m->cp_hqd_pq_rptr);
 
-	acquire_queue(kgd, pipe_id, queue_id);
+	WREG32(mmCP_HQD_PQ_WPTR_POLL_ADDR, m->cp_hqd_pq_wptr_poll_addr_lo);
+	WREG32(mmCP_HQD_PQ_WPTR_POLL_ADDR_HI, m->cp_hqd_pq_wptr_poll_addr_hi);
 
-	DUMP_REG(mmCOMPUTE_STATIC_THREAD_MGMT_SE0);
-	DUMP_REG(mmCOMPUTE_STATIC_THREAD_MGMT_SE1);
-	DUMP_REG(mmCOMPUTE_STATIC_THREAD_MGMT_SE2);
-	DUMP_REG(mmCOMPUTE_STATIC_THREAD_MGMT_SE3);
+	WREG32(mmCP_HQD_PQ_DOORBELL_CONTROL, m->cp_hqd_pq_doorbell_control);
 
-	for (reg = mmCP_MQD_BASE_ADDR; reg <= mmCP_MQD_CONTROL; reg++)
-		DUMP_REG(reg);
+	WREG32(mmCP_HQD_VMID, m->cp_hqd_vmid);
 
-	release_queue(kgd);
+	WREG32(mmCP_HQD_QUANTUM, m->cp_hqd_quantum);
+
+	WREG32(mmCP_HQD_PIPE_PRIORITY, m->cp_hqd_pipe_priority);
+	WREG32(mmCP_HQD_QUEUE_PRIORITY, m->cp_hqd_queue_priority);
 
-	WARN_ON_ONCE(i != HQD_N_REGS);
-	*n_regs = i;
+	WREG32(mmCP_HQD_IQ_RPTR, m->cp_hqd_iq_rptr);
+
+	if (is_wptr_shadow_valid)
+		WREG32(mmCP_HQD_PQ_WPTR, wptr_shadow);
+
+	WREG32(mmCP_HQD_ACTIVE, m->cp_hqd_active);
+	release_queue(kgd);
 
 	return 0;
 }
 
-static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm)
+static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 	struct cik_sdma_rlc_registers *m;
 	uint32_t sdma_base_addr;
-	uint32_t temp, timeout = 2000;
-	uint32_t data;
 
 	m = get_sdma_mqd(mqd);
 	sdma_base_addr = get_sdma_base_addr(m);
 
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL,
-		m->sdma_rlc_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
-
-	while (true) {
-		temp = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
-		if (temp & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
-			break;
-		if (timeout == 0)
-			return -ETIME;
-		msleep(10);
-		timeout -= 10;
-	}
-	if (m->sdma_engine_id) {
-		data = RREG32(mmSDMA1_GFX_CONTEXT_CNTL);
-		data = REG_SET_FIELD(data, SDMA1_GFX_CONTEXT_CNTL,
-				RESUME_CTX, 0);
-		WREG32(mmSDMA1_GFX_CONTEXT_CNTL, data);
-	} else {
-		data = RREG32(mmSDMA0_GFX_CONTEXT_CNTL);
-		data = REG_SET_FIELD(data, SDMA0_GFX_CONTEXT_CNTL,
-				RESUME_CTX, 0);
-		WREG32(mmSDMA0_GFX_CONTEXT_CNTL, data);
-	}
-
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_DOORBELL, m->sdma_rlc_doorbell);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR, m->sdma_rlc_rb_rptr);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_VIRTUAL_ADDR,
+			m->sdma_rlc_virtual_addr);
 
-	if (read_user_wptr(mm, wptr, &data))
-		WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_WPTR, data);
-	else
-		WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_WPTR,
-		       m->sdma_rlc_rb_rptr);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_BASE,
+			m->sdma_rlc_rb_base);
 
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_VIRTUAL_ADDR, m->sdma_rlc_virtual_addr);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_BASE, m->sdma_rlc_rb_base);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_BASE_HI, m->sdma_rlc_rb_base_hi);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR_ADDR_LO, m->sdma_rlc_rb_rptr_addr_lo);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR_ADDR_HI, m->sdma_rlc_rb_rptr_addr_hi);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL, m->sdma_rlc_rb_cntl);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_BASE_HI,
+			m->sdma_rlc_rb_base_hi);
 
-	return 0;
-}
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR_ADDR_LO,
+			m->sdma_rlc_rb_rptr_addr_lo);
 
-static int kgd_hqd_sdma_dump(struct kgd_dev *kgd,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs)
-{
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	uint32_t sdma_offset = engine_id * SDMA1_REGISTER_OFFSET +
-		queue_id * KFD_CIK_SDMA_QUEUE_OFFSET;
-	uint32_t i = 0, reg;
-#undef HQD_N_REGS
-#define HQD_N_REGS (19+4)
-
-	*dump = kmalloc(HQD_N_REGS*2*sizeof(uint32_t), GFP_KERNEL);
-	if (*dump == NULL)
-		return -ENOMEM;
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR_ADDR_HI,
+			m->sdma_rlc_rb_rptr_addr_hi);
 
-	for (reg = mmSDMA0_RLC0_RB_CNTL; reg <= mmSDMA0_RLC0_DOORBELL; reg++)
-		DUMP_REG(sdma_offset + reg);
-	for (reg = mmSDMA0_RLC0_VIRTUAL_ADDR; reg <= mmSDMA0_RLC0_WATERMARK;
-	     reg++)
-		DUMP_REG(sdma_offset + reg);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_DOORBELL,
+			m->sdma_rlc_doorbell);
 
-	WARN_ON_ONCE(i != HQD_N_REGS);
-	*n_regs = i;
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL,
+			m->sdma_rlc_rb_cntl);
 
 	return 0;
 }
@@ -574,100 +436,30 @@ static bool kgd_hqd_sdma_is_occupied(struct kgd_dev *kgd, void *mqd)
 	return false;
 }
 
-static int kgd_hqd_destroy(struct kgd_dev *kgd,
-				enum kfd_preempt_type reset_type,
-				unsigned int timeout, uint32_t pipe_id,
+static int kgd_hqd_destroy(struct kgd_dev *kgd, uint32_t reset_type,
+				unsigned int utimeout, uint32_t pipe_id,
 				uint32_t queue_id)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 	uint32_t temp;
-	enum hqd_dequeue_request_type type;
-	unsigned long flags, end_jiffies;
-	int retry;
+	int timeout = utimeout;
 
 	acquire_queue(kgd, pipe_id, queue_id);
-
 	WREG32(mmCP_HQD_PQ_DOORBELL_CONTROL, 0);
 
-	switch (reset_type) {
-	case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
-		type = DRAIN_PIPE;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
-		type = RESET_WAVES;
-		break;
-	default:
-		type = DRAIN_PIPE;
-		break;
-	}
-
-	/* Workaround: If IQ timer is active and the wait time is close to or
-	 * equal to 0, dequeueing is not safe. Wait until either the wait time
-	 * is larger or timer is cleared. Also, ensure that IQ_REQ_PEND is
-	 * cleared before continuing. Also, ensure wait times are set to at
-	 * least 0x3.
-	 */
-	local_irq_save(flags);
-	preempt_disable();
-	retry = 5000; /* wait for 500 usecs at maximum */
-	while (true) {
-		temp = RREG32(mmCP_HQD_IQ_TIMER);
-		if (REG_GET_FIELD(temp, CP_HQD_IQ_TIMER, PROCESSING_IQ)) {
-			pr_debug("HW is processing IQ\n");
-			goto loop;
-		}
-		if (REG_GET_FIELD(temp, CP_HQD_IQ_TIMER, ACTIVE)) {
-			if (REG_GET_FIELD(temp, CP_HQD_IQ_TIMER, RETRY_TYPE)
-					== 3) /* SEM-rearm is safe */
-				break;
-			/* Wait time 3 is safe for CP, but our MMIO read/write
-			 * time is close to 1 microsecond, so check for 10 to
-			 * leave more buffer room
-			 */
-			if (REG_GET_FIELD(temp, CP_HQD_IQ_TIMER, WAIT_TIME)
-					>= 10)
-				break;
-			pr_debug("IQ timer is active\n");
-		} else
-			break;
-	loop:
-		if (!retry) {
-			pr_err("kfd: CP HQD IQ timer status time out\n");
-			break;
-		}
-		ndelay(100);
-		--retry;
-	}
-	retry = 1000;
-	while (true) {
-		temp = RREG32(mmCP_HQD_DEQUEUE_REQUEST);
-		if (!(temp & CP_HQD_DEQUEUE_REQUEST__IQ_REQ_PEND_MASK))
-			break;
-		pr_debug("Dequeue request is pending\n");
-
-		if (!retry) {
-			pr_err("kfd: CP HQD dequeue request time out\n");
-			break;
-		}
-		ndelay(100);
-		--retry;
-	}
-	local_irq_restore(flags);
-	preempt_enable();
-
-	WREG32(mmCP_HQD_DEQUEUE_REQUEST, type);
+	WREG32(mmCP_HQD_DEQUEUE_REQUEST, reset_type);
 
-	end_jiffies = (timeout * HZ / 1000) + jiffies;
 	while (true) {
 		temp = RREG32(mmCP_HQD_ACTIVE);
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
+		if (temp & CP_HQD_ACTIVE__ACTIVE_MASK)
 			break;
-		if (time_after(jiffies, end_jiffies)) {
-			pr_err("kfd: cp queue preemption time out\n");
+		if (timeout <= 0) {
+			pr_err("kfd: cp queue preemption time out.\n");
 			release_queue(kgd);
 			return -ETIME;
 		}
-		usleep_range(500, 1000);
+		msleep(20);
+		timeout -= 20;
 	}
 
 	release_queue(kgd);
@@ -675,13 +467,13 @@ static int kgd_hqd_destroy(struct kgd_dev *kgd,
 }
 
 static int kgd_hqd_sdma_destroy(struct kgd_dev *kgd, void *mqd,
-				unsigned int timeout)
+				unsigned int utimeout)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 	struct cik_sdma_rlc_registers *m;
 	uint32_t sdma_base_addr;
 	uint32_t temp;
-	unsigned long end_jiffies = (timeout * HZ / 1000) + jiffies;
+	int timeout = utimeout;
 
 	m = get_sdma_mqd(mqd);
 	sdma_base_addr = get_sdma_base_addr(m);
@@ -692,19 +484,18 @@ static int kgd_hqd_sdma_destroy(struct kgd_dev *kgd, void *mqd,
 
 	while (true) {
 		temp = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
-		if (temp & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
+		if (temp & SDMA0_STATUS_REG__RB_CMD_IDLE__SHIFT)
 			break;
-		if (time_after(jiffies, end_jiffies))
+		if (timeout <= 0)
 			return -ETIME;
-		usleep_range(500, 1000);
+		msleep(20);
+		timeout -= 20;
 	}
 
 	WREG32(sdma_base_addr + mmSDMA0_RLC0_DOORBELL, 0);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL,
-		RREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL) |
-		SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
-
-	m->sdma_rlc_rb_rptr = RREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR, 0);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_WPTR, 0);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_BASE, 0);
 
 	return 0;
 }
@@ -723,8 +514,8 @@ static int kgd_address_watch_disable(struct kgd_dev *kgd)
 
 	/* Turning off this address until we set all the registers */
 	for (i = 0; i < MAX_WATCH_ADDRESSES; i++)
-		WREG32(watchRegs[i * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_CNTL],
-			cntl.u32All);
+		WREG32(watchRegs[i * ADDRESS_WATCH_REG_MAX +
+			ADDRESS_WATCH_REG_CNTL], cntl.u32All);
 
 	return 0;
 }
@@ -742,20 +533,20 @@ static int kgd_address_watch_execute(struct kgd_dev *kgd,
 
 	/* Turning off this watch point until we set all the registers */
 	cntl.bitfields.valid = 0;
-	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_CNTL],
-		cntl.u32All);
+	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX +
+		ADDRESS_WATCH_REG_CNTL], cntl.u32All);
 
-	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_ADDR_HI],
-		addr_hi);
+	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX +
+		ADDRESS_WATCH_REG_ADDR_HI], addr_hi);
 
-	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_ADDR_LO],
-		addr_lo);
+	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX +
+		ADDRESS_WATCH_REG_ADDR_LO], addr_lo);
 
 	/* Enable the watch point */
 	cntl.bitfields.valid = 1;
 
-	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_CNTL],
-		cntl.u32All);
+	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX +
+		ADDRESS_WATCH_REG_CNTL], cntl.u32All);
 
 	return 0;
 }
@@ -809,7 +600,7 @@ static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
 
 	reg = RREG32(mmATC_VMID0_PASID_MAPPING + vmid);
-	return reg & ATC_VMID0_PASID_MAPPING__PASID_MASK;
+	return reg & ATC_VMID0_PASID_MAPPING__VALID_MASK;
 }
 
 static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
@@ -819,33 +610,6 @@ static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
 	WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
 }
 
-static int write_config_static_mem(struct kgd_dev *kgd, bool swizzle_enable,
-		uint8_t element_size, uint8_t index_stride, uint8_t mtype)
-{
-	uint32_t reg;
-	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
-
-	reg = swizzle_enable << SH_STATIC_MEM_CONFIG__SWIZZLE_ENABLE__SHIFT |
-		element_size << SH_STATIC_MEM_CONFIG__ELEMENT_SIZE__SHIFT |
-		index_stride << SH_STATIC_MEM_CONFIG__INDEX_STRIDE__SHIFT |
-		mtype << SH_STATIC_MEM_CONFIG__PRIVATE_MTYPE__SHIFT;
-
-	WREG32(mmSH_STATIC_MEM_CONFIG, reg);
-	return 0;
-}
-static int alloc_memory_of_scratch(struct kgd_dev *kgd,
-				 uint64_t va, uint32_t vmid)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
-
-	lock_srbm(kgd, 0, 0, 0, vmid);
-	WREG32(mmSH_HIDDEN_PRIVATE_BASE_VMID, va);
-	unlock_srbm(kgd);
-
-	return 0;
-}
-
-
 static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
@@ -905,42 +669,3 @@ static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type)
 	return hdr->common.ucode_version;
 }
 
-static void set_num_of_requests(struct kgd_dev *dev, uint8_t num_of_req)
-{
-	uint32_t value;
-	struct amdgpu_device *adev = get_amdgpu_device(dev);
-
-	value = RREG32(mmATC_ATS_DEBUG);
-	value &= ~ATC_ATS_DEBUG__NUM_REQUESTS_AT_ERR_MASK;
-	value |= (num_of_req << ATC_ATS_DEBUG__NUM_REQUESTS_AT_ERR__SHIFT);
-
-	WREG32(mmATC_ATS_DEBUG, value);
-}
-
-static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
-			uint32_t page_table_base)
-{
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	/* TODO: Don't use hardcoded VMIDs */
-	if (vmid < 8 || vmid > 15) {
-		pr_err("amdkfd: trying to set page table base for wrong VMID\n");
-		return;
-	}
-	WREG32(mmVM_CONTEXT8_PAGE_TABLE_BASE_ADDR + vmid - 8, page_table_base);
-}
-
- /**
-  * read_vmid_from_vmfault_reg - read vmid from register
-  *
-  * adev: amdgpu_device pointer
-  * @vmid: vmid pointer
-  * read vmid from register (CIK).
-  */
-static uint32_t read_vmid_from_vmfault_reg(struct kgd_dev *kgd)
-{
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-
-	uint32_t status = RREG32(mmVM_CONTEXT1_PROTECTION_FAULT_STATUS);
-
-	return REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS, VMID);
-}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
index 1cff04e..6697612 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
@@ -28,7 +28,6 @@
 #include "amdgpu.h"
 #include "amdgpu_amdkfd.h"
 #include "amdgpu_ucode.h"
-#include "amdgpu_amdkfd_gfx_v8.h"
 #include "gca/gfx_8_0_sh_mask.h"
 #include "gca/gfx_8_0_d.h"
 #include "gca/gfx_8_0_enum.h"
@@ -39,34 +38,9 @@
 #include "vi_structs.h"
 #include "vid.h"
 
-#define VI_PIPE_PER_MEC		(4)
-#define VI_QUEUES_PER_PIPE_MEC	(8)
+#define VI_PIPE_PER_MEC	(4)
 
-enum hqd_dequeue_request_type {
-	NO_ACTION = 0,
-	DRAIN_PIPE,
-	RESET_WAVES,
-	SAVE_WAVES
-};
-
-static const uint32_t watchRegs[MAX_WATCH_ADDRESSES * ADDRESS_WATCH_REG_MAX] = {
-	mmTCP_WATCH0_ADDR_H, mmTCP_WATCH0_ADDR_L, mmTCP_WATCH0_CNTL,
-	mmTCP_WATCH1_ADDR_H, mmTCP_WATCH1_ADDR_L, mmTCP_WATCH1_CNTL,
-	mmTCP_WATCH2_ADDR_H, mmTCP_WATCH2_ADDR_L, mmTCP_WATCH2_CNTL,
-	mmTCP_WATCH3_ADDR_H, mmTCP_WATCH3_ADDR_L, mmTCP_WATCH3_CNTL
-};
-
-
-struct vi_sdma_mqd;
-
-static int create_process_gpumem(struct kgd_dev *kgd, uint64_t va, size_t size,
-		void *vm, struct kgd_mem **mem);
-static void destroy_process_gpumem(struct kgd_dev *kgd, struct kgd_mem *mem);
-
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm,
-				int fd, uint32_t handle, struct kgd_mem **mem);
-
-static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type);
+struct cik_sdma_rlc_registers;
 
 /*
  * Register access functions
@@ -82,26 +56,16 @@ static int kgd_init_pipeline(struct kgd_dev *kgd, uint32_t pipe_id,
 		uint32_t hpd_size, uint64_t hpd_gpu_addr);
 static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id);
 static int kgd_hqd_load(struct kgd_dev *kgd, void *mqd, uint32_t pipe_id,
-			uint32_t queue_id, uint32_t __user *wptr,
-			uint32_t wptr_shift, uint32_t wptr_mask,
-			struct mm_struct *mm);
-static int kgd_hqd_dump(struct kgd_dev *kgd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs);
-static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm);
-static int kgd_hqd_sdma_dump(struct kgd_dev *kgd,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs);
+		uint32_t queue_id, uint32_t __user *wptr);
+static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd);
 static bool kgd_hqd_is_occupied(struct kgd_dev *kgd, uint64_t queue_address,
 		uint32_t pipe_id, uint32_t queue_id);
 static bool kgd_hqd_sdma_is_occupied(struct kgd_dev *kgd, void *mqd);
-static int kgd_hqd_destroy(struct kgd_dev *kgd,
-				enum kfd_preempt_type reset_type,
-				unsigned int timeout, uint32_t pipe_id,
+static int kgd_hqd_destroy(struct kgd_dev *kgd, uint32_t reset_type,
+				unsigned int utimeout, uint32_t pipe_id,
 				uint32_t queue_id);
 static int kgd_hqd_sdma_destroy(struct kgd_dev *kgd, void *mqd,
-				unsigned int timeout);
+				unsigned int utimeout);
 static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid);
 static int kgd_address_watch_disable(struct kgd_dev *kgd);
 static int kgd_address_watch_execute(struct kgd_dev *kgd,
@@ -121,60 +85,20 @@ static bool get_atc_vmid_pasid_mapping_valid(struct kgd_dev *kgd,
 static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 		uint8_t vmid);
 static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid);
-static void set_num_of_requests(struct kgd_dev *kgd,
-			uint8_t num_of_requests);
-static int alloc_memory_of_scratch(struct kgd_dev *kgd,
-				 uint64_t va, uint32_t vmid);
-static int write_config_static_mem(struct kgd_dev *kgd, bool swizzle_enable,
-		uint8_t element_size, uint8_t index_stride, uint8_t mtype);
-static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
-		uint32_t page_table_base);
-
-/* Because of REG_GET_FIELD() being used, we put this function in the
- * asic specific file.
- */
-static int amdgpu_amdkfd_get_tile_config(struct kgd_dev *kgd,
-		struct tile_config *config)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
-
-	config->gb_addr_config = adev->gfx.config.gb_addr_config;
-	config->num_banks = REG_GET_FIELD(adev->gfx.config.mc_arb_ramcfg,
-				MC_ARB_RAMCFG, NOOFBANK);
-	config->num_ranks = REG_GET_FIELD(adev->gfx.config.mc_arb_ramcfg,
-				MC_ARB_RAMCFG, NOOFRANKS);
-
-	config->tile_config_ptr = adev->gfx.config.tile_mode_array;
-	config->num_tile_configs =
-			ARRAY_SIZE(adev->gfx.config.tile_mode_array);
-	config->macro_tile_config_ptr =
-			adev->gfx.config.macrotile_mode_array;
-	config->num_macro_tile_configs =
-			ARRAY_SIZE(adev->gfx.config.macrotile_mode_array);
-
-	return 0;
-}
+static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type);
 
 static const struct kfd2kgd_calls kfd2kgd = {
 	.init_gtt_mem_allocation = alloc_gtt_mem,
 	.free_gtt_mem = free_gtt_mem,
-	.get_local_mem_info = get_local_mem_info,
+	.get_vmem_size = get_vmem_size,
 	.get_gpu_clock_counter = get_gpu_clock_counter,
 	.get_max_engine_clock_in_mhz = get_max_engine_clock_in_mhz,
-	.create_process_vm = amdgpu_amdkfd_gpuvm_create_process_vm,
-	.destroy_process_vm = amdgpu_amdkfd_gpuvm_destroy_process_vm,
-	.create_process_gpumem = create_process_gpumem,
-	.destroy_process_gpumem = destroy_process_gpumem,
-	.get_process_page_dir = amdgpu_amdkfd_gpuvm_get_process_page_dir,
-	.open_graphic_handle = open_graphic_handle,
 	.program_sh_mem_settings = kgd_program_sh_mem_settings,
 	.set_pasid_vmid_mapping = kgd_set_pasid_vmid_mapping,
 	.init_pipeline = kgd_init_pipeline,
 	.init_interrupts = kgd_init_interrupts,
 	.hqd_load = kgd_hqd_load,
 	.hqd_sdma_load = kgd_hqd_sdma_load,
-	.hqd_dump = kgd_hqd_dump,
-	.hqd_sdma_dump = kgd_hqd_sdma_dump,
 	.hqd_is_occupied = kgd_hqd_is_occupied,
 	.hqd_sdma_is_occupied = kgd_hqd_sdma_is_occupied,
 	.hqd_destroy = kgd_hqd_destroy,
@@ -188,52 +112,14 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.get_atc_vmid_pasid_mapping_valid =
 			get_atc_vmid_pasid_mapping_valid,
 	.write_vmid_invalidate_request = write_vmid_invalidate_request,
-	.alloc_memory_of_gpu = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu,
-	.free_memory_of_gpu = amdgpu_amdkfd_gpuvm_free_memory_of_gpu,
-	.map_memory_to_gpu = amdgpu_amdkfd_gpuvm_map_memory_to_gpu,
-	.unmap_memory_to_gpu = amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu,
-	.get_fw_version = get_fw_version,
-	.set_num_of_requests = set_num_of_requests,
-	.get_cu_info = get_cu_info,
-	.alloc_memory_of_scratch = alloc_memory_of_scratch,
-	.write_config_static_mem = write_config_static_mem,
-	.mmap_bo = amdgpu_amdkfd_gpuvm_mmap_bo,
-	.map_gtt_bo_to_kernel = amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel,
-	.set_vm_context_page_table_base = set_vm_context_page_table_base,
-	.pin_get_sg_table_bo = amdgpu_amdkfd_gpuvm_pin_get_sg_table,
-	.unpin_put_sg_table_bo = amdgpu_amdkfd_gpuvm_unpin_put_sg_table,
-	.get_dmabuf_info = amdgpu_amdkfd_get_dmabuf_info,
-	.import_dmabuf = amdgpu_amdkfd_gpuvm_import_dmabuf,
-	.export_dmabuf = amdgpu_amdkfd_gpuvm_export_dmabuf,
-	.get_vm_fault_info = amdgpu_amdkfd_gpuvm_get_vm_fault_info,
-	.submit_ib = amdgpu_amdkfd_submit_ib,
-	.get_tile_config = amdgpu_amdkfd_get_tile_config,
-	.restore_process_bos = amdgpu_amdkfd_gpuvm_restore_process_bos
+	.get_fw_version = get_fw_version
 };
 
-struct kfd2kgd_calls *amdgpu_amdkfd_gfx_8_0_get_functions()
+struct kfd2kgd_calls *amdgpu_amdkfd_gfx_8_0_get_functions(void)
 {
 	return (struct kfd2kgd_calls *)&kfd2kgd;
 }
 
-static int create_process_gpumem(struct kgd_dev *kgd, uint64_t va, size_t size,
-				void *vm, struct kgd_mem **mem)
-{
-	return 0;
-}
-
-/* Destroys the GPU allocation and frees the kgd_mem structure */
-static void destroy_process_gpumem(struct kgd_dev *kgd, struct kgd_mem *mem)
-{
-
-}
-
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm,
-				int fd, uint32_t handle, struct kgd_mem **mem)
-{
-	return 0;
-}
-
 static inline struct amdgpu_device *get_amdgpu_device(struct kgd_dev *kgd)
 {
 	return (struct amdgpu_device *)kgd;
@@ -340,15 +226,9 @@ static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id)
 	return 0;
 }
 
-static inline uint32_t get_sdma_base_addr(struct vi_sdma_mqd *m)
+static inline uint32_t get_sdma_base_addr(struct cik_sdma_rlc_registers *m)
 {
-	uint32_t retval;
-
-	retval = m->sdma_engine_id * SDMA1_REGISTER_OFFSET +
-		m->sdma_queue_id * KFD_VI_SDMA_QUEUE_OFFSET;
-	pr_debug("kfd: sdma base address: 0x%x\n", retval);
-
-	return retval;
+	return 0;
 }
 
 static inline struct vi_mqd *get_mqd(void *mqd)
@@ -356,183 +236,75 @@ static inline struct vi_mqd *get_mqd(void *mqd)
 	return (struct vi_mqd *)mqd;
 }
 
-static inline struct vi_sdma_mqd *get_sdma_mqd(void *mqd)
+static inline struct cik_sdma_rlc_registers *get_sdma_mqd(void *mqd)
 {
-	return (struct vi_sdma_mqd *)mqd;
+	return (struct cik_sdma_rlc_registers *)mqd;
 }
 
 static int kgd_hqd_load(struct kgd_dev *kgd, void *mqd, uint32_t pipe_id,
-			uint32_t queue_id, uint32_t __user *wptr,
-			uint32_t wptr_shift, uint32_t wptr_mask,
-			struct mm_struct *mm)
+			uint32_t queue_id, uint32_t __user *wptr)
 {
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 	struct vi_mqd *m;
-	uint32_t *mqd_hqd;
-	uint32_t reg, wptr_val;
+	uint32_t shadow_wptr, valid_wptr;
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 
 	m = get_mqd(mqd);
 
+	valid_wptr = copy_from_user(&shadow_wptr, wptr, sizeof(shadow_wptr));
 	acquire_queue(kgd, pipe_id, queue_id);
 
-	/* HQD registers extend from CP_MQD_BASE_ADDR to CP_HQD_EOP_WPTR_MEM. */
-	mqd_hqd = &m->cp_mqd_base_addr_lo;
-
-	for (reg = mmCP_HQD_VMID; reg <= mmCP_HQD_EOP_CONTROL; reg++)
-		WREG32(reg, mqd_hqd[reg - mmCP_MQD_BASE_ADDR]);
-
-	/* Tonga errata: EOP RPTR/WPTR should be left unmodified.
-	 * This is safe since EOP RPTR==WPTR for any inactive HQD
-	 * on ASICs that do not support context-save.
-	 * EOP writes/reads can start anywhere in the ring.
-	 */
-	if (get_amdgpu_device(kgd)->asic_type != CHIP_TONGA) {
-		WREG32(mmCP_HQD_EOP_RPTR, m->cp_hqd_eop_rptr);
-		WREG32(mmCP_HQD_EOP_WPTR, m->cp_hqd_eop_wptr);
-		WREG32(mmCP_HQD_EOP_WPTR_MEM, m->cp_hqd_eop_wptr_mem);
-	}
-
-	for (reg = mmCP_HQD_EOP_EVENTS; reg <= mmCP_HQD_ERROR; reg++)
-		WREG32(reg, mqd_hqd[reg - mmCP_MQD_BASE_ADDR]);
-
-	/* Copy userspace write pointer value to register.
-	 * Doorbell logic is active and will monitor subsequent changes.
-	 */
-	if (read_user_wptr(mm, wptr, &wptr_val))
-		WREG32(mmCP_HQD_PQ_WPTR, (wptr_val << wptr_shift) & wptr_mask);
-
-	/* Write CP_HQD_ACTIVE last. */
-	for (reg = mmCP_MQD_BASE_ADDR; reg <= mmCP_HQD_ACTIVE; reg++)
-		WREG32(reg, mqd_hqd[reg - mmCP_MQD_BASE_ADDR]);
+	WREG32(mmCP_MQD_CONTROL, m->cp_mqd_control);
+	WREG32(mmCP_MQD_BASE_ADDR, m->cp_mqd_base_addr_lo);
+	WREG32(mmCP_MQD_BASE_ADDR_HI, m->cp_mqd_base_addr_hi);
+
+	WREG32(mmCP_HQD_VMID, m->cp_hqd_vmid);
+	WREG32(mmCP_HQD_PERSISTENT_STATE, m->cp_hqd_persistent_state);
+	WREG32(mmCP_HQD_PIPE_PRIORITY, m->cp_hqd_pipe_priority);
+	WREG32(mmCP_HQD_QUEUE_PRIORITY, m->cp_hqd_queue_priority);
+	WREG32(mmCP_HQD_QUANTUM, m->cp_hqd_quantum);
+	WREG32(mmCP_HQD_PQ_BASE, m->cp_hqd_pq_base_lo);
+	WREG32(mmCP_HQD_PQ_BASE_HI, m->cp_hqd_pq_base_hi);
+	WREG32(mmCP_HQD_PQ_RPTR_REPORT_ADDR, m->cp_hqd_pq_rptr_report_addr_lo);
+	WREG32(mmCP_HQD_PQ_RPTR_REPORT_ADDR_HI,
+			m->cp_hqd_pq_rptr_report_addr_hi);
+
+	if (valid_wptr > 0)
+		WREG32(mmCP_HQD_PQ_WPTR, shadow_wptr);
+
+	WREG32(mmCP_HQD_PQ_CONTROL, m->cp_hqd_pq_control);
+	WREG32(mmCP_HQD_PQ_DOORBELL_CONTROL, m->cp_hqd_pq_doorbell_control);
+
+	WREG32(mmCP_HQD_EOP_BASE_ADDR, m->cp_hqd_eop_base_addr_lo);
+	WREG32(mmCP_HQD_EOP_BASE_ADDR_HI, m->cp_hqd_eop_base_addr_hi);
+	WREG32(mmCP_HQD_EOP_CONTROL, m->cp_hqd_eop_control);
+	WREG32(mmCP_HQD_EOP_RPTR, m->cp_hqd_eop_rptr);
+	WREG32(mmCP_HQD_EOP_WPTR, m->cp_hqd_eop_wptr);
+	WREG32(mmCP_HQD_EOP_EVENTS, m->cp_hqd_eop_done_events);
+
+	WREG32(mmCP_HQD_CTX_SAVE_BASE_ADDR_LO, m->cp_hqd_ctx_save_base_addr_lo);
+	WREG32(mmCP_HQD_CTX_SAVE_BASE_ADDR_HI, m->cp_hqd_ctx_save_base_addr_hi);
+	WREG32(mmCP_HQD_CTX_SAVE_CONTROL, m->cp_hqd_ctx_save_control);
+	WREG32(mmCP_HQD_CNTL_STACK_OFFSET, m->cp_hqd_cntl_stack_offset);
+	WREG32(mmCP_HQD_CNTL_STACK_SIZE, m->cp_hqd_cntl_stack_size);
+	WREG32(mmCP_HQD_WG_STATE_OFFSET, m->cp_hqd_wg_state_offset);
+	WREG32(mmCP_HQD_CTX_SAVE_SIZE, m->cp_hqd_ctx_save_size);
+
+	WREG32(mmCP_HQD_IB_CONTROL, m->cp_hqd_ib_control);
+
+	WREG32(mmCP_HQD_DEQUEUE_REQUEST, m->cp_hqd_dequeue_request);
+	WREG32(mmCP_HQD_ERROR, m->cp_hqd_error);
+	WREG32(mmCP_HQD_EOP_WPTR_MEM, m->cp_hqd_eop_wptr_mem);
+	WREG32(mmCP_HQD_EOP_DONES, m->cp_hqd_eop_dones);
+
+	WREG32(mmCP_HQD_ACTIVE, m->cp_hqd_active);
 
 	release_queue(kgd);
 
 	return 0;
 }
 
-static int kgd_hqd_dump(struct kgd_dev *kgd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs)
-{
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	uint32_t i = 0, reg;
-#define HQD_N_REGS (54+4)
-#define DUMP_REG(addr) do {				\
-		if (WARN_ON_ONCE(i >= HQD_N_REGS))	\
-			break;				\
-		(*dump)[i][0] = (addr) << 2;		\
-		(*dump)[i++][1] = RREG32(addr);		\
-	} while (0)
-
-	*dump = kmalloc(HQD_N_REGS*2*sizeof(uint32_t), GFP_KERNEL);
-	if (*dump == NULL)
-		return -ENOMEM;
-
-	acquire_queue(kgd, pipe_id, queue_id);
-
-	DUMP_REG(mmCOMPUTE_STATIC_THREAD_MGMT_SE0);
-	DUMP_REG(mmCOMPUTE_STATIC_THREAD_MGMT_SE1);
-	DUMP_REG(mmCOMPUTE_STATIC_THREAD_MGMT_SE2);
-	DUMP_REG(mmCOMPUTE_STATIC_THREAD_MGMT_SE3);
-
-	for (reg = mmCP_MQD_BASE_ADDR; reg <= mmCP_HQD_EOP_DONES; reg++)
-		DUMP_REG(reg);
-
-	release_queue(kgd);
-
-	WARN_ON_ONCE(i != HQD_N_REGS);
-	*n_regs = i;
-
-	return 0;
-}
-
-static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm)
-{
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	struct vi_sdma_mqd *m;
-	uint32_t sdma_base_addr;
-	uint32_t temp, timeout = 2000;
-	uint32_t data;
-
-	m = get_sdma_mqd(mqd);
-	sdma_base_addr = get_sdma_base_addr(m);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL,
-		m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
-
-	while (true) {
-		temp = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
-		if (temp & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
-			break;
-		if (timeout == 0)
-			return -ETIME;
-		msleep(10);
-		timeout -= 10;
-	}
-	if (m->sdma_engine_id) {
-		data = RREG32(mmSDMA1_GFX_CONTEXT_CNTL);
-		data = REG_SET_FIELD(data, SDMA1_GFX_CONTEXT_CNTL,
-				RESUME_CTX, 0);
-		WREG32(mmSDMA1_GFX_CONTEXT_CNTL, data);
-	} else {
-		data = RREG32(mmSDMA0_GFX_CONTEXT_CNTL);
-		data = REG_SET_FIELD(data, SDMA0_GFX_CONTEXT_CNTL,
-				RESUME_CTX, 0);
-		WREG32(mmSDMA0_GFX_CONTEXT_CNTL, data);
-	}
-
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_DOORBELL, m->sdmax_rlcx_doorbell);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR, m->sdmax_rlcx_rb_rptr);
-
-	if (read_user_wptr(mm, wptr, &data))
-		WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_WPTR, data);
-	else
-		WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_WPTR,
-		       m->sdmax_rlcx_rb_rptr);
-
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_VIRTUAL_ADDR, m->sdmax_rlcx_virtual_addr);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_BASE, m->sdmax_rlcx_rb_base);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_BASE_HI, m->sdmax_rlcx_rb_base_hi);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR_ADDR_LO, m->sdmax_rlcx_rb_rptr_addr_lo);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR_ADDR_HI, m->sdmax_rlcx_rb_rptr_addr_hi);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL, m->sdmax_rlcx_rb_cntl);
-
-	return 0;
-}
-
-static int kgd_hqd_sdma_dump(struct kgd_dev *kgd,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs)
+static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd)
 {
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	uint32_t sdma_offset = engine_id * SDMA1_REGISTER_OFFSET +
-		queue_id * KFD_VI_SDMA_QUEUE_OFFSET;
-	uint32_t i = 0, reg;
-#undef HQD_N_REGS
-#define HQD_N_REGS (19+4+2+3+7)
-
-	*dump = kmalloc(HQD_N_REGS*2*sizeof(uint32_t), GFP_KERNEL);
-	if (*dump == NULL)
-		return -ENOMEM;
-
-	for (reg = mmSDMA0_RLC0_RB_CNTL; reg <= mmSDMA0_RLC0_DOORBELL; reg++)
-		DUMP_REG(sdma_offset + reg);
-	for (reg = mmSDMA0_RLC0_VIRTUAL_ADDR; reg <= mmSDMA0_RLC0_WATERMARK;
-	     reg++)
-		DUMP_REG(sdma_offset + reg);
-	for (reg = mmSDMA0_RLC0_CSA_ADDR_LO; reg <= mmSDMA0_RLC0_CSA_ADDR_HI;
-	     reg++)
-		DUMP_REG(sdma_offset + reg);
-	for (reg = mmSDMA0_RLC0_IB_SUB_REMAIN; reg <= mmSDMA0_RLC0_DUMMY_REG;
-	     reg++)
-		DUMP_REG(sdma_offset + reg);
-	for (reg = mmSDMA0_RLC0_MIDCMD_DATA0; reg <= mmSDMA0_RLC0_MIDCMD_CNTL;
-	     reg++)
-		DUMP_REG(sdma_offset + reg);
-
-	WARN_ON_ONCE(i != HQD_N_REGS);
-	*n_regs = i;
-
 	return 0;
 }
 
@@ -561,7 +333,7 @@ static bool kgd_hqd_is_occupied(struct kgd_dev *kgd, uint64_t queue_address,
 static bool kgd_hqd_sdma_is_occupied(struct kgd_dev *kgd, void *mqd)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	struct vi_sdma_mqd *m;
+	struct cik_sdma_rlc_registers *m;
 	uint32_t sdma_base_addr;
 	uint32_t sdma_rlc_rb_cntl;
 
@@ -576,98 +348,29 @@ static bool kgd_hqd_sdma_is_occupied(struct kgd_dev *kgd, void *mqd)
 	return false;
 }
 
-static int kgd_hqd_destroy(struct kgd_dev *kgd,
-				enum kfd_preempt_type reset_type,
-				unsigned int timeout, uint32_t pipe_id,
+static int kgd_hqd_destroy(struct kgd_dev *kgd, uint32_t reset_type,
+				unsigned int utimeout, uint32_t pipe_id,
 				uint32_t queue_id)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 	uint32_t temp;
-	enum hqd_dequeue_request_type type;
-	unsigned long flags, end_jiffies;
-	int retry;
+	int timeout = utimeout;
 
 	acquire_queue(kgd, pipe_id, queue_id);
 
-	switch (reset_type) {
-	case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
-		type = DRAIN_PIPE;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
-		type = RESET_WAVES;
-		break;
-	default:
-		type = DRAIN_PIPE;
-		break;
-	}
-
-	/* Workaround: If IQ timer is active and the wait time is close to or
-	 * equal to 0, dequeueing is not safe. Wait until either the wait time
-	 * is larger or timer is cleared. Also, ensure that IQ_REQ_PEND is
-	 * cleared before continuing. Also, ensure wait times are set to at
-	 * least 0x3.
-	 */
-	local_irq_save(flags);
-	preempt_disable();
-	retry = 5000; /* wait for 500 usecs at maximum */
-	while (true) {
-		temp = RREG32(mmCP_HQD_IQ_TIMER);
-		if (REG_GET_FIELD(temp, CP_HQD_IQ_TIMER, PROCESSING_IQ)) {
-			pr_debug("HW is processing IQ\n");
-			goto loop;
-		}
-		if (REG_GET_FIELD(temp, CP_HQD_IQ_TIMER, ACTIVE)) {
-			if (REG_GET_FIELD(temp, CP_HQD_IQ_TIMER, RETRY_TYPE)
-					== 3) /* SEM-rearm is safe */
-				break;
-			/* Wait time 3 is safe for CP, but our MMIO read/write
-			 * time is close to 1 microsecond, so check for 10 to
-			 * leave more buffer room
-			 */
-			if (REG_GET_FIELD(temp, CP_HQD_IQ_TIMER, WAIT_TIME)
-					>= 10)
-				break;
-			pr_debug("IQ timer is active\n");
-		} else
-			break;
-	loop:
-		if (!retry) {
-			pr_err("kfd: CP HQD IQ timer status time out\n");
-			break;
-		}
-		ndelay(100);
-		--retry;
-	}
-	retry = 1000;
-	while (true) {
-		temp = RREG32(mmCP_HQD_DEQUEUE_REQUEST);
-		if (!(temp & CP_HQD_DEQUEUE_REQUEST__IQ_REQ_PEND_MASK))
-			break;
-		pr_debug("Dequeue request is pending\n");
-
-		if (!retry) {
-			pr_err("kfd: CP HQD dequeue request time out\n");
-			break;
-		}
-		ndelay(100);
-		--retry;
-	}
-	local_irq_restore(flags);
-	preempt_enable();
+	WREG32(mmCP_HQD_DEQUEUE_REQUEST, reset_type);
 
-	WREG32(mmCP_HQD_DEQUEUE_REQUEST, type);
-
-	end_jiffies = (timeout * HZ / 1000) + jiffies;
 	while (true) {
 		temp = RREG32(mmCP_HQD_ACTIVE);
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
+		if (temp & CP_HQD_ACTIVE__ACTIVE_MASK)
 			break;
-		if (time_after(jiffies, end_jiffies)) {
+		if (timeout <= 0) {
 			pr_err("kfd: cp queue preemption time out.\n");
 			release_queue(kgd);
 			return -ETIME;
 		}
-		usleep_range(500, 1000);
+		msleep(20);
+		timeout -= 20;
 	}
 
 	release_queue(kgd);
@@ -675,13 +378,13 @@ static int kgd_hqd_destroy(struct kgd_dev *kgd,
 }
 
 static int kgd_hqd_sdma_destroy(struct kgd_dev *kgd, void *mqd,
-				unsigned int timeout)
+				unsigned int utimeout)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	struct vi_sdma_mqd *m;
+	struct cik_sdma_rlc_registers *m;
 	uint32_t sdma_base_addr;
 	uint32_t temp;
-	unsigned long end_jiffies = (timeout * HZ / 1000) + jiffies;
+	int timeout = utimeout;
 
 	m = get_sdma_mqd(mqd);
 	sdma_base_addr = get_sdma_base_addr(m);
@@ -692,19 +395,18 @@ static int kgd_hqd_sdma_destroy(struct kgd_dev *kgd, void *mqd,
 
 	while (true) {
 		temp = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
-		if (temp & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
+		if (temp & SDMA0_STATUS_REG__RB_CMD_IDLE__SHIFT)
 			break;
-		if (time_after(jiffies, end_jiffies))
+		if (timeout <= 0)
 			return -ETIME;
-		usleep_range(500, 1000);
+		msleep(20);
+		timeout -= 20;
 	}
 
 	WREG32(sdma_base_addr + mmSDMA0_RLC0_DOORBELL, 0);
-	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL,
-		RREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL) |
-		SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
-
-	m->sdmax_rlcx_rb_rptr = RREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_RPTR, 0);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_WPTR, 0);
+	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_BASE, 0);
 
 	return 0;
 }
@@ -726,7 +428,7 @@ static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
 
 	reg = RREG32(mmATC_VMID0_PASID_MAPPING + vmid);
-	return reg & ATC_VMID0_PASID_MAPPING__PASID_MASK;
+	return reg & ATC_VMID0_PASID_MAPPING__VALID_MASK;
 }
 
 static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
@@ -738,21 +440,6 @@ static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
 
 static int kgd_address_watch_disable(struct kgd_dev *kgd)
 {
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	union TCP_WATCH_CNTL_BITS cntl;
-	unsigned int i;
-
-	cntl.u32All = 0;
-
-	cntl.bitfields.valid = 0;
-	cntl.bitfields.mask = ADDRESS_WATCH_REG_CNTL_DEFAULT_MASK;
-	cntl.bitfields.atc = 1;
-
-	/* Turning off this address until we set all the registers */
-	for (i = 0; i < MAX_WATCH_ADDRESSES; i++)
-		WREG32(watchRegs[i * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_CNTL],
-			cntl.u32All);
-
 	return 0;
 }
 
@@ -762,28 +449,6 @@ static int kgd_address_watch_execute(struct kgd_dev *kgd,
 					uint32_t addr_hi,
 					uint32_t addr_lo)
 {
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	union TCP_WATCH_CNTL_BITS cntl;
-
-	cntl.u32All = cntl_val;
-
-	/* Turning off this watch point until we set all the registers */
-	cntl.bitfields.valid = 0;
-	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_CNTL],
-			cntl.u32All);
-
-	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_ADDR_HI],
-			addr_hi);
-
-	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_ADDR_LO],
-			addr_lo);
-
-	/* Enable the watch point */
-	cntl.bitfields.valid = 1;
-
-	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_CNTL],
-			cntl.u32All);
-
 	return 0;
 }
 
@@ -816,32 +481,6 @@ static uint32_t kgd_address_watch_get_offset(struct kgd_dev *kgd,
 					unsigned int watch_point_id,
 					unsigned int reg_offset)
 {
-	return watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + reg_offset];
-}
-
-static int write_config_static_mem(struct kgd_dev *kgd, bool swizzle_enable,
-		uint8_t element_size, uint8_t index_stride, uint8_t mtype)
-{
-	uint32_t reg;
-	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
-
-	reg = swizzle_enable << SH_STATIC_MEM_CONFIG__SWIZZLE_ENABLE__SHIFT |
-		element_size << SH_STATIC_MEM_CONFIG__ELEMENT_SIZE__SHIFT |
-		index_stride << SH_STATIC_MEM_CONFIG__INDEX_STRIDE__SHIFT |
-		mtype << SH_STATIC_MEM_CONFIG__PRIVATE_MTYPE__SHIFT;
-
-	WREG32(mmSH_STATIC_MEM_CONFIG, reg);
-	return 0;
-}
-static int alloc_memory_of_scratch(struct kgd_dev *kgd,
-				 uint64_t va, uint32_t vmid)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
-
-	lock_srbm(kgd, 0, 0, 0, vmid);
-	WREG32(mmSH_HIDDEN_PRIVATE_BASE_VMID, va);
-	unlock_srbm(kgd);
-
 	return 0;
 }
 
@@ -903,21 +542,3 @@ static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type)
 	/* Only 12 bit in use*/
 	return hdr->common.ucode_version;
 }
-
-static void set_num_of_requests(struct kgd_dev *kgd,
-			uint8_t num_of_requests)
-{
-	pr_debug("in %s this is a stub\n", __func__);
-}
-
-static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
-		uint32_t page_table_base)
-{
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	/* TODO: Don't use hardcoded VMIDs */
-	if (vmid < 8 || vmid > 15) {
-		pr_err("amdkfd: trying to set page table base for wrong VMID\n");
-		return;
-	}
-	WREG32(mmVM_CONTEXT8_PAGE_TABLE_BASE_ADDR + vmid - 8, page_table_base);
-}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h
deleted file mode 100644
index e75b712..0000000
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- */
-
-#ifndef AMDGPU_AMDKFD_GFX_V8_H_INCLUDED
-#define AMDGPU_AMDKFD_GFX_V8_H_INCLUDED
-
-#include <linux/types.h>
-
-enum {
-	MAX_TRAPID = 8,		/* 3 bits in the bitfield. */
-	MAX_WATCH_ADDRESSES = 4
-};
-
-enum {
-	ADDRESS_WATCH_REG_ADDR_HI = 0,
-	ADDRESS_WATCH_REG_ADDR_LO,
-	ADDRESS_WATCH_REG_CNTL,
-	ADDRESS_WATCH_REG_MAX
-};
-
-/*  not defined in the VI reg file  */
-enum {
-	ADDRESS_WATCH_REG_CNTL_ATC_BIT = 0x10000000UL,
-	ADDRESS_WATCH_REG_CNTL_DEFAULT_MASK = 0x00FFFFFF,
-	ADDRESS_WATCH_REG_ADDLOW_MASK_EXTENSION = 0x03000000,
-	/* extend the mask to 26 bits in order to match the low address field. */
-	ADDRESS_WATCH_REG_ADDLOW_SHIFT = 6,
-	ADDRESS_WATCH_REG_ADDHIGH_MASK = 0xFFFF
-};
-
-union TCP_WATCH_CNTL_BITS {
-	struct {
-		uint32_t mask:24;
-		uint32_t vmid:4;
-		uint32_t atc:1;
-		uint32_t mode:2;
-		uint32_t valid:1;
-	} bitfields, bits;
-	uint32_t u32All;
-	signed int i32All;
-	float f32All;
-};
-#endif
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
deleted file mode 100644
index e877221..0000000
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
+++ /dev/null
@@ -1,2251 +0,0 @@
-/*
- * Copyright 2014 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- */
-
-#include <linux/module.h>
-#include <linux/fdtable.h>
-#include <linux/uaccess.h>
-#include <linux/firmware.h>
-#include <linux/list.h>
-#include <drm/drmP.h>
-#include <linux/dma-buf.h>
-#include "amdgpu_amdkfd.h"
-#include "amdgpu_ucode.h"
-#include "gca/gfx_8_0_sh_mask.h"
-#include "gca/gfx_8_0_d.h"
-#include "gca/gfx_8_0_enum.h"
-#include "oss/oss_3_0_sh_mask.h"
-#include "oss/oss_3_0_d.h"
-#include "gmc/gmc_8_1_sh_mask.h"
-#include "gmc/gmc_8_1_d.h"
-#include "vi_structs.h"
-#include "vid.h"
-
-/* Special VM and GART address alignment needed for VI pre-Fiji due to
- * a HW bug. */
-#define VI_BO_SIZE_ALIGN (0x8000)
-
-/* Impose limit on how much memory KFD can use */
-struct kfd_mem_usage_limit {
-	uint64_t max_system_mem_limit;
-	uint64_t max_userptr_mem_limit;
-	int64_t system_mem_used;
-	int64_t userptr_mem_used;
-	spinlock_t mem_limit_lock;
-};
-
-static struct kfd_mem_usage_limit kfd_mem_limit;
-
-/* Struct used for amdgpu_amdkfd_bo_validate */
-struct amdgpu_vm_parser {
-	uint32_t        domain;
-	bool            wait;
-};
-
-
-static inline struct amdgpu_device *get_amdgpu_device(struct kgd_dev *kgd)
-{
-	return (struct amdgpu_device *)kgd;
-}
-
-static bool check_if_add_bo_to_vm(struct amdgpu_vm *avm,
-		struct kgd_mem *mem)
-{
-	struct kfd_bo_va_list *entry;
-
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list)
-		if (entry->bo_va->vm == avm)
-			return false;
-
-	return true;
-}
-
-bool check_if_large_bar(struct kgd_dev *kgd)
-{
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-
-	if (adev->mc.visible_vram_size > 0 &&
-		adev->mc.real_vram_size == adev->mc.visible_vram_size)
-		return true;
-
-	return false;
-}
-
-/* Set memory usage limits. Current, limits are
- *  System (kernel) memory - 3/8th System RAM
- *  Userptr memory - 3/4th System RAM
- */
-void amdgpu_amdkfd_gpuvm_init_mem_limits(void)
-{
-	struct sysinfo si;
-	uint64_t mem;
-
-	si_meminfo(&si);
-	mem = si.totalram - si.totalhigh;
-	mem *= si.mem_unit;
-
-	spin_lock_init(&kfd_mem_limit.mem_limit_lock);
-	kfd_mem_limit.max_system_mem_limit = (mem >> 1) - (mem >> 3);
-	kfd_mem_limit.max_userptr_mem_limit = mem - (mem >> 2);
-	pr_debug("Kernel memory limit %lluM, userptr limit %lluM\n",
-		(kfd_mem_limit.max_system_mem_limit >> 20),
-		(kfd_mem_limit.max_userptr_mem_limit >> 20));
-}
-
-static int amdgpu_amdkfd_reserve_system_mem_limit(struct amdgpu_device *adev,
-					      uint64_t size, u32 domain)
-{
-	size_t acc_size;
-	int ret = 0;
-
-	acc_size = ttm_bo_dma_acc_size(&adev->mman.bdev, size,
-				       sizeof(struct amdgpu_bo));
-
-	spin_lock(&kfd_mem_limit.mem_limit_lock);
-	if (domain == AMDGPU_GEM_DOMAIN_GTT) {
-		if (kfd_mem_limit.system_mem_used + (acc_size + size) >
-			kfd_mem_limit.max_system_mem_limit) {
-			ret = -ENOMEM;
-			goto err_no_mem;
-		}
-		kfd_mem_limit.system_mem_used += (acc_size + size);
-
-	} else if (domain == AMDGPU_GEM_DOMAIN_CPU) {
-		if ((kfd_mem_limit.system_mem_used + acc_size >
-			kfd_mem_limit.max_system_mem_limit) &&
-			(kfd_mem_limit.userptr_mem_used + (size + acc_size) >
-			kfd_mem_limit.max_userptr_mem_limit)) {
-			ret = -ENOMEM;
-			goto err_no_mem;
-		}
-		kfd_mem_limit.system_mem_used += acc_size;
-		kfd_mem_limit.userptr_mem_used += size;
-	}
-err_no_mem:
-	spin_unlock(&kfd_mem_limit.mem_limit_lock);
-	return ret;
-}
-
-void amdgpu_amdkfd_unreserve_system_memory_limit(struct amdgpu_bo *bo)
-{
-	spin_lock(&kfd_mem_limit.mem_limit_lock);
-
-	if (bo->prefered_domains == AMDGPU_GEM_DOMAIN_GTT)
-		kfd_mem_limit.system_mem_used -=
-			(bo->tbo.acc_size + amdgpu_bo_size(bo));
-	else if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {
-		kfd_mem_limit.system_mem_used -= bo->tbo.acc_size;
-		kfd_mem_limit.userptr_mem_used -= amdgpu_bo_size(bo);
-	}
-	if (kfd_mem_limit.system_mem_used < 0) {
-		pr_warn("kfd system memory size ref. error\n");
-		kfd_mem_limit.system_mem_used = 0;
-	}
-	if (kfd_mem_limit.userptr_mem_used < 0) {
-		pr_warn("kfd userptr memory size ref. error\n");
-		kfd_mem_limit.userptr_mem_used = 0;
-	}
-
-	spin_unlock(&kfd_mem_limit.mem_limit_lock);
-}
-
-
-/* amdgpu_amdkfd_remove_eviction_fence - Removes eviction fence(s) from BO's
- *  reservation object.
- *
- * @bo: [IN] Remove eviction fence(s) from this BO
- * @ef: [IN] If ef is specified, then this eviction fence is removed if it
- *  is present in the shared list.
- * @ef_list: [OUT] Returns list of eviction fences. These fences are removed
- *  from BO's reservation object shared list.
- * @ef_count: [OUT] Number of fences in ef_list.
- *
- * NOTE: If called with ef_list, then amdgpu_amdkfd_add_eviction_fence must be
- *  called to restore the eviction fences and to avoid memory leak. This is
- *  useful for shared BOs.
- * NOTE: Must be called with BO reserved i.e. bo->tbo.resv->lock held.
- */
-static int amdgpu_amdkfd_remove_eviction_fence(struct amdgpu_bo *bo,
-					struct amdgpu_amdkfd_fence *ef,
-					struct amdgpu_amdkfd_fence ***ef_list,
-					unsigned int *ef_count)
-{
-	struct reservation_object_list *fobj;
-	struct reservation_object *resv;
-	unsigned int i = 0, j = 0, k = 0, shared_count;
-	unsigned int count = 0;
-	struct amdgpu_amdkfd_fence **fence_list;
-
-	if (!ef && !ef_list)
-		return -EINVAL;
-
-	if (ef_list) {
-		*ef_list = NULL;
-		*ef_count = 0;
-	}
-
-	resv = bo->tbo.resv;
-	fobj = reservation_object_get_list(resv);
-
-	if (!fobj)
-		return 0;
-
-	preempt_disable();
-	write_seqcount_begin(&resv->seq);
-
-	/* Go through all the shared fences in the resevation object. If
-	 * ef is specified and it exists in the list, remove it and reduce the
-	 * count. If ef is not specified, then get the count of eviction fences
-	 * present.
-	 */
-	shared_count = fobj->shared_count;
-	for (i = 0; i < shared_count; ++i) {
-		struct fence *f;
-
-		f = rcu_dereference_protected(fobj->shared[i],
-					      reservation_object_held(resv));
-
-		if (ef) {
-			if (f->context == ef->base.context) {
-				fence_put(f);
-				fobj->shared_count--;
-			} else
-				RCU_INIT_POINTER(fobj->shared[j++], f);
-
-		} else if (to_amdgpu_amdkfd_fence(f))
-			count++;
-	}
-	write_seqcount_end(&resv->seq);
-	preempt_enable();
-
-	if (ef || !count)
-		return 0;
-
-	/* Alloc memory for count number of eviction fence pointers. Fill the
-	 * ef_list array and ef_count
-	 */
-
-	fence_list = kcalloc(count, sizeof(struct amdgpu_amdkfd_fence *),
-			     GFP_KERNEL);
-	if (!fence_list)
-		return -ENOMEM;
-
-	preempt_disable();
-	write_seqcount_begin(&resv->seq);
-
-	j = 0;
-	for (i = 0; i < shared_count; ++i) {
-		struct fence *f;
-		struct amdgpu_amdkfd_fence *efence;
-
-		f = rcu_dereference_protected(fobj->shared[i],
-			reservation_object_held(resv));
-
-		efence = to_amdgpu_amdkfd_fence(f);
-		if (efence) {
-			fence_list[k++] = efence;
-			fobj->shared_count--;
-		} else
-			RCU_INIT_POINTER(fobj->shared[j++], f);
-	}
-
-	write_seqcount_end(&resv->seq);
-	preempt_enable();
-
-	*ef_list = fence_list;
-	*ef_count = k;
-
-	return 0;
-}
-
-/* amdgpu_amdkfd_add_eviction_fence - Adds eviction fence(s) back into BO's
- *  reservation object.
- *
- * @bo: [IN] Add eviction fences to this BO
- * @ef_list: [IN] List of eviction fences to be added
- * @ef_count: [IN] Number of fences in ef_list.
- *
- * NOTE: Must call amdgpu_amdkfd_remove_eviction_fence before calling this
- *  function.
- */
-static void amdgpu_amdkfd_add_eviction_fence(struct amdgpu_bo *bo,
-				struct amdgpu_amdkfd_fence **ef_list,
-				unsigned int ef_count)
-{
-	int i;
-
-	if (!ef_list || !ef_count)
-		return;
-
-	for (i = 0; i < ef_count; i++) {
-		amdgpu_bo_fence(bo, &ef_list[i]->base, true);
-		/* Readding the fence takes an additional reference. Drop that
-		 * reference.
-		 */
-		fence_put(&ef_list[i]->base);
-	}
-
-	kfree(ef_list);
-}
-
-static int add_bo_to_vm(struct amdgpu_device *adev, struct kgd_mem *mem,
-		struct amdgpu_vm *avm, bool is_aql,
-		struct kfd_bo_va_list **p_bo_va_entry)
-{
-	int ret;
-	struct kfd_bo_va_list *bo_va_entry;
-	struct amdgpu_bo *bo = mem->bo;
-	uint64_t va = mem->va;
-	struct list_head *list_bo_va = &mem->bo_va_list;
-
-	if (is_aql)
-		va += bo->tbo.mem.size;
-
-	bo_va_entry = kzalloc(sizeof(*bo_va_entry), GFP_KERNEL);
-	if (!bo_va_entry)
-		return -ENOMEM;
-
-	BUG_ON(va == 0);
-
-	pr_debug("amdkfd: adding bo_va to bo %p and va 0x%llx id 0x%x\n",
-			bo, va, adev->dev->id);
-
-	/* Add BO to VM internal data structures*/
-	bo_va_entry->bo_va = amdgpu_vm_bo_add(adev, avm, bo);
-	if (bo_va_entry->bo_va == NULL) {
-		ret = -EINVAL;
-		pr_err("amdkfd: Failed to add BO object to VM. ret == %d\n",
-				ret);
-		goto err_vmadd;
-	}
-
-	bo_va_entry->va = va;
-	bo_va_entry->kgd_dev = (void *)adev;
-	list_add(&bo_va_entry->bo_list, list_bo_va);
-
-	if (p_bo_va_entry)
-		*p_bo_va_entry = bo_va_entry;
-
-	return 0;
-
-err_vmadd:
-	kfree(bo_va_entry);
-	return ret;
-}
-
-static void remove_bo_from_vm(struct amdgpu_device *adev,
-		struct kfd_bo_va_list *entry)
-{
-	amdgpu_vm_bo_rmv(adev, entry->bo_va);
-	list_del(&entry->bo_list);
-	kfree(entry);
-}
-
-static int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,
-				     bool wait)
-{
-	int ret = 0;
-
-	if (!amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {
-		amdgpu_ttm_placement_from_domain(bo, domain);
-
-		ret = ttm_bo_validate(&bo->tbo, &bo->placement,
-				      false, false);
-		if (ret)
-			goto validate_fail;
-		if (wait) {
-			struct amdgpu_amdkfd_fence **ef_list;
-			unsigned int ef_count;
-
-			ret = amdgpu_amdkfd_remove_eviction_fence(bo, NULL,
-								  &ef_list,
-								  &ef_count);
-			if (ret)
-				goto validate_fail;
-
-			ttm_bo_wait(&bo->tbo, false, false, false);
-			amdgpu_amdkfd_add_eviction_fence(bo, ef_list,
-							 ef_count);
-		}
-	} else {
-		/* Userptrs are not pinned. Therefore we can use the
-		 * bo->pin_count for our version of pinning without conflict.
-		 */
-		if (bo->pin_count == 0) {
-			amdgpu_ttm_placement_from_domain(bo, domain);
-			ret = ttm_bo_validate(&bo->tbo, &bo->placement,
-					      true, false);
-			if (ret)
-				goto validate_fail;
-			if (wait)
-				ttm_bo_wait(&bo->tbo, false, false, false);
-		}
-		bo->pin_count++;
-	}
-
-validate_fail:
-	return ret;
-}
-
-static int amdgpu_amdkfd_validate(void *param, struct amdgpu_bo *bo)
-{
-	struct amdgpu_vm_parser *p = param;
-
-	return amdgpu_amdkfd_bo_validate(bo, bo->prefered_domains, p->wait);
-}
-
-static int amdgpu_amdkfd_bo_invalidate(struct amdgpu_bo *bo)
-{
-	int ret = 0;
-
-	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) &&
-		(--bo->pin_count == 0)) {
-		amdgpu_ttm_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);
-		ret = ttm_bo_validate(&bo->tbo, &bo->placement, true, false);
-		if (ret != 0)
-			pr_err("amdgpu: failed to invalidate userptr BO\n");
-	}
-	return ret;
-}
-
-static int validate_pt_pd_bos(struct amdgpu_vm *vm)
-{
-	int i, ret = 0;
-	struct amdgpu_bo *bo, *pd = vm->page_directory;
-	struct amdkfd_vm *kvm = container_of(vm, struct amdkfd_vm, base);
-
-	/* Remove eviction fence so that validate can wait on move fences */
-	amdgpu_amdkfd_remove_eviction_fence(pd, kvm->eviction_fence,
-					    NULL, NULL);
-
-	/* PTs share same reservation object as PD. So only fence PD */
-	for (i = 0; i <= vm->max_pde_used; ++i) {
-		bo = vm->page_tables[i].bo;
-
-		if (!bo)
-			continue;
-
-		ret = amdgpu_amdkfd_bo_validate(bo, AMDGPU_GEM_DOMAIN_VRAM,
-						true);
-		if (ret != 0) {
-			pr_err("amdgpu: failed to validate PTE %d\n", i);
-			break;
-		}
-	}
-
-	ret = amdgpu_amdkfd_bo_validate(pd, AMDGPU_GEM_DOMAIN_VRAM,
-					true);
-	if (ret != 0) {
-		pr_err("amdgpu: failed to validate PD\n");
-		return ret;
-	}
-
-	/* Add the eviction fence back */
-	amdgpu_bo_fence(pd, &kvm->master->eviction_fence->base, true);
-
-	return ret;
-}
-
-/**
- * amdgpu_vm_clear_bo - initially clear the VRAM pages
- *
- * @adev: amdgpu_device pointer
- * @bo: bo to clear
- * @vm: requested vm
- * need to reserve bo first before calling it.
- */
-static int amdgpu_amdkfd_gpuvm_clear_bo(struct amdgpu_device *adev,
-			      struct amdgpu_vm *vm,
-			      struct amdgpu_bo *bo)
-{
-	struct amdgpu_ring *ring;
-	struct fence *fence = NULL;
-	struct amdgpu_job *job;
-	unsigned entries;
-	uint64_t addr;
-	int r;
-
-	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
-
-	r = reservation_object_reserve_shared(bo->tbo.resv);
-	if (r)
-		return r;
-
-	r = ttm_bo_validate(&bo->tbo, &bo->placement, true, false);
-	if (r)
-		goto error;
-
-	addr = amdgpu_bo_gpu_offset(bo);
-	entries = amdgpu_bo_size(bo);
-
-	r = amdgpu_job_alloc_with_ib(adev, 64, &job);
-	if (r)
-		goto error;
-
-	amdgpu_emit_fill_buffer(adev, &job->ibs[0], 0, addr, entries);
-	amdgpu_ring_pad_ib(ring, &job->ibs[0]);
-
-	WARN_ON(job->ibs[0].length_dw > 64);
-	r = amdgpu_job_submit(job, ring, &vm->entity,
-			      AMDGPU_FENCE_OWNER_VM, &fence);
-	if (r)
-		goto error_free;
-
-	amdgpu_bo_fence(bo, fence, true);
-	fence_put(fence);
-	return 0;
-
-error_free:
-	amdgpu_job_free(job);
-
-error:
-	return r;
-}
-
-static void add_kgd_mem_to_kfd_bo_list(struct kgd_mem *mem,
-				       struct amdkfd_vm *kfd_vm)
-{
-	struct amdgpu_bo_list_entry *entry = &mem->bo_list_entry;
-	struct amdgpu_bo *bo = mem->bo;
-
-	entry->robj = bo;
-	INIT_LIST_HEAD(&entry->tv.head);
-	entry->tv.shared = true;
-	entry->tv.bo = &bo->tbo;
-	mutex_lock(&kfd_vm->master->lock);
-	list_add_tail(&entry->tv.head, &kfd_vm->master->kfd_bo_list);
-	mutex_unlock(&kfd_vm->master->lock);
-}
-
-static int __alloc_memory_of_gpu(struct kgd_dev *kgd, uint64_t va,
-		uint64_t size, void *vm, struct kgd_mem **mem,
-		uint64_t *offset, void **kptr,
-		u32 domain, u64 flags, struct sg_table *sg, bool aql_queue,
-		bool readonly, bool execute, bool no_sub, bool userptr)
-{
-	struct amdgpu_device *adev;
-	int ret;
-	struct amdgpu_bo *bo;
-	uint64_t user_addr = 0;
-	int byte_align;
-	u32 alloc_domain;
-	uint32_t pte_flags;
-	struct amdkfd_vm *kfd_vm = (struct amdkfd_vm *)vm;
-
-	BUG_ON(kgd == NULL);
-	BUG_ON(size == 0);
-	BUG_ON(mem == NULL);
-	BUG_ON(kfd_vm == NULL);
-
-	if (aql_queue)
-		size = size >> 1;
-	if (userptr) {
-		if (!offset || !*offset)
-			return -EINVAL;
-		user_addr = *offset;
-	}
-
-	adev = get_amdgpu_device(kgd);
-	byte_align = (adev->family == AMDGPU_FAMILY_VI &&
-			adev->asic_type != CHIP_FIJI &&
-			adev->asic_type != CHIP_POLARIS10 &&
-			adev->asic_type != CHIP_POLARIS11) ?
-			VI_BO_SIZE_ALIGN : 1;
-
-	*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);
-	if (*mem == NULL) {
-		ret = -ENOMEM;
-		goto err;
-	}
-	INIT_LIST_HEAD(&(*mem)->bo_va_list);
-	mutex_init(&(*mem)->lock);
-
-	(*mem)->no_substitute = no_sub;
-	(*mem)->aql_queue = aql_queue;
-
-	pte_flags = AMDGPU_PTE_READABLE | AMDGPU_PTE_VALID;
-	if (!readonly)
-		pte_flags |= AMDGPU_PTE_WRITEABLE;
-	if (execute)
-		pte_flags |= AMDGPU_PTE_EXECUTABLE;
-
-	(*mem)->pte_flags = pte_flags;
-
-	alloc_domain = userptr ? AMDGPU_GEM_DOMAIN_CPU : domain;
-	pr_debug("amdkfd: allocating BO on domain %d with size %llu\n",
-				alloc_domain, size);
-
-	ret = amdgpu_amdkfd_reserve_system_mem_limit(adev, size, alloc_domain);
-	if (ret) {
-		pr_err("amdkfd: Insufficient system memory\n");
-		goto err_bo_create;
-	}
-
-
-	/* Allocate buffer object. Userptr objects need to start out
-	 * in the CPU domain, get moved to GTT when pinned. */
-	ret = amdgpu_bo_create(adev, size, byte_align, false,
-				alloc_domain,
-			       flags, sg, NULL, &bo);
-	if (ret != 0) {
-		pr_err("amdkfd: failed to create BO on domain %d. ret %d\n",
-				alloc_domain, ret);
-		goto err_bo_create;
-	}
-	bo->kfd_bo = *mem;
-	(*mem)->bo = bo;
-
-	pr_debug("amdkfd: created BO on domain %d with size %llu\n",
-				alloc_domain, size);
-
-	if (userptr) {
-		ret = amdgpu_ttm_tt_set_userptr(bo->tbo.ttm, user_addr, 0);
-		if (ret) {
-			dev_err(adev->dev,
-				"(%d) failed to set userptr\n", ret);
-			goto allocate_mem_set_userptr_failed;
-		}
-
-		ret = amdgpu_mn_register(bo, user_addr);
-		if (ret) {
-			dev_err(adev->dev,
-				"(%d) failed to register MMU notifier\n", ret);
-			goto allocate_mem_set_userptr_failed;
-		}
-	}
-
-	if (kptr) {
-		ret = amdgpu_bo_reserve(bo, true);
-		if (ret) {
-			dev_err(adev->dev, "(%d) failed to reserve bo for amdkfd\n", ret);
-			goto allocate_mem_reserve_bo_failed;
-		}
-
-		ret = amdgpu_bo_pin(bo, domain,
-					NULL);
-		if (ret) {
-			dev_err(adev->dev, "(%d) failed to pin bo for amdkfd\n", ret);
-			goto allocate_mem_pin_bo_failed;
-		}
-
-		ret = amdgpu_bo_kmap(bo, kptr);
-		if (ret) {
-			dev_err(adev->dev,
-				"(%d) failed to map bo to kernel for amdkfd\n", ret);
-			goto allocate_mem_kmap_bo_failed;
-		}
-		(*mem)->kptr = *kptr;
-
-		amdgpu_bo_unreserve(bo);
-	}
-
-	(*mem)->va = va;
-	(*mem)->domain = domain;
-	(*mem)->mapped_to_gpu_memory = 0;
-	add_kgd_mem_to_kfd_bo_list(*mem, kfd_vm);
-
-	if (offset)
-		*offset = amdgpu_bo_mmap_offset(bo);
-
-	return 0;
-
-allocate_mem_kmap_bo_failed:
-	amdgpu_bo_unpin(bo);
-allocate_mem_pin_bo_failed:
-	amdgpu_bo_unreserve(bo);
-allocate_mem_reserve_bo_failed:
-	if (userptr)
-		amdgpu_mn_unregister(bo);
-allocate_mem_set_userptr_failed:
-err_bo_clear:
-	amdgpu_bo_unref(&bo);
-err_bo_create:
-	kfree(*mem);
-err:
-	return ret;
-}
-
-/* Reserving a BO and its page table BOs must happen atomically to
- * avoid deadlocks. When updating userptrs we need to temporarily
- * back-off the reservation and then reacquire it. Track all the
- * reservation info in a context structure. Buffers can be mapped to
- * multiple VMs simultaneously (buffers being restored on multiple
- * GPUs). */
-struct bo_vm_reservation_context {
-	struct amdgpu_bo_list_entry kfd_bo;
-	unsigned n_vms;
-	struct amdgpu_bo_list_entry *vm_pd;
-	struct ww_acquire_ctx ticket;
-	struct list_head list, duplicates;
-	struct amdgpu_sync sync;
-	bool reserved;
-};
-
-/**
- * reserve_bo_and_vm - reserve a BO and a VM unconditionally.
- * @mem: KFD BO structure.
- * @vm: the VM to reserve.
- * @ctx: the struct that will be used in unreserve_bo_and_vms().
- */
-static int reserve_bo_and_vm(struct kgd_mem *mem,
-			      struct amdgpu_vm *vm,
-			      struct bo_vm_reservation_context *ctx)
-{
-	struct amdgpu_bo *bo = mem->bo;
-	struct amdgpu_vm_parser param;
-	int ret;
-
-	WARN_ON(!vm);
-
-	ctx->reserved = false;
-	ctx->n_vms = 1;
-	amdgpu_sync_create(&ctx->sync);
-
-	INIT_LIST_HEAD(&ctx->list);
-	INIT_LIST_HEAD(&ctx->duplicates);
-
-	ctx->vm_pd = kzalloc(sizeof(struct amdgpu_bo_list_entry)
-			      * ctx->n_vms, GFP_KERNEL);
-	if (ctx->vm_pd == NULL)
-		return -ENOMEM;
-
-	ctx->kfd_bo.robj = bo;
-	ctx->kfd_bo.priority = 0;
-	ctx->kfd_bo.tv.bo = &bo->tbo;
-	ctx->kfd_bo.tv.shared = true;
-	ctx->kfd_bo.user_pages = NULL;
-	list_add(&ctx->kfd_bo.tv.head, &ctx->list);
-
-	amdgpu_vm_get_pd_bo(vm, &ctx->list, &ctx->vm_pd[0]);
-	param.domain = bo->prefered_domains;
-	param.wait = false;
-	amdgpu_vm_validate_pt_bos(amdgpu_ttm_adev(bo->tbo.bdev), vm,
-			amdgpu_amdkfd_validate, &param);
-
-	ret = ttm_eu_reserve_buffers(&ctx->ticket, &ctx->list,
-				     false, &ctx->duplicates);
-	if (!ret)
-		ctx->reserved = true;
-	else
-		pr_err("amdkfd: Failed to reserve buffers in ttm\n");
-
-	if (ret) {
-		kfree(ctx->vm_pd);
-		ctx->vm_pd = NULL;
-	}
-
-	return ret;
-}
-
-enum VA_TYPE {
-	VA_NOT_MAPPED = 0,
-	VA_MAPPED,
-	VA_DO_NOT_CARE,
-};
-
-/**
- * reserve_bo_and_vm - reserve a BO and some VMs that the BO has been added
- * to, conditionally based on map_type.
- * @mem: KFD BO structure.
- * @vm: the VM to reserve. If NULL, then all VMs associated with the BO
- * is used. Otherwise, a single VM associated with the BO.
- * @map_type: the mapping status that will be used to filter the VMs.
- * @ctx: the struct that will be used in unreserve_bo_and_vms().
- */
-static int reserve_bo_and_cond_vms(struct kgd_mem *mem,
-			      struct amdgpu_vm *vm, enum VA_TYPE map_type,
-			      struct bo_vm_reservation_context *ctx)
-{
-	struct amdgpu_bo *bo = mem->bo;
-	struct kfd_bo_va_list *entry;
-	struct amdgpu_vm_parser param;
-	unsigned i;
-	int ret;
-
-	ctx->reserved = false;
-	ctx->n_vms = 0;
-	ctx->vm_pd = NULL;
-	amdgpu_sync_create(&ctx->sync);
-
-	INIT_LIST_HEAD(&ctx->list);
-	INIT_LIST_HEAD(&ctx->duplicates);
-
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
-		if ((vm && vm != entry->bo_va->vm) ||
-			(entry->is_mapped != map_type
-			&& map_type != VA_DO_NOT_CARE))
-			continue;
-
-		ctx->n_vms++;
-	}
-
-	if (ctx->n_vms != 0) {
-		ctx->vm_pd = kzalloc(sizeof(struct amdgpu_bo_list_entry)
-			      * ctx->n_vms, GFP_KERNEL);
-		if (ctx->vm_pd == NULL)
-			return -ENOMEM;
-	}
-
-	ctx->kfd_bo.robj = bo;
-	ctx->kfd_bo.priority = 0;
-	ctx->kfd_bo.tv.bo = &bo->tbo;
-	ctx->kfd_bo.tv.shared = true;
-	ctx->kfd_bo.user_pages = NULL;
-	list_add(&ctx->kfd_bo.tv.head, &ctx->list);
-
-	i = 0;
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
-		if ((vm && vm != entry->bo_va->vm) ||
-			(entry->is_mapped != map_type
-			&& map_type != VA_DO_NOT_CARE))
-			continue;
-
-		amdgpu_vm_get_pd_bo(entry->bo_va->vm, &ctx->list,
-				&ctx->vm_pd[i]);
-		i++;
-	}
-
-	if (vm) {
-		param.domain = bo->prefered_domains;
-		param.wait = false;
-		amdgpu_vm_validate_pt_bos(amdgpu_ttm_adev(bo->tbo.bdev), vm,
-				amdgpu_amdkfd_validate, &param);
-	}
-
-	ret = ttm_eu_reserve_buffers(&ctx->ticket, &ctx->list,
-				     false, &ctx->duplicates);
-	if (!ret)
-		ctx->reserved = true;
-	else
-		pr_err("amdkfd: Failed to reserve buffers in ttm\n");
-
-	if (ret) {
-		kfree(ctx->vm_pd);
-		ctx->vm_pd = NULL;
-	}
-
-	return ret;
-}
-
-static void unreserve_bo_and_vms(struct bo_vm_reservation_context *ctx,
-				 bool wait)
-{
-	if (wait)
-		amdgpu_sync_wait(&ctx->sync);
-
-	if (ctx->reserved)
-		ttm_eu_backoff_reservation(&ctx->ticket, &ctx->list);
-	kfree(ctx->vm_pd);
-
-	amdgpu_sync_free(&ctx->sync);
-	ctx->reserved = false;
-	ctx->vm_pd = NULL;
-}
-
-/* Must be called with mem->lock held and a BO/VM reservation
- * context. Temporarily drops the lock and reservation for updating
- * user pointers, to avoid circular lock dependencies between MM locks
- * and buffer reservations. If user pages are invalidated while the
- * lock and reservation are dropped, try again. */
-static int update_user_pages(struct kgd_mem *mem, struct mm_struct *mm,
-			     struct bo_vm_reservation_context *ctx)
-{
-	struct amdgpu_bo *bo;
-	unsigned tries = 10;
-	int ret;
-
-	bo = mem->bo;
-	if (!amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))
-		return 0;
-
-	if (bo->tbo.ttm->state != tt_bound) {
-		struct page **pages;
-		int invalidated;
-
-		/* get user pages without locking the BO to avoid
-		 * circular lock dependency with MMU notifier. Retry
-		 * until we have the current version. */
-		ttm_eu_backoff_reservation(&ctx->ticket, &ctx->list);
-		ctx->reserved = false;
-		pages = drm_calloc_large(bo->tbo.ttm->num_pages,
-					 sizeof(struct page *));
-		if (!pages)
-			return -ENOMEM;
-
-		mutex_unlock(&mem->lock);
-
-		while (true) {
-			down_read(&mm->mmap_sem);
-			ret = amdgpu_ttm_tt_get_user_pages(bo->tbo.ttm, pages);
-			up_read(&mm->mmap_sem);
-
-			mutex_lock(&mem->lock);
-			if (ret != 0)
-				return ret;
-
-			BUG_ON(bo != mem->bo);
-
-			ret = ttm_eu_reserve_buffers(&ctx->ticket, &ctx->list,
-						     false, &ctx->duplicates);
-			if (unlikely(ret != 0)) {
-				release_pages(pages, bo->tbo.ttm->num_pages, 0);
-				drm_free_large(pages);
-				return ret;
-			}
-			ctx->reserved = true;
-			if (!amdgpu_ttm_tt_userptr_invalidated(bo->tbo.ttm,
-							       &invalidated) ||
-			    bo->tbo.ttm->state == tt_bound ||
-			    --tries == 0)
-				break;
-
-			release_pages(pages, bo->tbo.ttm->num_pages, 0);
-			ttm_eu_backoff_reservation(&ctx->ticket, &ctx->list);
-			ctx->reserved = false;
-			mutex_unlock(&mem->lock);
-		}
-
-		/* If someone else already bound it, release our pages
-		 * array, otherwise copy it into the ttm BO. */
-		if (bo->tbo.ttm->state == tt_bound || tries == 0)
-			release_pages(pages, bo->tbo.ttm->num_pages, 0);
-		else
-			memcpy(bo->tbo.ttm->pages, pages,
-			       sizeof(struct page *) * bo->tbo.ttm->num_pages);
-		drm_free_large(pages);
-	}
-
-	if (tries == 0) {
-		pr_err("Gave up trying to update user pages\n");
-		return -EDEADLK;
-	}
-
-	return 0;
-}
-
-static int unmap_bo_from_gpuvm(struct amdgpu_device *adev,
-				struct amdgpu_bo *bo,
-				struct kfd_bo_va_list *entry,
-				struct amdgpu_sync *sync)
-{
-	struct amdgpu_bo_va *bo_va = entry->bo_va;
-	struct amdgpu_vm *vm = bo_va->vm;
-
-	amdgpu_vm_bo_unmap(adev, bo_va, entry->va);
-
-	amdgpu_vm_clear_freed(adev, vm);
-
-	amdgpu_sync_fence(adev, sync, bo_va->last_pt_update);
-
-	amdgpu_amdkfd_bo_invalidate(bo);
-
-	return 0;
-}
-
-static int update_gpuvm_pte(struct amdgpu_device *adev, struct amdgpu_bo *bo,
-		struct kfd_bo_va_list *entry,
-		struct amdgpu_sync *sync)
-{
-	int ret;
-	struct amdgpu_vm *vm;
-	struct amdgpu_bo_va *bo_va;
-
-	bo_va = entry->bo_va;
-	vm = bo_va->vm;
-	/* Validate PT / PTs */
-	ret = validate_pt_pd_bos(vm);
-	if (ret != 0) {
-		pr_err("amdkfd: Failed to validate_pt_pd_bos\n");
-		return ret;
-	}
-
-	/* Update the page directory */
-	ret = amdgpu_vm_update_page_directory(adev, vm);
-	if (ret != 0) {
-		pr_err("amdkfd: Failed to amdgpu_vm_update_page_directory\n");
-		return ret;
-	}
-
-	amdgpu_sync_fence(adev, sync, vm->page_directory_fence);
-
-	/* Update the page tables  */
-	ret = amdgpu_vm_bo_update(adev, bo_va, false);
-	if (ret != 0) {
-		pr_err("amdkfd: Failed to amdgpu_vm_bo_update\n");
-		return ret;
-	}
-
-	amdgpu_sync_fence(adev, sync, bo_va->last_pt_update);
-
-	/* Remove PTs from LRU list (reservation removed PD only) */
-	amdgpu_vm_move_pt_bos_in_lru(adev, vm);
-
-	return 0;
-}
-
-static int map_bo_to_gpuvm(struct amdgpu_device *adev, struct amdgpu_bo *bo,
-		struct kfd_bo_va_list *entry, uint32_t pte_flags,
-		struct amdgpu_sync *sync)
-{
-	int ret;
-	struct amdgpu_bo_va *bo_va;
-
-	bo_va = entry->bo_va;
-	/* Set virtual address for the allocation, allocate PTs,
-	 * if needed, and zero them.
-	 */
-	ret = amdgpu_vm_bo_map(adev, bo_va,
-			entry->va, 0, amdgpu_bo_size(bo),
-			pte_flags);
-	if (ret != 0) {
-		pr_err("amdkfd: Failed to map bo in vm. ret == %d (0x%llx)\n",
-				ret, entry->va);
-		return ret;
-	}
-
-	ret = update_gpuvm_pte(adev, bo, entry, sync);
-	if (ret != 0) {
-		pr_err("amdkfd: update_gpuvm_pte() failed\n");
-		goto update_gpuvm_pte_failed;
-	}
-
-	return 0;
-
-update_gpuvm_pte_failed:
-	unmap_bo_from_gpuvm(adev, bo, entry, sync);
-	return ret;
-}
-
-static struct sg_table *create_doorbell_sg(uint64_t addr, uint32_t size)
-{
-	struct sg_table *sg = kmalloc(sizeof(struct sg_table), GFP_KERNEL);
-
-	if (!sg)
-		return NULL;
-	if (sg_alloc_table(sg, 1, GFP_KERNEL)) {
-		kfree(sg);
-		return NULL;
-	}
-	sg->sgl->dma_address = addr;
-	sg->sgl->length = size;
-#ifdef CONFIG_NEED_SG_DMA_LENGTH
-	sg->sgl->dma_length = size;
-#endif
-	return sg;
-}
-
-#define BOOL_TO_STR(b)	(b == true) ? "true" : "false"
-
-int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
-		struct kgd_dev *kgd, uint64_t va, uint64_t size,
-		void *vm, struct kgd_mem **mem,
-		uint64_t *offset, void **kptr,
-		uint32_t flags)
-{
-	bool aql_queue, public, readonly, execute, no_sub, userptr;
-	u64 alloc_flag;
-	uint32_t domain;
-	uint64_t *temp_offset;
-	struct sg_table *sg = NULL;
-
-	if (!(flags & ALLOC_MEM_FLAGS_NONPAGED)) {
-		pr_err("amdgpu: current hw doesn't support paged memory\n");
-		return -EINVAL;
-	}
-
-	domain = 0;
-	alloc_flag = 0;
-	temp_offset = NULL;
-
-	aql_queue = (flags & ALLOC_MEM_FLAGS_AQL_QUEUE_MEM) ? true : false;
-	public    = (flags & ALLOC_MEM_FLAGS_PUBLIC) ? true : false;
-	readonly  = (flags & ALLOC_MEM_FLAGS_READONLY) ? true : false;
-	execute   = (flags & ALLOC_MEM_FLAGS_EXECUTE_ACCESS) ? true : false;
-	no_sub    = (flags & ALLOC_MEM_FLAGS_NO_SUBSTITUTE) ? true : false;
-	userptr   = (flags & ALLOC_MEM_FLAGS_USERPTR) ? true : false;
-
-	if (userptr && kptr) {
-		pr_err("amdgpu: userptr can't be mapped to kernel\n");
-		return -EINVAL;
-	}
-
-	/*
-	 * Check on which domain to allocate BO
-	 */
-	if (flags & ALLOC_MEM_FLAGS_VRAM) {
-		domain = AMDGPU_GEM_DOMAIN_VRAM;
-		alloc_flag = AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
-		if (public) {
-			alloc_flag = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
-			temp_offset = offset;
-		}
-	} else if (flags & (ALLOC_MEM_FLAGS_GTT | ALLOC_MEM_FLAGS_USERPTR)) {
-		domain = AMDGPU_GEM_DOMAIN_GTT;
-		alloc_flag = 0;
-		temp_offset = offset;
-	} else if (flags & ALLOC_MEM_FLAGS_DOORBELL) {
-		domain = AMDGPU_GEM_DOMAIN_GTT;
-		alloc_flag = 0;
-		temp_offset = offset;
-		if (size > UINT_MAX)
-			return -EINVAL;
-		sg = create_doorbell_sg(*offset, size);
-		if (!sg)
-			return -ENOMEM;
-	}
-
-	if (offset && !userptr)
-		*offset = 0;
-
-	pr_debug("amdgpu: allocating BO domain %d alloc_flag 0x%llu public %s readonly %s execute %s no substitue %s va 0x%llx\n",
-			domain,
-			alloc_flag,
-			BOOL_TO_STR(public),
-			BOOL_TO_STR(readonly),
-			BOOL_TO_STR(execute),
-			BOOL_TO_STR(no_sub),
-			va);
-
-	return __alloc_memory_of_gpu(kgd, va, size, vm, mem,
-			temp_offset, kptr, domain,
-			alloc_flag, sg,
-			aql_queue, readonly, execute,
-			no_sub, userptr);
-}
-
-int amdgpu_amdkfd_gpuvm_free_memory_of_gpu(
-		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm)
-{
-	struct amdgpu_device *adev;
-	struct kfd_bo_va_list *entry, *tmp;
-	struct bo_vm_reservation_context ctx;
-	int ret;
-	struct amdgpu_bo_list_entry *bo_list_entry;
-	struct amdkfd_vm *master_vm;
-
-	BUG_ON(kgd == NULL);
-	BUG_ON(mem == NULL);
-	BUG_ON(vm == NULL);
-
-	adev = get_amdgpu_device(kgd);
-	master_vm = ((struct amdkfd_vm *)vm)->master;
-	BUG_ON(master_vm == NULL);
-
-	mutex_lock(&mem->lock);
-
-	if (mem->mapped_to_gpu_memory > 0) {
-		pr_err("BO with size %lu bytes is mapped to GPU. Need to unmap it before release va 0x%llx\n",
-			mem->bo->tbo.mem.size, mem->va);
-		mutex_unlock(&mem->lock);
-		return -EBUSY;
-	}
-
-	mutex_unlock(&mem->lock);
-	/* lock is not needed after this, since mem is unused and will
-	 * be freed anyway */
-
-	amdgpu_mn_unregister(mem->bo);
-	if (mem->work.work.func)
-		cancel_delayed_work_sync(&mem->work);
-
-	ret = reserve_bo_and_cond_vms(mem, NULL, VA_DO_NOT_CARE, &ctx);
-	if (unlikely(ret != 0))
-		return ret;
-
-	/* The eviction fence should be removed by the last unmap.
-	 * TODO: Log an error condition if the bo still has the eviction fence
-	 * attached
-	 */
-	amdgpu_amdkfd_remove_eviction_fence(mem->bo, master_vm->eviction_fence,
-					    NULL, NULL);
-	pr_debug("Releasing BO with VA 0x%llx, size %lu bytes\n",
-					mem->va,
-					mem->bo->tbo.mem.size);
-
-	/* Remove from VM internal data structures */
-	list_for_each_entry_safe(entry, tmp, &mem->bo_va_list, bo_list) {
-		pr_debug("\t remove from amdgpu_bo_va %p\n", entry->bo_va);
-		remove_bo_from_vm((struct amdgpu_device *)entry->kgd_dev,
-				entry);
-	}
-
-	unreserve_bo_and_vms(&ctx, false);
-
-	/* If the SG is not NULL, it's one we created for a doorbell
-	 * BO. We need to free it.
-	 */
-	if (mem->bo->tbo.sg) {
-		sg_free_table(mem->bo->tbo.sg);
-		kfree(mem->bo->tbo.sg);
-	}
-
-	/* Free the BO*/
-	bo_list_entry = &mem->bo_list_entry;
-	mutex_lock(&master_vm->lock);
-	list_del(&bo_list_entry->tv.head);
-	mutex_unlock(&master_vm->lock);
-
-	amdgpu_bo_unref(&mem->bo);
-	kfree(mem);
-
-	return 0;
-}
-
-int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
-		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm)
-{
-	struct amdgpu_device *adev;
-	int ret;
-	struct amdgpu_bo *bo;
-	uint32_t domain;
-	struct kfd_bo_va_list *entry;
-	struct bo_vm_reservation_context ctx;
-	struct kfd_bo_va_list *bo_va_entry = NULL;
-	struct kfd_bo_va_list *bo_va_entry_aql = NULL;
-	struct amdkfd_vm *kfd_vm = (struct amdkfd_vm *)vm;
-	int num_to_quiesce = 0;
-
-	BUG_ON(kgd == NULL);
-	BUG_ON(mem == NULL);
-
-	adev = get_amdgpu_device(kgd);
-
-	mutex_lock(&mem->lock);
-
-	bo = mem->bo;
-
-	BUG_ON(bo == NULL);
-
-	domain = mem->domain;
-
-	pr_debug("amdgpu: try to map VA 0x%llx domain %d\n",
-			mem->va, domain);
-
-	ret = reserve_bo_and_vm(mem, vm, &ctx);
-	if (unlikely(ret != 0))
-		goto bo_reserve_failed;
-
-	if (check_if_add_bo_to_vm((struct amdgpu_vm *)vm, mem)) {
-		pr_debug("amdkfd: add new BO_VA to list 0x%llx\n",
-				mem->va);
-		ret = add_bo_to_vm(adev, mem, (struct amdgpu_vm *)vm, false,
-				&bo_va_entry);
-		if (ret != 0)
-			goto add_bo_to_vm_failed;
-		if (mem->aql_queue) {
-			ret = add_bo_to_vm(adev, mem, (struct amdgpu_vm *)vm,
-					true, &bo_va_entry_aql);
-			if (ret != 0)
-				goto add_bo_to_vm_failed_aql;
-		}
-	}
-
-	if (!mem->evicted) {
-		ret = update_user_pages(mem, current->mm, &ctx);
-		if (ret != 0)
-			goto update_user_pages_failed;
-	}
-
-	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {
-		ret = amdgpu_amdkfd_bo_validate(bo, domain, true);
-		if (ret) {
-			pr_debug("amdkfd: userptr: Validate failed\n");
-			goto map_bo_to_gpuvm_failed;
-		}
-	} else if (mem->mapped_to_gpu_memory == 0) {
-		/* Validate BO only once. The eviction fence gets added to BO
-		 * the first time it is mapped. Validate will wait for all
-		 * background evictions to complete.
-		 */
-		ret = amdgpu_amdkfd_bo_validate(bo, domain, true);
-		if (ret) {
-			pr_debug("amdkfd: Validate failed\n");
-			goto map_bo_to_gpuvm_failed;
-		}
-	}
-
-
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
-		if (entry->bo_va->vm == vm && !entry->is_mapped) {
-			if (mem->evicted) {
-				/* If the BO is evicted, just mark the
-				 * mapping as mapped and the GPU's queues
-				 * will be stopped later.
-				 */
-				entry->is_mapped = true;
-				mem->mapped_to_gpu_memory++;
-				num_to_quiesce++;
-				continue;
-			}
-
-			pr_debug("amdkfd: Trying to map VA 0x%llx to vm %p\n",
-					mem->va, vm);
-
-			ret = map_bo_to_gpuvm(adev, bo, entry, mem->pte_flags,
-					&ctx.sync);
-			if (ret != 0) {
-				pr_err("amdkfd: Failed to map radeon bo to gpuvm\n");
-				goto map_bo_to_gpuvm_failed;
-			}
-			entry->is_mapped = true;
-			mem->mapped_to_gpu_memory++;
-				pr_debug("amdgpu: INC mapping count %d\n",
-					mem->mapped_to_gpu_memory);
-		}
-	}
-
-	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) == NULL)
-		amdgpu_bo_fence(bo, &kfd_vm->master->eviction_fence->base,
-				true);
-	unreserve_bo_and_vms(&ctx, true);
-
-	while (num_to_quiesce--) {
-		/* Now stop the GPU's queues while bo and VMs are unreserved.
-		 * quiesce_mm() is reference counted, and that is why we can
-		 * call it multiple times.
-		 */
-		ret = kgd2kfd->quiesce_mm(adev->kfd, current->mm);
-		if (ret != 0) {
-			pr_err("quiesce_mm() failed\n");
-			reserve_bo_and_vm(mem, vm, &ctx);
-			goto map_bo_to_gpuvm_failed;
-		}
-	}
-
-	mutex_unlock(&mem->lock);
-	return ret;
-
-map_bo_to_gpuvm_failed:
-update_user_pages_failed:
-	if (bo_va_entry_aql)
-		remove_bo_from_vm(adev, bo_va_entry_aql);
-add_bo_to_vm_failed_aql:
-	if (bo_va_entry)
-		remove_bo_from_vm(adev, bo_va_entry);
-add_bo_to_vm_failed:
-	unreserve_bo_and_vms(&ctx, false);
-bo_reserve_failed:
-	mutex_unlock(&mem->lock);
-	return ret;
-}
-
-static u64 get_vm_pd_gpu_offset(void *vm)
-{
-	struct amdgpu_vm *avm = (struct amdgpu_vm *) vm;
-	u64 offset;
-
-	BUG_ON(avm == NULL);
-
-	amdgpu_bo_reserve(avm->page_directory, false);
-
-	offset = amdgpu_bo_gpu_offset(avm->page_directory);
-
-	amdgpu_bo_unreserve(avm->page_directory);
-
-	return offset;
-}
-
-int amdgpu_amdkfd_gpuvm_create_process_vm(struct kgd_dev *kgd, void **vm,
-					  void *master_vm)
-{
-	int ret;
-	struct amdkfd_vm *new_vm;
-	bool vm_update_using_cpu;
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-
-	BUG_ON(kgd == NULL);
-	BUG_ON(vm == NULL);
-
-	new_vm = kzalloc(sizeof(struct amdkfd_vm), GFP_KERNEL);
-	if (new_vm == NULL)
-		return -ENOMEM;
-
-	/* Initialize the VM context, allocate the page directory and zero it */
-	vm_update_using_cpu = !!(amdgpu_vm_update_context &
-				AMDGPU_VM_USE_CPU_UPDATE_FOR_COMPUTE_MASK);
-	ret = amdgpu_vm_init(adev, &new_vm->base,
-			     (vm_update_using_cpu && check_if_large_bar(kgd)),
-			     true);
-	if (ret != 0) {
-		pr_err("amdgpu: failed init vm ret %d\n", ret);
-		/* Undo everything related to the new VM context */
-		goto vm_init_fail;
-	}
-	new_vm->adev = adev;
-	mutex_init(&new_vm->lock);
-	INIT_LIST_HEAD(&new_vm->kfd_bo_list);
-	INIT_LIST_HEAD(&new_vm->kfd_vm_list);
-
-	if (master_vm == NULL) {
-		new_vm->master = new_vm;
-		new_vm->eviction_fence =
-			amdgpu_amdkfd_fence_create(fence_context_alloc(1),
-						   current->mm);
-		if (new_vm->master->eviction_fence == NULL) {
-			pr_err("Failed to create eviction fence\n");
-			goto evict_fence_fail;
-		}
-	} else {
-		new_vm->master = master_vm;
-		list_add_tail(&new_vm->kfd_vm_list,
-			      &((struct amdkfd_vm *)master_vm)->kfd_vm_list);
-	}
-	new_vm->master->n_vms++;
-	*vm = (void *) new_vm;
-
-	pr_debug("amdgpu: created process vm with address 0x%llx\n",
-			get_vm_pd_gpu_offset(&new_vm->base));
-
-	return ret;
-
-evict_fence_fail:
-	amdgpu_vm_fini(adev, &new_vm->base);
-vm_init_fail:
-	kfree(new_vm);
-	return ret;
-
-}
-
-void amdgpu_amdkfd_gpuvm_destroy_process_vm(struct kgd_dev *kgd, void *vm)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
-	struct amdkfd_vm *kfd_vm = (struct amdkfd_vm *) vm;
-	struct amdgpu_vm *avm = &kfd_vm->base;
-	struct amdgpu_bo *pd;
-
-	BUG_ON(kgd == NULL);
-	BUG_ON(vm == NULL);
-
-	pr_debug("Destroying process vm with address %p\n", vm);
-	/* Release eviction fence from PD */
-	pd = avm->page_directory;
-	amdgpu_bo_reserve(pd, false);
-	amdgpu_bo_fence(pd, NULL, false);
-	amdgpu_bo_unreserve(pd);
-
-	/* Release eviction fence */
-	if (kfd_vm->master == kfd_vm && kfd_vm->eviction_fence != NULL)
-		fence_put(&kfd_vm->eviction_fence->base);
-
-	/* Release the VM context */
-	amdgpu_vm_fini(adev, avm);
-	kfree(vm);
-}
-
-uint32_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *vm)
-{
-	return get_vm_pd_gpu_offset(vm) >> AMDGPU_GPU_PAGE_SHIFT;
-}
-
-int amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct kgd_dev *kgd,
-					      struct kfd_vm_fault_info *mem)
-{
-	struct amdgpu_device *adev;
-
-	BUG_ON(kgd == NULL);
-	adev = (struct amdgpu_device *) kgd;
-	if (atomic_read(&adev->mc.vm_fault_info_updated) == 1) {
-		*mem = *adev->mc.vm_fault_info;
-		mb();
-		atomic_set(&adev->mc.vm_fault_info_updated, 0);
-	}
-	return 0;
-}
-
-static bool is_mem_on_local_device(struct kgd_dev *kgd,
-		struct list_head *bo_va_list, void *vm)
-{
-	struct kfd_bo_va_list *entry;
-
-	list_for_each_entry(entry, bo_va_list, bo_list) {
-		if (entry->kgd_dev == kgd && entry->bo_va->vm == vm)
-			return true;
-	}
-
-	return false;
-}
-
-int amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(
-		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm)
-{
-	struct kfd_bo_va_list *entry;
-	struct amdgpu_device *adev;
-	unsigned mapped_before;
-	int ret = 0;
-	struct bo_vm_reservation_context ctx;
-	struct amdkfd_vm *master_vm;
-	int num_to_resume = 0;
-
-	BUG_ON(kgd == NULL);
-	BUG_ON(mem == NULL);
-
-	adev = (struct amdgpu_device *) kgd;
-	master_vm = ((struct amdkfd_vm *)vm)->master;
-
-	mutex_lock(&mem->lock);
-
-	/*
-	 * Make sure that this BO mapped on KGD before unmappping it
-	 */
-	if (!is_mem_on_local_device(kgd, &mem->bo_va_list, vm)) {
-		ret = -EINVAL;
-		goto out;
-	}
-
-	if (mem->mapped_to_gpu_memory == 0) {
-		pr_debug("BO size %lu bytes at va 0x%llx is not mapped\n",
-			 mem->bo->tbo.mem.size, mem->va);
-		ret = -EINVAL;
-		goto out;
-	}
-	mapped_before = mem->mapped_to_gpu_memory;
-
-	ret = reserve_bo_and_cond_vms(mem, vm, VA_MAPPED, &ctx);
-	if (unlikely(ret != 0))
-		goto out;
-
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
-		if (entry->bo_va->vm == vm && entry->is_mapped) {
-			if (mem->evicted) {
-				/* If the BO is evicted, just mark the
-				 * mapping as unmapped and the GPU's queues
-				 * will be resumed later.
-				 */
-				entry->is_mapped = false;
-				mem->mapped_to_gpu_memory--;
-				num_to_resume++;
-				continue;
-			}
-
-			pr_debug("unmapping BO with VA 0x%llx, size %lu bytes from GPU memory\n",
-				mem->va,
-				mem->bo->tbo.mem.size);
-
-			ret = unmap_bo_from_gpuvm(adev, mem->bo,
-						entry, &ctx.sync);
-			if (ret == 0) {
-				entry->is_mapped = false;
-			} else {
-				pr_err("amdgpu: failed unmap va 0x%llx\n",
-						mem->va);
-				goto unreserve_out;
-			}
-
-			mem->mapped_to_gpu_memory--;
-			pr_debug("amdgpu: DEC mapping count %d\n",
-					mem->mapped_to_gpu_memory);
-		}
-	}
-
-	/* If BO is unmapped from all VMs, unfence it. It can be evicted if
-	 * required.
-	 */
-	if (mem->mapped_to_gpu_memory == 0)
-		amdgpu_amdkfd_remove_eviction_fence(mem->bo,
-						    master_vm->eviction_fence,
-						    NULL, NULL);
-
-	if (mapped_before == mem->mapped_to_gpu_memory) {
-		pr_debug("BO size %lu bytes at va 0x%llx is not mapped on GPU %x:%x.%x\n",
-			 mem->bo->tbo.mem.size, mem->va,
-			 adev->pdev->bus->number, PCI_SLOT(adev->pdev->devfn),
-			 PCI_FUNC(adev->pdev->devfn));
-		ret = -EINVAL;
-	}
-
-unreserve_out:
-	unreserve_bo_and_vms(&ctx, false);
-
-	while (num_to_resume--) {
-		/* Now resume GPU's queues while bo and VMs are unreserved.
-		 * resume_mm() is reference counted, and that is why we can
-		 * call it multiple times.
-		 */
-		ret = kgd2kfd->resume_mm(adev->kfd, current->mm);
-		if (ret != 0) {
-			pr_err("resume_mm() failed.\n");
-			break;
-		}
-	}
-out:
-	mutex_unlock(&mem->lock);
-	return ret;
-}
-
-int amdgpu_amdkfd_gpuvm_mmap_bo(struct kgd_dev *kgd, struct vm_area_struct *vma)
-{
-	struct amdgpu_device *adev;
-
-	adev = get_amdgpu_device(kgd);
-	BUG_ON(!adev);
-
-	return amdgpu_bo_mmap(NULL, vma, &adev->mman.bdev);
-}
-
-int amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_dev *kgd,
-		struct kgd_mem *mem, void **kptr)
-{
-	int ret;
-	struct amdgpu_device *adev;
-	struct amdgpu_bo *bo;
-
-	adev = get_amdgpu_device(kgd);
-
-	mutex_lock(&mem->lock);
-
-	bo = mem->bo;
-	/* map the buffer */
-	ret = amdgpu_bo_reserve(bo, true);
-	if (ret) {
-		dev_err(adev->dev, "(%d) failed to reserve bo for amdkfd\n", ret);
-		mutex_unlock(&mem->lock);
-		return ret;
-	}
-
-	ret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT,
-			NULL);
-	if (ret) {
-		dev_err(adev->dev, "(%d) failed to pin bo for amdkfd\n", ret);
-		amdgpu_bo_unreserve(bo);
-		mutex_unlock(&mem->lock);
-		return ret;
-	}
-
-	ret = amdgpu_bo_kmap(bo, kptr);
-	if (ret) {
-		dev_err(adev->dev,
-			"(%d) failed to map bo to kernel for amdkfd\n", ret);
-		amdgpu_bo_unpin(bo);
-		amdgpu_bo_unreserve(bo);
-		mutex_unlock(&mem->lock);
-		return ret;
-	}
-
-	mem->kptr = *kptr;
-
-	amdgpu_bo_unreserve(bo);
-	mutex_unlock(&mem->lock);
-
-	return 0;
-}
-
-static int pin_bo_wo_map(struct kgd_mem *mem)
-{
-	struct amdgpu_bo *bo = mem->bo;
-	int ret = 0;
-
-	ret = amdgpu_bo_reserve(bo, false);
-	if (unlikely(ret != 0))
-		return ret;
-
-	ret = amdgpu_bo_pin(bo, mem->domain, NULL);
-	amdgpu_bo_unreserve(bo);
-
-	return ret;
-}
-
-static void unpin_bo_wo_map(struct kgd_mem *mem)
-{
-	struct amdgpu_bo *bo = mem->bo;
-	int ret = 0;
-
-	ret = amdgpu_bo_reserve(bo, false);
-	if (unlikely(ret != 0))
-		return;
-
-	amdgpu_bo_unpin(bo);
-	amdgpu_bo_unreserve(bo);
-}
-
-#define AMD_GPU_PAGE_SHIFT	PAGE_SHIFT
-#define AMD_GPU_PAGE_SIZE (_AC(1, UL) << AMD_GPU_PAGE_SHIFT)
-
-static int get_sg_table(struct amdgpu_device *adev,
-		struct kgd_mem *mem, uint64_t offset,
-		uint64_t size, struct sg_table **ret_sg)
-{
-	struct amdgpu_bo *bo = mem->bo;
-	struct sg_table *sg = NULL;
-	unsigned long bus_addr;
-	unsigned int chunks;
-	unsigned int i;
-	struct scatterlist *s;
-	uint64_t offset_in_page;
-	unsigned int page_size;
-	int ret;
-
-	sg = kmalloc(sizeof(struct sg_table), GFP_KERNEL);
-	if (!sg) {
-		ret = -ENOMEM;
-		goto out;
-	}
-
-	if (bo->prefered_domains == AMDGPU_GEM_DOMAIN_VRAM)
-		page_size = AMD_GPU_PAGE_SIZE;
-	else
-		page_size = PAGE_SIZE;
-
-
-	offset_in_page = offset & (page_size - 1);
-	chunks = (size  + offset_in_page + page_size - 1)
-			/ page_size;
-
-	ret = sg_alloc_table(sg, chunks, GFP_KERNEL);
-	if (unlikely(ret))
-		goto out;
-
-	if (bo->prefered_domains == AMDGPU_GEM_DOMAIN_VRAM) {
-		bus_addr = bo->tbo.offset + adev->mc.aper_base + offset;
-
-		for_each_sg(sg->sgl, s, sg->orig_nents, i) {
-			uint64_t chunk_size, length;
-
-			chunk_size = page_size - offset_in_page;
-			length = min(size, chunk_size);
-
-			sg_set_page(s, NULL, length, offset_in_page);
-			s->dma_address = bus_addr;
-			s->dma_length = length;
-
-			size -= length;
-			offset_in_page = 0;
-			bus_addr += length;
-		}
-	} else {
-		struct page **pages;
-		unsigned int cur_page;
-
-		pages = bo->tbo.ttm->pages;
-
-		cur_page = offset / page_size;
-		for_each_sg(sg->sgl, s, sg->orig_nents, i) {
-			uint64_t chunk_size, length;
-
-			chunk_size = page_size - offset_in_page;
-			length = min(size, chunk_size);
-
-			sg_set_page(s, pages[cur_page], length, offset_in_page);
-			s->dma_address = page_to_phys(pages[cur_page]);
-			s->dma_length = length;
-
-			size -= length;
-			offset_in_page = 0;
-			cur_page++;
-		}
-	}
-
-	*ret_sg = sg;
-	return 0;
-out:
-	kfree(sg);
-	*ret_sg = NULL;
-	return ret;
-}
-
-int amdgpu_amdkfd_gpuvm_pin_get_sg_table(struct kgd_dev *kgd,
-		struct kgd_mem *mem, uint64_t offset,
-		uint64_t size, struct sg_table **ret_sg)
-{
-	int ret;
-	struct amdgpu_device *adev;
-
-	ret = pin_bo_wo_map(mem);
-	if (unlikely(ret != 0))
-		return ret;
-
-	adev = get_amdgpu_device(kgd);
-
-	ret = get_sg_table(adev, mem, offset, size, ret_sg);
-	if (ret)
-		unpin_bo_wo_map(mem);
-
-	return ret;
-}
-
-void amdgpu_amdkfd_gpuvm_unpin_put_sg_table(
-		struct kgd_mem *mem, struct sg_table *sg)
-{
-	sg_free_table(sg);
-	kfree(sg);
-
-	unpin_bo_wo_map(mem);
-}
-
-int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
-				      struct dma_buf *dma_buf,
-				      uint64_t va, void *vm,
-				      struct kgd_mem **mem, uint64_t *size,
-				      uint64_t *mmap_offset)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
-	struct drm_gem_object *obj;
-	struct amdgpu_bo *bo;
-	struct amdkfd_vm *kfd_vm = (struct amdkfd_vm *)vm;
-
-	if (dma_buf->ops != &drm_gem_prime_dmabuf_ops)
-		/* Can't handle non-graphics buffers */
-		return -EINVAL;
-
-	obj = dma_buf->priv;
-	if (obj->dev->dev_private != adev)
-		/* Can't handle buffers from other devices */
-		return -EINVAL;
-
-	bo = gem_to_amdgpu_bo(obj);
-	if (!(bo->prefered_domains & (AMDGPU_GEM_DOMAIN_VRAM |
-				    AMDGPU_GEM_DOMAIN_GTT)))
-		/* Only VRAM and GTT BOs are supported */
-		return -EINVAL;
-
-	*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);
-	if (*mem == NULL)
-		return -ENOMEM;
-
-	if (size)
-		*size = amdgpu_bo_size(bo);
-
-	if (mmap_offset)
-		*mmap_offset = amdgpu_bo_mmap_offset(bo);
-
-	INIT_LIST_HEAD(&(*mem)->bo_va_list);
-	mutex_init(&(*mem)->lock);
-	(*mem)->pte_flags = AMDGPU_PTE_READABLE | AMDGPU_PTE_VALID
-			| AMDGPU_PTE_WRITEABLE | AMDGPU_PTE_EXECUTABLE;
-
-	(*mem)->bo = amdgpu_bo_ref(bo);
-	(*mem)->va = va;
-	(*mem)->domain = (bo->prefered_domains & AMDGPU_GEM_DOMAIN_VRAM) ?
-		AMDGPU_GEM_DOMAIN_VRAM : AMDGPU_GEM_DOMAIN_GTT;
-	(*mem)->mapped_to_gpu_memory = 0;
-	add_kgd_mem_to_kfd_bo_list(*mem, kfd_vm);
-
-	return 0;
-}
-
-int amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_dev *kgd, void *vm,
-				      struct kgd_mem *mem,
-				      struct dma_buf **dmabuf)
-{
-	struct amdgpu_device *adev = NULL;
-	struct amdgpu_bo *bo = NULL;
-	struct drm_gem_object *gobj = NULL;
-
-	if (!dmabuf || !kgd || !vm || !mem)
-		return -EINVAL;
-
-	adev = get_amdgpu_device(kgd);
-	bo = mem->bo;
-
-	gobj = amdgpu_gem_prime_foreign_bo(adev, bo);
-	if (gobj == NULL) {
-		pr_err("Export BO failed. Unable to find/create GEM object\n");
-		return -EINVAL;
-	}
-
-	*dmabuf = amdgpu_gem_prime_export(adev->ddev, gobj, 0);
-	return 0;
-}
-
-/* Runs out of process context. mem->lock must be held. */
-int amdgpu_amdkfd_gpuvm_evict_mem(struct kgd_mem *mem, struct mm_struct *mm)
-{
-	struct kfd_bo_va_list *entry;
-	unsigned int n_evicted = 0, n_unmapped = 0;
-	int r = 0;
-	struct bo_vm_reservation_context ctx;
-
-	pr_debug("Evicting buffer %p\n", mem);
-
-	if (mem->mapped_to_gpu_memory == 0)
-		return 0;
-
-	/* Remove all GPU mappings of the buffer, but don't change any
-	 * of the is_mapped flags so we can restore it later. The
-	 * queues of the affected GPUs are quiesced first. Count the
-	 * number of evicted mappings so we can roll back if something
-	 * goes wrong. */
-
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
-		struct amdgpu_device *adev;
-
-		if (!entry->is_mapped)
-			continue;
-
-		adev = (struct amdgpu_device *)entry->kgd_dev;
-
-		r = kgd2kfd->quiesce_mm(adev->kfd, mm);
-		if (r != 0) {
-			pr_err("failed to quiesce KFD\n");
-			goto fail;
-		}
-
-		n_evicted++;
-	}
-
-	r = reserve_bo_and_cond_vms(mem, NULL, VA_MAPPED, &ctx);
-	if (unlikely(r != 0))
-		goto fail;
-
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
-		struct amdgpu_device *adev;
-
-		if (!entry->is_mapped)
-			continue;
-
-		adev = (struct amdgpu_device *)entry->kgd_dev;
-
-		r = unmap_bo_from_gpuvm(adev, mem->bo,
-					entry, &ctx.sync);
-		if (r != 0) {
-			pr_err("failed unmap va 0x%llx\n",
-			       mem->va);
-			unreserve_bo_and_vms(&ctx, true);
-			goto fail;
-		}
-
-		n_unmapped++;
-	}
-
-	unreserve_bo_and_vms(&ctx, true);
-
-	return 0;
-
-fail:
-	/* To avoid hangs and keep state consistent, roll back partial
-	 * eviction by restoring queues and marking mappings as
-	 * unmapped. Access to now unmapped buffers will fault. */
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
-		struct amdgpu_device *adev;
-
-		if (n_evicted == 0)
-			break;
-		if (!entry->is_mapped)
-			continue;
-
-		if (n_unmapped) {
-			entry->is_mapped = false;
-			n_unmapped--;
-		}
-
-		adev = (struct amdgpu_device *)entry->kgd_dev;
-		if (kgd2kfd->resume_mm(adev->kfd, mm))
-			pr_err("Failed to resume KFD\n");
-		n_evicted--;
-	}
-
-	return r;
-}
-
-/* Runs out of process context. mem->lock must be held. */
-int amdgpu_amdkfd_gpuvm_restore_mem(struct kgd_mem *mem, struct mm_struct *mm)
-{
-	struct bo_vm_reservation_context ctx;
-	struct kfd_bo_va_list *entry;
-	uint32_t domain;
-	int r, ret = 0;
-	bool have_pages = false;
-
-	pr_debug("Restoring buffer %p\n", mem);
-
-	if (mem->mapped_to_gpu_memory == 0)
-		return 0;
-
-	domain = mem->domain;
-
-	ret = reserve_bo_and_cond_vms(mem, NULL, VA_MAPPED, &ctx);
-	if (likely(ret == 0)) {
-		ret = update_user_pages(mem, mm, &ctx);
-		have_pages = !ret;
-		if (!have_pages)
-			unreserve_bo_and_vms(&ctx, false);
-	}
-
-	/* update_user_pages drops the lock briefly. Check if someone
-	 * else evicted or restored the buffer in the mean time */
-	if (mem->evicted != 1) {
-		unreserve_bo_and_vms(&ctx, false);
-		return 0;
-	}
-
-	/* Try to restore all mappings. Mappings that fail to restore
-	 * will be marked as unmapped. If we failed to get the user
-	 * pages, all mappings will be marked as unmapped. */
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
-		struct amdgpu_device *adev;
-
-		if (!entry->is_mapped)
-			continue;
-
-		adev = (struct amdgpu_device *)entry->kgd_dev;
-
-		if (unlikely(!have_pages)) {
-			pr_err("get_user_pages failed. Probably userptr is freed. %d\n",
-				ret);
-			entry->map_fail = true;
-			continue;
-		}
-
-		r = amdgpu_amdkfd_bo_validate(mem->bo, domain, true);
-		if (unlikely(r != 0)) {
-			pr_err("Failed to validate BO\n");
-			entry->map_fail = true;
-			if (ret == 0)
-				ret = r;
-			continue;
-		}
-
-		r = map_bo_to_gpuvm(adev, mem->bo, entry, mem->pte_flags,
-				&ctx.sync);
-		if (unlikely(r != 0)) {
-			pr_err("Failed to map BO to gpuvm\n");
-			entry->map_fail = true;
-			if (ret == 0)
-				ret = r;
-		}
-	}
-
-	if (have_pages)
-		unreserve_bo_and_vms(&ctx, true);
-
-	/* Resume queues after unreserving the BOs and most
-	 * importantly, waiting for the BO fences to guarantee that
-	 * the page table updates have completed.
-	 */
-	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
-		struct amdgpu_device *adev;
-
-		if (!entry->is_mapped)
-			continue;
-
-		/* Mapping failed. To be in a consistent state, mark the
-		 * buffer as unmapped, but state of the buffer will be
-		 * not evicted. A vm fault will generated if user space tries
-		 * to access this buffer.
-		 */
-		if (entry->map_fail) {
-			entry->is_mapped = false;
-			mem->mapped_to_gpu_memory--;
-		}
-		adev = (struct amdgpu_device *)entry->kgd_dev;
-
-		r = kgd2kfd->resume_mm(adev->kfd, mm);
-		if (r != 0) {
-			pr_err("Failed to resume KFD\n");
-			if (ret == 0)
-				ret = r;
-		}
-	}
-
-	return ret;
-}
-
-/** amdgpu_amdkfd_gpuvm_restore_process_bos - Restore all BOs for the given
- *   KFD process identified by master_vm
- *
- * @master_vm: Master VM of the KFD process
- *
- * After memory eviction, restore thread calls this function. The function
- * should be called when the Process is still valid. BO restore involves -
- *
- * 1.  Release old eviction fence and create new one
- * 2.  Get PD BO list and PT BO list from all the VMs.
- * 3.  Merge PD BO list into KFD BO list. Reserve all BOs.
- * 4.  Split the list into PD BO list and KFD BO list
- * 5.  Validate of PD and PT BOs and add new fence to the BOs
- * 6.  Validate all KFD BOs and Map them and add new fence
- * 7.  Merge PD BO list into KFD BO list. Unreserve all BOs
- * 8.  Restore back KFD BO list by removing PD BO entries
- */
-
-int amdgpu_amdkfd_gpuvm_restore_process_bos(void *m_vm)
-{
-	struct amdgpu_bo_list_entry *pd_bo_list, *last_pd_bo_entry, *entry;
-	struct amdkfd_vm *master_vm = (struct amdkfd_vm *)m_vm, *peer_vm;
-	struct kgd_mem *mem;
-	struct bo_vm_reservation_context ctx;
-	struct amdgpu_amdkfd_fence *old_fence;
-	int ret = 0, i;
-	struct amdgpu_vm_parser param;
-
-	if (WARN_ON(master_vm == NULL || master_vm->master != master_vm))
-		return -EINVAL;
-
-	INIT_LIST_HEAD(&ctx.list);
-	INIT_LIST_HEAD(&ctx.duplicates);
-
-	pd_bo_list = kcalloc(master_vm->n_vms,
-			     sizeof(struct amdgpu_bo_list_entry),
-			     GFP_KERNEL);
-	if (pd_bo_list == NULL)
-		return -ENOMEM;
-
-	/* Release old eviction fence and create new one. Use context and mm
-	 * from the old fence.
-	 */
-	old_fence = master_vm->eviction_fence;
-	master_vm->eviction_fence =
-		amdgpu_amdkfd_fence_create(old_fence->base.context,
-					   old_fence->mm);
-	fence_put(&old_fence->base);
-	if (master_vm->eviction_fence == NULL) {
-		pr_err("Failed to create eviction fence\n");
-		goto evict_fence_fail;
-	}
-
-	/* Get PD BO list and PT BO list from all the VMs */
-	amdgpu_vm_get_pd_bo(&master_vm->base, &ctx.list,
-			    &pd_bo_list[0]);
-#if 0				
-	amdgpu_vm_get_pt_bos(master_vm->adev, &master_vm->base,
-			     &ctx.duplicates);
-#else
-	//param.domain = bo->prefered_domains;
-	param.wait = false;
-	amdgpu_vm_validate_pt_bos(master_vm->adev, &master_vm->base,
-			amdgpu_amdkfd_validate, &param);
-
-#endif			 
-
-	i = 1;
-	list_for_each_entry(peer_vm, &master_vm->kfd_vm_list, kfd_vm_list) {
-		amdgpu_vm_get_pd_bo(&peer_vm->base, &ctx.list,
-				    &pd_bo_list[i]);
-#if 0					
-		amdgpu_vm_get_pt_bos(peer_vm->adev, &peer_vm->base,
-				     &ctx.duplicates);
-#else
-	//param.domain = bo->prefered_domains;
-	param.wait = false;
-	amdgpu_vm_validate_pt_bos(peer_vm->adev, &peer_vm->base,
-			amdgpu_amdkfd_validate, &param);
-#endif				 
-		i++;
-	}
-
-	/* Needed to splicing and cutting the lists */
-	last_pd_bo_entry = list_last_entry(&ctx.list,
-					   struct amdgpu_bo_list_entry,
-					   tv.head);
-
-	/* Reserve all BOs and page tables/directory. */
-	mutex_lock(&master_vm->lock);
-	list_splice_init(&ctx.list, &master_vm->kfd_bo_list);
-	ret = ttm_eu_reserve_buffers(&ctx.ticket, &master_vm->kfd_bo_list,
-				     false, &ctx.duplicates);
-	if (ret) {
-		pr_debug("Memory eviction: TTM Reserve Failed. Try again\n");
-		goto ttm_reserve_fail;
-	}
-
-	/* Restore kfd_bo_list. ctx.list contains only PDs */
-	list_cut_position(&ctx.list, &master_vm->kfd_bo_list,
-			  &last_pd_bo_entry->tv.head);
-
-	amdgpu_sync_create(&ctx.sync);
-
-	/* Validate PDs and PTs */
-	list_for_each_entry(entry, &ctx.list, tv.head) {
-		struct amdgpu_bo *bo = entry->robj;
-
-		ret = amdgpu_amdkfd_bo_validate(bo, bo->prefered_domains,
-						false);
-		if (ret) {
-			pr_debug("Memory eviction: Validate failed. Try again\n");
-			goto validate_map_fail;
-		}
-	}
-	list_for_each_entry(entry, &ctx.duplicates, tv.head) {
-		struct amdgpu_bo *bo = entry->robj;
-
-		ret = amdgpu_amdkfd_bo_validate(bo, bo->prefered_domains,
-						false);
-		if (ret) {
-			pr_debug("Memory eviction: Validate failed. Try again\n");
-			goto validate_map_fail;
-		}
-	}
-
-	/* Wait for PT/PD validate to finish and attach eviction fence.
-	 * PD/PT share the same reservation object
-	 */
-	list_for_each_entry(entry, &ctx.list, tv.head) {
-		struct amdgpu_bo *bo = entry->robj;
-
-		ttm_bo_wait(&bo->tbo, false, false, false);
-	}
-
-
-	/* Validate BOs and map them to GPUVM (update VM page tables). */
-	list_for_each_entry(mem, &master_vm->kfd_bo_list,
-			    bo_list_entry.tv.head) {
-
-		struct amdgpu_bo *bo = mem->bo;
-		uint32_t domain = mem->domain;
-		struct kfd_bo_va_list *bo_va_entry;
-
-		ret = amdgpu_amdkfd_bo_validate(bo, domain, false);
-		if (ret) {
-			pr_debug("Memory eviction: Validate failed. Try again\n");
-			goto validate_map_fail;
-		}
-
-		list_for_each_entry(bo_va_entry, &mem->bo_va_list,
-				    bo_list) {
-			ret = update_gpuvm_pte((struct amdgpu_device *)
-					      bo_va_entry->kgd_dev,
-					      bo, bo_va_entry,
-					      &ctx.sync);
-			if (ret) {
-				pr_debug("Memory eviction: Map failed. Try again\n");
-				goto validate_map_fail;
-			}
-		}
-	}
-
-	amdgpu_sync_wait(&ctx.sync);
-
-	/* Wait for validate to finish and attach new eviction fence */
-	list_for_each_entry(mem, &master_vm->kfd_bo_list,
-		bo_list_entry.tv.head) {
-		struct amdgpu_bo *bo = mem->bo;
-
-		ttm_bo_wait(&bo->tbo, false, false, false);
-		amdgpu_bo_fence(bo, &master_vm->eviction_fence->base, true);
-	}
-	list_for_each_entry(entry, &ctx.list, tv.head) {
-		struct amdgpu_bo *bo = entry->robj;
-
-		amdgpu_bo_fence(bo, &master_vm->eviction_fence->base, true);
-	}
-validate_map_fail:
-	/* Add PDs to kfd_bo_list for unreserve */
-	list_splice_init(&ctx.list, &master_vm->kfd_bo_list);
-	ttm_eu_backoff_reservation(&ctx.ticket, &master_vm->kfd_bo_list);
-	amdgpu_sync_free(&ctx.sync);
-ttm_reserve_fail:
-	/* Restore kfd_bo_list */
-	list_cut_position(&ctx.list, &master_vm->kfd_bo_list,
-			  &last_pd_bo_entry->tv.head);
-	mutex_unlock(&master_vm->lock);
-evict_fence_fail:
-	kfree(pd_bo_list);
-	return ret;
-}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
index d42a976..6c343a9 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c
@@ -34,6 +34,7 @@ struct amdgpu_atpx {
 
 static struct amdgpu_atpx_priv {
 	bool atpx_detected;
+	bool bridge_pm_usable;
 	/* handle for device - and atpx */
 	acpi_handle dhandle;
 	acpi_handle other_handle;
@@ -204,17 +205,12 @@ static int amdgpu_atpx_validate(struct amdgpu_atpx *atpx)
 
 	atpx->is_hybrid = false;
 	if (valid_bits & ATPX_MS_HYBRID_GFX_SUPPORTED) {
-                printk("Hybrid Graphics, ATPX dGPU power cntl disabled\n");
-#if 1
-		/* This is a temporary hack until the D3 cold support
-		 * makes it upstream.  The ATPX power_control method seems
-		 * to still work on even if the system should be using
-		 * the new standardized hybrid D3 cold ACPI interface.
+		printk("ATPX Hybrid Graphics\n");
+		/*
+		 * Disable legacy PM methods only when pcie port PM is usable,
+		 * otherwise the device might fail to power off or power on.
 		 */
-		atpx->functions.power_cntl = true;
-#else
-		atpx->functions.power_cntl = false;
-#endif
+		atpx->functions.power_cntl = !amdgpu_atpx_priv.bridge_pm_usable;
 		atpx->is_hybrid = true;
 	}
 
@@ -564,17 +560,25 @@ static bool amdgpu_atpx_detect(void)
 	struct pci_dev *pdev = NULL;
 	bool has_atpx = false;
 	int vga_count = 0;
+	bool d3_supported = false;
+	struct pci_dev *parent_pdev;
 
 	while ((pdev = pci_get_class(PCI_CLASS_DISPLAY_VGA << 8, pdev)) != NULL) {
 		vga_count++;
 
 		has_atpx |= (amdgpu_atpx_pci_probe_handle(pdev) == true);
+
+		parent_pdev = pci_upstream_bridge(pdev);
+		d3_supported |= parent_pdev && parent_pdev->bridge_d3;
 	}
 
 	while ((pdev = pci_get_class(PCI_CLASS_DISPLAY_OTHER << 8, pdev)) != NULL) {
 		vga_count++;
 
 		has_atpx |= (amdgpu_atpx_pci_probe_handle(pdev) == true);
+
+		parent_pdev = pci_upstream_bridge(pdev);
+		d3_supported |= parent_pdev && parent_pdev->bridge_d3;
 	}
 
 	if (has_atpx && vga_count == 2) {
@@ -582,6 +586,7 @@ static bool amdgpu_atpx_detect(void)
 		printk(KERN_INFO "vga_switcheroo: detected switching method %s handle\n",
 		       acpi_method_name);
 		amdgpu_atpx_priv.atpx_detected = true;
+		amdgpu_atpx_priv.bridge_pm_usable = d3_supported;
 		amdgpu_atpx_init();
 		return true;
 	}
@@ -596,13 +601,14 @@ static bool amdgpu_atpx_detect(void)
 void amdgpu_register_atpx_handler(void)
 {
 	bool r;
+	enum vga_switcheroo_handler_flags_t handler_flags = 0;
 
 	/* detect if we have any ATPX + 2 VGA in the system */
 	r = amdgpu_atpx_detect();
 	if (!r)
 		return;
 
-        vga_switcheroo_register_handler(&amdgpu_atpx_handler);
+	vga_switcheroo_register_handler(&amdgpu_atpx_handler, handler_flags);
 }
 
 /**
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c
index 844214d..c02db01f6 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bo_list.c
@@ -107,7 +107,7 @@ static int amdgpu_bo_list_set(struct amdgpu_device *adev,
 		struct amdgpu_bo *bo;
 		struct mm_struct *usermm;
 
-		gobj = drm_gem_object_lookup(adev->ddev, filp, info[i].bo_handle);
+		gobj = drm_gem_object_lookup(filp, info[i].bo_handle);
 		if (!gobj) {
 			r = -ENOENT;
 			goto error_free;
@@ -132,7 +132,7 @@ static int amdgpu_bo_list_set(struct amdgpu_device *adev,
 		entry->priority = min(info[i].bo_priority,
 				      AMDGPU_BO_LIST_MAX_PRIORITY);
 		entry->tv.bo = &entry->robj->tbo;
-		entry->tv.shared = true;
+		entry->tv.shared = !entry->robj->prime_shared_count;
 
 		if (entry->robj->prefered_domains == AMDGPU_GEM_DOMAIN_GDS)
 			gds_obj = entry->robj;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_connectors.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_connectors.c
index 76a7830..8d1cf2d 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_connectors.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_connectors.c
@@ -168,12 +168,12 @@ int amdgpu_connector_get_monitor_bpc(struct drm_connector *connector)
 		}
 
 		/* Any defined maximum tmds clock limit we must not exceed? */
-		if (connector->max_tmds_clock > 0) {
+		if (connector->display_info.max_tmds_clock > 0) {
 			/* mode_clock is clock in kHz for mode to be modeset on this connector */
 			mode_clock = amdgpu_connector->pixelclock_for_modeset;
 
 			/* Maximum allowable input clock in kHz */
-			max_tmds_clock = connector->max_tmds_clock * 1000;
+			max_tmds_clock = connector->display_info.max_tmds_clock;
 
 			DRM_DEBUG("%s: hdmi mode dotclock %d kHz, max tmds input clock %d kHz.\n",
 				  connector->name, mode_clock, max_tmds_clock);
@@ -765,14 +765,20 @@ amdgpu_connector_lvds_detect(struct drm_connector *connector, bool force)
 	return ret;
 }
 
-static void amdgpu_connector_destroy(struct drm_connector *connector)
+static void amdgpu_connector_unregister(struct drm_connector *connector)
 {
 	struct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);
 
-	if (amdgpu_connector->ddc_bus->has_aux) {
+	if (amdgpu_connector->ddc_bus && amdgpu_connector->ddc_bus->has_aux) {
 		drm_dp_aux_unregister(&amdgpu_connector->ddc_bus->aux);
 		amdgpu_connector->ddc_bus->has_aux = false;
 	}
+}
+
+static void amdgpu_connector_destroy(struct drm_connector *connector)
+{
+	struct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);
+
 	amdgpu_connector_free_edid(connector);
 	kfree(amdgpu_connector->con_priv);
 	drm_connector_unregister(connector);
@@ -826,6 +832,7 @@ static const struct drm_connector_funcs amdgpu_connector_lvds_funcs = {
 	.dpms = drm_helper_connector_dpms,
 	.detect = amdgpu_connector_lvds_detect,
 	.fill_modes = drm_helper_probe_single_connector_modes,
+	.early_unregister = amdgpu_connector_unregister,
 	.destroy = amdgpu_connector_destroy,
 	.set_property = amdgpu_connector_set_lcd_property,
 };
@@ -936,6 +943,7 @@ static const struct drm_connector_funcs amdgpu_connector_vga_funcs = {
 	.dpms = drm_helper_connector_dpms,
 	.detect = amdgpu_connector_vga_detect,
 	.fill_modes = drm_helper_probe_single_connector_modes,
+	.early_unregister = amdgpu_connector_unregister,
 	.destroy = amdgpu_connector_destroy,
 	.set_property = amdgpu_connector_set_property,
 };
@@ -1203,6 +1211,7 @@ static const struct drm_connector_funcs amdgpu_connector_dvi_funcs = {
 	.detect = amdgpu_connector_dvi_detect,
 	.fill_modes = drm_helper_probe_single_connector_modes,
 	.set_property = amdgpu_connector_set_property,
+	.early_unregister = amdgpu_connector_unregister,
 	.destroy = amdgpu_connector_destroy,
 	.force = amdgpu_connector_dvi_force,
 };
@@ -1493,6 +1502,7 @@ static const struct drm_connector_funcs amdgpu_connector_dp_funcs = {
 	.detect = amdgpu_connector_dp_detect,
 	.fill_modes = drm_helper_probe_single_connector_modes,
 	.set_property = amdgpu_connector_set_property,
+	.early_unregister = amdgpu_connector_unregister,
 	.destroy = amdgpu_connector_destroy,
 	.force = amdgpu_connector_dvi_force,
 };
@@ -1502,6 +1512,7 @@ static const struct drm_connector_funcs amdgpu_connector_edp_funcs = {
 	.detect = amdgpu_connector_dp_detect,
 	.fill_modes = drm_helper_probe_single_connector_modes,
 	.set_property = amdgpu_connector_set_lcd_property,
+	.early_unregister = amdgpu_connector_unregister,
 	.destroy = amdgpu_connector_destroy,
 	.force = amdgpu_connector_dvi_force,
 };
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
index b1617be..99e1e8b 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c
@@ -30,47 +30,6 @@
 #include "amdgpu.h"
 #include "amdgpu_trace.h"
 
-#define AMDGPU_CS_MAX_PRIORITY		32u
-#define AMDGPU_CS_NUM_BUCKETS		(AMDGPU_CS_MAX_PRIORITY + 1)
-
-/* This is based on the bucket sort with O(n) time complexity.
- * An item with priority "i" is added to bucket[i]. The lists are then
- * concatenated in descending order.
- */
-struct amdgpu_cs_buckets {
-	struct list_head bucket[AMDGPU_CS_NUM_BUCKETS];
-};
-
-static void amdgpu_cs_buckets_init(struct amdgpu_cs_buckets *b)
-{
-	unsigned i;
-
-	for (i = 0; i < AMDGPU_CS_NUM_BUCKETS; i++)
-		INIT_LIST_HEAD(&b->bucket[i]);
-}
-
-static void amdgpu_cs_buckets_add(struct amdgpu_cs_buckets *b,
-				  struct list_head *item, unsigned priority)
-{
-	/* Since buffers which appear sooner in the relocation list are
-	 * likely to be used more often than buffers which appear later
-	 * in the list, the sort mustn't change the ordering of buffers
-	 * with the same priority, i.e. it must be stable.
-	 */
-	list_add_tail(item, &b->bucket[min(priority, AMDGPU_CS_MAX_PRIORITY)]);
-}
-
-static void amdgpu_cs_buckets_get_list(struct amdgpu_cs_buckets *b,
-				       struct list_head *out_list)
-{
-	unsigned i;
-
-	/* Connect the sorted buckets in the output list. */
-	for (i = 0; i < AMDGPU_CS_NUM_BUCKETS; i++) {
-		list_splice(&b->bucket[i], out_list);
-	}
-}
-
 int amdgpu_cs_get_ring(struct amdgpu_device *adev, u32 ip_type,
 		       u32 ip_instance, u32 ring,
 		       struct amdgpu_ring **out_ring)
@@ -134,7 +93,7 @@ static int amdgpu_cs_user_fence_chunk(struct amdgpu_cs_parser *p,
 	struct drm_gem_object *gobj;
 	unsigned long size;
 
-	gobj = drm_gem_object_lookup(p->adev->ddev, p->filp, data->handle);
+	gobj = drm_gem_object_lookup(p->filp, data->handle);
 	if (gobj == NULL)
 		return -EINVAL;
 
@@ -1215,7 +1174,7 @@ static int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,
 				     struct drm_amdgpu_fence *fences)
 {
 	uint32_t fence_count = wait->in.fence_count;
-	unsigned i;
+	unsigned int i;
 	long r = 1;
 
 	for (i = 0; i < fence_count; i++) {
@@ -1259,12 +1218,12 @@ static int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,
 	uint32_t fence_count = wait->in.fence_count;
 	uint32_t first = ~0;
 	struct fence **array;
-	unsigned i;
+	unsigned int i;
 	long r;
 
 	/* Prepare the fence array */
-	array = (struct fence **)kcalloc(fence_count, sizeof(struct fence *),
-			GFP_KERNEL);
+	array = kcalloc(fence_count, sizeof(struct fence *), GFP_KERNEL);
+
 	if (array == NULL)
 		return -ENOMEM;
 
@@ -1283,7 +1242,8 @@ static int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,
 		}
 	}
 
-	r = fence_wait_any_timeout(array, fence_count, true, timeout, &first);
+	r = fence_wait_any_timeout(array, fence_count, true, timeout,
+				   &first);
 	if (r < 0)
 		goto err_free_fence_array;
 
@@ -1370,11 +1330,7 @@ amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,
 		struct amdgpu_bo_list_entry *lobj;
 
 		lobj = &parser->bo_list->array[i];
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)		
-		if (!lobj->bo_va || amdgpu_ttm_adev(lobj->bo_va->bo->tbo.bdev) != parser->adev)
-#else
 		if (!lobj->bo_va)
-#endif
 			continue;
 
 		list_for_each_entry(mapping, &lobj->bo_va->valids, list) {
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
index 66e95ac..9a76614 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
@@ -52,9 +52,7 @@
 #include "bif/bif_4_1_d.h"
 #include <linux/pci.h>
 #include <linux/firmware.h>
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#include "amdgpu_amdkfd.h"
-#endif
+
 static int amdgpu_debugfs_regs_init(struct amdgpu_device *adev);
 static void amdgpu_debugfs_regs_cleanup(struct amdgpu_device *adev);
 
@@ -953,11 +951,6 @@ static bool amdgpu_check_pot_argument(int arg)
  */
 static void amdgpu_check_arguments(struct amdgpu_device *adev)
 {
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	struct sysinfo si;
-	int phys_ram_gb, amdgpu_vm_size_aligned;
-#endif
-
 	if (amdgpu_sched_jobs < 4) {
 		dev_warn(adev->dev, "sched jobs (%d) must be at least 4\n",
 			 amdgpu_sched_jobs);
@@ -977,30 +970,6 @@ static void amdgpu_check_arguments(struct amdgpu_device *adev)
 		}
 	}
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* Compute the GPU VM space only if the user
-	 * hasn't changed it from the default.
-	 */
-	if (amdgpu_vm_size == -1) {
-		/* Computation depends on the amount of physical RAM available.
-		 * Cannot exceed 1TB.
-		 */
-		si_meminfo(&si);
-		phys_ram_gb = ((uint64_t)si.totalram * si.mem_unit) >> 30;
-		amdgpu_vm_size = min(phys_ram_gb * 3 + 16, 1024);
-
-		/* GPUVM sizes are almost never perfect powers of two.
-		 * Round up to nearest power of two starting from
-		 * the minimum allowed but aligned size of 32GB */
-		amdgpu_vm_size_aligned = 32;
-		while (amdgpu_vm_size > amdgpu_vm_size_aligned)
-			amdgpu_vm_size_aligned *= 2;
-
-		amdgpu_vm_size = amdgpu_vm_size_aligned;
-	}
-#endif
-
-
 	if (!amdgpu_check_pot_argument(amdgpu_vm_size)) {
 		dev_warn(adev->dev, "VM size (%d) must be a power of 2\n",
 			 amdgpu_vm_size);
@@ -1551,7 +1520,7 @@ static int amdgpu_fini(struct amdgpu_device *adev)
 	return 0;
 }
 
-static int amdgpu_suspend(struct amdgpu_device *adev)
+int amdgpu_suspend(struct amdgpu_device *adev)
 {
 	int i, r;
 
@@ -1604,28 +1573,36 @@ static int amdgpu_resume(struct amdgpu_device *adev)
 	return 0;
 }
 
+static void amdgpu_device_detect_sriov_bios(struct amdgpu_device *adev)
+{
+	if (amdgpu_atombios_has_gpu_virtualization_table(adev))
+		adev->virtualization.virtual_caps |= AMDGPU_SRIOV_CAPS_SRIOV_VBIOS;
+}
+
 bool amdgpu_device_asic_has_dc_support(enum amd_asic_type asic_type)
 {
 	switch (asic_type) {
+#if defined(CONFIG_DRM_AMD_DC)
 	case CHIP_BONAIRE:
 	case CHIP_HAWAII:
 		return amdgpu_dc != 0;
 	case CHIP_CARRIZO:
 	case CHIP_STONEY:
 	case CHIP_POLARIS11:
-	case CHIP_POLARIS12:
 	case CHIP_POLARIS10:
+	case CHIP_POLARIS12:
 		return amdgpu_dc != 0;
 	case CHIP_TONGA:
 	case CHIP_FIJI:
 		return amdgpu_dc != 0;
+#endif
 	default:
 		return false;
 	}
 }
 
 /**
- * amdgpu_device_has_dc_support - check if dal is supported
+ * amdgpu_device_has_dc_support - check if dc is supported
  *
  * @adev: amdgpu_device_pointer
  *
@@ -1636,12 +1613,6 @@ bool amdgpu_device_has_dc_support(struct amdgpu_device *adev)
 	return amdgpu_device_asic_has_dc_support(adev->asic_type);
 }
 
-static void amdgpu_device_detect_sriov_bios(struct amdgpu_device *adev)
-{
-	if (amdgpu_atombios_has_gpu_virtualization_table(adev))
-		adev->virtualization.virtual_caps |= AMDGPU_SRIOV_CAPS_SRIOV_VBIOS;
-}
-
 /**
  * amdgpu_device_init - initialize the driver
  *
@@ -1928,6 +1899,7 @@ void amdgpu_device_fini(struct amdgpu_device *adev)
 
 	DRM_INFO("amdgpu: finishing device.\n");
 	adev->shutdown = true;
+	drm_crtc_force_disable_all(adev->ddev);
 	/* evict vram memory */
 	amdgpu_bo_evict_vram(adev);
 	amdgpu_ib_pool_fini(adev);
@@ -1983,8 +1955,7 @@ int amdgpu_device_suspend(struct drm_device *dev, bool suspend, bool fbcon)
 
 	adev = dev->dev_private;
 
-	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF ||
-	    dev->switch_power_state == DRM_SWITCH_POWER_DYNAMIC_OFF)
+	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
 		return 0;
 
 	drm_kms_helper_poll_disable(dev);
@@ -1998,10 +1969,6 @@ int amdgpu_device_suspend(struct drm_device *dev, bool suspend, bool fbcon)
 		drm_modeset_unlock_all(dev);
 	}
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	amdgpu_amdkfd_suspend(adev);
-#endif
-
 	/* unpin the front buffers and cursors */
 	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
 		struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
@@ -2079,8 +2046,7 @@ int amdgpu_device_resume(struct drm_device *dev, bool resume, bool fbcon)
 	struct drm_crtc *crtc;
 	int r;
 
-	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF ||
-	    dev->switch_power_state == DRM_SWITCH_POWER_DYNAMIC_OFF)
+	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
 		return 0;
 
 	if (fbcon)
@@ -2141,11 +2107,6 @@ int amdgpu_device_resume(struct drm_device *dev, bool resume, bool fbcon)
 			}
 		}
 	}
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	r = amdgpu_amdkfd_resume(adev);
-	if (r)
-		return r;
-#endif
 
 	/* blat the mode back in */
 	if (fbcon) {
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
index fb00b3b..a2ed4ed 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_display.c
@@ -41,7 +41,7 @@ static void amdgpu_flip_callback(struct fence *f, struct fence_cb *cb)
 		container_of(cb, struct amdgpu_flip_work, cb);
 
 	fence_put(f);
-	schedule_work(&work->flip_work);
+	schedule_work(&work->flip_work.work);
 }
 
 static bool amdgpu_flip_handle_fence(struct amdgpu_flip_work *work,
@@ -63,16 +63,17 @@ static bool amdgpu_flip_handle_fence(struct amdgpu_flip_work *work,
 
 static void amdgpu_flip_work_func(struct work_struct *__work)
 {
+	struct delayed_work *delayed_work =
+		container_of(__work, struct delayed_work, work);
 	struct amdgpu_flip_work *work =
-		container_of(__work, struct amdgpu_flip_work, flip_work);
+		container_of(delayed_work, struct amdgpu_flip_work, flip_work);
 	struct amdgpu_device *adev = work->adev;
-	struct amdgpu_crtc *amdgpuCrtc = adev->mode_info.crtcs[work->crtc_id];
+	struct amdgpu_crtc *amdgpu_crtc = adev->mode_info.crtcs[work->crtc_id];
 
-	struct drm_crtc *crtc = &amdgpuCrtc->base;
+	struct drm_crtc *crtc = &amdgpu_crtc->base;
 	unsigned long flags;
-	unsigned i, repcnt = 4;
-	int vpos, hpos, stat, min_udelay = 0;
-	struct drm_vblank_crtc *vblank = &crtc->dev->vblank[work->crtc_id];
+	unsigned i;
+	int vpos, hpos;
 
 	if (amdgpu_flip_handle_fence(work, &work->excl))
 		return;
@@ -81,66 +82,34 @@ static void amdgpu_flip_work_func(struct work_struct *__work)
 		if (amdgpu_flip_handle_fence(work, &work->shared[i]))
 			return;
 
-	/* We borrow the event spin lock for protecting flip_status */
-	spin_lock_irqsave(&crtc->dev->event_lock, flags);
-
-	/* If this happens to execute within the "virtually extended" vblank
-	 * interval before the start of the real vblank interval then it needs
-	 * to delay programming the mmio flip until the real vblank is entered.
-	 * This prevents completing a flip too early due to the way we fudge
-	 * our vblank counter and vblank timestamps in order to work around the
-	 * problem that the hw fires vblank interrupts before actual start of
-	 * vblank (when line buffer refilling is done for a frame). It
-	 * complements the fudging logic in amdgpu_get_crtc_scanoutpos() for
-	 * timestamping and amdgpu_get_vblank_counter_kms() for vblank counts.
-	 *
-	 * In practice this won't execute very often unless on very fast
-	 * machines because the time window for this to happen is very small.
+	/* Wait until we're out of the vertical blank period before the one
+	 * targeted by the flip
 	 */
-	while (amdgpuCrtc->enabled && --repcnt) {
-		/* GET_DISTANCE_TO_VBLANKSTART returns distance to real vblank
-		 * start in hpos, and to the "fudged earlier" vblank start in
-		 * vpos.
-		 */
-		stat = amdgpu_get_crtc_scanoutpos(adev->ddev, work->crtc_id,
-						  GET_DISTANCE_TO_VBLANKSTART,
-						  &vpos, &hpos, NULL, NULL,
-						  &crtc->hwmode);
-
-		if ((stat & (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_ACCURATE)) !=
-		    (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_ACCURATE) ||
-		    !(vpos >= 0 && hpos <= 0))
-			break;
-
-		/* Sleep at least until estimated real start of hw vblank */
-		min_udelay = (-hpos + 1) * max(vblank->linedur_ns / 1000, 5);
-		if (min_udelay > vblank->framedur_ns / 2000) {
-			/* Don't wait ridiculously long - something is wrong */
-			repcnt = 0;
-			break;
-		}
-		spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
-		usleep_range(min_udelay, 2 * min_udelay);
-		spin_lock_irqsave(&crtc->dev->event_lock, flags);
+	if (amdgpu_crtc->enabled &&
+	    (amdgpu_get_crtc_scanoutpos(adev->ddev, work->crtc_id, 0,
+					&vpos, &hpos, NULL, NULL,
+					&crtc->hwmode)
+	     & (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_IN_VBLANK)) ==
+	    (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_IN_VBLANK) &&
+	    (int)(work->target_vblank -
+		  amdgpu_get_vblank_counter_kms(adev->ddev, amdgpu_crtc->crtc_id)) > 0) {
+		schedule_delayed_work(&work->flip_work, usecs_to_jiffies(1000));
+		return;
 	}
 
-	if (!repcnt)
-		DRM_DEBUG_DRIVER("Delay problem on crtc %d: min_udelay %d, "
-				 "framedur %d, linedur %d, stat %d, vpos %d, "
-				 "hpos %d\n", work->crtc_id, min_udelay,
-				 vblank->framedur_ns / 1000,
-				 vblank->linedur_ns / 1000, stat, vpos, hpos);
+	/* We borrow the event spin lock for protecting flip_status */
+	spin_lock_irqsave(&crtc->dev->event_lock, flags);
 
 	/* Do the flip (mmio) */
 	adev->mode_info.funcs->page_flip(adev, work->crtc_id, work->base, work->async);
 
 	/* Set the flip status */
-	amdgpuCrtc->pflip_status = AMDGPU_FLIP_SUBMITTED;
+	amdgpu_crtc->pflip_status = AMDGPU_FLIP_SUBMITTED;
 	spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
 
 
 	DRM_DEBUG_DRIVER("crtc:%d[%p], pflip_stat:AMDGPU_FLIP_SUBMITTED, work: %p,\n",
-					 amdgpuCrtc->crtc_id, amdgpuCrtc, work);
+					 amdgpu_crtc->crtc_id, amdgpu_crtc, work);
 
 }
 
@@ -169,10 +138,10 @@ static void amdgpu_unpin_work_func(struct work_struct *__work)
 	kfree(work);
 }
 
-int amdgpu_crtc_page_flip(struct drm_crtc *crtc,
-			  struct drm_framebuffer *fb,
-			  struct drm_pending_vblank_event *event,
-			  uint32_t page_flip_flags)
+int amdgpu_crtc_page_flip_target(struct drm_crtc *crtc,
+				 struct drm_framebuffer *fb,
+				 struct drm_pending_vblank_event *event,
+				 uint32_t page_flip_flags, uint32_t target)
 {
 	struct drm_device *dev = crtc->dev;
 	struct amdgpu_device *adev = dev->dev_private;
@@ -191,7 +160,7 @@ int amdgpu_crtc_page_flip(struct drm_crtc *crtc,
 	if (work == NULL)
 		return -ENOMEM;
 
-	INIT_WORK(&work->flip_work, amdgpu_flip_work_func);
+	INIT_DELAYED_WORK(&work->flip_work, amdgpu_flip_work_func);
 	INIT_WORK(&work->unpin_work, amdgpu_unpin_work_func);
 
 	work->event = event;
@@ -211,14 +180,6 @@ int amdgpu_crtc_page_flip(struct drm_crtc *crtc,
 	obj = new_amdgpu_fb->obj;
 	new_abo = gem_to_amdgpu_bo(obj);
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	if (amdgpu_ttm_adev(new_abo->tbo.bdev) != adev) {
-		DRM_ERROR("Foreign BOs not allowed in the display engine\n");
-		r = -EINVAL;
-		goto cleanup;
-	}
-#endif
-
 	/* pin the new buffer */
 	r = amdgpu_bo_reserve(new_abo, false);
 	if (unlikely(r != 0)) {
@@ -245,12 +206,8 @@ int amdgpu_crtc_page_flip(struct drm_crtc *crtc,
 	amdgpu_bo_unreserve(new_abo);
 
 	work->base = base;
-
-	r = drm_vblank_get(crtc->dev, amdgpu_crtc->crtc_id);
-	if (r) {
-		DRM_ERROR("failed to get vblank before flip\n");
-		goto pflip_cleanup;
-	}
+	work->target_vblank = target - drm_crtc_vblank_count(crtc) +
+		amdgpu_get_vblank_counter_kms(dev, work->crtc_id);
 
 	/* we borrow the event spin lock for protecting flip_wrok */
 	spin_lock_irqsave(&crtc->dev->event_lock, flags);
@@ -258,7 +215,7 @@ int amdgpu_crtc_page_flip(struct drm_crtc *crtc,
 		DRM_DEBUG_DRIVER("flip queue: crtc already busy\n");
 		spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
 		r = -EBUSY;
-		goto vblank_cleanup;
+		goto pflip_cleanup;
 	}
 
 	amdgpu_crtc->pflip_status = AMDGPU_FLIP_PENDING;
@@ -270,12 +227,9 @@ int amdgpu_crtc_page_flip(struct drm_crtc *crtc,
 	/* update crtc fb */
 	crtc->primary->fb = fb;
 	spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
-	amdgpu_flip_work_func(&work->flip_work);
+	amdgpu_flip_work_func(&work->flip_work.work);
 	return 0;
 
-vblank_cleanup:
-	drm_vblank_put(crtc->dev, amdgpu_crtc->crtc_id);
-
 pflip_cleanup:
 	if (unlikely(amdgpu_bo_reserve(new_abo, false) != 0)) {
 		DRM_ERROR("failed to reserve new abo in error path\n");
@@ -572,7 +526,7 @@ amdgpu_user_framebuffer_create(struct drm_device *dev,
 	struct amdgpu_framebuffer *amdgpu_fb;
 	int ret;
 
-	obj = drm_gem_object_lookup(dev, file_priv, mode_cmd->handles[0]);
+	obj = drm_gem_object_lookup(file_priv, mode_cmd->handles[0]);
 	if (obj ==  NULL) {
 		dev_err(&dev->pdev->dev, "No GEM object associated to handle 0x%08X, "
 			"can't create framebuffer\n", mode_cmd->handles[0]);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h
index a6f9e5d..6c58c7d 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dpm.h
@@ -266,16 +266,6 @@ struct amdgpu_dpm_funcs {
 	int (*set_sclk_od)(struct amdgpu_device *adev, uint32_t value);
 	int (*get_mclk_od)(struct amdgpu_device *adev);
 	int (*set_mclk_od)(struct amdgpu_device *adev, uint32_t value);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	int (*reset_power_profile_state)(struct amdgpu_device *adev,
-			struct pp_profile *request);
-	int (*get_power_profile_state)(struct amdgpu_device *adev,
-			struct pp_profile *query);
-	int (*set_power_profile_state)(struct amdgpu_device *adev,
-			struct pp_profile *request);
-	int (*switch_power_profile)(struct amdgpu_device *adev,
-			enum pp_profile_type type);
-#endif
 	int (*check_state_equal)(struct amdgpu_device *adev,
 				struct amdgpu_ps *cps,
 				struct amdgpu_ps *rps,
@@ -383,24 +373,6 @@ struct amdgpu_dpm_funcs {
 #define amdgpu_dpm_set_mclk_od(adev, value) \
 	((adev)->powerplay.pp_funcs->set_mclk_od((adev)->powerplay.pp_handle, value))
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#define amdgpu_dpm_reset_power_profile_state(adev, request) \
-	((adev)->powerplay.pp_funcs->reset_power_profile_state(\
-			(adev)->powerplay.pp_handle, request))
-
-#define amdgpu_dpm_get_power_profile_state(adev, query) \
-	((adev)->powerplay.pp_funcs->get_power_profile_state(\
-			(adev)->powerplay.pp_handle, query))
-
-#define amdgpu_dpm_set_power_profile_state(adev, request) \
-	((adev)->powerplay.pp_funcs->set_power_profile_state(\
-			(adev)->powerplay.pp_handle, request))
-
-#define amdgpu_dpm_switch_power_profile(adev, type) \
-	((adev)->powerplay.pp_funcs->switch_power_profile(\
-			(adev)->powerplay.pp_handle, type))
-#endif
-
 #define amdgpu_dpm_dispatch_task(adev, event_id, input, output)		\
 	(adev)->powerplay.pp_funcs->dispatch_tasks((adev)->powerplay.pp_handle, (event_id), (input), (output))
 
@@ -490,7 +462,7 @@ struct amdgpu_pm {
 	const struct amdgpu_dpm_funcs *funcs;
 	uint32_t                pcie_gen_mask;
 	uint32_t                pcie_mlw_mask;
-	struct amd_pp_display_configuration pm_display_cfg;/* set by DC */
+	struct amd_pp_display_configuration pm_display_cfg;/* set by dc */
 };
 
 #define R600_SSTU_DFLT                               0
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
index c3ffb9e..cf90913 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
@@ -82,17 +82,10 @@ int amdgpu_runtime_pm = -1;
 unsigned amdgpu_ip_block_mask = 0xffffffff;
 int amdgpu_bapm = -1;
 int amdgpu_deep_color = 0;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-int amdgpu_vm_size = -1;
-#else
 int amdgpu_vm_size = 64;
-#endif
 int amdgpu_vm_block_size = -1;
 int amdgpu_vm_fault_stop = 0;
 int amdgpu_vm_debug = 0;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-int amdgpu_vm_update_context = 2;
-#endif
 int amdgpu_vram_page_split = 1024;
 int amdgpu_exp_hw_support = 0;
 int amdgpu_dc = -1;
@@ -108,9 +101,7 @@ unsigned amdgpu_pg_mask = 0xffffffff;
 char *amdgpu_disable_cu = NULL;
 char *amdgpu_virtual_display = NULL;
 unsigned amdgpu_pp_feature_mask = 0xffffffff;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-unsigned amdgpu_sdma_phase_quantum = 32;
-#endif
+
 MODULE_PARM_DESC(vramlimit, "Restrict VRAM for testing, in megabytes");
 module_param_named(vramlimit, amdgpu_vram_limit, int, 0600);
 
@@ -177,18 +168,13 @@ module_param_named(vm_fault_stop, amdgpu_vm_fault_stop, int, 0444);
 MODULE_PARM_DESC(vm_debug, "Debug VM handling (0 = disabled (default), 1 = enabled)");
 module_param_named(vm_debug, amdgpu_vm_debug, int, 0644);
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-MODULE_PARM_DESC(vm_update_context, "VM update using CPU on large BAR. (0 = never, 1 = Graphics only, 2 = Compute only (default), 3 = Both");
-module_param_named(vm_update_context, amdgpu_vm_update_context, int, 0444);
-#endif
-
 MODULE_PARM_DESC(vram_page_split, "Number of pages after we split VRAM allocations (default 1024, -1 = disable)");
 module_param_named(vram_page_split, amdgpu_vram_page_split, int, 0444);
 
 MODULE_PARM_DESC(exp_hw_support, "experimental hw support (1 = enable, 0 = disable (default))");
 module_param_named(exp_hw_support, amdgpu_exp_hw_support, int, 0444);
 
-MODULE_PARM_DESC(dc, "display core driver (1 = enable, 0 = disable, -1 = auto (default))");
+MODULE_PARM_DESC(dc, "Display Core driver (1 = enable, 0 = disable, -1 = auto (default))");
 module_param_named(dc, amdgpu_dc, int, 0444);
 
 MODULE_PARM_DESC(sched_jobs, "the max number of jobs supported in the sw queue (default 32)");
@@ -224,10 +210,7 @@ module_param_named(disable_cu, amdgpu_disable_cu, charp, 0444);
 MODULE_PARM_DESC(virtual_display,
 		 "Enable virtual display feature (the virtual_display will be set like xxxx:xx:xx.x,x;xxxx:xx:xx.x,x)");
 module_param_named(virtual_display, amdgpu_virtual_display, charp, 0444);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-MODULE_PARM_DESC(sdma_phase_quantum, "SDMA context switch phase quantum (x 1K GPU clock cycles, 0 = no change (default 32))");
-module_param_named(sdma_phase_quantum, amdgpu_sdma_phase_quantum, uint, 0444);
-#endif
+
 static const struct pci_device_id pciidlist[] = {
 #ifdef  CONFIG_DRM_AMDGPU_SI
 	{0x1002, 0x6780, PCI_ANY_ID, PCI_ANY_ID, 0, 0, CHIP_TAHITI},
@@ -466,7 +449,7 @@ static int amdgpu_kick_out_firmware_fb(struct pci_dev *pdev)
 #ifdef CONFIG_X86
 	primary = pdev->resource[PCI_ROM_RESOURCE].flags & IORESOURCE_ROM_SHADOW;
 #endif
-	remove_conflicting_framebuffers(ap, "amdgpudrmfb", primary);
+	drm_fb_helper_remove_conflicting_framebuffers(ap, "amdgpudrmfb", primary);
 	kfree(ap);
 
 	return 0;
@@ -484,6 +467,14 @@ static int amdgpu_pci_probe(struct pci_dev *pdev,
 		return -ENODEV;
 	}
 
+	/*
+	 * Initialize amdkfd before starting radeon. If it was not loaded yet,
+	 * defer radeon probing
+	 */
+	ret = amdgpu_amdkfd_init();
+	if (ret == -EPROBE_DEFER)
+		return ret;
+
 	/* Get rid of things like offb */
 	ret = amdgpu_kick_out_firmware_fb(pdev);
 	if (ret)
@@ -503,12 +494,15 @@ amdgpu_pci_remove(struct pci_dev *pdev)
 static void
 amdgpu_pci_shutdown(struct pci_dev *pdev)
 {
+	struct drm_device *dev = pci_get_drvdata(pdev);
+	struct amdgpu_device *adev = dev->dev_private;
+
 	/* if we are running in a VM, make sure the device
 	 * torn down properly on reboot/shutdown.
 	 * unfortunately we can't detect certain
 	 * hypervisors so just do this all the time.
 	 */
-	amdgpu_pci_remove(pdev);
+	amdgpu_suspend(adev);
 }
 
 static int amdgpu_pmops_suspend(struct device *dev)
@@ -726,11 +720,7 @@ static struct drm_driver kms_driver = {
 	.prime_handle_to_fd = drm_gem_prime_handle_to_fd,
 	.prime_fd_to_handle = drm_gem_prime_fd_to_handle,
 	.gem_prime_export = amdgpu_gem_prime_export,
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	.gem_prime_import = amdgpu_gem_prime_import,
-#else
 	.gem_prime_import = drm_gem_prime_import,
-#endif
 	.gem_prime_pin = amdgpu_gem_prime_pin,
 	.gem_prime_unpin = amdgpu_gem_prime_unpin,
 	.gem_prime_res_obj = amdgpu_gem_prime_res_obj,
@@ -786,9 +776,6 @@ static int __init amdgpu_init(void)
 	pdriver = &amdgpu_kms_pci_driver;
 	driver->num_ioctls = amdgpu_max_kms_ioctl;
 	amdgpu_register_atpx_handler();
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
-	amdgpu_amdkfd_init();
-#endif
 	/* let modprobe override vga console setting */
 	return drm_pci_init(driver, pdriver);
 
@@ -804,9 +791,7 @@ error_sync:
 
 static void __exit amdgpu_exit(void)
 {
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	amdgpu_amdkfd_fini();
-#endif
 	drm_pci_exit(driver, pdriver);
 	amdgpu_unregister_atpx_handler();
 	amdgpu_sync_fini();
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_fb.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_fb.c
index 73c8449..1e735c4 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_fb.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_fb.c
@@ -25,7 +25,6 @@
  */
 #include <linux/module.h>
 #include <linux/slab.h>
-#include <linux/fb.h>
 #include <linux/pm_runtime.h>
 
 #include <drm/drmP.h>
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c
index f11c845..5772ef2 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c
@@ -633,7 +633,7 @@ static int amdgpu_debugfs_fence_info(struct seq_file *m, void *data)
 		seq_printf(m, "--- ring %d (%s) ---\n", i, ring->name);
 		seq_printf(m, "Last signaled fence 0x%08x\n",
 			   atomic_read(&ring->fence_drv.last_seq));
-		seq_printf(m, "Last emitted	   0x%08x\n",
+		seq_printf(m, "Last emitted        0x%08x\n",
 			   ring->fence_drv.sync_seq);
 	}
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
index 6e7329e..c0820de 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
@@ -33,22 +33,6 @@
 
 void amdgpu_gem_object_free(struct drm_gem_object *gobj)
 {
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	struct amdgpu_gem_object *aobj;
-
-	aobj = container_of((gobj), struct amdgpu_gem_object, base);
-	if (aobj->base.import_attach)
-		drm_prime_gem_destroy(&aobj->base, aobj->bo->tbo.sg);
-
-	ww_mutex_lock(&aobj->bo->tbo.resv->lock, NULL);
-	list_del(&aobj->list);
-	ww_mutex_unlock(&aobj->bo->tbo.resv->lock);
-
-	amdgpu_mn_unregister(aobj->bo);
-	amdgpu_bo_unref(&aobj->bo);
-	drm_gem_object_release(&aobj->base);
-	kfree(aobj);
-#else
 	struct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);
 
 	if (robj) {
@@ -57,7 +41,6 @@ void amdgpu_gem_object_free(struct drm_gem_object *gobj)
 		amdgpu_mn_unregister(robj);
 		amdgpu_bo_unref(&robj);
 	}
-#endif
 }
 
 int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,
@@ -66,9 +49,6 @@ int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,
 				struct drm_gem_object **obj)
 {
 	struct amdgpu_bo *robj;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	struct amdgpu_gem_object *gobj;
-#endif
 	unsigned long max_size;
 	int r;
 
@@ -103,26 +83,7 @@ retry:
 		}
 		return r;
 	}
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	*obj = &robj->gem_base;
-#else
-	gobj = kzalloc(sizeof(struct amdgpu_gem_object), GFP_KERNEL);
-	if (unlikely(!gobj)) {
-		amdgpu_bo_unref(&robj);
-		return -ENOMEM;
-	}
-
-	r = drm_gem_object_init(adev->ddev, &gobj->base, amdgpu_bo_size(robj));
-	if (unlikely(r)) {
-		kfree(gobj);
-		amdgpu_bo_unref(&robj);
-		return r;
-	}
-
-	list_add(&gobj->list, &robj->gem_objects);
-	gobj->bo = robj;
-	*obj = &gobj->base;
-#endif
 
 	return 0;
 }
@@ -378,7 +339,7 @@ int amdgpu_mode_dumb_mmap(struct drm_file *filp,
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *robj;
 
-	gobj = drm_gem_object_lookup(dev, filp, handle);
+	gobj = drm_gem_object_lookup(filp, handle);
 	if (gobj == NULL) {
 		return -ENOENT;
 	}
@@ -442,7 +403,7 @@ int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
 	int r = 0;
 	long ret;
 
-	gobj = drm_gem_object_lookup(dev, filp, handle);
+	gobj = drm_gem_object_lookup(filp, handle);
 	if (gobj == NULL) {
 		return -ENOENT;
 	}
@@ -476,7 +437,7 @@ int amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,
 	int r = -1;
 
 	DRM_DEBUG("%d \n", args->handle);
-	gobj = drm_gem_object_lookup(dev, filp, args->handle);
+	gobj = drm_gem_object_lookup(filp, args->handle);
 	if (gobj == NULL)
 		return -ENOENT;
 	robj = gem_to_amdgpu_bo(gobj);
@@ -630,7 +591,7 @@ int amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,
 		return -EINVAL;
 	}
 
-	gobj = drm_gem_object_lookup(dev, filp, args->handle);
+	gobj = drm_gem_object_lookup(filp, args->handle);
 	if (gobj == NULL)
 		return -ENOENT;
 	abo = gem_to_amdgpu_bo(gobj);
@@ -690,7 +651,7 @@ int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
 	struct amdgpu_bo *robj;
 	int r;
 
-	gobj = drm_gem_object_lookup(dev, filp, args->handle);
+	gobj = drm_gem_object_lookup(filp, args->handle);
 	if (gobj == NULL) {
 		return -ENOENT;
 	}
@@ -705,11 +666,7 @@ int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
 		struct drm_amdgpu_gem_create_in info;
 		void __user *out = (void __user *)(long)args->value;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-		info.bo_size = amdgpu_bo_size(robj);
-#else
 		info.bo_size = robj->gem_base.size;
-#endif
 		info.alignment = robj->tbo.mem.page_alignment << PAGE_SHIFT;
 		info.domains = robj->prefered_domains;
 		info.domain_flags = robj->flags;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c
index e8443df..3ab4c65 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ih.c
@@ -169,11 +169,9 @@ restart_ih:
 	while (adev->irq.ih.rptr != wptr) {
 		u32 ring_index = adev->irq.ih.rptr >> 2;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 		/* Before dispatching irq to IP blocks, send it to amdkfd */
 		amdgpu_amdkfd_interrupt(adev,
 				(const void *) &adev->irq.ih.ring[ring_index]);
-#endif
 
 		entry.iv_entry = (const uint32_t *)
 			&adev->irq.ih.ring[ring_index];
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
index 0aae9dc..3839506 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
@@ -65,9 +65,7 @@ int amdgpu_driver_unload_kms(struct drm_device *dev)
 		pm_runtime_forbid(dev->dev);
 	}
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	amdgpu_amdkfd_device_fini(adev);
-#endif
 
 	amdgpu_acpi_fini(adev);
 
@@ -128,11 +126,9 @@ int amdgpu_driver_load_kms(struct drm_device *dev, unsigned long flags)
 				"Error during ACPI methods call\n");
 	}
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	amdgpu_amdkfd_load_interface(adev);
 	amdgpu_amdkfd_device_probe(adev);
 	amdgpu_amdkfd_device_init(adev);
-#endif
 
 	if (amdgpu_device_is_px(dev)) {
 		pm_runtime_use_autosuspend(dev->dev);
@@ -648,15 +644,7 @@ int amdgpu_driver_open_kms(struct drm_device *dev, struct drm_file *file_priv)
 		goto out_suspend;
 	}
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	r = amdgpu_vm_init(adev, &fpriv->vm,
-		!!(amdgpu_vm_update_context &
-		AMDGPU_VM_USE_CPU_UPDATE_FOR_GRAPHICS_MASK),
-		false);
-#else
 	r = amdgpu_vm_init(adev, &fpriv->vm);
-#endif
-
 	if (r) {
 		kfree(fpriv);
 		goto out_suspend;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c
index b512c4e..7ea3cac 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c
@@ -35,23 +35,12 @@
 #include <drm/drm.h>
 
 #include "amdgpu.h"
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#include "amdgpu_amdkfd.h"
-
-enum amdgpu_mn_type {
-	AMDGPU_MN_TYPE_GFX,
-	AMDGPU_MN_TYPE_HSA,
-};
-#endif
 
 struct amdgpu_mn {
 	/* constant after initialisation */
 	struct amdgpu_device	*adev;
 	struct mm_struct	*mm;
 	struct mmu_notifier	mn;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	enum amdgpu_mn_type	type;
-#endif
 
 	/* only used on destruction */
 	struct work_struct	work;
@@ -91,11 +80,6 @@ static void amdgpu_mn_destroy(struct work_struct *work)
 		list_for_each_entry_safe(bo, next_bo, &node->bos, mn_list) {
 			bo->mn = NULL;
 			list_del_init(&bo->mn_list);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-			if (rmn->type == AMDGPU_MN_TYPE_HSA)
-				amdgpu_amdkfd_cancel_restore_mem(
-					adev, bo->kfd_bo);
-#endif
 		}
 		kfree(node);
 	}
@@ -192,7 +176,7 @@ static void amdgpu_mn_invalidate_page(struct mmu_notifier *mn,
 }
 
 /**
- * amdgpu_mn_invalidate_range_start_gfx - callback to notify about mm change
+ * amdgpu_mn_invalidate_range_start - callback to notify about mm change
  *
  * @mn: our notifier
  * @mn: the mm this callback is about
@@ -202,17 +186,10 @@ static void amdgpu_mn_invalidate_page(struct mmu_notifier *mn,
  * We block for all BOs between start and end to be idle and
  * unmap them by move them into system domain again.
  */
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static void amdgpu_mn_invalidate_range_start_gfx(struct mmu_notifier *mn,
-						 struct mm_struct *mm,
-						 unsigned long start,
-						 unsigned long end)
-#else
 static void amdgpu_mn_invalidate_range_start(struct mmu_notifier *mn,
 					     struct mm_struct *mm,
 					     unsigned long start,
 					     unsigned long end)
-#endif
 {
 	struct amdgpu_mn *rmn = container_of(mn, struct amdgpu_mn, mn);
 	struct interval_tree_node *it;
@@ -235,141 +212,23 @@ static void amdgpu_mn_invalidate_range_start(struct mmu_notifier *mn,
 	mutex_unlock(&rmn->lock);
 }
 
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 static const struct mmu_notifier_ops amdgpu_mn_ops = {
 	.release = amdgpu_mn_release,
 	.invalidate_page = amdgpu_mn_invalidate_page,
 	.invalidate_range_start = amdgpu_mn_invalidate_range_start,
 };
-#else
-/**
- * amdgpu_mn_invalidate_range_start_hsa - callback to notify about mm change
- *
- * @mn: our notifier
- * @mn: the mm this callback is about
- * @start: start of updated range
- * @end: end of updated range
- *
- * We temporarily evict all BOs between start and end. This
- * necessitates evicting all user-mode queues of the process. The BOs
- * are restorted in amdgpu_mn_invalidate_range_end_hsa.
- */
-static void amdgpu_mn_invalidate_range_start_hsa(struct mmu_notifier *mn,
-						 struct mm_struct *mm,
-						 unsigned long start,
-						 unsigned long end)
-{
-	struct amdgpu_mn *rmn = container_of(mn, struct amdgpu_mn, mn);
-	struct interval_tree_node *it;
-
-	/* notification is exclusive, but interval is inclusive */
-	end -= 1;
-
-	it = interval_tree_iter_first(&rmn->objects, start, end);
-	while (it) {
-		struct amdgpu_mn_node *node;
-		struct amdgpu_bo *bo;
-
-		node = container_of(it, struct amdgpu_mn_node, it);
-		it = interval_tree_iter_next(it, start, end);
-
-		list_for_each_entry(bo, &node->bos, mn_list) {
-			struct kgd_mem *mem = bo->kfd_bo;
-
-			if (amdgpu_ttm_tt_affect_userptr(bo->tbo.ttm,
-							 start, end))
-				amdgpu_amdkfd_evict_mem(
-						amdgpu_ttm_adev(bo->tbo.bdev),
-						mem, mm);
-		}
-	}
-}
-
-/**
- * amdgpu_mn_invalidate_range_end_hsa - callback to notify about mm change
- *
- * @mn: our notifier
- * @mn: the mm this callback is about
- * @start: start of updated range
- * @end: end of updated range
- *
- * Restore BOs between start and end. Once the last BO is restored,
- * the queues can be reenabled. Restoring a BO can itself trigger
- * another recursive MMU notifier. Therefore this needs to be
- * scheduled in a worker thread. Adding a slight delay (1 jiffy)
- * avoids excessive repeated evictions.
- */
-static void amdgpu_mn_invalidate_range_end_hsa(struct mmu_notifier *mn,
-					       struct mm_struct *mm,
-					       unsigned long start,
-					       unsigned long end)
-{
-	struct amdgpu_mn *rmn = container_of(mn, struct amdgpu_mn, mn);
-	struct interval_tree_node *it;
-
-	/* notification is exclusive, but interval is inclusive */
-	end -= 1;
-
-	it = interval_tree_iter_first(&rmn->objects, start, end);
-	while (it) {
-		struct amdgpu_mn_node *node;
-		struct amdgpu_bo *bo;
-
-		node = container_of(it, struct amdgpu_mn_node, it);
-		it = interval_tree_iter_next(it, start, end);
-
-		list_for_each_entry(bo, &node->bos, mn_list) {
-			struct kgd_mem *mem = bo->kfd_bo;
-
-			if (amdgpu_ttm_tt_affect_userptr(bo->tbo.ttm,
-							 start, end))
-				amdgpu_amdkfd_schedule_restore_mem(
-						amdgpu_ttm_adev(bo->tbo.bdev),
-						mem, mm, 1);
-		}
-	}
-}
-
-static const struct mmu_notifier_ops amdgpu_mn_ops[] = {
-	[AMDGPU_MN_TYPE_GFX] = {
-		.release = amdgpu_mn_release,
-		.invalidate_page = amdgpu_mn_invalidate_page,
-		.invalidate_range_start = amdgpu_mn_invalidate_range_start_gfx,
-	},
-	[AMDGPU_MN_TYPE_HSA] = {
-		.release = amdgpu_mn_release,
-		.invalidate_page = amdgpu_mn_invalidate_page,
-		.invalidate_range_start = amdgpu_mn_invalidate_range_start_hsa,
-		.invalidate_range_end = amdgpu_mn_invalidate_range_end_hsa,
-	},
-};
-
-/* Low bits of any reasonable mm pointer will be unused due to struct
- * alignment. Use these bits to make a unique key from the mm pointer
- * and notifier type. */
-#define AMDGPU_MN_KEY(mm, type) ((unsigned long)(mm) + (type))
-#endif
 
 /**
  * amdgpu_mn_get - create notifier context
  *
  * @adev: amdgpu device pointer
- * @type: type of MMU notifier context
  *
  * Creates a notifier context for current->mm.
  */
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev,
-				       enum amdgpu_mn_type type)
-#else
 static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)
-#endif
 {
 	struct mm_struct *mm = current->mm;
 	struct amdgpu_mn *rmn;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	unsigned long key = AMDGPU_MN_KEY(mm, type);
-#endif
 	int r;
 
 	mutex_lock(&adev->mn_lock);
@@ -378,13 +237,8 @@ static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)
 		return ERR_PTR(-EINTR);
 	}
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	hash_for_each_possible(adev->mn_hash, rmn, node, key)
-		if (AMDGPU_MN_KEY(rmn->mm, rmn->type) == key)
-#else
 	hash_for_each_possible(adev->mn_hash, rmn, node, (unsigned long)mm)
 		if (rmn->mm == mm)
-#endif
 			goto release_locks;
 
 	rmn = kzalloc(sizeof(*rmn), GFP_KERNEL);
@@ -395,12 +249,7 @@ static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)
 
 	rmn->adev = adev;
 	rmn->mm = mm;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	rmn->type = type;
-	rmn->mn.ops = &amdgpu_mn_ops[type];
-#else
 	rmn->mn.ops = &amdgpu_mn_ops;
-#endif
 	mutex_init(&rmn->lock);
 	rmn->objects = RB_ROOT;
 
@@ -408,11 +257,7 @@ static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)
 	if (r)
 		goto free_rmn;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	hash_add(adev->mn_hash, &rmn->node, AMDGPU_MN_KEY(mm, type));
-#else
 	hash_add(adev->mn_hash, &rmn->node, (unsigned long)mm);
-#endif
 
 release_locks:
 	up_write(&mm->mmap_sem);
@@ -441,20 +286,12 @@ int amdgpu_mn_register(struct amdgpu_bo *bo, unsigned long addr)
 {
 	unsigned long end = addr + amdgpu_bo_size(bo) - 1;
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	enum amdgpu_mn_type type =
-		bo->kfd_bo ? AMDGPU_MN_TYPE_HSA : AMDGPU_MN_TYPE_GFX;
-#endif
 	struct amdgpu_mn *rmn;
 	struct amdgpu_mn_node *node = NULL;
 	struct list_head bos;
 	struct interval_tree_node *it;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	rmn = amdgpu_mn_get(adev, type);
-#else
 	rmn = amdgpu_mn_get(adev);
-#endif
 	if (IS_ERR(rmn))
 		return PTR_ERR(rmn);
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h
index 314c013..4c0a86e 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h
@@ -40,11 +40,10 @@
 #include <drm/drm_fb_helper.h>
 #include <linux/i2c.h>
 #include <linux/i2c-algo-bit.h>
-
-#include <drm/drm_dp_mst_helper.h>
 #include <linux/hrtimer.h>
 #include "amdgpu_irq.h"
 
+#include <drm/drm_dp_mst_helper.h>
 #include "modules/inc/mod_freesync.h"
 
 struct amdgpu_bo;
@@ -439,14 +438,14 @@ struct amdgpu_crtc {
 	u32 wm_high;
 	u32 lb_vblank_lead_lines;
 	struct drm_display_mode hw_mode;
+	/* for virtual dce */
+	struct hrtimer vblank_timer;
+	enum amdgpu_interrupt_state vsync_timer_enabled;
 
 	int otg_inst;
 	uint32_t flip_flags;
 	/* After Set Mode target will be non-NULL */
 	struct dc_target *target;
-	/* for virtual dce */
-	struct hrtimer vblank_timer;
-	enum amdgpu_interrupt_state vsync_timer_enabled;
 };
 
 struct amdgpu_encoder_atom_dig {
@@ -674,10 +673,10 @@ int amdgpu_align_pitch(struct amdgpu_device *adev, int width, int bpp, bool tile
 void amdgpu_print_display_setup(struct drm_device *dev);
 int amdgpu_modeset_create_props(struct amdgpu_device *adev);
 int amdgpu_crtc_set_config(struct drm_mode_set *set);
-int amdgpu_crtc_page_flip(struct drm_crtc *crtc,
-			  struct drm_framebuffer *fb,
-			  struct drm_pending_vblank_event *event,
-			  uint32_t page_flip_flags);
+int amdgpu_crtc_page_flip_target(struct drm_crtc *crtc,
+				 struct drm_framebuffer *fb,
+				 struct drm_pending_vblank_event *event,
+				 uint32_t page_flip_flags, uint32_t target);
 extern const struct drm_mode_config_funcs amdgpu_mode_funcs;
 
 #endif
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
index 0b3fbb0..f45066c 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
@@ -36,9 +36,7 @@
 #include <drm/drm_cache.h>
 #include "amdgpu.h"
 #include "amdgpu_trace.h"
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#include "amdgpu_amdkfd.h"
-#endif
+
 
 
 static u64 amdgpu_get_vis_part_size(struct amdgpu_device *adev,
@@ -95,15 +93,9 @@ static void amdgpu_ttm_bo_destroy(struct ttm_buffer_object *tbo)
 
 	bo = container_of(tbo, struct amdgpu_bo, tbo);
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	if (bo->kfd_bo)
-		amdgpu_amdkfd_unreserve_system_memory_limit(bo);
-#endif
 	amdgpu_update_memory_usage(adev, &bo->tbo.mem, NULL);
 
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	drm_gem_object_release(&bo->gem_base);
-#endif
 	amdgpu_bo_unref(&bo->parent);
 	if (!list_empty(&bo->shadow_list)) {
 		mutex_lock(&adev->shadow_list_lock);
@@ -352,19 +344,13 @@ int amdgpu_bo_create_restricted(struct amdgpu_device *adev,
 	bo = kzalloc(sizeof(struct amdgpu_bo), GFP_KERNEL);
 	if (bo == NULL)
 		return -ENOMEM;
-
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	r = drm_gem_object_init(adev->ddev, &bo->gem_base, size);
 	if (unlikely(r)) {
 		kfree(bo);
 		return r;
 	}
-#endif
 	INIT_LIST_HEAD(&bo->shadow_list);
 	INIT_LIST_HEAD(&bo->va);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	INIT_LIST_HEAD(&bo->gem_objects);
-#endif
 	bo->prefered_domains = domain & (AMDGPU_GEM_DOMAIN_VRAM |
 					 AMDGPU_GEM_DOMAIN_GTT |
 					 AMDGPU_GEM_DOMAIN_CPU |
@@ -765,6 +751,10 @@ static const char *amdgpu_vram_names[] = {
 
 int amdgpu_bo_init(struct amdgpu_device *adev)
 {
+	/* reserve PAT memory space to WC for VRAM */
+	arch_io_reserve_memtype_wc(adev->mc.aper_base,
+				   adev->mc.aper_size);
+
 	/* Add an MTRR for the VRAM */
 	adev->mc.vram_mtrr = arch_phys_wc_add(adev->mc.aper_base,
 					      adev->mc.aper_size);
@@ -780,6 +770,7 @@ void amdgpu_bo_fini(struct amdgpu_device *adev)
 {
 	amdgpu_ttm_fini(adev);
 	arch_phys_wc_del(adev->mc.vram_mtrr);
+	arch_io_free_memtype_wc(adev->mc.aper_base, adev->mc.aper_size);
 }
 
 int amdgpu_bo_fbdev_mmap(struct amdgpu_bo *bo,
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h
index 0b9d4df..d3baf83 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h
@@ -74,7 +74,7 @@ static inline int amdgpu_bo_reserve(struct amdgpu_bo *bo, bool no_intr)
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
 	int r;
 
-	r = ttm_bo_reserve(&bo->tbo, !no_intr, false, false, NULL);
+	r = ttm_bo_reserve(&bo->tbo, !no_intr, false, NULL);
 	if (unlikely(r != 0)) {
 		if (r != -ERESTARTSYS)
 			dev_err(adev->dev, "%p reserve failed\n", bo);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c
index 2a54c31..439fb2f 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_pm.c
@@ -567,177 +567,6 @@ fail:
 	return count;
 }
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static ssize_t amdgpu_get_pp_power_profile(struct device *dev,
-		char *buf, struct pp_profile *query)
-{
-	struct drm_device *ddev = dev_get_drvdata(dev);
-	struct amdgpu_device *adev = ddev->dev_private;
-	int ret = 0;
-
-	if (adev->pp_enabled)
-		ret = amdgpu_dpm_get_power_profile_state(
-				adev, query);
-	else if (adev->pm.funcs->get_power_profile_state)
-		ret = adev->pm.funcs->get_power_profile_state(
-				adev, query);
-
-	if (ret)
-		return ret;
-
-	return snprintf(buf, PAGE_SIZE,
-			"%d %d %d %d %d\n",
-			query->min_sclk / 100,
-			query->min_mclk / 100,
-			query->activity_threshold,
-			query->up_hyst,
-			query->down_hyst);
-}
-
-static ssize_t amdgpu_get_pp_gfx_power_profile(struct device *dev,
-		struct device_attribute *attr,
-		char *buf)
-{
-	struct pp_profile query = {0};
-
-	query.type = PP_GFX_PROFILE;
-
-	return amdgpu_get_pp_power_profile(dev, buf, &query);
-}
-
-static ssize_t amdgpu_get_pp_compute_power_profile(struct device *dev,
-		struct device_attribute *attr,
-		char *buf)
-{
-	struct pp_profile query = {0};
-
-	query.type = PP_COMPUTE_PROFILE;
-
-	return amdgpu_get_pp_power_profile(dev, buf, &query);
-}
-
-static ssize_t amdgpu_set_pp_power_profile(struct device *dev,
-		const char *buf,
-		size_t count,
-		struct pp_profile *request)
-{
-	struct drm_device *ddev = dev_get_drvdata(dev);
-	struct amdgpu_device *adev = ddev->dev_private;
-	uint32_t loop = 0;
-	char *sub_str, buf_cpy[128], *tmp_str;
-	const char delimiter[3] = {' ', '\n', '\0'};
-	long int value;
-	int ret = 0;
-
-	if (strncmp("reset", buf, strlen("reset")) == 0) {
-		if (adev->pp_enabled)
-			ret = amdgpu_dpm_reset_power_profile_state(
-					adev, request);
-		else if (adev->pm.funcs->reset_power_profile_state)
-			ret = adev->pm.funcs->reset_power_profile_state(
-					adev, request);
-		if (ret) {
-			count = -EINVAL;
-			goto fail;
-		}
-		return count;
-	}
-
-	if (strncmp("set", buf, strlen("set")) == 0) {
-		if (adev->pp_enabled)
-			ret = amdgpu_dpm_set_power_profile_state(
-					adev, request);
-		else if (adev->pm.funcs->set_power_profile_state)
-			ret = adev->pm.funcs->set_power_profile_state(
-					adev, request);
-		if (ret) {
-			count = -EINVAL;
-			goto fail;
-		}
-		return count;
-	}
-
-	if (count+1 >= 128) {
-		count = -EINVAL;
-		goto fail;
-	}
-
-	memcpy(buf_cpy, buf, count+1);
-	tmp_str = buf_cpy;
-
-	while (tmp_str[0]) {
-		sub_str = strsep(&tmp_str, delimiter);
-		ret = kstrtol(sub_str, 0, &value);
-		if (ret) {
-			count = -EINVAL;
-			goto fail;
-		}
-
-		switch (loop) {
-		case 0:
-			/* input unit MHz convert to dpm table unit 10KHz*/
-			request->min_sclk = (uint32_t)value * 100;
-			break;
-		case 1:
-			/* input unit MHz convert to dpm table unit 10KHz*/
-			request->min_mclk = (uint32_t)value * 100;
-			break;
-		case 2:
-			request->activity_threshold = (uint16_t)value;
-			break;
-		case 3:
-			request->up_hyst = (uint8_t)value;
-			break;
-		case 4:
-			request->down_hyst = (uint8_t)value;
-			break;
-		default:
-			count = -EINVAL;
-			goto fail;
-		}
-
-		loop++;
-	}
-
-	if (adev->pp_enabled)
-		ret = amdgpu_dpm_set_power_profile_state(
-				adev, request);
-	else if (adev->pm.funcs->set_power_profile_state)
-		ret = adev->pm.funcs->set_power_profile_state(
-				adev, request);
-
-	if (ret)
-		count = -EINVAL;
-
-fail:
-	return count;
-}
-
-static ssize_t amdgpu_set_pp_gfx_power_profile(struct device *dev,
-		struct device_attribute *attr,
-		const char *buf,
-		size_t count)
-{
-	struct pp_profile request = {0};
-
-	request.type = PP_GFX_PROFILE;
-
-	return amdgpu_set_pp_power_profile(dev, buf, count, &request);
-}
-
-static ssize_t amdgpu_set_pp_compute_power_profile(struct device *dev,
-		struct device_attribute *attr,
-		const char *buf,
-		size_t count)
-{
-	struct pp_profile request = {0};
-
-	request.type = PP_COMPUTE_PROFILE;
-
-	return amdgpu_set_pp_power_profile(dev, buf, count, &request);
-}
-#endif
-
 static DEVICE_ATTR(power_dpm_state, S_IRUGO | S_IWUSR, amdgpu_get_dpm_state, amdgpu_set_dpm_state);
 static DEVICE_ATTR(power_dpm_force_performance_level, S_IRUGO | S_IWUSR,
 		   amdgpu_get_dpm_forced_performance_level,
@@ -765,14 +594,6 @@ static DEVICE_ATTR(pp_sclk_od, S_IRUGO | S_IWUSR,
 static DEVICE_ATTR(pp_mclk_od, S_IRUGO | S_IWUSR,
 		amdgpu_get_pp_mclk_od,
 		amdgpu_set_pp_mclk_od);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static DEVICE_ATTR(pp_gfx_power_profile, S_IRUGO | S_IWUSR,
-		amdgpu_get_pp_gfx_power_profile,
-		amdgpu_set_pp_gfx_power_profile);
-static DEVICE_ATTR(pp_compute_power_profile, S_IRUGO | S_IWUSR,
-		amdgpu_get_pp_compute_power_profile,
-		amdgpu_set_pp_compute_power_profile);
-#endif
 
 static ssize_t amdgpu_hwmon_show_temp(struct device *dev,
 				      struct device_attribute *attr,
@@ -1206,7 +1027,6 @@ static void amdgpu_dpm_change_power_state_locked(struct amdgpu_device *adev)
 		printk("switching to power state:\n");
 		amdgpu_dpm_print_power_state(adev, adev->pm.dpm.requested_ps);
 	}
-
 	/* update whether vce is active */
 	ps->vce_active = adev->pm.dpm.vce_active;
 
@@ -1381,22 +1201,6 @@ int amdgpu_pm_sysfs_init(struct amdgpu_device *adev)
 		DRM_ERROR("failed to create device file pp_mclk_od\n");
 		return ret;
 	}
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	ret = device_create_file(adev->dev,
-			&dev_attr_pp_gfx_power_profile);
-	if (ret) {
-		DRM_ERROR("failed to create device file	"
-				"pp_gfx_power_profile\n");
-		return ret;
-	}
-	ret = device_create_file(adev->dev,
-			&dev_attr_pp_compute_power_profile);
-	if (ret) {
-		DRM_ERROR("failed to create device file	"
-				"pp_compute_power_profile\n");
-		return ret;
-	}
-#endif
 
 	ret = amdgpu_debugfs_pm_init(adev);
 	if (ret) {
@@ -1426,12 +1230,6 @@ void amdgpu_pm_sysfs_fini(struct amdgpu_device *adev)
 	device_remove_file(adev->dev, &dev_attr_pp_dpm_pcie);
 	device_remove_file(adev->dev, &dev_attr_pp_sclk_od);
 	device_remove_file(adev->dev, &dev_attr_pp_mclk_od);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	device_remove_file(adev->dev,
-			&dev_attr_pp_gfx_power_profile);
-	device_remove_file(adev->dev,
-			&dev_attr_pp_compute_power_profile);
-#endif
 }
 
 void amdgpu_pm_compute_clocks(struct amdgpu_device *adev)
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c
index 5676e21..3826d5a 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_prime.c
@@ -65,9 +65,6 @@ amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
 	struct reservation_object *resv = attach->dmabuf->resv;
 	struct amdgpu_device *adev = dev->dev_private;
 	struct amdgpu_bo *bo;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	struct amdgpu_gem_object *gobj;
-#endif
 	int ret;
 
 	ww_mutex_lock(&resv->lock, NULL);
@@ -76,41 +73,37 @@ amdgpu_gem_prime_import_sg_table(struct drm_device *dev,
 	ww_mutex_unlock(&resv->lock);
 	if (ret)
 		return ERR_PTR(ret);
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
-	return &bo->gem_base;
-#else
-
-	gobj = kzalloc(sizeof(struct amdgpu_gem_object), GFP_KERNEL);
-	if (unlikely(!gobj)) {
-		amdgpu_bo_unref(&bo);
-		return ERR_PTR(-ENOMEM);
-	}
-
-	ret = drm_gem_object_init(adev->ddev, &gobj->base, amdgpu_bo_size(bo));
-	if (unlikely(ret)) {
-		kfree(gobj);
-		amdgpu_bo_unref(&bo);
-		return ERR_PTR(ret);
-	}
-
-	list_add(&gobj->list, &bo->gem_objects);
-	gobj->bo = amdgpu_bo_ref(bo);
 
-	return &gobj->base;
-#endif
+	bo->prime_shared_count = 1;
+	return &bo->gem_base;
 }
 
 int amdgpu_gem_prime_pin(struct drm_gem_object *obj)
 {
 	struct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);
-	int ret = 0;
+	long ret = 0;
 
 	ret = amdgpu_bo_reserve(bo, false);
 	if (unlikely(ret != 0))
 		return ret;
 
+	/*
+	 * Wait for all shared fences to complete before we switch to future
+	 * use of exclusive fence on this prime shared bo.
+	 */
+	ret = reservation_object_wait_timeout_rcu(bo->tbo.resv, true, false,
+						  MAX_SCHEDULE_TIMEOUT);
+	if (unlikely(ret < 0)) {
+		DRM_DEBUG_PRIME("Fence wait failed: %li\n", ret);
+		amdgpu_bo_unreserve(bo);
+		return ret;
+	}
+
 	/* pin buffer into GTT */
 	ret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT, NULL);
+	if (likely(ret == 0))
+		bo->prime_shared_count++;
+
 	amdgpu_bo_unreserve(bo);
 	return ret;
 }
@@ -125,6 +118,8 @@ void amdgpu_gem_prime_unpin(struct drm_gem_object *obj)
 		return;
 
 	amdgpu_bo_unpin(bo);
+	if (bo->prime_shared_count)
+		bo->prime_shared_count--;
 	amdgpu_bo_unreserve(bo);
 }
 
@@ -146,64 +141,3 @@ struct dma_buf *amdgpu_gem_prime_export(struct drm_device *dev,
 
 	return drm_gem_prime_export(dev, gobj, flags);
 }
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-struct drm_gem_object *
-amdgpu_gem_prime_foreign_bo(struct amdgpu_device *adev, struct amdgpu_bo *bo)
-{
-	struct amdgpu_gem_object *gobj;
-	int r;
-
-	ww_mutex_lock(&bo->tbo.resv->lock, NULL);
-
-	list_for_each_entry(gobj, &bo->gem_objects, list) {
-		if (gobj->base.dev != adev->ddev)
-			continue;
-
-		ww_mutex_unlock(&bo->tbo.resv->lock);
-		drm_gem_object_reference(&gobj->base);
-		return &gobj->base;
-	}
-
-
-	gobj = kzalloc(sizeof(struct amdgpu_gem_object), GFP_KERNEL);
-	if (unlikely(!gobj)) {
-		ww_mutex_unlock(&bo->tbo.resv->lock);
-		return ERR_PTR(-ENOMEM);
-	}
-
-	r = drm_gem_object_init(adev->ddev, &gobj->base, amdgpu_bo_size(bo));
-	if (unlikely(r)) {
-		kfree(gobj);
-		ww_mutex_unlock(&bo->tbo.resv->lock);
-		return ERR_PTR(r);
-	}
-
-	list_add(&gobj->list, &bo->gem_objects);
-	gobj->bo = amdgpu_bo_ref(bo);
-	bo->flags |= AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
-
-	ww_mutex_unlock(&bo->tbo.resv->lock);
-
-	return &gobj->base;
-}
-
-struct drm_gem_object *amdgpu_gem_prime_import(struct drm_device *dev,
-					       struct dma_buf *dma_buf)
-{
-	struct amdgpu_device *adev = dev->dev_private;
-
-	if (dma_buf->ops == &drm_gem_prime_dmabuf_ops) {
-		struct drm_gem_object *obj = dma_buf->priv;
-
-		if (obj->dev != dev && obj->dev->driver == dev->driver) {
-			/* It's a amdgpu_bo from a different driver instance */
-			struct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);
-
-			return amdgpu_gem_prime_foreign_bo(adev, bo);
-		}
-	}
-
-	return drm_gem_prime_import(dev, dma_buf);
-}
-#endif
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c
index ad7c851..4c99282 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c
@@ -327,9 +327,6 @@ static const struct file_operations amdgpu_debugfs_ring_fops = {
 	.llseek = default_llseek
 };
 
-static struct drm_info_list amdgpu_debugfs_ring_info_list[AMDGPU_MAX_RINGS];
-static char amdgpu_debugfs_ring_names[AMDGPU_MAX_RINGS][32];
-
 #endif
 
 static int amdgpu_debugfs_ring_init(struct amdgpu_device *adev,
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h
index cc7d0a5..92bc89b 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h
@@ -35,7 +35,7 @@
 /* some special values for the owner field */
 #define AMDGPU_FENCE_OWNER_UNDEFINED	((void*)0ul)
 #define AMDGPU_FENCE_OWNER_VM		((void*)1ul)
-#define AMDGPU_FENCE_OWNER_KFD		((void *)2ul)
+
 #define AMDGPU_FENCE_FLAG_64BIT         (1 << 0)
 #define AMDGPU_FENCE_FLAG_INT           (1 << 1)
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_sa.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_sa.c
index e7276d4..7b56d99 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sa.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sa.c
@@ -360,7 +360,8 @@ int amdgpu_sa_bo_new(struct amdgpu_sa_manager *sa_manager,
 		if (count) {
 			spin_unlock(&sa_manager->wq.lock);
 			t = fence_wait_any_timeout(fences, count, false,
-						   MAX_SCHEDULE_TIMEOUT, NULL);
+						   MAX_SCHEDULE_TIMEOUT,
+						   NULL);
 			for (i = 0; i < count; ++i)
 				fence_put(fences[i]);
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_sched.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_sched.c
deleted file mode 100644
index e69de29..0000000
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c
index 25b1de4..5c8d302 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c
@@ -31,9 +31,6 @@
 #include <drm/drmP.h>
 #include "amdgpu.h"
 #include "amdgpu_trace.h"
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#include "amdgpu_amdkfd.h"
-#endif
 
 struct amdgpu_sync_entry {
 	struct hlist_node	node;
@@ -86,26 +83,11 @@ static bool amdgpu_sync_same_dev(struct amdgpu_device *adev, struct fence *f)
  */
 static void *amdgpu_sync_get_owner(struct fence *f)
 {
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	struct amd_sched_fence *s_fence = to_amd_sched_fence(f);
-#else
-	struct amd_sched_fence *s_fence;
-	struct amdgpu_amdkfd_fence *kfd_fence;
 
-	if (f == NULL)
-		return AMDGPU_FENCE_OWNER_UNDEFINED;
-
-	s_fence = to_amd_sched_fence(f);
-#endif
 	if (s_fence)
 		return s_fence->owner;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	kfd_fence = to_amdgpu_amdkfd_fence(f);
-	if (kfd_fence)
-		return AMDGPU_FENCE_OWNER_KFD;
-#endif
-
 	return AMDGPU_FENCE_OWNER_UNDEFINED;
 }
 
@@ -187,8 +169,7 @@ int amdgpu_sync_fence(struct amdgpu_device *adev, struct amdgpu_sync *sync,
  * @resv: reservation object with embedded fence
  * @shared: true if we should only sync to the exclusive fence
  *
- * Sync to the fence except if it is KFD eviction fence and owner is
- * AMDGPU_FENCE_OWNER_VM.
+ * Sync to the fence
  */
 int amdgpu_sync_resv(struct amdgpu_device *adev,
 		     struct amdgpu_sync *sync,
@@ -204,15 +185,9 @@ int amdgpu_sync_resv(struct amdgpu_device *adev,
 	if (resv == NULL)
 		return -EINVAL;
 
+	/* always sync to the exclusive fence */
 	f = reservation_object_get_excl(resv);
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	r = amdgpu_sync_fence(adev, sync, f);
-#else
-	fence_owner = amdgpu_sync_get_owner(f);
-	if (fence_owner != AMDGPU_FENCE_OWNER_KFD ||
-			owner != AMDGPU_FENCE_OWNER_VM)
-		r = amdgpu_sync_fence(adev, sync, f);
-#endif
 
 	flist = reservation_object_get_list(resv);
 	if (!flist || r)
@@ -221,20 +196,11 @@ int amdgpu_sync_resv(struct amdgpu_device *adev,
 	for (i = 0; i < flist->shared_count; ++i) {
 		f = rcu_dereference_protected(flist->shared[i],
 					      reservation_object_held(resv));
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-		fence_owner = amdgpu_sync_get_owner(f);
-		if (fence_owner == AMDGPU_FENCE_OWNER_KFD &&
-				owner == AMDGPU_FENCE_OWNER_VM)
-			continue;
-#endif
-
 		if (amdgpu_sync_same_dev(adev, f)) {
 			/* VM updates are only interesting
 			 * for other VM updates and moves.
 			 */
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 			fence_owner = amdgpu_sync_get_owner(f);
-#endif
 			if ((owner != AMDGPU_FENCE_OWNER_UNDEFINED) &&
 			    (fence_owner != AMDGPU_FENCE_OWNER_UNDEFINED) &&
 			    ((owner == AMDGPU_FENCE_OWNER_VM) !=
@@ -330,27 +296,6 @@ struct fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync)
 	return NULL;
 }
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-int amdgpu_sync_wait(struct amdgpu_sync *sync)
-{
-	struct amdgpu_sync_entry *e;
-	struct hlist_node *tmp;
-	int i, r;
-
-	hash_for_each_safe(sync->fences, i, tmp, e, node) {
-		r = fence_wait(e->fence, false);
-		if (r)
-			return r;
-
-		hash_del(&e->node);
-		fence_put(e->fence);
-		kmem_cache_free(amdgpu_sync_slab, e);
-	}
-
-	return 0;
-}
-#endif
-
 /**
  * amdgpu_sync_free - free the sync object
  *
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h
index 172e8d8..405f379 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.h
@@ -49,9 +49,6 @@ int amdgpu_sync_resv(struct amdgpu_device *adev,
 struct fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,
 				     struct amdgpu_ring *ring);
 struct fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-int amdgpu_sync_wait(struct amdgpu_sync *sync);
-#endif
 void amdgpu_sync_free(struct amdgpu_sync *sync);
 int amdgpu_sync_init(void);
 void amdgpu_sync_fini(void);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
index 7330ead..49af568 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
@@ -44,9 +44,7 @@
 #include <linux/debugfs.h>
 #include "amdgpu.h"
 #include "bif/bif_4_1_d.h"
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#include "amdgpu_amdkfd.h"
-#endif
+
 #define DRM_FILE_PAGE_OFFSET (0x100000000ULL >> PAGE_SHIFT)
 
 static int amdgpu_ttm_debugfs_init(struct amdgpu_device *adev);
@@ -234,46 +232,16 @@ static void amdgpu_evict_flags(struct ttm_buffer_object *bo,
 
 static int amdgpu_verify_access(struct ttm_buffer_object *bo, struct file *filp)
 {
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	struct amdgpu_bo *abo = container_of(bo, struct amdgpu_bo, tbo);
-#else
-	struct amdgpu_bo *abo;
-	struct drm_file *file_priv;
-	struct amdgpu_gem_object *gobj;
-
-
-	abo = container_of(bo, struct amdgpu_bo, tbo);
-	/*
-	 * Don't verify access for KFD BO as it doesn't necessary has
-	 * KGD file pointer
-	 */
-	if (!abo || abo->kfd_bo || !filp)
-		return 0;
-	file_priv = filp->private_data;
-#endif
 
 	if (amdgpu_ttm_tt_get_usermm(bo->ttm))
 		return -EPERM;
-
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
-	return drm_vma_node_verify_access(&abo->gem_base.vma_node, filp);
-#else
-	ww_mutex_lock(&abo->tbo.resv->lock, NULL);
-	list_for_each_entry(gobj, &abo->gem_objects, list) {
-		if (gobj->base.dev != file_priv->minor->dev)
-			continue;
-
-		ww_mutex_unlock(&abo->tbo.resv->lock);
-		return drm_vma_node_verify_access(&gobj->base.vma_node, filp);
-	}
-	ww_mutex_unlock(&abo->tbo.resv->lock);
-
-	return -EPERM;
-#endif
+	return drm_vma_node_verify_access(&abo->gem_base.vma_node,
+					  filp->private_data);
 }
 
 static void amdgpu_move_null(struct ttm_buffer_object *bo,
-			struct ttm_mem_reg *new_mem)
+			     struct ttm_mem_reg *new_mem)
 {
 	struct ttm_mem_reg *old_mem = &bo->mem;
 
@@ -382,9 +350,8 @@ static int amdgpu_move_blit(struct ttm_buffer_object *bo,
 			new_start += cur_pages * PAGE_SIZE;
 		}
 	}
- 
-    r = ttm_bo_move_accel_cleanup(bo, fence,
-                                      evict, no_wait_gpu, new_mem);
+
+	r = ttm_bo_pipeline_move(bo, fence, evict, new_mem);
 	fence_put(fence);
 	return r;
 
@@ -436,7 +403,7 @@ static int amdgpu_move_vram_ram(struct ttm_buffer_object *bo,
 	if (unlikely(r)) {
 		goto out_cleanup;
 	}
-	r = ttm_bo_move_ttm(bo, true, no_wait_gpu, new_mem);
+	r = ttm_bo_move_ttm(bo, interruptible, no_wait_gpu, new_mem);
 out_cleanup:
 	ttm_bo_mem_put(bo, &tmp_mem);
 	return r;
@@ -469,7 +436,7 @@ static int amdgpu_move_ram_vram(struct ttm_buffer_object *bo,
 	if (unlikely(r)) {
 		return r;
 	}
-	r = ttm_bo_move_ttm(bo, true, no_wait_gpu, &tmp_mem);
+	r = ttm_bo_move_ttm(bo, interruptible, no_wait_gpu, &tmp_mem);
 	if (unlikely(r)) {
 		goto out_cleanup;
 	}
@@ -536,8 +503,7 @@ static int amdgpu_bo_move(struct ttm_buffer_object *bo,
 
 	if (r) {
 memcpy:
-		r = ttm_bo_move_memcpy(bo, evict,
-				       no_wait_gpu, new_mem);
+		r = ttm_bo_move_memcpy(bo, interruptible, no_wait_gpu, new_mem);
 		if (r) {
 			return r;
 		}
@@ -620,11 +586,7 @@ struct amdgpu_ttm_tt {
 	struct amdgpu_device	*adev;
 	u64			offset;
 	uint64_t		userptr;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	struct task_struct	*usertask;
-#else
-	struct mm_struct        *usermm;
-#endif
+	struct mm_struct	*usermm;
 	uint32_t		userflags;
 	spinlock_t              guptasklock;
 	struct list_head        guptasks;
@@ -635,21 +597,20 @@ struct amdgpu_ttm_tt {
 int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)
 {
 	struct amdgpu_ttm_tt *gtt = (void *)ttm;
-	int write = !(gtt->userflags & AMDGPU_GEM_USERPTR_READONLY);
+	unsigned int flags = 0;
 	unsigned pinned = 0;
 	int r;
 
+	if (!(gtt->userflags & AMDGPU_GEM_USERPTR_READONLY))
+		flags |= FOLL_WRITE;
+
 	if (gtt->userflags & AMDGPU_GEM_USERPTR_ANONONLY) {
 		/* check that we only use anonymous memory
 		   to prevent problems with writeback */
 		unsigned long end = gtt->userptr + ttm->num_pages * PAGE_SIZE;
 		struct vm_area_struct *vma;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-		vma = find_vma(gtt->usertask->mm, gtt->userptr);
-#else
 		vma = find_vma(gtt->usermm, gtt->userptr);
-#endif
 		if (!vma || vma->vm_file || vma->vm_end < end)
 			return -EPERM;
 	}
@@ -665,13 +626,8 @@ int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)
 		list_add(&guptask.list, &gtt->guptasks);
 		spin_unlock(&gtt->guptasklock);
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-		r = get_user_pages(gtt->usertask, gtt->usertask->mm, userptr, num_pages,
-				   write, 0, p, NULL);
-#else
-		r = get_user_pages(current, current->mm, userptr, num_pages,
-				   write, 0, p, NULL);
-#endif
+		r = get_user_pages(userptr, num_pages, flags, p, NULL);
+
 		spin_lock(&gtt->guptasklock);
 		list_del(&guptask.list);
 		spin_unlock(&gtt->guptasklock);
@@ -1003,11 +959,7 @@ int amdgpu_ttm_tt_set_userptr(struct ttm_tt *ttm, uint64_t addr,
 		return -EINVAL;
 
 	gtt->userptr = addr;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	gtt->usertask = current->group_leader;
-#else
 	gtt->usermm = current->mm;
-#endif
 	gtt->userflags = flags;
 	spin_lock_init(&gtt->guptasklock);
 	INIT_LIST_HEAD(&gtt->guptasks);
@@ -1023,14 +975,7 @@ struct mm_struct *amdgpu_ttm_tt_get_usermm(struct ttm_tt *ttm)
 	if (gtt == NULL)
 		return NULL;
 
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	return gtt->usermm;
-#else
-	if (gtt->usertask == NULL)
-		return NULL;
-
-	return gtt->usertask->mm;
-#endif
 }
 
 bool amdgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
@@ -1160,12 +1105,6 @@ static struct list_head *amdgpu_ttm_swap_lru_tail(struct ttm_buffer_object *tbo)
 static bool amdgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
 					    const struct ttm_place *place)
 {
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	struct reservation_object_list *flist;
-	struct fence *f;
-	int i;
-#endif
-
 	if (bo->mem.mem_type == TTM_PL_VRAM &&
 	    bo->mem.start == AMDGPU_BO_INVALID_OFFSET) {
 		unsigned long num_pages = bo->mem.num_pages;
@@ -1183,31 +1122,8 @@ static bool amdgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
 
 		return false;
 	}
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
-	return ttm_bo_eviction_valuable(bo, place);
-#else
-	/* Don't evict this BO if it's outside of the
-	 * requested placement range
-	 */
-	if (!ttm_bo_eviction_valuable(bo, place))
-		return false;
-	/* If bo is a KFD BO, check if the bo belongs to the current process.
-	 * If true, then return false as any KFD process needs all its BOs to
-	 * be resident to run successfully
-	 */
-	flist = reservation_object_get_list(bo->resv);
-	if (!flist)
-		return true;
-
-	for (i = 0; i < flist->shared_count; ++i) {
-		f = rcu_dereference_protected(flist->shared[i],
-			reservation_object_held(bo->resv));
-		if (amd_kfd_fence_check_mm(f, current->mm))
-			return false;
-	}
 
-	return true;
-#endif
+	return ttm_bo_eviction_valuable(bo, place);
 }
 
 static struct ttm_bo_driver amdgpu_bo_driver = {
@@ -1280,7 +1196,6 @@ int amdgpu_ttm_init(struct amdgpu_device *adev)
 	if (r) {
 		return r;
 	}
-
 	r = amdgpu_bo_reserve(adev->stollen_vga_memory, false);
 	if (r)
 		return r;
@@ -1290,7 +1205,6 @@ int amdgpu_ttm_init(struct amdgpu_device *adev)
 		amdgpu_bo_unref(&adev->stollen_vga_memory);
 		return r;
 	}
-
 	DRM_INFO("amdgpu: %uM of VRAM memory ready\n",
 		 (unsigned) (adev->mc.real_vram_size / (1024 * 1024)));
 	r = ttm_bo_init_mm(&adev->mman.bdev, TTM_PL_TT,
@@ -1350,7 +1264,6 @@ void amdgpu_ttm_fini(struct amdgpu_device *adev)
 	if (!adev->mman.initialized)
 		return;
 	amdgpu_ttm_debugfs_fini(adev);
-
 	if (adev->stollen_vga_memory) {
 		r = amdgpu_bo_reserve(adev->stollen_vga_memory, false);
 		if (r == 0) {
@@ -1359,7 +1272,6 @@ void amdgpu_ttm_fini(struct amdgpu_device *adev)
 		}
 		amdgpu_bo_unref(&adev->stollen_vga_memory);
 	}
-
 	ttm_bo_clean_mm(&adev->mman.bdev, TTM_PL_VRAM);
 	ttm_bo_clean_mm(&adev->mman.bdev, TTM_PL_TT);
 	ttm_bo_clean_mm(&adev->mman.bdev, AMDGPU_PL_GDS);
@@ -1386,144 +1298,6 @@ void amdgpu_ttm_set_active_vram_size(struct amdgpu_device *adev, u64 size)
 	man->size = size >> PAGE_SHIFT;
 }
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static struct vm_operations_struct amdgpu_ttm_vm_ops;
-static const struct vm_operations_struct *ttm_vm_ops /* = NULL;
-						      * (appease checkpatch) */;
-static int amdgpu_ttm_bo_access_vram(struct amdgpu_bo *abo,
-				     unsigned long offset,
-				     void *buf, int len, int write)
-{
-	struct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);
-	uint64_t pos = amdgpu_bo_gpu_offset(abo) + offset;
-	uint32_t value = 0;
-	unsigned long flags;
-	int result = 0;
-
-	while (len && pos < adev->mc.mc_vram_size) {
-		uint64_t aligned_pos = pos & ~(uint64_t)3;
-		uint32_t bytes = 4 - (pos & 3);
-		uint32_t shift = (pos & 3) * 8;
-		uint32_t mask = 0xffffffff << shift;
-
-		if (len < bytes) {
-			mask &= 0xffffffff >> (bytes - len) * 8;
-			bytes = len;
-		}
-
-		spin_lock_irqsave(&adev->mmio_idx_lock, flags);
-		WREG32(mmMM_INDEX, ((uint32_t)aligned_pos) | 0x80000000);
-		WREG32(mmMM_INDEX_HI, aligned_pos >> 31);
-		if (!write || mask != 0xffffffff)
-			value = RREG32(mmMM_DATA);
-		if (write) {
-			value &= ~mask;
-			value |= (*(uint32_t *)buf << shift) & mask;
-			WREG32(mmMM_DATA, value);
-		}
-		spin_unlock_irqrestore(&adev->mmio_idx_lock, flags);
-		if (!write) {
-			value = (value & mask) >> shift;
-			memcpy(buf, &value, bytes);
-		}
-
-		result += bytes;
-		buf = (uint8_t *)buf + bytes;
-		pos += bytes;
-		len -= bytes;
-	}
-
-	return result;
-}
-
-static int amdgpu_ttm_bo_access_kmap(struct amdgpu_bo *abo,
-				     unsigned long offset,
-				     void *buf, int len, int write)
-{
-	struct ttm_buffer_object *bo = &abo->tbo;
-	struct ttm_bo_kmap_obj map;
-	void *ptr;
-	bool is_iomem;
-	int r;
-
-	r = ttm_bo_kmap(bo, 0, bo->num_pages, &map);
-	if (r)
-		return r;
-	ptr = (uint8_t *)ttm_kmap_obj_virtual(&map, &is_iomem) + offset;
-	WARN_ON(is_iomem);
-	if (write)
-		memcpy(ptr, buf, len);
-	else
-		memcpy(buf, ptr, len);
-	ttm_bo_kunmap(&map);
-
-	return len;
-}
-
-static int amdgpu_ttm_vm_access(struct vm_area_struct *vma, unsigned long addr,
-				void *buf, int len, int write)
-{
-	unsigned long offset = (addr) - vma->vm_start;
-	struct ttm_buffer_object *bo = vma->vm_private_data;
-	struct amdgpu_bo *abo = container_of(bo, struct amdgpu_bo, tbo);
-	unsigned domain;
-	int result;
-
-	result = amdgpu_bo_reserve(abo, false);
-	if (result != 0)
-		return result;
-
-	domain = amdgpu_mem_type_to_domain(bo->mem.mem_type);
-	if (domain == AMDGPU_GEM_DOMAIN_VRAM)
-		result = amdgpu_ttm_bo_access_vram(abo, offset,
-						   buf, len, write);
-	else
-		result = amdgpu_ttm_bo_access_kmap(abo, offset,
-						   buf, len, write);
-	amdgpu_bo_unreserve(abo);
-
-	return len;
-}
-/* This function is a tweak variance of ttm_bo_vm_open() just to avoid the
- * warning message when fork() with KFD BOs on DGPU.
- */
-static void amdgpu_ttm_vm_open(struct vm_area_struct *vma)
-{
-	struct ttm_buffer_object *bo =
-	    (struct ttm_buffer_object *)vma->vm_private_data;
-	struct amdgpu_bo *abo = container_of(bo, struct amdgpu_bo, tbo);
-
-	/* Because vma->vm_file for /dev/kfd can not be associated with any
-	 * ttm_bo_device due to the one to many mapping between kfd and
-	 * amdgpu devices, for KFD BOs we should just skip the check.
-	 */
-	if (!abo->kfd_bo)
-		WARN_ON(bo->bdev->dev_mapping != vma->vm_file->f_mapping);
-
-	(void)ttm_bo_reference(bo);
-}
-
-int amdgpu_bo_mmap(struct file *filp, struct vm_area_struct *vma,
-		   struct ttm_bo_device *bdev)
-{
-	int r;
-
-	r = ttm_bo_mmap(filp, vma, bdev);
-	if (unlikely(r != 0))
-		return r;
-
-	if (unlikely(ttm_vm_ops == NULL)) {
-		ttm_vm_ops = vma->vm_ops;
-		amdgpu_ttm_vm_ops = *ttm_vm_ops;
-		amdgpu_ttm_vm_ops.access = &amdgpu_ttm_vm_access;
-		amdgpu_ttm_vm_ops.open = &amdgpu_ttm_vm_open;
-	}
-	vma->vm_ops = &amdgpu_ttm_vm_ops;
-
-	return 0;
-}
-#endif
-
 int amdgpu_mmap(struct file *filp, struct vm_area_struct *vma)
 {
 	struct drm_file *file_priv;
@@ -1537,11 +1311,7 @@ int amdgpu_mmap(struct file *filp, struct vm_area_struct *vma)
 	if (adev == NULL)
 		return -EINVAL;
 
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	return ttm_bo_mmap(filp, vma, &adev->mman.bdev);
-#else
-	return amdgpu_bo_mmap(filp, vma, &adev->mman.bdev);
-#endif
 }
 
 int amdgpu_copy_buffer(struct amdgpu_ring *ring,
@@ -1583,8 +1353,8 @@ int amdgpu_copy_buffer(struct amdgpu_ring *ring,
 	for (i = 0; i < num_loops; i++) {
 		uint32_t cur_size_in_bytes = min(byte_count, max_bytes);
 
-		amdgpu_emit_copy_buffer(adev, &job->ibs[0], src_offset, dst_offset,
-					cur_size_in_bytes);
+		amdgpu_emit_copy_buffer(adev, &job->ibs[0], src_offset,
+					dst_offset, cur_size_in_bytes);
 
 		src_offset += cur_size_in_bytes;
 		dst_offset += cur_size_in_bytes;
@@ -1614,7 +1384,8 @@ error_free:
 	return r;
 }
 
-int amdgpu_fill_buffer(struct amdgpu_bo *bo, uint32_t src_data,
+int amdgpu_fill_buffer(struct amdgpu_bo *bo,
+		       uint32_t src_data,
 		       struct reservation_object *resv,
 		       struct fence **fence)
 {
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h
index cf0f0a7..d1c00c0 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.h
@@ -26,13 +26,13 @@
 
 #include "gpu_scheduler.h"
 
-#define AMDGPU_PL_GDS		(TTM_PL_PRIV0 + 0)
-#define AMDGPU_PL_GWS		(TTM_PL_PRIV0 + 1)
-#define AMDGPU_PL_OA		(TTM_PL_PRIV0 + 2)
+#define AMDGPU_PL_GDS		(TTM_PL_PRIV + 0)
+#define AMDGPU_PL_GWS		(TTM_PL_PRIV + 1)
+#define AMDGPU_PL_OA		(TTM_PL_PRIV + 2)
 
-#define AMDGPU_PL_FLAG_GDS		(TTM_PL_FLAG_PRIV0 << 0)
-#define AMDGPU_PL_FLAG_GWS		(TTM_PL_FLAG_PRIV0 << 1)
-#define AMDGPU_PL_FLAG_OA		(TTM_PL_FLAG_PRIV0 << 2)
+#define AMDGPU_PL_FLAG_GDS		(TTM_PL_FLAG_PRIV << 0)
+#define AMDGPU_PL_FLAG_GWS		(TTM_PL_FLAG_PRIV << 1)
+#define AMDGPU_PL_FLAG_OA		(TTM_PL_FLAG_PRIV << 2)
 
 #define AMDGPU_TTM_LRU_SIZE	20
 
@@ -87,8 +87,5 @@ int amdgpu_fill_buffer(struct amdgpu_bo *bo,
 int amdgpu_mmap(struct file *filp, struct vm_area_struct *vma);
 bool amdgpu_ttm_is_bound(struct ttm_tt *ttm);
 int amdgpu_ttm_bind(struct ttm_buffer_object *bo, struct ttm_mem_reg *bo_mem);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-int amdgpu_bo_mmap(struct file *filp, struct vm_area_struct *vma,
-                   struct ttm_bo_device *bdev);
-#endif				   
+
 #endif
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c
index 5ac371e..6ba67cb 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c
@@ -405,7 +405,7 @@ int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 {
 	const unsigned ib_size_dw = 1024;
 	struct amdgpu_job *job;
-	struct amdgpu_ib *ib = NULL;
+	struct amdgpu_ib *ib;
 	struct fence *f = NULL;
 	uint64_t dummy;
 	int i, r;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index c44aae2..d05546e 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -59,10 +59,6 @@ struct amdgpu_pte_update_params {
 	struct amdgpu_device *adev;
 	/* address where to copy page table entries from */
 	uint64_t src;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* DMA addresses to use for mapping */
-	dma_addr_t *pages_addr;
-#endif
 	/* indirect buffer to fill with commands */
 	struct amdgpu_ib *ib;
 	/* Function which actually does the update */
@@ -71,48 +67,8 @@ struct amdgpu_pte_update_params {
 		     uint32_t flags);
 	/* indicate update pt or its shadow */
 	bool shadow;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
-	/* Kernel pointer of PD/PT BO that needs to be updated */
-	void *kptr;
-#endif	
 };
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-/**
- * write_pte_using_cpu - update the page tables using CPU
- *
- * @adev: amdgpu_device pointer
- * @pte_update_params: see structure definition
- * @pe_start: offset from start of the page table
- * @addr: dst addr to write into pe
- * @count: number of page entries to update
- * @incr: increase next addr by incr bytes
- * @flags: hw access flags
- *
- */
-static void write_pte_using_cpu(struct amdgpu_device *adev,
-				struct amdgpu_pte_update_params *params,
-				uint64_t pe_start, uint64_t addr,
-				unsigned count, uint32_t incr,
-				uint32_t flags)
-{
-	unsigned int i;
-	uint64_t page_idx, value;
-
-	page_idx = pe_start / 8;
-
-	for (i = 0; i < count; i++) {
-		value = amdgpu_vm_map_gart(params->pages_addr, addr);
-		amdgpu_gart_set_pte_pde(adev, params->kptr,
-			page_idx, value, flags);
-		addr += incr;
-		page_idx++;
-	}
-	mb();
-	amdgpu_gart_flush_gpu_tlb(adev, 0);
-}
-#endif
-
 /**
  * amdgpu_vm_num_pde - return the number of page directory entries
  *
@@ -528,9 +484,8 @@ struct amdgpu_bo_va *amdgpu_vm_bo_find(struct amdgpu_vm *vm,
  * @incr: increase next addr by incr bytes
  * @flags: hw access flags
  *
- * If kptr is available, then update pde/pte directly using CPU. Otherwise,
- * assume ib is valid and call the right asic functions to setup the page table
- * using the DMA. Also traces the parameters.
+ * Traces the parameters and calls the right asic functions
+ * to setup the page table using the DMA.
  */
 static void amdgpu_vm_do_set_ptes(struct amdgpu_pte_update_params *params,
 				  uint64_t pe, uint64_t addr,
@@ -539,20 +494,13 @@ static void amdgpu_vm_do_set_ptes(struct amdgpu_pte_update_params *params,
 {
 	trace_amdgpu_vm_set_ptes(pe, addr, count, incr, flags);
 
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	if (count < 3) {
-#else
-	if (params->kptr) {
-		write_pte_using_cpu(params->adev, params, pe, addr,
-			count, incr, flags);
-	} else if (count < 3) {
-#endif
 		amdgpu_vm_write_pte(params->adev, params->ib, pe,
 				    addr | flags, count, incr);
 
 	} else {
 		amdgpu_vm_set_pte_pde(params->adev, params->ib, pe, addr,
-				count, incr, flags);
+				      count, incr, flags);
 	}
 }
 
@@ -590,33 +538,15 @@ static void amdgpu_vm_do_copy_ptes(struct amdgpu_pte_update_params *params,
  * Look up the physical address of the page that the pte resolves
  * to and return the pointer for the page table entry.
  */
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
-#else
 static uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
-#endif
 {
 	uint64_t result;
 
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	/* page table offset */
 	result = pages_addr[addr >> PAGE_SHIFT];
 
 	/* in case cpu page size != gpu page size*/
 	result |= addr & (~PAGE_MASK);
-#else
-	if (pages_addr) {
-		/* page table offset */
-		result = pages_addr[addr >> PAGE_SHIFT];
-
-		/* in case cpu page size != gpu page size*/
-		result |= addr & (~PAGE_MASK);
-
-	} else {
-		/* No mapping required */
-		result = addr;
-	}
-#endif
 
 	result &= 0xFFFFFFFFFFFFF000ULL;
 
@@ -640,8 +570,7 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 {
 	struct amdgpu_bo *shadow;
 	struct amdgpu_ring *ring;
-	uint64_t pd_addr = 0;
-	uint64_t shadow_addr = 0;
+	uint64_t pd_addr, shadow_addr;
 	uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
 	uint64_t last_pde = ~0, last_pt = ~0, last_shadow = ~0;
 	unsigned count = 0, pt_idx, ndw;
@@ -649,7 +578,7 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 	struct amdgpu_pte_update_params params;
 	struct fence *fence = NULL;
 
-	int r = 0;
+	int r;
 
 	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
 	shadow = vm->page_directory->shadow;
@@ -660,10 +589,6 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 	/* assume the worst case */
 	ndw += vm->max_pde_used * 6;
 
-	memset(&params, 0, sizeof(params));
-	params.adev = adev;
-
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	pd_addr = amdgpu_bo_gpu_offset(vm->page_directory);
 	if (shadow) {
 		r = amdgpu_ttm_bind(&shadow->tbo, &shadow->tbo.mem);
@@ -678,46 +603,11 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 	r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
 	if (r)
 		return r;
-	params.ib = &job->ibs[0];
-#else
-	/* If is_vm_update_mode_cpu flag is set, then try to update PD table
-	 * using CPU. If failed, fallback to use SDMA
-	 */
-	if (vm->is_vm_update_mode_cpu) {
-		struct amdgpu_sync sync;
-		struct amdgpu_bo *pd = vm->page_directory;
-
-		r = amdgpu_bo_kmap(vm->page_directory,
-			&params.kptr);
-		if (r)
-			dev_warn(adev->dev, "PD table update using CPU failed. Fallback to SDMA\n");
-		else {
-			amdgpu_sync_create(&sync);
-			amdgpu_sync_resv(adev, &sync, pd->tbo.resv,
-				AMDGPU_FENCE_OWNER_VM);
-			amdgpu_sync_wait(&sync);
-			amdgpu_sync_free(&sync);
-		}
-	}
-	if (r || !vm->is_vm_update_mode_cpu) {
-		pd_addr = amdgpu_bo_gpu_offset(vm->page_directory);
-		if (shadow) {
-			r = amdgpu_ttm_bind(&shadow->tbo, &shadow->tbo.mem);
-			if (r)
-				return r;
-			shadow_addr = amdgpu_bo_gpu_offset(shadow);
-			ndw *= 2;
-		} else {
-			shadow_addr = 0;
-		}
 
-		r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
-		if (r)
-			return r;
+	memset(&params, 0, sizeof(params));
+	params.adev = adev;
+	params.ib = &job->ibs[0];
 
-		params.ib = &job->ibs[0];
-	}
-#endif
 	/* walk over the address space and update the page directory */
 	for (pt_idx = 0; pt_idx <= vm->max_pde_used; ++pt_idx) {
 		struct amdgpu_bo *bo = vm->page_tables[pt_idx].bo;
@@ -777,36 +667,28 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 				      count, incr, AMDGPU_PTE_VALID);
 	}
 
-	/* Submit IB if updating using SDMA */
-	if (params.ib) {
-		if (params.ib->length_dw == 0) {
-			amdgpu_job_free(job);
-			return 0;
-		}
-		amdgpu_ring_pad_ib(ring, params.ib);
-		amdgpu_sync_resv(adev, &job->sync, vm->page_directory->tbo.resv,
+	if (params.ib->length_dw == 0) {
+		amdgpu_job_free(job);
+		return 0;
+	}
+
+	amdgpu_ring_pad_ib(ring, params.ib);
+	amdgpu_sync_resv(adev, &job->sync, vm->page_directory->tbo.resv,
+			 AMDGPU_FENCE_OWNER_VM);
+	if (shadow)
+		amdgpu_sync_resv(adev, &job->sync, shadow->tbo.resv,
 				 AMDGPU_FENCE_OWNER_VM);
-		if (shadow)
-			amdgpu_sync_resv(adev, &job->sync, shadow->tbo.resv,
-					AMDGPU_FENCE_OWNER_VM);
 
-		WARN_ON(params.ib->length_dw > ndw);
-		r = amdgpu_job_submit(job, ring, &vm->entity,
-				AMDGPU_FENCE_OWNER_VM, &fence);
-		if (r)
-			goto error_free;
+	WARN_ON(params.ib->length_dw > ndw);
+	r = amdgpu_job_submit(job, ring, &vm->entity,
+			      AMDGPU_FENCE_OWNER_VM, &fence);
+	if (r)
+		goto error_free;
 
-		amdgpu_bo_fence(vm->page_directory, fence, true);
-		fence_put(vm->page_directory_fence);
-		vm->page_directory_fence = fence_get(fence);
-		fence_put(fence);
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
-	}
-#else
-	} else if (params.kptr) {
-		amdgpu_bo_kunmap(vm->page_directory);
-	}
-#endif
+	amdgpu_bo_fence(vm->page_directory, fence, true);
+	fence_put(vm->page_directory_fence);
+	vm->page_directory_fence = fence_get(fence);
+	fence_put(fence);
 
 	return 0;
 
@@ -816,7 +698,7 @@ error_free:
 }
 
 /**
- * amdgpu_vm_update_ptes_sdma - make sure that page tables are valid
+ * amdgpu_vm_update_ptes - make sure that page tables are valid
  *
  * @params: see amdgpu_pte_update_params definition
  * @vm: requested vm
@@ -825,19 +707,12 @@ error_free:
  * @dst: destination address to map to, the next dst inside the function
  * @flags: mapping flags
  *
- * Update the page tables in the range @start - @end using SDMA
+ * Update the page tables in the range @start - @end.
  */
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static void amdgpu_vm_update_ptes_sdma(struct amdgpu_pte_update_params *params,
-				  struct amdgpu_vm *vm,
-				  uint64_t start, uint64_t end,
-				  uint64_t dst, uint32_t flags)
-#else
 static void amdgpu_vm_update_ptes(struct amdgpu_pte_update_params *params,
 				  struct amdgpu_vm *vm,
 				  uint64_t start, uint64_t end,
 				  uint64_t dst, uint32_t flags)
-#endif
 {
 	const uint64_t mask = AMDGPU_VM_PTE_COUNT - 1;
 
@@ -914,96 +789,6 @@ static void amdgpu_vm_update_ptes(struct amdgpu_pte_update_params *params,
 		     AMDGPU_GPU_PAGE_SIZE, flags);
 }
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-/*
- * amdgpu_vm_frag_ptes - add fragment information to PTEs
- *
- * @params: see amdgpu_pte_update_params definition
- * @vm: requested vm
- * @start: first PTE to handle
- * @end: last PTE to handle
- * @dst: addr those PTEs should point to
- * @flags: hw mapping flags
- */
-static void amdgpu_vm_frag_ptes(struct amdgpu_pte_update_params	*params,
-				struct amdgpu_vm *vm,
-				uint64_t start, uint64_t end,
-				uint64_t dst, uint32_t flags)
-{
-	/**
-	 * The MC L1 TLB supports variable sized pages, based on a fragment
-	 * field in the PTE. When this field is set to a non-zero value, page
-	 * granularity is increased from 4KB to (1 << (12 + frag)). The PTE
-	 * flags are considered valid for all PTEs within the fragment range
-	 * and corresponding mappings are assumed to be physically contiguous.
-	 *
-	 * The L1 TLB can store a single PTE for the whole fragment,
-	 * significantly increasing the space available for translation
-	 * caching. This leads to large improvements in throughput when the
-	 * TLB is under pressure.
-	 *
-	 * The L2 TLB distributes small and large fragments into two
-	 * asymmetric partitions. The large fragment cache is significantly
-	 * larger. Thus, we try to use large fragments wherever possible.
-	 * Userspace can support this by aligning virtual base address and
-	 * allocation size to the fragment size.
-	 */
-
-	/* SI and newer are optimized for 64KB */
-	uint64_t frag_flags = AMDGPU_PTE_FRAG(AMDGPU_LOG2_PAGES_PER_FRAG);
-	uint64_t frag_align = 1 << AMDGPU_LOG2_PAGES_PER_FRAG;
-
-	uint64_t frag_start = ALIGN(start, frag_align);
-	uint64_t frag_end = end & ~(frag_align - 1);
-
-	unsigned count;
-	uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
-	
-	/* system pages are non continuously */
-	if (params->src || params->pages_addr || !(flags & AMDGPU_PTE_VALID) ||
-	    (frag_start >= frag_end)) {
-
-		count = (end - start)/8;
-		if (params->kptr)
-			write_pte_using_cpu(params->adev, params, start, dst, count, incr, flags);
-		else
-			amdgpu_vm_update_ptes_sdma(params, vm, start, end, dst, flags);
-		return;
-	}
-
-	/* handle the 4K area at the beginning */
-	if (start != frag_start) {
-	
-		count = (frag_start - start)/8;
-		if (params->kptr)
-			write_pte_using_cpu(params->adev, params, start, dst, count, incr, flags);
-		else
-			amdgpu_vm_update_ptes_sdma(params, vm, start, frag_start,
-				      	dst, flags);
-		dst += (frag_start - start) * AMDGPU_GPU_PAGE_SIZE;
-	}
-
-	/* handle the area in the middle */
-	count = (frag_end - frag_start)/8;
-	if (params->kptr)
-		write_pte_using_cpu(params->adev, params, frag_start, dst, count, incr, flags);
-	else	
-		amdgpu_vm_update_ptes_sdma(params, vm, frag_start, frag_end, dst,
-		      flags | frag_flags);
-
-	/* handle the 4K area at the end */
-	if (frag_end != end) {
-		dst += (frag_end - frag_start) * AMDGPU_GPU_PAGE_SIZE;
-		count = (end - frag_end)/8;
-		if (params->kptr)
-			write_pte_using_cpu(params->adev, params, frag_end, dst, count, incr, flags);
-		else		
-			amdgpu_vm_update_ptes_sdma(params, vm, frag_end, end, dst, flags);
-	}
-}
-
-#else
-
 /*
  * amdgpu_vm_frag_ptes - add fragment information to PTEs
  *
@@ -1071,64 +856,6 @@ static void amdgpu_vm_frag_ptes(struct amdgpu_pte_update_params	*params,
 	}
 }
 
-#endif
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-/**
- * amdgpu_vm_update_ptes_cpu - make sure that page tables are valid
- *
- * @adev: amdgpu_device pointer
- * @pte_update_params: see amdgpu_vm_update_params definition
- * @vm: requested vm
- * @start: start of GPU address range
- * @end: end of GPU address range
- * @dst: destination address to map to
- * @flags: mapping flags
- *
- * Update the page tables in the range @start - @end using CPU
- */
-static int amdgpu_vm_update_ptes_cpu(struct amdgpu_pte_update_params
-					*vm_update_params,
-				     struct amdgpu_vm *vm,
-				     uint64_t start, uint64_t end,
-				     uint64_t dst, uint32_t flags)
-{
-	const uint64_t mask = AMDGPU_VM_PTE_COUNT - 1;
-	int r;
-	uint64_t addr;
-	struct amdgpu_bo *pt = NULL;
-
-	/* walk over the address space and update the page tables */
-	for (addr = start; addr < end; ) {
-		uint64_t pt_idx = addr >> amdgpu_vm_block_size;
-		unsigned nptes;
-		uint64_t pe_start;
-
-		pt = vm->page_tables[pt_idx].bo;
-		r = amdgpu_bo_kmap(pt, &vm_update_params->kptr);
-		if (r)
-			return r;
-
-		if ((addr & ~mask) == (end & ~mask))
-			nptes = end - addr;
-		else
-			nptes = AMDGPU_VM_PTE_COUNT - (addr & mask);
-
-		pe_start = (addr & mask) * 8;
-
-		amdgpu_vm_frag_ptes(vm_update_params, vm,
-			pe_start, pe_start + 8 * nptes,
-			dst, flags);
-
-		addr += nptes;
-		dst += nptes * AMDGPU_GPU_PAGE_SIZE;
-
-		amdgpu_bo_kunmap(pt);
-	}
-	return 0;
-}
-#endif
-
 /**
  * amdgpu_vm_bo_update_mapping - update a mapping in the vm page table
  *
@@ -1166,39 +893,12 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 	memset(&params, 0, sizeof(params));
 	params.adev = adev;
 	params.src = src;
-	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 
+	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
 
-	params.pages_addr = pages_addr;
-
-	/* If update flag is set to CPU, then try to update PT entries directly
-	 * by CPU. If failed, fallback to SDMA update.
-	 */
-	if (vm->is_vm_update_mode_cpu) {
-		struct amdgpu_sync sync;
-
-		amdgpu_sync_create(&sync);
-		r = amdgpu_sync_resv(adev, &sync, vm->page_directory->tbo.resv,
-			owner);
-		if (r) {
-			dev_warn(adev->dev, "CPU pte update failed. Fallback to SDMA\n");
-			goto fallback_sdma_update;
-		}
-		amdgpu_sync_wait(&sync);
-		amdgpu_sync_free(&sync);
-
-		r = amdgpu_vm_update_ptes_cpu(&params,
-			vm, start, last + 1, addr, flags);
-		if (r) {
-			dev_info(adev->dev, "CPU pte update failed. Fallback to sdma\n");
-			goto fallback_sdma_update;
-		}
-		return r;
-	}
-
-fallback_sdma_update:
-#endif
+	memset(&params, 0, sizeof(params));
+	params.adev = adev;
+	params.src = src;
 
 	/* sync to everything on unmapping */
 	if (!(flags & AMDGPU_PTE_VALID))
@@ -1534,11 +1234,7 @@ int amdgpu_vm_clear_invalids(struct amdgpu_device *adev,
 	}
 	spin_unlock(&vm->status_lock);
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	if (bo_va && sync)
-#else
 	if (bo_va)
-#endif
 		r = amdgpu_sync_fence(adev, sync, bo_va->last_pt_update);
 
 	return r;
@@ -1673,13 +1369,7 @@ int amdgpu_vm_bo_map(struct amdgpu_device *adev,
 		r = amdgpu_bo_create(adev, AMDGPU_VM_PTE_COUNT * 8,
 				     AMDGPU_GPU_PAGE_SIZE, true,
 				     AMDGPU_GEM_DOMAIN_VRAM,
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-				     vm->is_vm_update_mode_cpu ?
-					AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED :
-					AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
-#else
 				     AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
-#endif
 				     AMDGPU_GEM_CREATE_SHADOW |
 				     AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS |
 				     AMDGPU_GEM_CREATE_VRAM_CLEARED,
@@ -1827,12 +1517,7 @@ void amdgpu_vm_bo_invalidate(struct amdgpu_device *adev,
  *
  * Init @vm fields.
  */
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		   bool vm_update_mode, bool is_kfd_vm)
-#else
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm)
-#endif
 {
 	const unsigned align = min(AMDGPU_VM_PTB_ALIGN_SIZE,
 		AMDGPU_VM_PTE_COUNT * 8);
@@ -1872,23 +1557,11 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 	if (r)
 		goto err;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	vm->is_vm_update_mode_cpu = vm_update_mode;
-	DRM_DEBUG_DRIVER("VM update mode is %s\n",
-			 vm_update_mode ? "CPU" : "SDMA");
-#endif
-
 	vm->page_directory_fence = NULL;
 
 	r = amdgpu_bo_create(adev, pd_size, align, true,
 			     AMDGPU_GEM_DOMAIN_VRAM,
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-			     vm->is_vm_update_mode_cpu ?
-				AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED :
-				AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
-#else
 			     AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
-#endif
 			     AMDGPU_GEM_CREATE_SHADOW |
 			     AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS |
 			     AMDGPU_GEM_CREATE_VRAM_CLEARED,
@@ -1903,25 +1576,6 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 	vm->last_eviction_counter = atomic64_read(&adev->num_evictions);
 	amdgpu_bo_unreserve(vm->page_directory);
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	vm->is_kfd_vm = is_kfd_vm;
-	if (is_kfd_vm) {
-		mutex_lock(&adev->vm_manager.lock);
-
-		if (adev->vm_manager.n_kfd_vms++ == 0) {
-			/* First KFD VM: enable compute power profile */
-			if (adev->pp_enabled)
-				amdgpu_dpm_switch_power_profile(adev,
-						PP_COMPUTE_PROFILE);
-			else if (adev->pm.funcs->switch_power_profile)
-				adev->pm.funcs->switch_power_profile(adev,
-						PP_COMPUTE_PROFILE);
-		}
-
-		mutex_unlock(&adev->vm_manager.lock);
-	}
-#endif
-
 	return 0;
 
 error_free_page_directory:
@@ -1952,26 +1606,6 @@ void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 	struct amdgpu_bo_va_mapping *mapping, *tmp;
 	int i;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	if (vm->is_kfd_vm) {
-		mutex_lock(&adev->vm_manager.lock);
-
-		WARN(adev->vm_manager.n_kfd_vms == 0, "Unbalanced number of KFD VMs");
-
-		if (--adev->vm_manager.n_kfd_vms == 0) {
-			/* Last KFD VM: enable graphics power profile */
-			if (adev->pp_enabled)
-				amdgpu_dpm_switch_power_profile(adev,
-						PP_GFX_PROFILE);
-			else if (adev->pm.funcs->switch_power_profile)
-				adev->pm.funcs->switch_power_profile(adev,
-						PP_GFX_PROFILE);
-		}
-
-		mutex_unlock(&adev->vm_manager.lock);
-	}
-#endif
-
 	amd_sched_entity_fini(vm->entity.sched, &vm->entity);
 
 	if (!RB_EMPTY_ROOT(&vm->va)) {
@@ -2030,10 +1664,6 @@ void amdgpu_vm_manager_init(struct amdgpu_device *adev)
 
 	atomic_set(&adev->vm_manager.vm_pte_next_ring, 0);
 	atomic64_set(&adev->vm_manager.client_counter, 0);
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	adev->vm_manager.n_kfd_vms = 0;
-#endif
 }
 
 /**
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
index 773c2d8..42a629b 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
@@ -70,16 +70,6 @@ struct amdgpu_bo_list_entry;
 #define AMDGPU_VM_FAULT_STOP_FIRST	1
 #define AMDGPU_VM_FAULT_STOP_ALWAYS	2
 
-/* amdgpu_vm_update_context - User parameter that controls how VM pde/pte are
- *  updated for Graphics and Compute. BIT0 controls Graphics and BIT1 Compute.
- *  BIT0 [= 0] Graphics updated by SDMA [= 1] by CPU
- *  BIT1 [= 0] Compute updated by SDMA [= 1] by CPU
- *
- *  NOTE: CPU update is supported only for Large BAR systems
- */
-#define AMDGPU_VM_USE_CPU_UPDATE_FOR_GRAPHICS_MASK (1 << 0)
-#define AMDGPU_VM_USE_CPU_UPDATE_FOR_COMPUTE_MASK (1 << 1)
-
 struct amdgpu_vm_pt {
 	struct amdgpu_bo	*bo;
 	uint64_t		addr;
@@ -121,13 +111,6 @@ struct amdgpu_vm {
 
 	/* client id */
 	u64                     client_id;
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* Flag to indicate if VM tables are updated by CPU or GPU (SDMA) */
-	bool			is_vm_update_mode_cpu : 1;
-	/* Flag to indicate if this is a KFD VM */
-	bool			is_kfd_vm : 1;
-#endif
 };
 
 struct amdgpu_vm_id {
@@ -174,21 +157,11 @@ struct amdgpu_vm_manager {
 	atomic_t				vm_pte_next_ring;
 	/* client id counter */
 	atomic64_t				client_counter;
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* Number of KFD VMs, used for detecting KFD activity */
-	unsigned				n_kfd_vms;
-#endif
 };
 
 void amdgpu_vm_manager_init(struct amdgpu_device *adev);
 void amdgpu_vm_manager_fini(struct amdgpu_device *adev);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		  bool vm_update_mode, bool is_kfd_vm);
-#else
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm);
-#endif
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 void amdgpu_vm_get_pd_bo(struct amdgpu_vm *vm,
 			 struct list_head *validated,
@@ -203,9 +176,6 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 		      struct amdgpu_job *job);
 int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job);
 void amdgpu_vm_reset_id(struct amdgpu_device *adev, unsigned vm_id);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr);
-#endif
 int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 				    struct amdgpu_vm *vm);
 int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
diff --git a/drivers/gpu/drm/amd/amdgpu/ci_dpm.c b/drivers/gpu/drm/amd/amdgpu/ci_dpm.c
index cba9c77..ab7d2bb 100644
--- a/drivers/gpu/drm/amd/amdgpu/ci_dpm.c
+++ b/drivers/gpu/drm/amd/amdgpu/ci_dpm.c
@@ -3673,42 +3673,6 @@ static int ci_find_boot_level(struct ci_single_dpm_table *table,
 	return ret;
 }
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static void ci_save_default_power_profile(struct amdgpu_device *adev)
-{
-	struct ci_power_info *pi = ci_get_pi(adev);
-	struct SMU7_Discrete_GraphicsLevel *levels =
-				pi->smc_state_table.GraphicsLevel;
-	uint32_t min_level = 0;
-
-	pi->default_gfx_power_profile.activity_threshold =
-			be16_to_cpu(levels[0].ActivityLevel);
-	pi->default_gfx_power_profile.up_hyst = levels[0].UpH;
-	pi->default_gfx_power_profile.down_hyst = levels[0].DownH;
-	pi->default_gfx_power_profile.type = PP_GFX_PROFILE;
-
-	pi->default_compute_power_profile = pi->default_gfx_power_profile;
-	pi->default_compute_power_profile.type = PP_COMPUTE_PROFILE;
-
-	/* Optimize compute power profile: Use only highest
-	 * 2 power levels (if more than 2 are available), Hysteresis:
-	 * 0ms up, 5ms down
-	 */
-	if (pi->smc_state_table.GraphicsDpmLevelCount > 2)
-		min_level = pi->smc_state_table.GraphicsDpmLevelCount - 2;
-	else if (pi->smc_state_table.GraphicsDpmLevelCount == 2)
-		min_level = 1;
-	pi->default_compute_power_profile.min_sclk =
-			be32_to_cpu(levels[min_level].SclkFrequency);
-
-	pi->default_compute_power_profile.up_hyst = 0;
-	pi->default_compute_power_profile.down_hyst = 5;
-
-	pi->gfx_power_profile = pi->default_gfx_power_profile;
-	pi->compute_power_profile = pi->default_compute_power_profile;
-}
-#endif
-
 static int ci_init_smc_table(struct amdgpu_device *adev)
 {
 	struct ci_power_info *pi = ci_get_pi(adev);
@@ -3854,10 +3818,6 @@ static int ci_init_smc_table(struct amdgpu_device *adev)
 	if (ret)
 		return ret;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	ci_save_default_power_profile(adev);
-#endif
-
 	return 0;
 }
 
@@ -6719,220 +6679,6 @@ static int ci_dpm_set_mclk_od(struct amdgpu_device *adev, uint32_t value)
 	return 0;
 }
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static int ci_dpm_get_power_profile_state(struct amdgpu_device *adev,
-		struct pp_profile *query)
-{
-	struct ci_power_info *pi = ci_get_pi(adev);
-
-	if (!pi || !query)
-		return -EINVAL;
-
-	if (query->type == PP_GFX_PROFILE)
-		memcpy(query, &pi->gfx_power_profile,
-				sizeof(struct pp_profile));
-	else if (query->type == PP_COMPUTE_PROFILE)
-		memcpy(query, &pi->compute_power_profile,
-				sizeof(struct pp_profile));
-	else
-		return -EINVAL;
-
-	return 0;
-}
-
-static int ci_populate_requested_graphic_levels(struct amdgpu_device *adev,
-		struct pp_profile *request)
-{
-	struct ci_power_info *pi = ci_get_pi(adev);
-	struct ci_dpm_table *dpm_table = &(pi->dpm_table);
-	struct SMU7_Discrete_GraphicsLevel *levels =
-			pi->smc_state_table.GraphicsLevel;
-	uint32_t array = pi->dpm_table_start +
-			offsetof(SMU7_Discrete_DpmTable, GraphicsLevel);
-	uint32_t array_size = sizeof(struct SMU7_Discrete_GraphicsLevel) *
-			SMU7_MAX_LEVELS_GRAPHICS;
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		levels[i].ActivityLevel =
-				cpu_to_be16(request->activity_threshold);
-		levels[i].EnabledForActivity = 1;
-		levels[i].UpH = request->up_hyst;
-		levels[i].DownH = request->down_hyst;
-	}
-
-	return amdgpu_ci_copy_bytes_to_smc(adev, array, (uint8_t *)levels,
-				array_size, pi->sram_end);
-}
-
-static void ci_find_min_clock_masks(struct amdgpu_device *adev,
-		uint32_t *sclk_mask, uint32_t *mclk_mask,
-		uint32_t min_sclk, uint32_t min_mclk)
-{
-	struct ci_power_info *pi = ci_get_pi(adev);
-	struct ci_dpm_table *dpm_table = &(pi->dpm_table);
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		if (dpm_table->sclk_table.dpm_levels[i].enabled &&
-			dpm_table->sclk_table.dpm_levels[i].value >= min_sclk)
-			*sclk_mask |= 1 << i;
-	}
-
-	for (i = 0; i < dpm_table->mclk_table.count; i++) {
-		if (dpm_table->mclk_table.dpm_levels[i].enabled &&
-			dpm_table->mclk_table.dpm_levels[i].value >= min_mclk)
-			*mclk_mask |= 1 << i;
-	}
-}
-
-static int ci_set_power_profile_state(struct amdgpu_device *adev,
-		struct pp_profile *request)
-{
-	struct ci_power_info *pi = ci_get_pi(adev);
-	int tmp_result, result = 0;
-	uint32_t sclk_mask = 0, mclk_mask = 0;
-
-	tmp_result = ci_freeze_sclk_mclk_dpm(adev);
-	if (tmp_result) {
-		DRM_ERROR("Failed to freeze SCLK MCLK DPM!");
-		result = tmp_result;
-	}
-
-	tmp_result = ci_populate_requested_graphic_levels(adev,
-			request);
-	if (tmp_result) {
-		DRM_ERROR("Failed to populate requested graphic levels!");
-		result = tmp_result;
-	}
-
-	tmp_result = ci_unfreeze_sclk_mclk_dpm(adev);
-	if (tmp_result) {
-		DRM_ERROR("Failed to unfreeze SCLK MCLK DPM!");
-		result = tmp_result;
-	}
-
-	ci_find_min_clock_masks(adev, &sclk_mask, &mclk_mask,
-			request->min_sclk, request->min_mclk);
-
-	if (sclk_mask) {
-		if (!pi->sclk_dpm_key_disabled)
-			amdgpu_ci_send_msg_to_smc_with_parameter(
-				adev,
-				PPSMC_MSG_SCLKDPM_SetEnabledMask,
-				pi->dpm_level_enable_mask.
-				sclk_dpm_enable_mask &
-				sclk_mask);
-	}
-
-	if (mclk_mask) {
-		if (!pi->mclk_dpm_key_disabled)
-			amdgpu_ci_send_msg_to_smc_with_parameter(
-				adev,
-				PPSMC_MSG_MCLKDPM_SetEnabledMask,
-				pi->dpm_level_enable_mask.
-				mclk_dpm_enable_mask &
-				mclk_mask);
-	}
-
-
-	return result;
-}
-
-static int ci_dpm_set_power_profile_state(struct amdgpu_device *adev,
-		struct pp_profile *request)
-{
-	struct ci_power_info *pi = ci_get_pi(adev);
-	int ret = -1;
-
-	if (!pi || !request)
-		return -EINVAL;
-
-	if (adev->pm.dpm.forced_level !=
-			AMDGPU_DPM_FORCED_LEVEL_AUTO)
-		return -EINVAL;
-
-	if (request->min_sclk ||
-		request->min_mclk ||
-		request->activity_threshold ||
-		request->up_hyst ||
-		request->down_hyst) {
-		if (request->type == PP_GFX_PROFILE)
-			memcpy(&pi->gfx_power_profile, request,
-					sizeof(struct pp_profile));
-		else if (request->type == PP_COMPUTE_PROFILE)
-			memcpy(&pi->compute_power_profile, request,
-					sizeof(struct pp_profile));
-		else
-			return -EINVAL;
-
-		if (request->type == pi->current_power_profile)
-			ret = ci_set_power_profile_state(
-					adev,
-					request);
-	} else {
-		/* set power profile if it exists */
-		switch (request->type) {
-		case PP_GFX_PROFILE:
-			ret = ci_set_power_profile_state(
-				adev,
-				&pi->gfx_power_profile);
-			break;
-		case PP_COMPUTE_PROFILE:
-			ret = ci_set_power_profile_state(
-				adev,
-				&pi->compute_power_profile);
-			break;
-		default:
-			return -EINVAL;
-		}
-	}
-
-	if (!ret)
-		pi->current_power_profile = request->type;
-
-	return 0;
-}
-
-static int ci_dpm_reset_power_profile_state(struct amdgpu_device *adev,
-		struct pp_profile *request)
-{
-	struct ci_power_info *pi = ci_get_pi(adev);
-
-	if (!pi || !request)
-		return -EINVAL;
-
-	if (request->type == PP_GFX_PROFILE) {
-		pi->gfx_power_profile = pi->default_gfx_power_profile;
-		return ci_dpm_set_power_profile_state(adev,
-					  &pi->gfx_power_profile);
-	} else if (request->type == PP_COMPUTE_PROFILE) {
-		pi->compute_power_profile =
-			pi->default_compute_power_profile;
-		return ci_dpm_set_power_profile_state(adev,
-					  &pi->compute_power_profile);
-	} else
-		return -EINVAL;
-}
-
-static int ci_dpm_switch_power_profile(struct amdgpu_device *adev,
-		enum pp_profile_type type)
-{
-	struct ci_power_info *pi = ci_get_pi(adev);
-	struct pp_profile request = {0};
-
-	if (!pi)
-		return -EINVAL;
-
-	if (pi->current_power_profile != type) {
-		request.type = type;
-		return ci_dpm_set_power_profile_state(adev, &request);
-	}
-
-	return 0;
-}
-#endif
-
 const struct amd_ip_funcs ci_dpm_ip_funcs = {
 	.name = "ci_dpm",
 	.early_init = ci_dpm_early_init,
@@ -6973,12 +6719,6 @@ static const struct amdgpu_dpm_funcs ci_dpm_funcs = {
 	.set_sclk_od = ci_dpm_set_sclk_od,
 	.get_mclk_od = ci_dpm_get_mclk_od,
 	.set_mclk_od = ci_dpm_set_mclk_od,
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	.get_power_profile_state = ci_dpm_get_power_profile_state,
-	.set_power_profile_state = ci_dpm_set_power_profile_state,
-	.reset_power_profile_state = ci_dpm_reset_power_profile_state,
-	.switch_power_profile = ci_dpm_switch_power_profile,
-#endif
 	.check_state_equal = ci_check_state_equal,
 	.get_vce_clock_state = amdgpu_get_vce_clock_state,
 };
diff --git a/drivers/gpu/drm/amd/amdgpu/ci_dpm.h b/drivers/gpu/drm/amd/amdgpu/ci_dpm.h
index 4e18fd7..91be299 100644
--- a/drivers/gpu/drm/amd/amdgpu/ci_dpm.h
+++ b/drivers/gpu/drm/amd/amdgpu/ci_dpm.h
@@ -25,9 +25,6 @@
 
 #include "amdgpu_atombios.h"
 #include "ppsmc.h"
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#include "amd_powerplay.h"
-#endif
 
 #define SMU__NUM_SCLK_DPM_STATE  8
 #define SMU__NUM_MCLK_DPM_LEVELS 6
@@ -298,15 +295,6 @@ struct ci_power_info {
 	bool fan_is_controlled_by_smc;
 	u32 t_min;
 	u32 fan_ctrl_default_mode;
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* power profile */
-	struct pp_profile gfx_power_profile;
-	struct pp_profile compute_power_profile;
-	struct pp_profile default_gfx_power_profile;
-	struct pp_profile default_compute_power_profile;
-	enum pp_profile_type current_power_profile;
-#endif
 };
 
 #define CISLANDS_VOLTAGE_CONTROL_NONE                   0x0
diff --git a/drivers/gpu/drm/amd/amdgpu/cik.c b/drivers/gpu/drm/amd/amdgpu/cik.c
index 571806d..ef3b302 100644
--- a/drivers/gpu/drm/amd/amdgpu/cik.c
+++ b/drivers/gpu/drm/amd/amdgpu/cik.c
@@ -1818,17 +1818,21 @@ static int cik_common_suspend(void *handle)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
 	amdgpu_amdkfd_suspend(adev);
-#endif
+
 	return cik_common_hw_fini(adev);
 }
 
 static int cik_common_resume(void *handle)
 {
+	int r;
 	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
 
-	return cik_common_hw_init(adev);
+	r = cik_common_hw_init(adev);
+	if (r)
+		return r;
+
+	return amdgpu_amdkfd_resume(adev);
 }
 
 static bool cik_common_is_idle(void *handle)
diff --git a/drivers/gpu/drm/amd/amdgpu/cik_sdma.c b/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
index 13c9a76..c7340b6 100644
--- a/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
+++ b/drivers/gpu/drm/amd/amdgpu/cik_sdma.c
@@ -342,65 +342,6 @@ static void cik_sdma_rlc_stop(struct amdgpu_device *adev)
 	/* XXX todo */
 }
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-/**
- * cik_ctx_switch_enable - stop the async dma engines context switch
- *
- * @adev: amdgpu_device pointer
- * @enable: enable/disable the DMA MEs context switch.
- *
- * Halt or unhalt the async dma engines context switch (VI).
- */
-static void cik_ctx_switch_enable(struct amdgpu_device *adev, bool enable)
-{
-	u32 f32_cntl, phase_quantum = 0;
-	int i;
-
-	if (amdgpu_sdma_phase_quantum) {
-		unsigned value = amdgpu_sdma_phase_quantum;
-		unsigned unit = 0;
-
-		while (value > (SDMA0_PHASE0_QUANTUM__VALUE_MASK >>
-				SDMA0_PHASE0_QUANTUM__VALUE__SHIFT)) {
-			value = (value + 1) >> 1;
-			unit++;
-		}
-		if (unit > (SDMA0_PHASE0_QUANTUM__UNIT_MASK >>
-			    SDMA0_PHASE0_QUANTUM__UNIT__SHIFT)) {
-			value = (SDMA0_PHASE0_QUANTUM__VALUE_MASK >>
-				 SDMA0_PHASE0_QUANTUM__VALUE__SHIFT);
-			unit = (SDMA0_PHASE0_QUANTUM__UNIT_MASK >>
-				SDMA0_PHASE0_QUANTUM__UNIT__SHIFT);
-			WARN_ONCE(1,
-			"clamping sdma_phase_quantum to %uK clock cycles\n",
-				  value << unit);
-		}
-		phase_quantum =
-			value << SDMA0_PHASE0_QUANTUM__VALUE__SHIFT |
-			unit  << SDMA0_PHASE0_QUANTUM__UNIT__SHIFT;
-	}
-
-	for (i = 0; i < adev->sdma.num_instances; i++) {
-		f32_cntl = RREG32(mmSDMA0_CNTL + sdma_offsets[i]);
-		if (enable) {
-			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
-					AUTO_CTXSW_ENABLE, 1);
-			if (amdgpu_sdma_phase_quantum) {
-				WREG32(mmSDMA0_PHASE0_QUANTUM + sdma_offsets[i],
-				       phase_quantum);
-				WREG32(mmSDMA0_PHASE1_QUANTUM + sdma_offsets[i],
-				       phase_quantum);
-			}
-		} else {
-			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
-					AUTO_CTXSW_ENABLE, 0);
-		}
-
-		WREG32(mmSDMA0_CNTL + sdma_offsets[i], f32_cntl);
-	}
-}
-#endif
-
 /**
  * cik_sdma_enable - stop the async dma engines
  *
@@ -597,10 +538,6 @@ static int cik_sdma_start(struct amdgpu_device *adev)
 
 	/* halt the engine before programing */
 	cik_sdma_enable(adev, false);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* enable sdma ring preemption */
-	cik_ctx_switch_enable(adev, true);
-#endif
 
 	/* start the gfx rings and rlc compute queues */
 	r = cik_sdma_gfx_resume(adev);
@@ -1045,9 +982,6 @@ static int cik_sdma_hw_fini(void *handle)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	cik_ctx_switch_enable(adev, false);
-#endif
 	cik_sdma_enable(adev, false);
 
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdgpu/cikd.h b/drivers/gpu/drm/amd/amdgpu/cikd.h
index bbf80fa..6cbd913 100644
--- a/drivers/gpu/drm/amd/amdgpu/cikd.h
+++ b/drivers/gpu/drm/amd/amdgpu/cikd.h
@@ -562,7 +562,7 @@
 #define	PRIVATE_BASE(x)	((x) << 0) /* scratch */
 #define	SHARED_BASE(x)	((x) << 16) /* LDS */
 
-#define KFD_CIK_SDMA_QUEUE_OFFSET (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL)
+#define KFD_CIK_SDMA_QUEUE_OFFSET	0x200
 
 /* valid for both DEFAULT_MTYPE and APE1_MTYPE */
 enum {
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_v10_0.c b/drivers/gpu/drm/amd/amdgpu/dce_v10_0.c
index 73d5d1c..4fa1293 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_v10_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/dce_v10_0.c
@@ -568,8 +568,8 @@ void dce_v10_0_resume_mc_access(struct amdgpu_device *adev,
 
 		if (save->crtc_enabled[i]) {
 			tmp = RREG32(mmMASTER_UPDATE_MODE + crtc_offsets[i]);
-			if (REG_GET_FIELD(tmp, MASTER_UPDATE_MODE, MASTER_UPDATE_MODE) != 3) {
-				tmp = REG_SET_FIELD(tmp, MASTER_UPDATE_MODE, MASTER_UPDATE_MODE, 3);
+			if (REG_GET_FIELD(tmp, MASTER_UPDATE_MODE, MASTER_UPDATE_MODE) != 0) {
+				tmp = REG_SET_FIELD(tmp, MASTER_UPDATE_MODE, MASTER_UPDATE_MODE, 0);
 				WREG32(mmMASTER_UPDATE_MODE + crtc_offsets[i], tmp);
 			}
 			tmp = RREG32(mmGRPH_UPDATE + crtc_offsets[i]);
@@ -2032,6 +2032,7 @@ static int dce_v10_0_crtc_do_set_base(struct drm_crtc *crtc,
 	u32 tmp, viewport_w, viewport_h;
 	int r;
 	bool bypass_lut = false;
+	char *format_name;
 
 	/* no fb bound */
 	if (!atomic && !crtc->primary->fb) {
@@ -2143,8 +2144,9 @@ static int dce_v10_0_crtc_do_set_base(struct drm_crtc *crtc,
 		bypass_lut = true;
 		break;
 	default:
-		DRM_ERROR("Unsupported screen format %s\n",
-			drm_get_format_name(target_fb->pixel_format));
+		format_name = drm_get_format_name(target_fb->pixel_format);
+		DRM_ERROR("Unsupported screen format %s\n", format_name);
+		kfree(format_name);
 		return -EINVAL;
 	}
 
@@ -2236,8 +2238,8 @@ static int dce_v10_0_crtc_do_set_base(struct drm_crtc *crtc,
 	WREG32(mmVIEWPORT_SIZE + amdgpu_crtc->crtc_offset,
 	       (viewport_w << 16) | viewport_h);
 
-	/* set pageflip to happen only at start of vblank interval (front porch) */
-	WREG32(mmMASTER_UPDATE_MODE + amdgpu_crtc->crtc_offset, 3);
+	/* set pageflip to happen anywhere in vblank interval */
+	WREG32(mmMASTER_UPDATE_MODE + amdgpu_crtc->crtc_offset, 0);
 
 	if (!atomic && fb && fb != crtc->primary->fb) {
 		amdgpu_fb = to_amdgpu_framebuffer(fb);
@@ -2554,7 +2556,7 @@ static int dce_v10_0_crtc_cursor_set2(struct drm_crtc *crtc,
 		return -EINVAL;
 	}
 
-	obj = drm_gem_object_lookup(crtc->dev, file_priv, handle);
+	obj = drm_gem_object_lookup(file_priv, handle);
 	if (!obj) {
 		DRM_ERROR("Cannot find cursor object %x for crtc %d\n", handle, amdgpu_crtc->crtc_id);
 		return -ENOENT;
@@ -2637,19 +2639,21 @@ static void dce_v10_0_cursor_reset(struct drm_crtc *crtc)
 	}
 }
 
-static void dce_v10_0_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,
-				    u16 *blue, uint32_t start, uint32_t size)
+static int dce_v10_0_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,
+				    u16 *blue, uint32_t size)
 {
 	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
-	int end = (start + size > 256) ? 256 : start + size, i;
+	int i;
 
 	/* userspace palettes are always correct as is */
-	for (i = start; i < end; i++) {
+	for (i = 0; i < size; i++) {
 		amdgpu_crtc->lut_r[i] = red[i] >> 6;
 		amdgpu_crtc->lut_g[i] = green[i] >> 6;
 		amdgpu_crtc->lut_b[i] = blue[i] >> 6;
 	}
 	dce_v10_0_crtc_load_lut(crtc);
+
+	return 0;
 }
 
 static void dce_v10_0_crtc_destroy(struct drm_crtc *crtc)
@@ -2666,7 +2670,7 @@ static const struct drm_crtc_funcs dce_v10_0_crtc_funcs = {
 	.gamma_set = dce_v10_0_crtc_gamma_set,
 	.set_config = amdgpu_crtc_set_config,
 	.destroy = dce_v10_0_crtc_destroy,
-	.page_flip = amdgpu_crtc_page_flip,
+	.page_flip_target = amdgpu_crtc_page_flip_target,
 };
 
 static void dce_v10_0_crtc_dpms(struct drm_crtc *crtc, int mode)
@@ -2687,13 +2691,13 @@ static void dce_v10_0_crtc_dpms(struct drm_crtc *crtc, int mode)
 		type = amdgpu_crtc_idx_to_irq_type(adev, amdgpu_crtc->crtc_id);
 		amdgpu_irq_update(adev, &adev->crtc_irq, type);
 		amdgpu_irq_update(adev, &adev->pageflip_irq, type);
-		drm_vblank_on(dev, amdgpu_crtc->crtc_id);
+		drm_crtc_vblank_on(crtc);
 		dce_v10_0_crtc_load_lut(crtc);
 		break;
 	case DRM_MODE_DPMS_STANDBY:
 	case DRM_MODE_DPMS_SUSPEND:
 	case DRM_MODE_DPMS_OFF:
-		drm_vblank_off(dev, amdgpu_crtc->crtc_id);
+		drm_crtc_vblank_off(crtc);
 		if (amdgpu_crtc->enabled) {
 			dce_v10_0_vga_enable(crtc, true);
 			amdgpu_atombios_crtc_blank(crtc, ATOM_ENABLE);
@@ -3344,7 +3348,7 @@ static int dce_v10_0_pageflip_irq(struct amdgpu_device *adev,
 
 	spin_unlock_irqrestore(&adev->ddev->event_lock, flags);
 
-	drm_vblank_put(adev->ddev, amdgpu_crtc->crtc_id);
+	drm_crtc_vblank_put(&amdgpu_crtc->base);
 	schedule_work(&works->unpin_work);
 
 	return 0;
@@ -3701,7 +3705,7 @@ static void dce_v10_0_encoder_add(struct amdgpu_device *adev,
 	case ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC1:
 	case ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC2:
 		drm_encoder_init(dev, encoder, &dce_v10_0_encoder_funcs,
-				 DRM_MODE_ENCODER_DAC);
+				 DRM_MODE_ENCODER_DAC, NULL);
 		drm_encoder_helper_add(encoder, &dce_v10_0_dac_helper_funcs);
 		break;
 	case ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DVO1:
@@ -3712,15 +3716,15 @@ static void dce_v10_0_encoder_add(struct amdgpu_device *adev,
 		if (amdgpu_encoder->devices & (ATOM_DEVICE_LCD_SUPPORT)) {
 			amdgpu_encoder->rmx_type = RMX_FULL;
 			drm_encoder_init(dev, encoder, &dce_v10_0_encoder_funcs,
-					 DRM_MODE_ENCODER_LVDS);
+					 DRM_MODE_ENCODER_LVDS, NULL);
 			amdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_lcd_info(amdgpu_encoder);
 		} else if (amdgpu_encoder->devices & (ATOM_DEVICE_CRT_SUPPORT)) {
 			drm_encoder_init(dev, encoder, &dce_v10_0_encoder_funcs,
-					 DRM_MODE_ENCODER_DAC);
+					 DRM_MODE_ENCODER_DAC, NULL);
 			amdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
 		} else {
 			drm_encoder_init(dev, encoder, &dce_v10_0_encoder_funcs,
-					 DRM_MODE_ENCODER_TMDS);
+					 DRM_MODE_ENCODER_TMDS, NULL);
 			amdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
 		}
 		drm_encoder_helper_add(encoder, &dce_v10_0_dig_helper_funcs);
@@ -3738,13 +3742,13 @@ static void dce_v10_0_encoder_add(struct amdgpu_device *adev,
 		amdgpu_encoder->is_ext_encoder = true;
 		if (amdgpu_encoder->devices & (ATOM_DEVICE_LCD_SUPPORT))
 			drm_encoder_init(dev, encoder, &dce_v10_0_encoder_funcs,
-					 DRM_MODE_ENCODER_LVDS);
+					 DRM_MODE_ENCODER_LVDS, NULL);
 		else if (amdgpu_encoder->devices & (ATOM_DEVICE_CRT_SUPPORT))
 			drm_encoder_init(dev, encoder, &dce_v10_0_encoder_funcs,
-					 DRM_MODE_ENCODER_DAC);
+					 DRM_MODE_ENCODER_DAC, NULL);
 		else
 			drm_encoder_init(dev, encoder, &dce_v10_0_encoder_funcs,
-					 DRM_MODE_ENCODER_TMDS);
+					 DRM_MODE_ENCODER_TMDS, NULL);
 		drm_encoder_helper_add(encoder, &dce_v10_0_ext_helper_funcs);
 		break;
 	}
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_v10_0.h b/drivers/gpu/drm/amd/amdgpu/dce_v10_0.h
index 00c396c..c29c10b 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_v10_0.h
+++ b/drivers/gpu/drm/amd/amdgpu/dce_v10_0.h
@@ -28,12 +28,12 @@
 extern const struct amdgpu_ip_block_version dce_v10_0_ip_block;
 extern const struct amdgpu_ip_block_version dce_v10_1_ip_block;
 
+void dce_v10_0_disable_dce(struct amdgpu_device *adev);
+
 void dce_v10_0_set_vga_render_state(struct amdgpu_device *adev,
 				    bool render);
 void dce_v10_0_stop_mc_access(struct amdgpu_device *adev,
 			      struct amdgpu_mode_mc_save *save);
 void dce_v10_0_resume_mc_access(struct amdgpu_device *adev,
 				struct amdgpu_mode_mc_save *save);
-void dce_v10_0_disable_dce(struct amdgpu_device *adev);
-bool dce_v10_0_is_display_hung(struct amdgpu_device *adev);
 #endif
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_v11_0.c b/drivers/gpu/drm/amd/amdgpu/dce_v11_0.c
index cbef5a2..297b1f2 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_v11_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/dce_v11_0.c
@@ -2016,6 +2016,7 @@ static int dce_v11_0_crtc_do_set_base(struct drm_crtc *crtc,
 	u32 tmp, viewport_w, viewport_h;
 	int r;
 	bool bypass_lut = false;
+	char *format_name;
 
 	/* no fb bound */
 	if (!atomic && !crtc->primary->fb) {
@@ -2127,8 +2128,9 @@ static int dce_v11_0_crtc_do_set_base(struct drm_crtc *crtc,
 		bypass_lut = true;
 		break;
 	default:
-		DRM_ERROR("Unsupported screen format %s\n",
-			drm_get_format_name(target_fb->pixel_format));
+		format_name = drm_get_format_name(target_fb->pixel_format);
+		DRM_ERROR("Unsupported screen format %s\n", format_name);
+		kfree(format_name);
 		return -EINVAL;
 	}
 
@@ -2220,8 +2222,8 @@ static int dce_v11_0_crtc_do_set_base(struct drm_crtc *crtc,
 	WREG32(mmVIEWPORT_SIZE + amdgpu_crtc->crtc_offset,
 	       (viewport_w << 16) | viewport_h);
 
-	/* set pageflip to happen only at start of vblank interval (front porch) */
-	WREG32(mmCRTC_MASTER_UPDATE_MODE + amdgpu_crtc->crtc_offset, 3);
+	/* set pageflip to happen anywhere in vblank interval */
+	WREG32(mmCRTC_MASTER_UPDATE_MODE + amdgpu_crtc->crtc_offset, 0);
 
 	if (!atomic && fb && fb != crtc->primary->fb) {
 		amdgpu_fb = to_amdgpu_framebuffer(fb);
@@ -2574,7 +2576,7 @@ static int dce_v11_0_crtc_cursor_set2(struct drm_crtc *crtc,
 		return -EINVAL;
 	}
 
-	obj = drm_gem_object_lookup(crtc->dev, file_priv, handle);
+	obj = drm_gem_object_lookup(file_priv, handle);
 	if (!obj) {
 		DRM_ERROR("Cannot find cursor object %x for crtc %d\n", handle, amdgpu_crtc->crtc_id);
 		return -ENOENT;
@@ -2657,19 +2659,21 @@ static void dce_v11_0_cursor_reset(struct drm_crtc *crtc)
 	}
 }
 
-static void dce_v11_0_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,
-				    u16 *blue, uint32_t start, uint32_t size)
+static int dce_v11_0_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,
+				    u16 *blue, uint32_t size)
 {
 	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
-	int end = (start + size > 256) ? 256 : start + size, i;
+	int i;
 
 	/* userspace palettes are always correct as is */
-	for (i = start; i < end; i++) {
+	for (i = 0; i < size; i++) {
 		amdgpu_crtc->lut_r[i] = red[i] >> 6;
 		amdgpu_crtc->lut_g[i] = green[i] >> 6;
 		amdgpu_crtc->lut_b[i] = blue[i] >> 6;
 	}
 	dce_v11_0_crtc_load_lut(crtc);
+
+	return 0;
 }
 
 static void dce_v11_0_crtc_destroy(struct drm_crtc *crtc)
@@ -2686,7 +2690,7 @@ static const struct drm_crtc_funcs dce_v11_0_crtc_funcs = {
 	.gamma_set = dce_v11_0_crtc_gamma_set,
 	.set_config = amdgpu_crtc_set_config,
 	.destroy = dce_v11_0_crtc_destroy,
-	.page_flip = amdgpu_crtc_page_flip,
+	.page_flip_target = amdgpu_crtc_page_flip_target,
 };
 
 static void dce_v11_0_crtc_dpms(struct drm_crtc *crtc, int mode)
@@ -2707,13 +2711,13 @@ static void dce_v11_0_crtc_dpms(struct drm_crtc *crtc, int mode)
 		type = amdgpu_crtc_idx_to_irq_type(adev, amdgpu_crtc->crtc_id);
 		amdgpu_irq_update(adev, &adev->crtc_irq, type);
 		amdgpu_irq_update(adev, &adev->pageflip_irq, type);
-		drm_vblank_on(dev, amdgpu_crtc->crtc_id);
+		drm_crtc_vblank_on(crtc);
 		dce_v11_0_crtc_load_lut(crtc);
 		break;
 	case DRM_MODE_DPMS_STANDBY:
 	case DRM_MODE_DPMS_SUSPEND:
 	case DRM_MODE_DPMS_OFF:
-		drm_vblank_off(dev, amdgpu_crtc->crtc_id);
+		drm_crtc_vblank_off(crtc);
 		if (amdgpu_crtc->enabled) {
 			dce_v11_0_vga_enable(crtc, true);
 			amdgpu_atombios_crtc_blank(crtc, ATOM_ENABLE);
@@ -2998,18 +3002,6 @@ static int dce_v11_0_early_init(void *handle)
 		adev->mode_info.num_hpd = 5;
 		adev->mode_info.num_dig = 5;
 		break;
-#if 0
-        case CHIP_ELLESMERE:
-                adev->mode_info.num_crtc = 6;
-                adev->mode_info.num_hpd = 6;
-                adev->mode_info.num_dig = 6;
-                break;
-        case CHIP_BAFFIN:
-                adev->mode_info.num_crtc = 5;
-                adev->mode_info.num_hpd = 5;
-                adev->mode_info.num_dig = 5;
-                break;
-#endif
 	default:
 		/* FIXME: not supported yet */
 		return -EINVAL;
@@ -3420,7 +3412,7 @@ static int dce_v11_0_pageflip_irq(struct amdgpu_device *adev,
 
 	spin_unlock_irqrestore(&adev->ddev->event_lock, flags);
 
-	drm_vblank_put(adev->ddev, amdgpu_crtc->crtc_id);
+	drm_crtc_vblank_put(&amdgpu_crtc->base);
 	schedule_work(&works->unpin_work);
 
 	return 0;
@@ -3776,7 +3768,7 @@ static void dce_v11_0_encoder_add(struct amdgpu_device *adev,
 	case ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC1:
 	case ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC2:
 		drm_encoder_init(dev, encoder, &dce_v11_0_encoder_funcs,
-				 DRM_MODE_ENCODER_DAC);
+				 DRM_MODE_ENCODER_DAC, NULL);
 		drm_encoder_helper_add(encoder, &dce_v11_0_dac_helper_funcs);
 		break;
 	case ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DVO1:
@@ -3787,15 +3779,15 @@ static void dce_v11_0_encoder_add(struct amdgpu_device *adev,
 		if (amdgpu_encoder->devices & (ATOM_DEVICE_LCD_SUPPORT)) {
 			amdgpu_encoder->rmx_type = RMX_FULL;
 			drm_encoder_init(dev, encoder, &dce_v11_0_encoder_funcs,
-					 DRM_MODE_ENCODER_LVDS);
+					 DRM_MODE_ENCODER_LVDS, NULL);
 			amdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_lcd_info(amdgpu_encoder);
 		} else if (amdgpu_encoder->devices & (ATOM_DEVICE_CRT_SUPPORT)) {
 			drm_encoder_init(dev, encoder, &dce_v11_0_encoder_funcs,
-					 DRM_MODE_ENCODER_DAC);
+					 DRM_MODE_ENCODER_DAC, NULL);
 			amdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
 		} else {
 			drm_encoder_init(dev, encoder, &dce_v11_0_encoder_funcs,
-					 DRM_MODE_ENCODER_TMDS);
+					 DRM_MODE_ENCODER_TMDS, NULL);
 			amdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
 		}
 		drm_encoder_helper_add(encoder, &dce_v11_0_dig_helper_funcs);
@@ -3813,13 +3805,13 @@ static void dce_v11_0_encoder_add(struct amdgpu_device *adev,
 		amdgpu_encoder->is_ext_encoder = true;
 		if (amdgpu_encoder->devices & (ATOM_DEVICE_LCD_SUPPORT))
 			drm_encoder_init(dev, encoder, &dce_v11_0_encoder_funcs,
-					 DRM_MODE_ENCODER_LVDS);
+					 DRM_MODE_ENCODER_LVDS, NULL);
 		else if (amdgpu_encoder->devices & (ATOM_DEVICE_CRT_SUPPORT))
 			drm_encoder_init(dev, encoder, &dce_v11_0_encoder_funcs,
-					 DRM_MODE_ENCODER_DAC);
+					 DRM_MODE_ENCODER_DAC, NULL);
 		else
 			drm_encoder_init(dev, encoder, &dce_v11_0_encoder_funcs,
-					 DRM_MODE_ENCODER_TMDS);
+					 DRM_MODE_ENCODER_TMDS, NULL);
 		drm_encoder_helper_add(encoder, &dce_v11_0_ext_helper_funcs);
 		break;
 	}
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_v11_0.h b/drivers/gpu/drm/amd/amdgpu/dce_v11_0.h
index cdd82de..6e9a7a9 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_v11_0.h
+++ b/drivers/gpu/drm/amd/amdgpu/dce_v11_0.h
@@ -27,12 +27,12 @@
 extern const struct amdgpu_ip_block_version dce_v11_0_ip_block;
 extern const struct amdgpu_ip_block_version dce_v11_2_ip_block;
 
+void dce_v11_0_disable_dce(struct amdgpu_device *adev);
+
 void dce_v11_0_set_vga_render_state(struct amdgpu_device *adev,
 				    bool render);
 void dce_v11_0_stop_mc_access(struct amdgpu_device *adev,
 			      struct amdgpu_mode_mc_save *save);
 void dce_v11_0_resume_mc_access(struct amdgpu_device *adev,
 				struct amdgpu_mode_mc_save *save);
-void dce_v11_0_disable_dce(struct amdgpu_device *adev);
-bool dce_v11_0_is_display_hung(struct amdgpu_device *adev);
 #endif
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_v6_0.c b/drivers/gpu/drm/amd/amdgpu/dce_v6_0.c
index ca1bcf7..2e7eb72 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_v6_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/dce_v6_0.c
@@ -1647,8 +1647,8 @@ static int dce_v6_0_crtc_do_set_base(struct drm_crtc *crtc,
 	WREG32(mmVIEWPORT_SIZE + amdgpu_crtc->crtc_offset,
 	       (viewport_w << 16) | viewport_h);
 
-	/* set pageflip to happen only at start of vblank interval (front porch) */
-	WREG32(mmMASTER_UPDATE_MODE + amdgpu_crtc->crtc_offset, 3);
+	/* set pageflip to happen anywhere in vblank interval */
+	WREG32(mmMASTER_UPDATE_MODE + amdgpu_crtc->crtc_offset, 0);
 
 	if (!atomic && fb && fb != crtc->primary->fb) {
 		amdgpu_fb = to_amdgpu_framebuffer(fb);
@@ -2003,19 +2003,21 @@ static void dce_v6_0_cursor_reset(struct drm_crtc *crtc)
 	}
 }
 
-static void dce_v6_0_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,
-				    u16 *blue, uint32_t start, uint32_t size)
+static int dce_v6_0_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,
+				   u16 *blue, uint32_t size)
 {
 	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
-	int end = (start + size > 256) ? 256 : start + size, i;
+	int i;
 
 	/* userspace palettes are always correct as is */
-	for (i = start; i < end; i++) {
+	for (i = 0; i < size; i++) {
 		amdgpu_crtc->lut_r[i] = red[i] >> 6;
 		amdgpu_crtc->lut_g[i] = green[i] >> 6;
 		amdgpu_crtc->lut_b[i] = blue[i] >> 6;
 	}
 	dce_v6_0_crtc_load_lut(crtc);
+
+	return 0;
 }
 
 static void dce_v6_0_crtc_destroy(struct drm_crtc *crtc)
@@ -2032,7 +2034,7 @@ static const struct drm_crtc_funcs dce_v6_0_crtc_funcs = {
 	.gamma_set = dce_v6_0_crtc_gamma_set,
 	.set_config = amdgpu_crtc_set_config,
 	.destroy = dce_v6_0_crtc_destroy,
-	.page_flip = amdgpu_crtc_page_flip,
+	.page_flip_target = amdgpu_crtc_page_flip_target,
 };
 
 static void dce_v6_0_crtc_dpms(struct drm_crtc *crtc, int mode)
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_v8_0.c b/drivers/gpu/drm/amd/amdgpu/dce_v8_0.c
index b7e8937..26db7cb 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/dce_v8_0.c
@@ -1910,6 +1910,7 @@ static int dce_v8_0_crtc_do_set_base(struct drm_crtc *crtc,
 	u32 viewport_w, viewport_h;
 	int r;
 	bool bypass_lut = false;
+	char *format_name;
 
 	/* no fb bound */
 	if (!atomic && !crtc->primary->fb) {
@@ -2014,8 +2015,9 @@ static int dce_v8_0_crtc_do_set_base(struct drm_crtc *crtc,
 		bypass_lut = true;
 		break;
 	default:
-		DRM_ERROR("Unsupported screen format %s\n",
-			  drm_get_format_name(target_fb->pixel_format));
+		format_name = drm_get_format_name(target_fb->pixel_format);
+		DRM_ERROR("Unsupported screen format %s\n", format_name);
+		kfree(format_name);
 		return -EINVAL;
 	}
 
@@ -2095,8 +2097,8 @@ static int dce_v8_0_crtc_do_set_base(struct drm_crtc *crtc,
 	WREG32(mmVIEWPORT_SIZE + amdgpu_crtc->crtc_offset,
 	       (viewport_w << 16) | viewport_h);
 
-	/* set pageflip to happen only at start of vblank interval (front porch) */
-	WREG32(mmMASTER_UPDATE_MODE + amdgpu_crtc->crtc_offset, 3);
+	/* set pageflip to happen anywhere in vblank interval */
+	WREG32(mmMASTER_UPDATE_MODE + amdgpu_crtc->crtc_offset, 0);
 
 	if (!atomic && fb && fb != crtc->primary->fb) {
 		amdgpu_fb = to_amdgpu_framebuffer(fb);
@@ -2405,7 +2407,7 @@ static int dce_v8_0_crtc_cursor_set2(struct drm_crtc *crtc,
 		return -EINVAL;
 	}
 
-	obj = drm_gem_object_lookup(crtc->dev,file_priv, handle);
+	obj = drm_gem_object_lookup(file_priv, handle);
 	if (!obj) {
 		DRM_ERROR("Cannot find cursor object %x for crtc %d\n", handle, amdgpu_crtc->crtc_id);
 		return -ENOENT;
@@ -2488,19 +2490,21 @@ static void dce_v8_0_cursor_reset(struct drm_crtc *crtc)
 	}
 }
 
-static void dce_v8_0_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,
-				    u16 *blue, uint32_t start, uint32_t size)
+static int dce_v8_0_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,
+				   u16 *blue, uint32_t size)
 {
 	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
-	int end = (start + size > 256) ? 256 : start + size, i;
+	int i;
 
 	/* userspace palettes are always correct as is */
-	for (i = start; i < end; i++) {
+	for (i = 0; i < size; i++) {
 		amdgpu_crtc->lut_r[i] = red[i] >> 6;
 		amdgpu_crtc->lut_g[i] = green[i] >> 6;
 		amdgpu_crtc->lut_b[i] = blue[i] >> 6;
 	}
 	dce_v8_0_crtc_load_lut(crtc);
+
+	return 0;
 }
 
 static void dce_v8_0_crtc_destroy(struct drm_crtc *crtc)
@@ -2517,7 +2521,7 @@ static const struct drm_crtc_funcs dce_v8_0_crtc_funcs = {
 	.gamma_set = dce_v8_0_crtc_gamma_set,
 	.set_config = amdgpu_crtc_set_config,
 	.destroy = dce_v8_0_crtc_destroy,
-	.page_flip = amdgpu_crtc_page_flip,
+	.page_flip_target = amdgpu_crtc_page_flip_target,
 };
 
 static void dce_v8_0_crtc_dpms(struct drm_crtc *crtc, int mode)
@@ -2538,13 +2542,13 @@ static void dce_v8_0_crtc_dpms(struct drm_crtc *crtc, int mode)
 		type = amdgpu_crtc_idx_to_irq_type(adev, amdgpu_crtc->crtc_id);
 		amdgpu_irq_update(adev, &adev->crtc_irq, type);
 		amdgpu_irq_update(adev, &adev->pageflip_irq, type);
-		drm_vblank_on(dev, amdgpu_crtc->crtc_id);
+		drm_crtc_vblank_on(crtc);
 		dce_v8_0_crtc_load_lut(crtc);
 		break;
 	case DRM_MODE_DPMS_STANDBY:
 	case DRM_MODE_DPMS_SUSPEND:
 	case DRM_MODE_DPMS_OFF:
-		drm_vblank_off(dev, amdgpu_crtc->crtc_id);
+		drm_crtc_vblank_off(crtc);
 		if (amdgpu_crtc->enabled) {
 			dce_v8_0_vga_enable(crtc, true);
 			amdgpu_atombios_crtc_blank(crtc, ATOM_ENABLE);
@@ -3262,7 +3266,7 @@ static int dce_v8_0_pageflip_irq(struct amdgpu_device *adev,
 
 	spin_unlock_irqrestore(&adev->ddev->event_lock, flags);
 
-	drm_vblank_put(adev->ddev, amdgpu_crtc->crtc_id);
+	drm_crtc_vblank_put(&amdgpu_crtc->base);
 	schedule_work(&works->unpin_work);
 
 	return 0;
@@ -3538,7 +3542,7 @@ static void dce_v8_0_encoder_add(struct amdgpu_device *adev,
 	case ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC1:
 	case ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC2:
 		drm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,
-				 DRM_MODE_ENCODER_DAC);
+				 DRM_MODE_ENCODER_DAC, NULL);
 		drm_encoder_helper_add(encoder, &dce_v8_0_dac_helper_funcs);
 		break;
 	case ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DVO1:
@@ -3549,15 +3553,15 @@ static void dce_v8_0_encoder_add(struct amdgpu_device *adev,
 		if (amdgpu_encoder->devices & (ATOM_DEVICE_LCD_SUPPORT)) {
 			amdgpu_encoder->rmx_type = RMX_FULL;
 			drm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,
-					 DRM_MODE_ENCODER_LVDS);
+					 DRM_MODE_ENCODER_LVDS, NULL);
 			amdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_lcd_info(amdgpu_encoder);
 		} else if (amdgpu_encoder->devices & (ATOM_DEVICE_CRT_SUPPORT)) {
 			drm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,
-					 DRM_MODE_ENCODER_DAC);
+					 DRM_MODE_ENCODER_DAC, NULL);
 			amdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
 		} else {
 			drm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,
-					 DRM_MODE_ENCODER_TMDS);
+					 DRM_MODE_ENCODER_TMDS, NULL);
 			amdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
 		}
 		drm_encoder_helper_add(encoder, &dce_v8_0_dig_helper_funcs);
@@ -3575,13 +3579,13 @@ static void dce_v8_0_encoder_add(struct amdgpu_device *adev,
 		amdgpu_encoder->is_ext_encoder = true;
 		if (amdgpu_encoder->devices & (ATOM_DEVICE_LCD_SUPPORT))
 			drm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,
-					 DRM_MODE_ENCODER_LVDS);
+					 DRM_MODE_ENCODER_LVDS, NULL);
 		else if (amdgpu_encoder->devices & (ATOM_DEVICE_CRT_SUPPORT))
 			drm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,
-					 DRM_MODE_ENCODER_DAC);
+					 DRM_MODE_ENCODER_DAC, NULL);
 		else
 			drm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,
-					 DRM_MODE_ENCODER_TMDS);
+					 DRM_MODE_ENCODER_TMDS, NULL);
 		drm_encoder_helper_add(encoder, &dce_v8_0_ext_helper_funcs);
 		break;
 	}
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_v8_0.h b/drivers/gpu/drm/amd/amdgpu/dce_v8_0.h
index a05d9f9..457a528 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_v8_0.h
+++ b/drivers/gpu/drm/amd/amdgpu/dce_v8_0.h
@@ -30,12 +30,12 @@ extern const struct amdgpu_ip_block_version dce_v8_2_ip_block;
 extern const struct amdgpu_ip_block_version dce_v8_3_ip_block;
 extern const struct amdgpu_ip_block_version dce_v8_5_ip_block;
 
+void dce_v8_0_disable_dce(struct amdgpu_device *adev);
+
 void dce_v8_0_set_vga_render_state(struct amdgpu_device *adev,
 				   bool render);
 void dce_v8_0_stop_mc_access(struct amdgpu_device *adev,
 			     struct amdgpu_mode_mc_save *save);
 void dce_v8_0_resume_mc_access(struct amdgpu_device *adev,
 			       struct amdgpu_mode_mc_save *save);
-void dce_v8_0_disable_dce(struct amdgpu_device *adev);
-bool dce_v8_0_is_display_hung(struct amdgpu_device *adev);
 #endif
diff --git a/drivers/gpu/drm/amd/amdgpu/dce_virtual.c b/drivers/gpu/drm/amd/amdgpu/dce_virtual.c
index e4e9aca..64dd266 100644
--- a/drivers/gpu/drm/amd/amdgpu/dce_virtual.c
+++ b/drivers/gpu/drm/amd/amdgpu/dce_virtual.c
@@ -163,18 +163,20 @@ static void dce_virtual_bandwidth_update(struct amdgpu_device *adev)
 	return;
 }
 
-static void dce_virtual_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,
-				    u16 *blue, uint32_t start, uint32_t size)
+static int dce_virtual_crtc_gamma_set(struct drm_crtc *crtc, u16 *red,
+				      u16 *green, u16 *blue, uint32_t size)
 {
 	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
-	int end = (start + size > 256) ? 256 : start + size, i;
+	int i;
 
 	/* userspace palettes are always correct as is */
-	for (i = start; i < end; i++) {
+	for (i = 0; i < size; i++) {
 		amdgpu_crtc->lut_r[i] = red[i] >> 6;
 		amdgpu_crtc->lut_g[i] = green[i] >> 6;
 		amdgpu_crtc->lut_b[i] = blue[i] >> 6;
 	}
+
+	return 0;
 }
 
 static void dce_virtual_crtc_destroy(struct drm_crtc *crtc)
@@ -191,7 +193,7 @@ static const struct drm_crtc_funcs dce_virtual_crtc_funcs = {
 	.gamma_set = dce_virtual_crtc_gamma_set,
 	.set_config = amdgpu_crtc_set_config,
 	.destroy = dce_virtual_crtc_destroy,
-	.page_flip = amdgpu_crtc_page_flip,
+	.page_flip_target = amdgpu_crtc_page_flip_target,
 };
 
 static void dce_virtual_crtc_dpms(struct drm_crtc *crtc, int mode)
@@ -537,11 +539,7 @@ static int dce_virtual_suspend(void *handle)
 
 static int dce_virtual_resume(void *handle)
 {
-	int ret;
-
-	ret = dce_virtual_hw_init(handle);
-
-	return ret;
+	return dce_virtual_hw_init(handle);
 }
 
 static bool dce_virtual_is_idle(void *handle)
@@ -659,7 +657,7 @@ static int dce_virtual_connector_encoder_init(struct amdgpu_device *adev,
 		return -ENOMEM;
 	encoder->possible_crtcs = 1 << index;
 	drm_encoder_init(adev->ddev, encoder, &dce_virtual_encoder_funcs,
-			 DRM_MODE_ENCODER_VIRTUAL);
+			 DRM_MODE_ENCODER_VIRTUAL, NULL);
 	drm_encoder_helper_add(encoder, &dce_virtual_encoder_helper_funcs);
 
 	connector = kzalloc(sizeof(struct drm_connector), GFP_KERNEL);
@@ -746,7 +744,7 @@ static int dce_virtual_pageflip(struct amdgpu_device *adev,
 
 	spin_unlock_irqrestore(&adev->ddev->event_lock, flags);
 
-	drm_vblank_put(adev->ddev, amdgpu_crtc->crtc_id);
+	drm_crtc_vblank_put(&amdgpu_crtc->base);
 	schedule_work(&works->unpin_work);
 
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
index 4c93873..adba80e 100644
--- a/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c
@@ -47,8 +47,6 @@
 #include "oss/oss_2_0_d.h"
 #include "oss/oss_2_0_sh_mask.h"
 
-#define NUM_SIMD_PER_CU 0x4
-
 #define GFX7_NUM_GFX_RINGS     1
 #define GFX7_NUM_COMPUTE_RINGS 8
 
@@ -2942,8 +2940,7 @@ static int gfx_v7_0_cp_compute_resume(struct amdgpu_device *adev)
 	u64 wb_gpu_addr;
 	u32 *buf;
 	struct bonaire_mqd *mqd;
-
-	gfx_v7_0_cp_compute_enable(adev, true);
+	struct amdgpu_ring *ring;
 
 	/* fix up chicken bits */
 	tmp = RREG32(mmCP_CPF_DEBUG);
@@ -2978,7 +2975,7 @@ static int gfx_v7_0_cp_compute_resume(struct amdgpu_device *adev)
 
 	/* init the queues.  Just two for now. */
 	for (i = 0; i < adev->gfx.num_compute_rings; i++) {
-		struct amdgpu_ring *ring = &adev->gfx.compute_ring[i];
+		ring = &adev->gfx.compute_ring[i];
 
 		if (ring->mqd_obj == NULL) {
 			r = amdgpu_bo_create(adev,
@@ -3157,6 +3154,13 @@ static int gfx_v7_0_cp_compute_resume(struct amdgpu_device *adev)
 		amdgpu_bo_unreserve(ring->mqd_obj);
 
 		ring->ready = true;
+	}
+
+	gfx_v7_0_cp_compute_enable(adev, true);
+
+	for (i = 0; i < adev->gfx.num_compute_rings; i++) {
+		ring = &adev->gfx.compute_ring[i];
+
 		r = amdgpu_ring_test_ring(ring);
 		if (r)
 			ring->ready = false;
@@ -5338,22 +5342,6 @@ static void gfx_v7_0_get_cu_info(struct amdgpu_device *adev)
 
 	cu_info->number = active_cu_number;
 	cu_info->ao_cu_mask = ao_cu_mask;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	cu_info->simd_per_cu = NUM_SIMD_PER_CU;
-	switch (adev->asic_type) {
-	case CHIP_KAVERI:
-	case CHIP_HAWAII:
-		cu_info->max_waves_per_simd = 10;
-		cu_info->max_scratch_slots_per_cu = 32;
-		cu_info->wave_front_size = 64;
-		cu_info->lds_size = 64;
-		break;
-	default:
-		dev_warn(adev->dev, "CU info asic_type [0x%x] not supported\n",
-				adev->asic_type);
-		break;
-	}
-#endif
 }
 
 const struct amdgpu_ip_block_version gfx_v7_0_ip_block =
diff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
index 36c73e5..8defec0 100644
--- a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
@@ -31,7 +31,6 @@
 #include "amdgpu_atombios.h"
 #include "atombios_i2c.h"
 #include "clearstate_vi.h"
-#include "gfx_v8_0.h"
 
 #include "gmc/gmc_8_2_d.h"
 #include "gmc/gmc_8_2_sh_mask.h"
@@ -1716,11 +1715,6 @@ static int gfx_v8_0_do_edc_gpr_workarounds(struct amdgpu_device *adev)
 		goto fail;
 	}
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* read back registers to clear the counters */
-	for (i = 0; i < ARRAY_SIZE(sec_ded_counter_registers); i++)
-		RREG32(sec_ded_counter_registers[i]);
-#else
 	tmp = REG_SET_FIELD(tmp, GB_EDC_MODE, DED_MODE, 2);
 	tmp = REG_SET_FIELD(tmp, GB_EDC_MODE, PROP_FED, 1);
 	WREG32(mmGB_EDC_MODE, tmp);
@@ -1733,7 +1727,6 @@ static int gfx_v8_0_do_edc_gpr_workarounds(struct amdgpu_device *adev)
 	/* read back registers to clear the counters */
 	for (i = 0; i < ARRAY_SIZE(sec_ded_counter_registers); i++)
 		RREG32(sec_ded_counter_registers[i]);
-#endif
 
 fail:
 	amdgpu_ib_free(adev, &ib, NULL);
@@ -4028,17 +4021,17 @@ static void gfx_v8_0_init_pg(struct amdgpu_device *adev)
 		gfx_v8_0_init_csb(adev);
 		gfx_v8_0_init_save_restore_list(adev);
 		gfx_v8_0_enable_save_restore_machine(adev);
-
-                WREG32(mmRLC_JUMP_TABLE_RESTORE, adev->gfx.rlc.cp_table_gpu_addr >> 8);
-                gfx_v8_0_init_power_gating(adev);
-                WREG32(mmRLC_PG_ALWAYS_ON_CU_MASK, adev->gfx.cu_info.ao_cu_mask);
+		WREG32(mmRLC_JUMP_TABLE_RESTORE, adev->gfx.rlc.cp_table_gpu_addr >> 8);
+		gfx_v8_0_init_power_gating(adev);
+		WREG32(mmRLC_PG_ALWAYS_ON_CU_MASK, adev->gfx.cu_info.ao_cu_mask);
 	} else if ((adev->asic_type == CHIP_POLARIS11) ||
-                   (adev->asic_type == CHIP_POLARIS12)) {
-                gfx_v8_0_init_csb(adev);
-                gfx_v8_0_init_save_restore_list(adev);
-                gfx_v8_0_enable_save_restore_machine(adev);
-                gfx_v8_0_init_power_gating(adev);
+		   (adev->asic_type == CHIP_POLARIS12)) {
+		gfx_v8_0_init_csb(adev);
+		gfx_v8_0_init_save_restore_list(adev);
+		gfx_v8_0_enable_save_restore_machine(adev);
+		gfx_v8_0_init_power_gating(adev);
 	}
+
 }
 
 static void gfx_v8_0_rlc_stop(struct amdgpu_device *adev)
@@ -5717,19 +5710,19 @@ static void gfx_v8_0_update_coarse_grain_clock_gating(struct amdgpu_device *adev
 		if (temp1 != data1)
 			WREG32(mmRLC_CGTT_MGCG_OVERRIDE, data1);
 
-		/* 2 wait for RLC_SERDES_CU_MASTER & RLC_SERDES_NONCU_MASTER idle */
+		/* : wait for RLC_SERDES_CU_MASTER & RLC_SERDES_NONCU_MASTER idle */
 		gfx_v8_0_wait_for_rlc_serdes(adev);
 
-		/* 3 - clear cgcg override */
+		/* 2 - clear cgcg override */
 		gfx_v8_0_send_serdes_cmd(adev, BPM_REG_CGCG_OVERRIDE, CLE_BPM_SERDES_CMD);
 
 		/* wait for RLC_SERDES_CU_MASTER & RLC_SERDES_NONCU_MASTER idle */
 		gfx_v8_0_wait_for_rlc_serdes(adev);
 
-		/* 4 - write cmd to set CGLS */
+		/* 3 - write cmd to set CGLS */
 		gfx_v8_0_send_serdes_cmd(adev, BPM_REG_CGLS_EN, SET_BPM_SERDES_CMD);
 
-		/* 5 - enable cgcg */
+		/* 4 - enable cgcg */
 		data |= RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
 
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGLS) {
@@ -6642,24 +6635,6 @@ static void gfx_v8_0_get_cu_info(struct amdgpu_device *adev)
 
 	cu_info->number = active_cu_number;
 	cu_info->ao_cu_mask = ao_cu_mask;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	cu_info->simd_per_cu = NUM_SIMD_PER_CU;
-	switch (adev->asic_type) {
-	case CHIP_CARRIZO:
-	case CHIP_TONGA:
-	case CHIP_FIJI:
-	case CHIP_POLARIS10:
-	case CHIP_POLARIS11:
-		cu_info->max_waves_per_simd = 10;
-		cu_info->max_scratch_slots_per_cu = 32;
-		cu_info->wave_front_size = 64;
-		cu_info->lds_size = 64;
-		break;
-	default:
-		dev_warn(adev->dev, "CU info asic_type [0x%x] not supported\n",
-					adev->asic_type);
-	}
-#endif
 }
 
 const struct amdgpu_ip_block_version gfx_v8_0_ip_block =
diff --git a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c b/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
index e2dca1b..273b16f 100644
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
@@ -27,9 +27,6 @@
 #include "cik.h"
 #include "gmc_v7_0.h"
 #include "amdgpu_ucode.h"
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#include "amdgpu_amdkfd.h"
-#endif
 
 #include "bif/bif_4_1_d.h"
 #include "bif/bif_4_1_sh_mask.h"
@@ -332,10 +329,6 @@ static int gmc_v7_0_mc_init(struct amdgpu_device *adev)
 {
 	u32 tmp;
 	int chansize, numchan;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	uint64_t gart_size_aligned;
-	struct sysinfo si;
-#endif
 
 	/* Get VRAM informations */
 	tmp = RREG32(mmMC_ARB_RAMCFG);
@@ -388,45 +381,11 @@ static int gmc_v7_0_mc_init(struct amdgpu_device *adev)
 	if (adev->mc.visible_vram_size > adev->mc.real_vram_size)
 		adev->mc.visible_vram_size = adev->mc.real_vram_size;
 
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	/* unless the user had overridden it, set the gart
 	 * size equal to the 1024 or vram, whichever is larger.
 	 */
 	if (amdgpu_gart_size == -1)
 		adev->mc.gtt_size = max((1024ULL << 20), adev->mc.mc_vram_size);
-#else
-	/* Unless the user has overridden it, compute the GART size */
-	if (amdgpu_gart_size == -1) {
-		/* Maximum GTT size is limited by the GART table size
-		 * in visible VRAM and the address space. Use at most
-		 * half of each. */
-		uint64_t max_gtt_size = min(
-			adev->mc.visible_vram_size / 8 * PAGE_SIZE / 2,
-			1ULL << 39);
-
-		si_meminfo(&si);
-		/* Set the GART to map the largest size between either
-		 * VRAM capacity or double the available physical RAM
-		 */
-		adev->mc.gtt_size = min(
-			max(
-				((uint64_t)si.totalram * si.mem_unit * 2),
-				adev->mc.mc_vram_size
-			),
-			max_gtt_size
-		);
-
-		/* GART sizes computed from physical RAM capacity
-		 * may not always be perfect powers of two.
-		 * Round up starting from the minimum size of 1GB.
-		 */
-		gart_size_aligned = 1024ULL << 20;
-		while (adev->mc.gtt_size > gart_size_aligned)
-			gart_size_aligned <<= 1;
-
-		adev->mc.gtt_size = gart_size_aligned;
-	}
-#endif
 	else
 		adev->mc.gtt_size = (uint64_t)amdgpu_gart_size << 20;
 
@@ -716,14 +675,6 @@ static int gmc_v7_0_vm_init(struct amdgpu_device *adev)
 	} else
 		adev->vm_manager.vram_base_offset = 0;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	adev->mc.vm_fault_info = kmalloc(sizeof(struct kfd_vm_fault_info),
-					GFP_KERNEL);
-	if (!adev->mc.vm_fault_info)
-		return -ENOMEM;
-	atomic_set(&adev->mc.vm_fault_info_updated, 0);
-#endif
-
 	return 0;
 }
 
@@ -751,9 +702,6 @@ static void gmc_v7_0_vm_decode_fault(struct amdgpu_device *adev,
 				     u32 status, u32 addr, u32 mc_client)
 {
 	u32 mc_id;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	struct kfd_vm_fault_info *info = adev->mc.vm_fault_info;
-#endif
 	u32 vmid = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS, VMID);
 	u32 protections = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,
 					PROTECTIONS);
@@ -768,20 +716,6 @@ static void gmc_v7_0_vm_decode_fault(struct amdgpu_device *adev,
 	       REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,
 			     MEMORY_CLIENT_RW) ?
 	       "write" : "read", block, mc_client, mc_id);
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	if (!atomic_read(&adev->mc.vm_fault_info_updated)) {
-		info->vmid = vmid;
-		info->mc_id = mc_id;
-		info->page_addr = addr;
-		info->prot_valid = protections & 0x6 ? true : false;
-		info->prot_read = protections & 0x8 ? true : false;
-		info->prot_write = protections & 0x10 ? true : false;
-		info->prot_exec = protections & 0x20 ? true : false;
-		mb();
-		atomic_set(&adev->mc.vm_fault_info_updated, 1);
-	}
-#endif
 }
 
 
diff --git a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c b/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
index 4ae8fa5..476bc9f 100644
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
@@ -25,9 +25,6 @@
 #include "amdgpu.h"
 #include "gmc_v8_0.h"
 #include "amdgpu_ucode.h"
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-#include "amdgpu_amdkfd.h"
-#endif
 
 #include "gmc/gmc_8_1_d.h"
 #include "gmc/gmc_8_1_sh_mask.h"
@@ -424,10 +421,6 @@ static int gmc_v8_0_mc_init(struct amdgpu_device *adev)
 {
 	u32 tmp;
 	int chansize, numchan;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	uint64_t gart_size_aligned;
-	struct sysinfo si;
-#endif
 
 	/* Get VRAM informations */
 	tmp = RREG32(mmMC_ARB_RAMCFG);
@@ -468,13 +461,6 @@ static int gmc_v8_0_mc_init(struct amdgpu_device *adev)
 		break;
 	}
 	adev->mc.vram_width = numchan * chansize;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* FIXME: The above calculation is outdated.
-	 * For HBM provide a temporary fix */
-	if (adev->mc.vram_type == AMDGPU_VRAM_TYPE_HBM)
-		adev->mc.vram_width = AMDGPU_VRAM_TYPE_HBM_WIDTH;
-#endif
-
 	/* Could aper size report 0 ? */
 	adev->mc.aper_base = pci_resource_start(adev->pdev, 0);
 	adev->mc.aper_size = pci_resource_len(adev->pdev, 0);
@@ -487,45 +473,11 @@ static int gmc_v8_0_mc_init(struct amdgpu_device *adev)
 	if (adev->mc.visible_vram_size > adev->mc.real_vram_size)
 		adev->mc.visible_vram_size = adev->mc.real_vram_size;
 
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 	/* unless the user had overridden it, set the gart
 	 * size equal to the 1024 or vram, whichever is larger.
 	 */
 	if (amdgpu_gart_size == -1)
 		adev->mc.gtt_size = max((1024ULL << 20), adev->mc.mc_vram_size);
-#else
-	/* Unless the user has overridden it, compute the GART size */
-	if (amdgpu_gart_size == -1) {
-		/* Maximum GTT size is limited by the GART table size
-		 * in visible VRAM and the address space. Use at most
-		 * half of each. */
-		uint64_t max_gtt_size = min(
-			adev->mc.visible_vram_size / 8 * PAGE_SIZE / 2,
-			1ULL << 39);
-
-		si_meminfo(&si);
-		/* Set the GART to map the largest size between either
-		 * VRAM capacity or double the available physical RAM
-		 */
-		adev->mc.gtt_size = min(
-			max(
-				((uint64_t)si.totalram * si.mem_unit * 2),
-				adev->mc.mc_vram_size
-			),
-			max_gtt_size
-		);
-
-		/* GART sizes computed from physical RAM capacity
-		 * may not always be perfect powers of two.
-		 * Round up starting from the minimum size of 1GB.
-		 */
-		gart_size_aligned = 1024ULL << 20;
-		while (adev->mc.gtt_size > gart_size_aligned)
-			gart_size_aligned <<= 1;
-
-		adev->mc.gtt_size = gart_size_aligned;
-	}
-#endif
 	else
 		adev->mc.gtt_size = (uint64_t)amdgpu_gart_size << 20;
 
@@ -854,14 +806,6 @@ static int gmc_v8_0_vm_init(struct amdgpu_device *adev)
 	} else
 		adev->vm_manager.vram_base_offset = 0;
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	adev->mc.vm_fault_info = kmalloc(sizeof(struct kfd_vm_fault_info),
-					GFP_KERNEL);
-	if (!adev->mc.vm_fault_info)
-		return -ENOMEM;
-	atomic_set(&adev->mc.vm_fault_info_updated, 0);
-#endif
-
 	return 0;
 }
 
@@ -889,9 +833,6 @@ static void gmc_v8_0_vm_decode_fault(struct amdgpu_device *adev,
 				     u32 status, u32 addr, u32 mc_client)
 {
 	u32 mc_id;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	struct kfd_vm_fault_info *info = adev->mc.vm_fault_info;
-#endif
 	u32 vmid = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS, VMID);
 	u32 protections = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,
 					PROTECTIONS);
@@ -906,20 +847,6 @@ static void gmc_v8_0_vm_decode_fault(struct amdgpu_device *adev,
 	       REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,
 			     MEMORY_CLIENT_RW) ?
 	       "write" : "read", block, mc_client, mc_id);
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	if (!atomic_read(&adev->mc.vm_fault_info_updated)) {
-		info->vmid = vmid;
-		info->mc_id = mc_id;
-		info->page_addr = addr;
-		info->prot_valid = protections & 0x6 ? true : false;
-		info->prot_read = protections & 0x8 ? true : false;
-		info->prot_write = protections & 0x10 ? true : false;
-		info->prot_exec = protections & 0x20 ? true : false;
-		mb();
-		atomic_set(&adev->mc.vm_fault_info_updated, 1);
-	}
-#endif
 }
 
 static int gmc_v8_0_convert_vram_type(int mc_seq_vram_type)
@@ -1052,12 +979,6 @@ static int gmc_v8_0_sw_init(void *handle)
 		adev->vm_manager.enabled = true;
 	}
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	/* Allow BIF to recode atomics to PCIe 3.0 AtomicOps.
-	 */
-	pci_enable_atomic_request(adev->pdev);
-#endif
-
 	return r;
 }
 
diff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c b/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c
index a26222d..e35df3d 100644
--- a/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c
+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c
@@ -190,12 +190,8 @@ out:
  */
 static uint32_t sdma_v2_4_ring_get_rptr(struct amdgpu_ring *ring)
 {
-	u32 rptr;
-
 	/* XXX check if swapping is necessary on BE */
-	rptr = ring->adev->wb.wb[ring->rptr_offs] >> 2;
-
-	return rptr;
+	return ring->adev->wb.wb[ring->rptr_offs] >> 2;
 }
 
 /**
diff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c b/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c
index e5a8b19..a34c51c 100644
--- a/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c
@@ -341,12 +341,8 @@ out:
  */
 static uint32_t sdma_v3_0_ring_get_rptr(struct amdgpu_ring *ring)
 {
-	u32 rptr;
-
 	/* XXX check if swapping is necessary on BE */
-	rptr = ring->adev->wb.wb[ring->rptr_offs] >> 2;
-
-	return rptr;
+	return ring->adev->wb.wb[ring->rptr_offs] >> 2;
 }
 
 /**
@@ -557,60 +553,17 @@ static void sdma_v3_0_rlc_stop(struct amdgpu_device *adev)
  */
 static void sdma_v3_0_ctx_switch_enable(struct amdgpu_device *adev, bool enable)
 {
-	int i;
 	u32 f32_cntl;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	u32 phase_quantum = 0;
-
-	if (amdgpu_sdma_phase_quantum) {
-		unsigned value = amdgpu_sdma_phase_quantum;
-		unsigned unit = 0;
-
-		while (value > (SDMA0_PHASE0_QUANTUM__VALUE_MASK >>
-				SDMA0_PHASE0_QUANTUM__VALUE__SHIFT)) {
-			value = (value + 1) >> 1;
-			unit++;
-		}
-		if (unit > (SDMA0_PHASE0_QUANTUM__UNIT_MASK >>
-			    SDMA0_PHASE0_QUANTUM__UNIT__SHIFT)) {
-			value = (SDMA0_PHASE0_QUANTUM__VALUE_MASK >>
-				 SDMA0_PHASE0_QUANTUM__VALUE__SHIFT);
-			unit = (SDMA0_PHASE0_QUANTUM__UNIT_MASK >>
-				SDMA0_PHASE0_QUANTUM__UNIT__SHIFT);
-			WARN_ONCE(1,
-			"clamping sdma_phase_quantum to %uK clock cycles\n",
-				  value << unit);
-		}
-		phase_quantum =
-			value << SDMA0_PHASE0_QUANTUM__VALUE__SHIFT |
-			unit  << SDMA0_PHASE0_QUANTUM__UNIT__SHIFT;
-	}
-#endif
+	int i;
 
 	for (i = 0; i < adev->sdma.num_instances; i++) {
 		f32_cntl = RREG32(mmSDMA0_CNTL + sdma_offsets[i]);
-		if (enable) {
+		if (enable)
 			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
 					AUTO_CTXSW_ENABLE, 1);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
-					ATC_L1_ENABLE, 1);
-			if (amdgpu_sdma_phase_quantum) {
-				WREG32(mmSDMA0_PHASE0_QUANTUM + sdma_offsets[i],
-				       phase_quantum);
-				WREG32(mmSDMA0_PHASE1_QUANTUM + sdma_offsets[i],
-				       phase_quantum);
-			}
-#endif
-		} else {
+		else
 			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
 					AUTO_CTXSW_ENABLE, 0);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-			f32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,
-					ATC_L1_ENABLE, 1);
-#endif
-		}
-
 		WREG32(mmSDMA0_CNTL + sdma_offsets[i], f32_cntl);
 	}
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c b/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
index a0db936..ba0bbf7 100644
--- a/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c
@@ -36,7 +36,6 @@
 #include "bif/bif_5_1_d.h"
 #include "gmc/gmc_8_1_d.h"
 #include "vi.h"
-#include "amdgpu_pm.h"
 
 static void uvd_v6_0_set_ring_funcs(struct amdgpu_device *adev);
 static void uvd_v6_0_set_irq_funcs(struct amdgpu_device *adev);
@@ -253,8 +252,6 @@ static int uvd_v6_0_resume(void *handle)
 	if (r)
 		return r;
 
-	amdgpu_dpm_enable_uvd(adev, true);	
-
 	return r;
 }
 
diff --git a/drivers/gpu/drm/amd/amdgpu/vi.c b/drivers/gpu/drm/amd/amdgpu/vi.c
index c577548..0da46d7 100644
--- a/drivers/gpu/drm/amd/amdgpu/vi.c
+++ b/drivers/gpu/drm/amd/amdgpu/vi.c
@@ -71,12 +71,12 @@
 #include "uvd_v5_0.h"
 #include "uvd_v6_0.h"
 #include "vce_v3_0.h"
+#include "amdgpu_powerplay.h"
 #if defined(CONFIG_DRM_AMD_ACP)
 #include "amdgpu_acp.h"
 #endif
-#include "amdgpu_dm.h"
-#include "amdgpu_powerplay.h"
 #include "dce_virtual.h"
+#include "amdgpu_dm.h"
 
 /*
  * Indirect registers accessor
@@ -1007,9 +1007,8 @@ static int vi_common_early_init(void *handle)
 			AMD_CG_SUPPORT_SDMA_MGCG |
 			AMD_CG_SUPPORT_SDMA_LS |
 			AMD_CG_SUPPORT_VCE_MGCG;
-		adev->pg_flags = 0;
-#if !defined(CONFIG_HSA_AMD) && !defined(CONFIG_HSA_AMD_MODULE)
 		/* rev0 hardware requires workarounds to support PG */
+		adev->pg_flags = 0;
 		if (adev->rev_id != 0x00) {
 			adev->pg_flags |= AMD_PG_SUPPORT_GFX_PG |
 				AMD_PG_SUPPORT_GFX_SMG |
@@ -1018,7 +1017,6 @@ static int vi_common_early_init(void *handle)
 				AMD_PG_SUPPORT_UVD |
 				AMD_PG_SUPPORT_VCE;
 		}
-#endif
 		adev->external_rev_id = adev->rev_id + 0x1;
 		break;
 	case CHIP_STONEY:
diff --git a/drivers/gpu/drm/amd/amdgpu/vid.h b/drivers/gpu/drm/amd/amdgpu/vid.h
index ef8c8fd..11746f2 100644
--- a/drivers/gpu/drm/amd/amdgpu/vid.h
+++ b/drivers/gpu/drm/amd/amdgpu/vid.h
@@ -27,8 +27,6 @@
 #define SDMA1_REGISTER_OFFSET                             0x200 /* not a register */
 #define SDMA_MAX_INSTANCE 2
 
-#define KFD_VI_SDMA_QUEUE_OFFSET                      0x80 /* not a register */
-
 /* crtc instance offsets */
 #define CRTC0_REGISTER_OFFSET                 (0x1b9c - 0x1b9c)
 #define CRTC1_REGISTER_OFFSET                 (0x1d9c - 0x1b9c)
diff --git a/drivers/gpu/drm/amd/amdkfd/Kconfig b/drivers/gpu/drm/amd/amdkfd/Kconfig
index ac49532..e13c67c 100644
--- a/drivers/gpu/drm/amd/amdkfd/Kconfig
+++ b/drivers/gpu/drm/amd/amdkfd/Kconfig
@@ -5,6 +5,5 @@
 config HSA_AMD
 	tristate "HSA kernel driver for AMD GPU devices"
 	depends on (DRM_RADEON || DRM_AMDGPU) && AMD_IOMMU_V2 && X86_64
-	select DRM_AMDGPU_USERPTR
 	help
 	  Enable this if you want to use HSA features on AMD GPU devices.
diff --git a/drivers/gpu/drm/amd/amdkfd/Makefile b/drivers/gpu/drm/amd/amdkfd/Makefile
index 6f7fbc2..7fc9b0f 100644
--- a/drivers/gpu/drm/amd/amdkfd/Makefile
+++ b/drivers/gpu/drm/amd/amdkfd/Makefile
@@ -2,11 +2,8 @@
 # Makefile for Heterogenous System Architecture support for AMD GPU devices
 #
 
-FULL_AMD_PATH=$(src)/..
-
-ccflags-y := -Iinclude/drm \
-		-I$(FULL_AMD_PATH)/include/  \
-		-I$(FULL_AMD_PATH)/include/asic_reg
+ccflags-y := -Iinclude/drm -Idrivers/gpu/drm/amd/include/  \
+		-Idrivers/gpu/drm/amd/include/asic_reg
 
 amdkfd-y	:= kfd_module.o kfd_device.o kfd_chardev.o kfd_topology.o \
 		kfd_pasid.o kfd_doorbell.o kfd_flat_memory.o \
@@ -17,9 +14,6 @@ amdkfd-y	:= kfd_module.o kfd_device.o kfd_chardev.o kfd_topology.o \
 		kfd_process_queue_manager.o kfd_device_queue_manager.o \
 		kfd_device_queue_manager_cik.o kfd_device_queue_manager_vi.o \
 		kfd_interrupt.o kfd_events.o cik_event_interrupt.o \
-		kfd_dbgdev.o kfd_dbgmgr.o kfd_flat_memory.o kfd_crat.o kfd_rdma.o \
-		kfd_peerdirect.o kfd_ipc.o
-
-amdkfd-$(CONFIG_DEBUG_FS) += kfd_debugfs.o
+		kfd_dbgdev.o kfd_dbgmgr.o
 
 obj-$(CONFIG_HSA_AMD)	+= amdkfd.o
diff --git a/drivers/gpu/drm/amd/amdkfd/cik_event_interrupt.c b/drivers/gpu/drm/amd/amdkfd/cik_event_interrupt.c
index c60a71a..211fc48 100644
--- a/drivers/gpu/drm/amd/amdkfd/cik_event_interrupt.c
+++ b/drivers/gpu/drm/amd/amdkfd/cik_event_interrupt.c
@@ -24,88 +24,40 @@
 #include "kfd_events.h"
 #include "cik_int.h"
 
-static bool is_cpc_vm_fault(struct kfd_dev *dev,
-					const uint32_t *ih_ring_entry)
-{
-	const struct cik_ih_ring_entry *ihre =
-			(const struct cik_ih_ring_entry *)ih_ring_entry;
-
-	if ((ihre->source_id == CIK_INTSRC_GFX_PAGE_INV_FAULT ||
-		ihre->source_id == CIK_INTSRC_GFX_MEM_PROT_FAULT) &&
-	    ihre->vmid >= dev->vm_info.first_vmid_kfd &&
-	    ihre->vmid <= dev->vm_info.last_vmid_kfd)
-		return true;
-	return false;
-}
-
 static bool cik_event_interrupt_isr(struct kfd_dev *dev,
-					const uint32_t *ih_ring_entry,
-					uint32_t *patched_ihre,
-					bool *patched_flag)
+					const uint32_t *ih_ring_entry)
 {
+	unsigned int pasid;
 	const struct cik_ih_ring_entry *ihre =
 			(const struct cik_ih_ring_entry *)ih_ring_entry;
-	const struct kfd2kgd_calls *f2g = dev->kfd2kgd;
-	struct cik_ih_ring_entry *tmp_ihre =
-			(struct cik_ih_ring_entry *) patched_ihre;
 
-	/* This workaround is due to HW/FW limitation on Hawaii that
-	 * VMID and PASID are not written into ih_ring_entry
-	 */
-	if ((ihre->source_id == CIK_INTSRC_GFX_PAGE_INV_FAULT ||
-		ihre->source_id == CIK_INTSRC_GFX_MEM_PROT_FAULT) &&
-		dev->device_info->asic_family == CHIP_HAWAII) {
-		*patched_flag = true;
-		*tmp_ihre = *ihre;
+	pasid = (ihre->ring_id & 0xffff0000) >> 16;
 
-		tmp_ihre->vmid = f2g->read_vmid_from_vmfault_reg(dev->kgd);
-		tmp_ihre->pasid = f2g->get_atc_vmid_pasid_mapping_pasid(
-						 dev->kgd, tmp_ihre->vmid);
-		return (tmp_ihre->pasid != 0) &&
-			tmp_ihre->vmid >= dev->vm_info.first_vmid_kfd &&
-			tmp_ihre->vmid <= dev->vm_info.last_vmid_kfd;
-	}
 	/* Do not process in ISR, just request it to be forwarded to WQ. */
-	return (ihre->pasid != 0) &&
+	return (pasid != 0) &&
 		(ihre->source_id == CIK_INTSRC_CP_END_OF_PIPE ||
-		ihre->source_id == CIK_INTSRC_SDMA_TRAP ||
 		ihre->source_id == CIK_INTSRC_SQ_INTERRUPT_MSG ||
-		ihre->source_id == CIK_INTSRC_CP_BAD_OPCODE ||
-		is_cpc_vm_fault(dev, ih_ring_entry));
+		ihre->source_id == CIK_INTSRC_CP_BAD_OPCODE);
 }
 
 static void cik_event_interrupt_wq(struct kfd_dev *dev,
 					const uint32_t *ih_ring_entry)
 {
+	unsigned int pasid;
 	const struct cik_ih_ring_entry *ihre =
 			(const struct cik_ih_ring_entry *)ih_ring_entry;
 
-	if (ihre->pasid == 0)
+	pasid = (ihre->ring_id & 0xffff0000) >> 16;
+
+	if (pasid == 0)
 		return;
 
 	if (ihre->source_id == CIK_INTSRC_CP_END_OF_PIPE)
-		kfd_signal_event_interrupt(ihre->pasid, 0, 0);
-	else if (ihre->source_id == CIK_INTSRC_SDMA_TRAP)
-		kfd_signal_event_interrupt(ihre->pasid, 0, 0);
+		kfd_signal_event_interrupt(pasid, 0, 0);
 	else if (ihre->source_id == CIK_INTSRC_SQ_INTERRUPT_MSG)
-		kfd_signal_event_interrupt(ihre->pasid, ihre->data & 0xFF, 8);
+		kfd_signal_event_interrupt(pasid, ihre->data & 0xFF, 8);
 	else if (ihre->source_id == CIK_INTSRC_CP_BAD_OPCODE)
-		kfd_signal_hw_exception_event(ihre->pasid);
-	else if (ihre->source_id == CIK_INTSRC_GFX_PAGE_INV_FAULT ||
-		ihre->source_id == CIK_INTSRC_GFX_MEM_PROT_FAULT) {
-		struct kfd_vm_fault_info info;
-
-		memset(&info, 0, sizeof(info));
-		dev->kfd2kgd->get_vm_fault_info(dev->kgd, &info);
-		kfd_process_vm_fault(dev->dqm, ihre->pasid);
-		if (!info.page_addr && !info.status)
-			return;
-
-		if (info.vmid == ihre->vmid)
-			kfd_signal_vm_fault_event(dev, ihre->pasid, &info);
-		else
-			kfd_signal_vm_fault_event(dev, ihre->pasid, NULL);
-	}
+		kfd_signal_hw_exception_event(pasid);
 }
 
 const struct kfd_event_interrupt_class event_interrupt_class_cik = {
diff --git a/drivers/gpu/drm/amd/amdkfd/cik_int.h b/drivers/gpu/drm/amd/amdkfd/cik_int.h
index 9054068..79a16d2 100644
--- a/drivers/gpu/drm/amd/amdkfd/cik_int.h
+++ b/drivers/gpu/drm/amd/amdkfd/cik_int.h
@@ -26,31 +26,16 @@
 #include <linux/types.h>
 
 struct cik_ih_ring_entry {
-	uint32_t source_id:8;
-	uint32_t reserved1:8;
-	uint32_t reserved2:16;
-
-	uint32_t data:28;
-	uint32_t reserved3:4;
-
-	/* pipeid, meid and unused3 are officially called RINGID,
-	 * but for our purposes, they always decode into pipe and ME. */
-	uint32_t pipeid:2;
-	uint32_t meid:2;
-	uint32_t reserved4:4;
-	uint32_t vmid:8;
-	uint32_t pasid:16;
-
-	uint32_t reserved5;
+	uint32_t source_id;
+	uint32_t data;
+	uint32_t ring_id;
+	uint32_t reserved;
 };
 
 #define CIK_INTSRC_DEQUEUE_COMPLETE	0xC6
 #define CIK_INTSRC_CP_END_OF_PIPE	0xB5
 #define CIK_INTSRC_CP_BAD_OPCODE	0xB7
 #define CIK_INTSRC_SQ_INTERRUPT_MSG	0xEF
-#define CIK_INTSRC_GFX_PAGE_INV_FAULT	0x92
-#define CIK_INTSRC_GFX_MEM_PROT_FAULT	0x93
-#define CIK_INTSRC_SDMA_TRAP		0xE0
 
 #endif
 
diff --git a/drivers/gpu/drm/amd/amdkfd/cik_regs.h b/drivers/gpu/drm/amd/amdkfd/cik_regs.h
index 607fc5c..48769d1 100644
--- a/drivers/gpu/drm/amd/amdkfd/cik_regs.h
+++ b/drivers/gpu/drm/amd/amdkfd/cik_regs.h
@@ -23,33 +23,11 @@
 #ifndef CIK_REGS_H
 #define CIK_REGS_H
 
-#define IH_VMID_0_LUT					0x3D40u
-
-#define BIF_DOORBELL_CNTL				0x530Cu
-
-#define	SRBM_GFX_CNTL					0xE44
-#define	PIPEID(x)					((x) << 0)
-#define	MEID(x)						((x) << 2)
-#define	VMID(x)						((x) << 4)
-#define	QUEUEID(x)					((x) << 8)
-
-#define	SQ_CONFIG					0x8C00
-
-#define	SH_MEM_BASES					0x8C28
 /* if PTR32, these are the bases for scratch and lds */
 #define	PRIVATE_BASE(x)					((x) << 0) /* scratch */
 #define	SHARED_BASE(x)					((x) << 16) /* LDS */
-#define	SH_MEM_APE1_BASE				0x8C2C
-/* if PTR32, this is the base location of GPUVM */
-#define	SH_MEM_APE1_LIMIT				0x8C30
-/* if PTR32, this is the upper limit of GPUVM */
-#define	SH_MEM_CONFIG					0x8C34
 #define	PTR32						(1 << 0)
-#define PRIVATE_ATC					(1 << 1)
 #define	ALIGNMENT_MODE(x)				((x) << 2)
-#define	SH_MEM_ALIGNMENT_MODE_DWORD			0
-#define	SH_MEM_ALIGNMENT_MODE_DWORD_STRICT		1
-#define	SH_MEM_ALIGNMENT_MODE_STRICT			2
 #define	SH_MEM_ALIGNMENT_MODE_UNALIGNED			3
 #define	DEFAULT_MTYPE(x)				((x) << 4)
 #define	APE1_MTYPE(x)					((x) << 7)
@@ -58,164 +36,37 @@
 #define	MTYPE_CACHED					0
 #define	MTYPE_NONCACHED					3
 
-
-#define SH_STATIC_MEM_CONFIG				0x9604u
-
-#define	TC_CFG_L1_LOAD_POLICY0				0xAC68
-#define	TC_CFG_L1_LOAD_POLICY1				0xAC6C
-#define	TC_CFG_L1_STORE_POLICY				0xAC70
-#define	TC_CFG_L2_LOAD_POLICY0				0xAC74
-#define	TC_CFG_L2_LOAD_POLICY1				0xAC78
-#define	TC_CFG_L2_STORE_POLICY0				0xAC7C
-#define	TC_CFG_L2_STORE_POLICY1				0xAC80
-#define	TC_CFG_L2_ATOMIC_POLICY				0xAC84
-#define	TC_CFG_L1_VOLATILE				0xAC88
-#define	TC_CFG_L2_VOLATILE				0xAC8C
-
-#define CP_PQ_WPTR_POLL_CNTL				0xC20C
-#define	WPTR_POLL_EN					(1 << 31)
-
-#define CPC_INT_CNTL					0xC2D0
-#define CP_ME1_PIPE0_INT_CNTL				0xC214
-#define CP_ME1_PIPE1_INT_CNTL				0xC218
-#define CP_ME1_PIPE2_INT_CNTL				0xC21C
-#define CP_ME1_PIPE3_INT_CNTL				0xC220
-#define CP_ME2_PIPE0_INT_CNTL				0xC224
-#define CP_ME2_PIPE1_INT_CNTL				0xC228
-#define CP_ME2_PIPE2_INT_CNTL				0xC22C
-#define CP_ME2_PIPE3_INT_CNTL				0xC230
-#define DEQUEUE_REQUEST_INT_ENABLE			(1 << 13)
-#define WRM_POLL_TIMEOUT_INT_ENABLE			(1 << 17)
-#define PRIV_REG_INT_ENABLE				(1 << 23)
-#define TIME_STAMP_INT_ENABLE				(1 << 26)
-#define GENERIC2_INT_ENABLE				(1 << 29)
-#define GENERIC1_INT_ENABLE				(1 << 30)
-#define GENERIC0_INT_ENABLE				(1 << 31)
-#define CP_ME1_PIPE0_INT_STATUS				0xC214
-#define CP_ME1_PIPE1_INT_STATUS				0xC218
-#define CP_ME1_PIPE2_INT_STATUS				0xC21C
-#define CP_ME1_PIPE3_INT_STATUS				0xC220
-#define CP_ME2_PIPE0_INT_STATUS				0xC224
-#define CP_ME2_PIPE1_INT_STATUS				0xC228
-#define CP_ME2_PIPE2_INT_STATUS				0xC22C
-#define CP_ME2_PIPE3_INT_STATUS				0xC230
-#define DEQUEUE_REQUEST_INT_STATUS			(1 << 13)
-#define WRM_POLL_TIMEOUT_INT_STATUS			(1 << 17)
-#define PRIV_REG_INT_STATUS				(1 << 23)
-#define TIME_STAMP_INT_STATUS				(1 << 26)
-#define GENERIC2_INT_STATUS				(1 << 29)
-#define GENERIC1_INT_STATUS				(1 << 30)
-#define GENERIC0_INT_STATUS				(1 << 31)
-
-#define CP_HPD_EOP_BASE_ADDR				0xC904
-#define CP_HPD_EOP_BASE_ADDR_HI				0xC908
-#define CP_HPD_EOP_VMID					0xC90C
-#define CP_HPD_EOP_CONTROL				0xC910
-#define	EOP_SIZE(x)					((x) << 0)
-#define	EOP_SIZE_MASK					(0x3f << 0)
-#define CP_MQD_BASE_ADDR				0xC914
-#define CP_MQD_BASE_ADDR_HI				0xC918
-#define CP_HQD_ACTIVE					0xC91C
-#define CP_HQD_VMID					0xC920
-
-#define CP_HQD_PERSISTENT_STATE				0xC924u
 #define	DEFAULT_CP_HQD_PERSISTENT_STATE			(0x33U << 8)
 #define	PRELOAD_REQ					(1 << 0)
 
-#define CP_HQD_PIPE_PRIORITY				0xC928u
-#define CP_HQD_QUEUE_PRIORITY				0xC92Cu
-#define CP_HQD_QUANTUM					0xC930u
+#define	MQD_CONTROL_PRIV_STATE_EN			(1U << 8)
+
+#define	DEFAULT_MIN_IB_AVAIL_SIZE			(3U << 20)
+
+#define	IB_ATC_EN					(1U << 23)
+
 #define	QUANTUM_EN					1U
 #define	QUANTUM_SCALE_1MS				(1U << 4)
 #define	QUANTUM_DURATION(x)				((x) << 8)
 
-#define CP_HQD_PQ_BASE					0xC934
-#define CP_HQD_PQ_BASE_HI				0xC938
-#define CP_HQD_PQ_RPTR					0xC93C
-#define CP_HQD_PQ_RPTR_REPORT_ADDR			0xC940
-#define CP_HQD_PQ_RPTR_REPORT_ADDR_HI			0xC944
-#define CP_HQD_PQ_WPTR_POLL_ADDR			0xC948
-#define CP_HQD_PQ_WPTR_POLL_ADDR_HI			0xC94C
-#define CP_HQD_PQ_DOORBELL_CONTROL			0xC950
-#define	DOORBELL_OFFSET(x)				((x) << 2)
-#define	DOORBELL_OFFSET_MASK				(0x1fffff << 2)
-#define	DOORBELL_SOURCE					(1 << 28)
-#define	DOORBELL_SCHD_HIT				(1 << 29)
-#define	DOORBELL_EN					(1 << 30)
-#define	DOORBELL_HIT					(1 << 31)
-#define CP_HQD_PQ_WPTR					0xC954
-#define CP_HQD_PQ_CONTROL				0xC958
-#define	QUEUE_SIZE(x)					((x) << 0)
-#define	QUEUE_SIZE_MASK					(0x3f << 0)
 #define	RPTR_BLOCK_SIZE(x)				((x) << 8)
-#define	RPTR_BLOCK_SIZE_MASK				(0x3f << 8)
 #define	MIN_AVAIL_SIZE(x)				((x) << 20)
-#define	PQ_ATC_EN					(1 << 23)
-#define	PQ_VOLATILE					(1 << 26)
-#define	NO_UPDATE_RPTR					(1 << 27)
-#define	UNORD_DISPATCH					(1 << 28)
-#define	ROQ_PQ_IB_FLIP					(1 << 29)
-#define	PRIV_STATE					(1 << 30)
-#define	KMD_QUEUE					(1 << 31)
-
 #define	DEFAULT_RPTR_BLOCK_SIZE				RPTR_BLOCK_SIZE(5)
 #define	DEFAULT_MIN_AVAIL_SIZE				MIN_AVAIL_SIZE(3)
 
-#define CP_HQD_IB_BASE_ADDR				0xC95Cu
-#define CP_HQD_IB_BASE_ADDR_HI				0xC960u
-#define CP_HQD_IB_RPTR					0xC964u
-#define CP_HQD_IB_CONTROL				0xC968u
-#define	IB_ATC_EN					(1U << 23)
-#define	DEFAULT_MIN_IB_AVAIL_SIZE			(3U << 20)
-
-#define CP_HQD_DEQUEUE_REQUEST				0xC974
-#define	DEQUEUE_REQUEST_DRAIN				1
-#define DEQUEUE_REQUEST_RESET				2
-#define		DEQUEUE_INT					(1U << 8)
+#define	PQ_ATC_EN					(1 << 23)
+#define	NO_UPDATE_RPTR					(1 << 27)
 
-#define CP_HQD_SEMA_CMD					0xC97Cu
-#define CP_HQD_MSG_TYPE					0xC980u
-#define CP_HQD_ATOMIC0_PREOP_LO				0xC984u
-#define CP_HQD_ATOMIC0_PREOP_HI				0xC988u
-#define CP_HQD_ATOMIC1_PREOP_LO				0xC98Cu
-#define CP_HQD_ATOMIC1_PREOP_HI				0xC990u
-#define CP_HQD_HQ_SCHEDULER0				0xC994u
-#define CP_HQD_HQ_SCHEDULER1				0xC998u
+#define	DOORBELL_OFFSET(x)				((x) << 2)
+#define	DOORBELL_EN					(1 << 30)
 
+#define	PRIV_STATE					(1 << 30)
+#define	KMD_QUEUE					(1 << 31)
 
-#define CP_MQD_CONTROL					0xC99C
-#define	MQD_VMID(x)					((x) << 0)
-#define	MQD_VMID_MASK					(0xf << 0)
-#define	MQD_CONTROL_PRIV_STATE_EN			(1U << 8)
+#define	AQL_ENABLE					1
 
 #define GRBM_GFX_INDEX					0x30800
-#define	INSTANCE_INDEX(x)				((x) << 0)
-#define	SH_INDEX(x)					((x) << 8)
-#define	SE_INDEX(x)					((x) << 16)
-#define	SH_BROADCAST_WRITES				(1 << 29)
-#define	INSTANCE_BROADCAST_WRITES			(1 << 30)
-#define	SE_BROADCAST_WRITES				(1 << 31)
 
-#define SQC_CACHES					0x30d20
-#define SQC_POLICY					0x8C38u
-#define SQC_VOLATILE					0x8C3Cu
-
-#define CP_PERFMON_CNTL					0x36020
-
-#define ATC_VMID0_PASID_MAPPING				0x339Cu
-#define	ATC_VMID_PASID_MAPPING_UPDATE_STATUS		0x3398u
 #define	ATC_VMID_PASID_MAPPING_VALID			(1U << 31)
 
-#define ATC_VM_APERTURE0_CNTL				0x3310u
-#define	ATS_ACCESS_MODE_NEVER				0
-#define	ATS_ACCESS_MODE_ALWAYS				1
-
-#define ATC_VM_APERTURE0_CNTL2				0x3318u
-#define ATC_VM_APERTURE0_HIGH_ADDR			0x3308u
-#define ATC_VM_APERTURE0_LOW_ADDR			0x3300u
-#define ATC_VM_APERTURE1_CNTL				0x3314u
-#define ATC_VM_APERTURE1_CNTL2				0x331Cu
-#define ATC_VM_APERTURE1_HIGH_ADDR			0x330Cu
-#define ATC_VM_APERTURE1_LOW_ADDR			0x3304u
-
 #endif
diff --git a/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h b/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h
deleted file mode 100644
index 4e34083..0000000
--- a/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h
+++ /dev/null
@@ -1,1356 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- */
-
-#if 0
-HW (VI) source code for CWSR trap handler
-#Version 9 + multiple trap handler
-
-var G8SR_WDMEM_HWREG_OFFSET = 0
-var G8SR_WDMEM_SGPR_OFFSET  = 128  // in bytes
-
-// Keep definition same as the app shader, These 2 time stamps are part of the app shader... Should before any Save and after restore.
-
-var G8SR_DEBUG_TIMESTAMP = 0
-var G8SR_DEBUG_TS_SAVE_D_OFFSET = 40*4  // ts_save_d timestamp offset relative to SGPR_SR_memory_offset
-var s_g8sr_ts_save_s    = s[34:35]   // save start
-var s_g8sr_ts_sq_save_msg  = s[36:37]   // The save shader send SAVEWAVE msg to spi
-var s_g8sr_ts_spi_wrexec   = s[38:39]   // the SPI write the sr address to SQ
-var s_g8sr_ts_save_d    = s[40:41]   // save end
-var s_g8sr_ts_restore_s = s[42:43]   // restore start
-var s_g8sr_ts_restore_d = s[44:45]   // restore end
-
-var G8SR_VGPR_SR_IN_DWX4 = 0
-var G8SR_SAVE_BUF_RSRC_WORD1_STRIDE_DWx4 = 0x00100000    // DWx4 stride is 4*4Bytes
-var G8SR_RESTORE_BUF_RSRC_WORD1_STRIDE_DWx4  = G8SR_SAVE_BUF_RSRC_WORD1_STRIDE_DWx4
-
-
-/*************************************************************************/
-/*					control on how to run the shader					 */
-/*************************************************************************/
-//any hack that needs to be made to run this code in EMU (either becasue various EMU code are not ready or no compute save & restore in EMU run)
-var EMU_RUN_HACK					=	0
-var EMU_RUN_HACK_RESTORE_NORMAL		=	0
-var EMU_RUN_HACK_SAVE_NORMAL_EXIT	=	0
-var	EMU_RUN_HACK_SAVE_SINGLE_WAVE	=	0
-var EMU_RUN_HACK_SAVE_FIRST_TIME	= 	0					//for interrupted restore in which the first save is through EMU_RUN_HACK
-var EMU_RUN_HACK_SAVE_FIRST_TIME_TBA_LO	= 	0					//for interrupted restore in which the first save is through EMU_RUN_HACK
-var EMU_RUN_HACK_SAVE_FIRST_TIME_TBA_HI	= 	0					//for interrupted restore in which the first save is through EMU_RUN_HACK
-var SAVE_LDS						= 	1
-var WG_BASE_ADDR_LO					=   0x9000a000
-var WG_BASE_ADDR_HI					=	0x0
-var WAVE_SPACE						=	0x5000				//memory size that each wave occupies in workgroup state mem
-var CTX_SAVE_CONTROL				=	0x0
-var CTX_RESTORE_CONTROL				=	CTX_SAVE_CONTROL
-var SIM_RUN_HACK					=	0					//any hack that needs to be made to run this code in SIM (either becasue various RTL code are not ready or no compute save & restore in RTL run)
-var	SGPR_SAVE_USE_SQC				=	1					//use SQC D$ to do the write
-var USE_MTBUF_INSTEAD_OF_MUBUF		=	0					//becasue TC EMU curently asserts on 0 of // overload DFMT field to carry 4 more bits of stride for MUBUF opcodes
-var SWIZZLE_EN						=	0					//whether we use swizzled buffer addressing
-
-/**************************************************************************/
-/*                     	variables							              */
-/**************************************************************************/
-var SQ_WAVE_STATUS_INST_ATC_SHIFT  = 23
-var SQ_WAVE_STATUS_INST_ATC_MASK   = 0x00800000
-var SQ_WAVE_STATUS_SPI_PRIO_MASK   = 0x00000006
-
-var SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT	= 12
-var SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE		= 9
-var SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT	= 8
-var SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE	= 6
-var SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT	= 24
-var SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE	= 3						//FIXME	 sq.blk still has 4 bits at this time while SQ programming guide has 3 bits
-
-var	SQ_WAVE_TRAPSTS_SAVECTX_MASK	=	0x400
-var SQ_WAVE_TRAPSTS_EXCE_MASK       =   0x1FF          			// Exception mask					
-var	SQ_WAVE_TRAPSTS_SAVECTX_SHIFT	=	10					
-var	SQ_WAVE_TRAPSTS_MEM_VIOL_MASK	=	0x100					
-var	SQ_WAVE_TRAPSTS_MEM_VIOL_SHIFT	=	8		
-var	SQ_WAVE_TRAPSTS_PRE_SAVECTX_MASK 	=	0x3FF
-var	SQ_WAVE_TRAPSTS_PRE_SAVECTX_SHIFT 	=	0x0
-var	SQ_WAVE_TRAPSTS_PRE_SAVECTX_SIZE 	=	10
-var	SQ_WAVE_TRAPSTS_POST_SAVECTX_MASK 	=	0xFFFFF800	
-var	SQ_WAVE_TRAPSTS_POST_SAVECTX_SHIFT 	=	11
-var	SQ_WAVE_TRAPSTS_POST_SAVECTX_SIZE 	=	21	
-
-var SQ_WAVE_IB_STS_RCNT_SHIFT			=	16					//FIXME
-var SQ_WAVE_IB_STS_RCNT_SIZE			=	4					//FIXME
-var SQ_WAVE_IB_STS_FIRST_REPLAY_SHIFT	=	15					//FIXME
-var SQ_WAVE_IB_STS_FIRST_REPLAY_SIZE	=	1					//FIXME
-var SQ_WAVE_IB_STS_RCNT_FIRST_REPLAY_MASK_NEG	= 0x00007FFF	//FIXME
- 
-var	SQ_BUF_RSRC_WORD1_ATC_SHIFT		=	24
-var	SQ_BUF_RSRC_WORD3_MTYPE_SHIFT	=	27
-
-
-/*      Save        */
-var	S_SAVE_BUF_RSRC_WORD1_STRIDE		=	0x00040000  		//stride is 4 bytes 
-var	S_SAVE_BUF_RSRC_WORD3_MISC			= 	0x00807FAC			//SQ_SEL_X/Y/Z/W, BUF_NUM_FORMAT_FLOAT, (0 for MUBUF stride[17:14] when ADD_TID_ENABLE and BUF_DATA_FORMAT_32 for MTBUF), ADD_TID_ENABLE			
-
-var	S_SAVE_SPI_INIT_ATC_MASK			=	0x08000000			//bit[27]: ATC bit
-var	S_SAVE_SPI_INIT_ATC_SHIFT			=	27
-var	S_SAVE_SPI_INIT_MTYPE_MASK			=	0x70000000			//bit[30:28]: Mtype
-var	S_SAVE_SPI_INIT_MTYPE_SHIFT			=	28
-var	S_SAVE_SPI_INIT_FIRST_WAVE_MASK		=	0x04000000			//bit[26]: FirstWaveInTG
-var	S_SAVE_SPI_INIT_FIRST_WAVE_SHIFT	=	26
-
-var S_SAVE_PC_HI_RCNT_SHIFT				=	28					//FIXME	 check with Brian to ensure all fields other than PC[47:0] can be used
-var S_SAVE_PC_HI_RCNT_MASK				=   0xF0000000			//FIXME
-var S_SAVE_PC_HI_FIRST_REPLAY_SHIFT		=	27					//FIXME
-var S_SAVE_PC_HI_FIRST_REPLAY_MASK		=	0x08000000			//FIXME
-
-var	s_save_spi_init_lo				=	exec_lo
-var s_save_spi_init_hi				=	exec_hi
-
-												//tba_lo and tba_hi need to be saved/restored
-var	s_save_pc_lo			=	ttmp0			//{TTMP1, TTMP0} = {3h0,pc_rewind[3:0], HT[0],trapID[7:0], PC[47:0]}
-var	s_save_pc_hi			=	ttmp1			
-var s_save_exec_lo			=	ttmp2
-var s_save_exec_hi			= 	ttmp3			
-var	s_save_status			=	ttmp4			
-var	s_save_trapsts			=	ttmp5			//not really used until the end of the SAVE routine
-var s_save_xnack_mask_lo	=	ttmp6
-var s_save_xnack_mask_hi	=	ttmp7
-var	s_save_buf_rsrc0		=	ttmp8
-var	s_save_buf_rsrc1		=	ttmp9
-var	s_save_buf_rsrc2		=	ttmp10
-var	s_save_buf_rsrc3		=	ttmp11
-
-var s_save_mem_offset		= 	tma_lo				
-var s_save_alloc_size		=	s_save_trapsts			//conflict
-var s_save_tmp              =   s_save_buf_rsrc2       	//shared with s_save_buf_rsrc2  (conflict: should not use mem access with s_save_tmp at the same time)
-var s_save_m0				=	tma_hi					
-
-/*      Restore     */
-var	S_RESTORE_BUF_RSRC_WORD1_STRIDE			=	S_SAVE_BUF_RSRC_WORD1_STRIDE 
-var	S_RESTORE_BUF_RSRC_WORD3_MISC			= 	S_SAVE_BUF_RSRC_WORD3_MISC		 
-
-var	S_RESTORE_SPI_INIT_ATC_MASK			    =	0x08000000			//bit[27]: ATC bit
-var	S_RESTORE_SPI_INIT_ATC_SHIFT			=	27
-var	S_RESTORE_SPI_INIT_MTYPE_MASK			=	0x70000000			//bit[30:28]: Mtype
-var	S_RESTORE_SPI_INIT_MTYPE_SHIFT			=	28
-var	S_RESTORE_SPI_INIT_FIRST_WAVE_MASK		=	0x04000000			//bit[26]: FirstWaveInTG
-var	S_RESTORE_SPI_INIT_FIRST_WAVE_SHIFT	    =	26
-
-var S_RESTORE_PC_HI_RCNT_SHIFT				=	S_SAVE_PC_HI_RCNT_SHIFT
-var S_RESTORE_PC_HI_RCNT_MASK				=   S_SAVE_PC_HI_RCNT_MASK
-var S_RESTORE_PC_HI_FIRST_REPLAY_SHIFT		=	S_SAVE_PC_HI_FIRST_REPLAY_SHIFT
-var S_RESTORE_PC_HI_FIRST_REPLAY_MASK		=	S_SAVE_PC_HI_FIRST_REPLAY_MASK
-
-var s_restore_spi_init_lo                   =   exec_lo
-var s_restore_spi_init_hi                   =   exec_hi
-
-var s_restore_mem_offset		= 	ttmp2
-var s_restore_alloc_size		=	ttmp3
-var s_restore_tmp           	=   ttmp6				//tba_lo/hi need to be restored
-var s_restore_mem_offset_save	= 	s_restore_tmp 		//no conflict
-
-var s_restore_m0			=	s_restore_alloc_size	//no conflict			
-
-var s_restore_mode			=  	ttmp7
-
-var	s_restore_pc_lo		    =	ttmp0			
-var	s_restore_pc_hi		    =	ttmp1
-var s_restore_exec_lo		=	tma_lo					//no conflict
-var s_restore_exec_hi		= 	tma_hi					//no conflict
-var	s_restore_status	    =	ttmp4			
-var	s_restore_trapsts	    =	ttmp5
-var s_restore_xnack_mask_lo	=	xnack_mask_lo
-var s_restore_xnack_mask_hi	=	xnack_mask_hi
-var	s_restore_buf_rsrc0		=	ttmp8
-var	s_restore_buf_rsrc1		=	ttmp9
-var	s_restore_buf_rsrc2		=	ttmp10
-var	s_restore_buf_rsrc3		=	ttmp11
-
-/**************************************************************************/
-/*                     	trap handler entry points			              */
-/**************************************************************************/
-/* Shader Main*/
-
-shader main
-  asic(CARRIZO)
-  type(CS)
-
-
-    if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_RESTORE_NORMAL)) 					//hack to use trap_id for determining save/restore
-		//FIXME VCCZ un-init assertion s_getreg_b32  	s_save_status, hwreg(HW_REG_STATUS)			//save STATUS since we will change SCC
-		s_and_b32 s_save_tmp, s_save_pc_hi, 0xffff0000 				//change SCC
-    	s_cmp_eq_u32 s_save_tmp, 0x007e0000  						//Save: trap_id = 0x7e. Restore: trap_id = 0x7f.  
-    	s_cbranch_scc0 L_JUMP_TO_RESTORE							//do not need to recover STATUS here  since we are going to RESTORE
-		//FIXME  s_setreg_b32 	hwreg(HW_REG_STATUS), 	s_save_status		//need to recover STATUS since we are going to SAVE	
-		s_branch L_SKIP_RESTORE 									//NOT restore, SAVE actually
-	else	
-		s_branch L_SKIP_RESTORE 									//NOT restore. might be a regular trap or save
-    end
-
-L_JUMP_TO_RESTORE:
-    s_branch L_RESTORE												//restore
-
-L_SKIP_RESTORE:
-	
-	s_getreg_b32  	s_save_status, hwreg(HW_REG_STATUS)								//save STATUS since we will change SCC
-	s_andn2_b32		s_save_status, s_save_status, SQ_WAVE_STATUS_SPI_PRIO_MASK      //check whether this is for save
-	s_getreg_b32  	s_save_trapsts, hwreg(HW_REG_TRAPSTS)    		 				
-	s_and_b32		s_save_trapsts, s_save_trapsts, SQ_WAVE_TRAPSTS_SAVECTX_MASK	//check whether this is for save  
-	s_cbranch_scc1	L_SAVE		     	  						//this is the operation for save
-
-    // *********    Handle non-CWSR traps       *******************
-if (!EMU_RUN_HACK)
-    /* read tba and tma for next level trap handler, ttmp4 is used as s_save_status */
-	s_load_dwordx4  [ttmp8,ttmp9,ttmp10, ttmp11], [tma_lo,tma_hi], 0 
-	s_waitcnt lgkmcnt(0)
-	s_or_b32        ttmp7, ttmp8, ttmp9
-	s_cbranch_scc0  L_NO_NEXT_TRAP //next level trap handler not been set
-	s_mov_b32       tma_lo, ttmp10  //set tma_lo/hi for next level trap handler
-	s_mov_b32       tma_hi, ttmp11 
-	s_setreg_b32    hwreg(HW_REG_STATUS), s_save_status //restore HW status(SCC)
-	s_setpc_b64     [ttmp8,ttmp9] //jump to next level trap handler 
-
-L_NO_NEXT_TRAP:
-	s_getreg_b32    s_save_trapsts, hwreg(HW_REG_TRAPSTS)
-	s_and_b32       s_save_trapsts, s_save_trapsts, SQ_WAVE_TRAPSTS_EXCE_MASK // Check whether it is an exception
-	s_cbranch_scc1  L_EXCP_CASE   // Exception, jump back to the shader program directly.
-	s_add_u32       ttmp0, ttmp0, 4   // S_TRAP case, add 4 to ttmp0
-	s_addc_u32	ttmp1, ttmp1, 0
-L_EXCP_CASE:
-	s_and_b32	ttmp1, ttmp1, 0xFFFF
-	s_setreg_b32    hwreg(HW_REG_STATUS), s_save_status //restore HW status(SCC)   
-	s_rfe_b64    	[ttmp0, ttmp1]
-end
-    // *********        End handling of non-CWSR traps   *******************
-
-/**************************************************************************/
-/*                     	save routine						              */
-/**************************************************************************/
-
-L_SAVE:	
-
-if G8SR_DEBUG_TIMESTAMP
-        s_memrealtime	s_g8sr_ts_save_s
-        s_waitcnt lgkmcnt(0)         //FIXME, will cause xnack??
-end
-
-	//check whether there is mem_viol
-	s_getreg_b32  	s_save_trapsts, hwreg(HW_REG_TRAPSTS)    		 				
-	s_and_b32	s_save_trapsts, s_save_trapsts, SQ_WAVE_TRAPSTS_MEM_VIOL_MASK			
-	s_cbranch_scc0	L_NO_PC_REWIND
-    
-	//if so, need rewind PC assuming GDS operation gets NACKed
-	s_mov_b32       s_save_tmp, 0															//clear mem_viol bit
-	s_setreg_b32	hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_MEM_VIOL_SHIFT, 1), s_save_tmp	//clear mem_viol bit 
-	s_and_b32 		s_save_pc_hi, s_save_pc_hi, 0x0000ffff    //pc[47:32]
-	s_sub_u32 		s_save_pc_lo, s_save_pc_lo, 8             //pc[31:0]-8
-	s_subb_u32 		s_save_pc_hi, s_save_pc_hi, 0x0			  // -scc
-
-L_NO_PC_REWIND:
-    s_mov_b32       s_save_tmp, 0															//clear saveCtx bit
-	s_setreg_b32	hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_SAVECTX_SHIFT, 1), s_save_tmp		//clear saveCtx bit   
-
-	s_mov_b32		s_save_xnack_mask_lo,	xnack_mask_lo									//save XNACK_MASK  
-	s_mov_b32		s_save_xnack_mask_hi,	xnack_mask_hi    //save XNACK must before any memory operation
-	s_getreg_b32	s_save_tmp, hwreg(HW_REG_IB_STS, SQ_WAVE_IB_STS_RCNT_SHIFT, SQ_WAVE_IB_STS_RCNT_SIZE)					//save RCNT
-	s_lshl_b32		s_save_tmp, s_save_tmp, S_SAVE_PC_HI_RCNT_SHIFT
-	s_or_b32		s_save_pc_hi, s_save_pc_hi, s_save_tmp
-	s_getreg_b32	s_save_tmp, hwreg(HW_REG_IB_STS, SQ_WAVE_IB_STS_FIRST_REPLAY_SHIFT, SQ_WAVE_IB_STS_FIRST_REPLAY_SIZE)	//save FIRST_REPLAY
-	s_lshl_b32		s_save_tmp, s_save_tmp, S_SAVE_PC_HI_FIRST_REPLAY_SHIFT
-	s_or_b32		s_save_pc_hi, s_save_pc_hi, s_save_tmp
-	s_getreg_b32	s_save_tmp, hwreg(HW_REG_IB_STS)										//clear RCNT and FIRST_REPLAY in IB_STS
-	s_and_b32		s_save_tmp, s_save_tmp, SQ_WAVE_IB_STS_RCNT_FIRST_REPLAY_MASK_NEG
-
-	s_setreg_b32	hwreg(HW_REG_IB_STS), s_save_tmp
-    
-	/*		inform SPI the readiness and wait for SPI's go signal */
-	s_mov_b32		s_save_exec_lo,	exec_lo													//save EXEC and use EXEC for the go signal from SPI
-	s_mov_b32		s_save_exec_hi,	exec_hi
-	s_mov_b64		exec, 	0x0																//clear EXEC to get ready to receive
-
-if G8SR_DEBUG_TIMESTAMP
-        s_memrealtime  s_g8sr_ts_sq_save_msg
-        s_waitcnt lgkmcnt(0)
-end
-
-	if (EMU_RUN_HACK)
-	
-	else
-		s_sendmsg	sendmsg(MSG_SAVEWAVE)  //send SPI a message and wait for SPI's write to EXEC  
-	end
-
-  L_SLEEP:		
-	s_sleep 0x2                // sleep 1 (64clk) is not enough for 8 waves per SIMD, which will cause SQ hang, since the 7,8th wave could not get arbit to exec inst, while other waves are stuck into the sleep-loop and waiting for wrexec!=0
-	
-	if (EMU_RUN_HACK)
-																							
-	else
-		s_cbranch_execz	L_SLEEP                                                         
-	end
-
-if G8SR_DEBUG_TIMESTAMP
-        s_memrealtime  s_g8sr_ts_spi_wrexec
-        s_waitcnt lgkmcnt(0)
-end
-
-	/*      setup Resource Contants    */
-	if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_SAVE_SINGLE_WAVE))	
-		//calculate wd_addr using absolute thread id 
-		v_readlane_b32 s_save_tmp, v9, 0
-		s_lshr_b32 s_save_tmp, s_save_tmp, 6
-		s_mul_i32 s_save_tmp, s_save_tmp, WAVE_SPACE
-		s_add_i32 s_save_spi_init_lo, s_save_tmp, WG_BASE_ADDR_LO
-		s_mov_b32 s_save_spi_init_hi, WG_BASE_ADDR_HI
-		s_and_b32 s_save_spi_init_hi, s_save_spi_init_hi, CTX_SAVE_CONTROL		
-	else
-	end
-	if ((EMU_RUN_HACK) && (EMU_RUN_HACK_SAVE_SINGLE_WAVE))
-		s_add_i32 s_save_spi_init_lo, s_save_tmp, WG_BASE_ADDR_LO
-		s_mov_b32 s_save_spi_init_hi, WG_BASE_ADDR_HI
-		s_and_b32 s_save_spi_init_hi, s_save_spi_init_hi, CTX_SAVE_CONTROL		
-	else
-	end
-	
-	
-	s_mov_b32		s_save_buf_rsrc0, 	s_save_spi_init_lo														//base_addr_lo
-	s_and_b32		s_save_buf_rsrc1, 	s_save_spi_init_hi, 0x0000FFFF											//base_addr_hi
-	s_or_b32		s_save_buf_rsrc1, 	s_save_buf_rsrc1,  S_SAVE_BUF_RSRC_WORD1_STRIDE
-    s_mov_b32       s_save_buf_rsrc2,   0                                               						//NUM_RECORDS initial value = 0 (in bytes) although not neccessarily inited
-	s_mov_b32		s_save_buf_rsrc3, 	S_SAVE_BUF_RSRC_WORD3_MISC
-	s_and_b32		s_save_tmp,         s_save_spi_init_hi, S_SAVE_SPI_INIT_ATC_MASK		
-	s_lshr_b32		s_save_tmp,  		s_save_tmp, (S_SAVE_SPI_INIT_ATC_SHIFT-SQ_BUF_RSRC_WORD1_ATC_SHIFT)			//get ATC bit into position
-	s_or_b32		s_save_buf_rsrc3, 	s_save_buf_rsrc3,  s_save_tmp											//or ATC
-	s_and_b32		s_save_tmp,         s_save_spi_init_hi, S_SAVE_SPI_INIT_MTYPE_MASK		
-	s_lshr_b32		s_save_tmp,  		s_save_tmp, (S_SAVE_SPI_INIT_MTYPE_SHIFT-SQ_BUF_RSRC_WORD3_MTYPE_SHIFT)		//get MTYPE bits into position
-	s_or_b32		s_save_buf_rsrc3, 	s_save_buf_rsrc3,  s_save_tmp											//or MTYPE	
-	
-	//FIXME  right now s_save_m0/s_save_mem_offset use tma_lo/tma_hi  (might need to save them before using them?)
-	s_mov_b32		s_save_m0,			m0																	//save M0
-	
-	/* 		global mem offset			*/
-	s_mov_b32		s_save_mem_offset, 	0x0																		//mem offset initial value = 0
-
-
-
-
-	/* 		save HW registers	*/
-	//////////////////////////////
-
-  L_SAVE_HWREG:
-        // HWREG SR memory offset : size(VGPR)+size(SGPR)
-       get_vgpr_size_bytes(s_save_mem_offset)
-       get_sgpr_size_bytes(s_save_tmp)
-       s_add_u32 s_save_mem_offset, s_save_mem_offset, s_save_tmp
-
-
-    s_mov_b32		s_save_buf_rsrc2, 0x4								//NUM_RECORDS	in bytes
-	if (SWIZZLE_EN)
-		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
-	else
-		s_mov_b32		s_save_buf_rsrc2,  0x1000000								//NUM_RECORDS in bytes
-	end
-
-	
-	write_hwreg_to_mem(s_save_m0, s_save_buf_rsrc0, s_save_mem_offset)					//M0
-
-	if ((EMU_RUN_HACK) && (EMU_RUN_HACK_SAVE_FIRST_TIME))      
-		s_add_u32 s_save_pc_lo, s_save_pc_lo, 4             //pc[31:0]+4
-		s_addc_u32 s_save_pc_hi, s_save_pc_hi, 0x0			//carry bit over
-		s_mov_b32	tba_lo, EMU_RUN_HACK_SAVE_FIRST_TIME_TBA_LO
-	    s_mov_b32	tba_hi, EMU_RUN_HACK_SAVE_FIRST_TIME_TBA_HI	
-	end
-
-	write_hwreg_to_mem(s_save_pc_lo, s_save_buf_rsrc0, s_save_mem_offset)					//PC
-	write_hwreg_to_mem(s_save_pc_hi, s_save_buf_rsrc0, s_save_mem_offset)
-	write_hwreg_to_mem(s_save_exec_lo, s_save_buf_rsrc0, s_save_mem_offset)				//EXEC
-	write_hwreg_to_mem(s_save_exec_hi, s_save_buf_rsrc0, s_save_mem_offset)
-	write_hwreg_to_mem(s_save_status, s_save_buf_rsrc0, s_save_mem_offset)				//STATUS 
-	
-	//s_save_trapsts conflicts with s_save_alloc_size
-	s_getreg_b32    s_save_trapsts, hwreg(HW_REG_TRAPSTS)
-	write_hwreg_to_mem(s_save_trapsts, s_save_buf_rsrc0, s_save_mem_offset)				//TRAPSTS
-	
-	write_hwreg_to_mem(s_save_xnack_mask_lo, s_save_buf_rsrc0, s_save_mem_offset)			//XNACK_MASK_LO
-	write_hwreg_to_mem(s_save_xnack_mask_hi, s_save_buf_rsrc0, s_save_mem_offset)			//XNACK_MASK_HI
-	
-	//use s_save_tmp would introduce conflict here between s_save_tmp and s_save_buf_rsrc2
-	s_getreg_b32 	s_save_m0, hwreg(HW_REG_MODE)                                                   //MODE
-	write_hwreg_to_mem(s_save_m0, s_save_buf_rsrc0, s_save_mem_offset)
-	write_hwreg_to_mem(tba_lo, s_save_buf_rsrc0, s_save_mem_offset)						//TBA_LO
-	write_hwreg_to_mem(tba_hi, s_save_buf_rsrc0, s_save_mem_offset)						//TBA_HI
-
-
-
-	/*      the first wave in the threadgroup    */
-        // save fist_wave bits in tba_hi unused bit.26
-	s_and_b32		s_save_tmp, s_save_spi_init_hi, S_SAVE_SPI_INIT_FIRST_WAVE_MASK     // extract fisrt wave bit
-    //s_or_b32        tba_hi, s_save_tmp, tba_hi                                        // save first wave bit to tba_hi.bits[26]
-    s_mov_b32        s_save_exec_hi, 0x0                                
-    s_or_b32         s_save_exec_hi, s_save_tmp, s_save_exec_hi                          // save first wave bit to s_save_exec_hi.bits[26]
-
-
-	/*      	save SGPRs	    */
-        // Save SGPR before LDS save, then the s0 to s4 can be used during LDS save...
-	//////////////////////////////
-
-    // SGPR SR memory offset : size(VGPR)	
-    get_vgpr_size_bytes(s_save_mem_offset)
-    // TODO, change RSRC word to rearrange memory layout for SGPRS
-
-	s_getreg_b32 	s_save_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE) 				//spgr_size
-	s_add_u32 		s_save_alloc_size, s_save_alloc_size, 1
-	s_lshl_b32 		s_save_alloc_size, s_save_alloc_size, 4 						//Number of SGPRs = (sgpr_size + 1) * 16   (non-zero value) 
-	
-	if (SGPR_SAVE_USE_SQC)
-		s_lshl_b32		s_save_buf_rsrc2,	s_save_alloc_size, 2					//NUM_RECORDS in bytes 
-	else
-		s_lshl_b32		s_save_buf_rsrc2,	s_save_alloc_size, 8					//NUM_RECORDS in bytes (64 threads)
-	end
-	
-	if (SWIZZLE_EN)
-		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
-	else
-		s_mov_b32		s_save_buf_rsrc2,  0x1000000								//NUM_RECORDS in bytes
-	end
-
-
-	// backup s_save_buf_rsrc0,1 to s_save_pc_lo/hi, since write_16sgpr_to_mem function will change the rsrc0
-    //s_mov_b64 s_save_pc_lo, s_save_buf_rsrc0
-    s_mov_b64 s_save_xnack_mask_lo, s_save_buf_rsrc0
-    s_add_u32 s_save_buf_rsrc0, s_save_buf_rsrc0, s_save_mem_offset
-
-	s_mov_b32 		m0, 0x0 						//SGPR initial index value =0		
-  L_SAVE_SGPR_LOOP: 					
-    // SGPR is allocated in 16 SGPR granularity					
-	s_movrels_b64 	s0, s0     //s0 = s[0+m0], s1 = s[1+m0]
-    s_movrels_b64   s2, s2     //s2 = s[2+m0], s3 = s[3+m0]
-	s_movrels_b64 	s4, s4     //s4 = s[4+m0], s5 = s[5+m0]
-    s_movrels_b64   s6, s6     //s6 = s[6+m0], s7 = s[7+m0]
-	s_movrels_b64 	s8, s8     //s8 = s[8+m0], s9 = s[9+m0]
-    s_movrels_b64   s10, s10   //s10 = s[10+m0], s11 = s[11+m0]
-	s_movrels_b64 	s12, s12   //s12 = s[12+m0], s13 = s[13+m0]
-    s_movrels_b64   s14, s14   //s14 = s[14+m0], s15 = s[15+m0]
-
-	write_16sgpr_to_mem(s0, s_save_buf_rsrc0, s_save_mem_offset) //PV: the best performance should be using s_buffer_store_dwordx4
-	s_add_u32		m0, m0, 16														//next sgpr index
-	s_cmp_lt_u32 	m0, s_save_alloc_size 											//scc = (m0 < s_save_alloc_size) ? 1 : 0
-	s_cbranch_scc1 	L_SAVE_SGPR_LOOP									//SGPR save is complete?
-    // restore s_save_buf_rsrc0,1
-    //s_mov_b64 s_save_buf_rsrc0, s_save_pc_lo
-    s_mov_b64 s_save_buf_rsrc0, s_save_xnack_mask_lo
-
-
-
-
-	/*      	save first 4 VGPR, then LDS save could use   */
-        // each wave will alloc 4 vgprs at least...
-	/////////////////////////////////////////////////////////////////////////////////////
-
-    s_mov_b32       s_save_mem_offset, 0
- 	s_mov_b32		exec_lo, 0xFFFFFFFF 											//need every thread from now on
-	s_mov_b32		exec_hi, 0xFFFFFFFF
-	
-	if (SWIZZLE_EN)
-		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
-	else
-		s_mov_b32		s_save_buf_rsrc2,  0x1000000								//NUM_RECORDS in bytes
-	end
-
-
-	// VGPR Allocated in 4-GPR granularity
-
-if G8SR_VGPR_SR_IN_DWX4
-        // the const stride for DWx4 is 4*4 bytes
-        s_and_b32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
-        s_or_b32  s_save_buf_rsrc1, s_save_buf_rsrc1, G8SR_SAVE_BUF_RSRC_WORD1_STRIDE_DWx4  // const stride to 4*4 bytes
-        
-        buffer_store_dwordx4 v0, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1
-
-        s_and_b32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
-        s_or_b32  s_save_buf_rsrc1, s_save_buf_rsrc1, S_SAVE_BUF_RSRC_WORD1_STRIDE  // reset const stride to 4 bytes
-else
-		buffer_store_dword v0, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1
-		buffer_store_dword v1, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256
-		buffer_store_dword v2, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256*2
-		buffer_store_dword v3, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256*3
-end
-
-
-
-	/*      	save LDS	    */
-	//////////////////////////////
-	
-  L_SAVE_LDS:
-
-        // Change EXEC to all threads...	
-	s_mov_b32		exec_lo, 0xFFFFFFFF   //need every thread from now on   
-	s_mov_b32		exec_hi, 0xFFFFFFFF
-	
-	s_getreg_b32 	s_save_alloc_size, hwreg(HW_REG_LDS_ALLOC,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE) 			//lds_size
-	s_and_b32		s_save_alloc_size, s_save_alloc_size, 0xFFFFFFFF				//lds_size is zero?
-	s_cbranch_scc0	L_SAVE_LDS_DONE                                                                            //no lds used? jump to L_SAVE_DONE
-
-	s_barrier               //LDS is used? wait for other waves in the same TG 
-	//s_and_b32		s_save_tmp, tba_hi, S_SAVE_SPI_INIT_FIRST_WAVE_MASK                //exec is still used here     
-    s_and_b32		s_save_tmp, s_save_exec_hi, S_SAVE_SPI_INIT_FIRST_WAVE_MASK                //exec is still used here
-	s_cbranch_scc0	L_SAVE_LDS_DONE
-
-        // first wave do LDS save;
-        
-	s_lshl_b32 		s_save_alloc_size, s_save_alloc_size, 6 						//LDS size in dwords = lds_size * 64dw
-	s_lshl_b32 		s_save_alloc_size, s_save_alloc_size, 2 						//LDS size in bytes
-	s_mov_b32		s_save_buf_rsrc2,  s_save_alloc_size  							//NUM_RECORDS in bytes
-
-    // LDS at offset: size(VGPR)+SIZE(SGPR)+SIZE(HWREG)
-    // 
-    get_vgpr_size_bytes(s_save_mem_offset)
-    get_sgpr_size_bytes(s_save_tmp)
-    s_add_u32  s_save_mem_offset, s_save_mem_offset, s_save_tmp
-    s_add_u32 s_save_mem_offset, s_save_mem_offset, get_hwreg_size_bytes()
-
-
-	if (SWIZZLE_EN)
-		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0	      //FIXME need to use swizzle to enable bounds checking?
-	else
-		s_mov_b32		s_save_buf_rsrc2,  0x1000000                  //NUM_RECORDS in bytes
-	end
-
-	s_mov_b32 		m0, 0x0                                               //lds_offset initial value = 0
-
-
-var LDS_DMA_ENABLE = 0
-var UNROLL = 0
-if UNROLL==0 && LDS_DMA_ENABLE==1
-        s_mov_b32  s3, 256*2
-        s_nop 0
-        s_nop 0
-        s_nop 0
-  L_SAVE_LDS_LOOP:
-        //TODO: looks the 2 buffer_store/load clause for s/r will hurt performance.???
-	if (SAVE_LDS)     //SPI always alloc LDS space in 128DW granularity
-            buffer_store_lds_dword s_save_buf_rsrc0, s_save_mem_offset lds:1            // first 64DW
-            buffer_store_lds_dword s_save_buf_rsrc0, s_save_mem_offset lds:1 offset:256 // second 64DW
-	end
-
-	s_add_u32		m0, m0, s3											//every buffer_store_lds does 256 bytes
-	s_add_u32		s_save_mem_offset, s_save_mem_offset, s3							//mem offset increased by 256 bytes
-	s_cmp_lt_u32	m0, s_save_alloc_size												//scc=(m0 < s_save_alloc_size) ? 1 : 0
-	s_cbranch_scc1  L_SAVE_LDS_LOOP														//LDS save is complete?
-
-elsif LDS_DMA_ENABLE==1 && UNROLL==1 // UNROOL  , has ichace miss
-      // store from higest LDS address to lowest
-      s_mov_b32  s3, 256*2
-      s_sub_u32  m0, s_save_alloc_size, s3
-      s_add_u32 s_save_mem_offset, s_save_mem_offset, m0
-      s_lshr_b32 s_save_alloc_size, s_save_alloc_size, 9   // how many 128 trunks...
-      s_sub_u32 s_save_alloc_size, 128, s_save_alloc_size   // store from higheset addr to lowest
-      s_mul_i32 s_save_alloc_size, s_save_alloc_size, 6*4   // PC offset increment,  each LDS save block cost 6*4 Bytes instruction
-      s_add_u32 s_save_alloc_size, s_save_alloc_size, 3*4   //2is the below 2 inst...//s_addc and s_setpc
-      s_nop 0
-      s_nop 0
-      s_nop 0   //pad 3 dw to let LDS_DMA align with 64Bytes
-      s_getpc_b64 s[0:1]                              // reuse s[0:1], since s[0:1] already saved
-      s_add_u32   s0, s0,s_save_alloc_size
-      s_addc_u32  s1, s1, 0
-      s_setpc_b64 s[0:1]
-      
-
-       for var i =0; i< 128; i++    
-            // be careful to make here a 64Byte aligned address, which could improve performance...
-            buffer_store_lds_dword s_save_buf_rsrc0, s_save_mem_offset lds:1 offset:0           // first 64DW
-            buffer_store_lds_dword s_save_buf_rsrc0, s_save_mem_offset lds:1 offset:256           // second 64DW
-
-	    if i!=127
-		s_sub_u32  m0, m0, s3      // use a sgpr to shrink 2DW-inst to 1DW inst to improve performance , i.e.  pack more LDS_DMA inst to one Cacheline
-	    	s_sub_u32  s_save_mem_offset, s_save_mem_offset,  s3
-            end
-       end
-
-else   // BUFFER_STORE
-      v_mbcnt_lo_u32_b32 v2, 0xffffffff, 0x0
-      v_mbcnt_hi_u32_b32 v3, 0xffffffff, v2     // tid
-      v_mul_i32_i24 v2, v3, 8   // tid*8
-      v_mov_b32 v3, 256*2
-      s_mov_b32 m0, 0x10000
-      s_mov_b32 s0, s_save_buf_rsrc3
-      s_and_b32 s_save_buf_rsrc3, s_save_buf_rsrc3, 0xFF7FFFFF    // disable add_tid 
-      s_or_b32 s_save_buf_rsrc3, s_save_buf_rsrc3, 0x58000   //DFMT
-
-L_SAVE_LDS_LOOP_VECTOR:
-      ds_read_b64 v[0:1], v2    //x =LDS[a], byte address
-      s_waitcnt lgkmcnt(0)
-      buffer_store_dwordx2  v[0:1], v2, s_save_buf_rsrc0, s_save_mem_offset offen:1  glc:1  slc:1
-//      s_waitcnt vmcnt(0)
-      v_add_u32 v2, vcc[0:1], v2, v3
-      v_cmp_lt_u32 vcc[0:1], v2, s_save_alloc_size
-      s_cbranch_vccnz L_SAVE_LDS_LOOP_VECTOR
-
-      // restore rsrc3
-      s_mov_b32 s_save_buf_rsrc3, s0
-      
-end
-
-L_SAVE_LDS_DONE:	
-
-
-	/*      	save VGPRs  - set the Rest VGPRs	    */
-	//////////////////////////////////////////////////////////////////////////////////////
-  L_SAVE_VGPR:
-    // VGPR SR memory offset: 0
-    // TODO rearrange the RSRC words to use swizzle for VGPR save...
-  
-    s_mov_b32       s_save_mem_offset, (0+256*4)                                    // for the rest VGPRs
- 	s_mov_b32		exec_lo, 0xFFFFFFFF 											//need every thread from now on
-	s_mov_b32		exec_hi, 0xFFFFFFFF
-	
-	s_getreg_b32 	s_save_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE) 					//vpgr_size
-	s_add_u32 		s_save_alloc_size, s_save_alloc_size, 1
-	s_lshl_b32 		s_save_alloc_size, s_save_alloc_size, 2 						//Number of VGPRs = (vgpr_size + 1) * 4    (non-zero value)   //FIXME for GFX, zero is possible 
-	s_lshl_b32		s_save_buf_rsrc2,  s_save_alloc_size, 8							//NUM_RECORDS in bytes (64 threads*4)
-	if (SWIZZLE_EN)
-		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
-	else
-		s_mov_b32		s_save_buf_rsrc2,  0x1000000								//NUM_RECORDS in bytes
-	end
-
-
-	// VGPR Allocated in 4-GPR granularity
-
-if G8SR_VGPR_SR_IN_DWX4
-        // the const stride for DWx4 is 4*4 bytes
-        s_and_b32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
-        s_or_b32  s_save_buf_rsrc1, s_save_buf_rsrc1, G8SR_SAVE_BUF_RSRC_WORD1_STRIDE_DWx4  // const stride to 4*4 bytes
-        
-        s_mov_b32         m0, 4     // skip first 4 VGPRs
-        s_cmp_lt_u32      m0, s_save_alloc_size
-        s_cbranch_scc0    L_SAVE_VGPR_LOOP_END      // no more vgprs
-
-        s_set_gpr_idx_on  m0, 0x1   // This will change M0
-        s_add_u32         s_save_alloc_size, s_save_alloc_size, 0x1000  // because above inst change m0
-L_SAVE_VGPR_LOOP:
-        v_mov_b32         v0, v0   // v0 = v[0+m0]
-        v_mov_b32         v1, v1
-        v_mov_b32         v2, v2
-        v_mov_b32         v3, v3
-
-
-        buffer_store_dwordx4 v0, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1
-        s_add_u32         m0, m0, 4
-        s_add_u32         s_save_mem_offset, s_save_mem_offset, 256*4
-        s_cmp_lt_u32      m0, s_save_alloc_size
-	s_cbranch_scc1 	L_SAVE_VGPR_LOOP												//VGPR save is complete?
-	s_set_gpr_idx_off
-L_SAVE_VGPR_LOOP_END:
-
-        s_and_b32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
-        s_or_b32  s_save_buf_rsrc1, s_save_buf_rsrc1, S_SAVE_BUF_RSRC_WORD1_STRIDE  // reset const stride to 4 bytes
-else
-    // VGPR store using dw burst	
-    s_mov_b32 		  m0, 0x4   //VGPR initial index value =0
-    s_cmp_lt_u32      m0, s_save_alloc_size
-    s_cbranch_scc0    L_SAVE_VGPR_END
-
-
-    s_set_gpr_idx_on    m0, 0x1 //M0[7:0] = M0[7:0] and M0[15:12] = 0x1
-    s_add_u32		s_save_alloc_size, s_save_alloc_size, 0x1000					//add 0x1000 since we compare m0 against it later	
-
-  L_SAVE_VGPR_LOOP: 										
-	v_mov_b32 		v0, v0				//v0 = v[0+m0]	
-	v_mov_b32 		v1, v1				//v0 = v[0+m0]	
-	v_mov_b32 		v2, v2				//v0 = v[0+m0]	
-	v_mov_b32 		v3, v3				//v0 = v[0+m0]	
-	    
-    if(USE_MTBUF_INSTEAD_OF_MUBUF)       
-		tbuffer_store_format_x v0, v0, s_save_buf_rsrc0, s_save_mem_offset format:BUF_NUM_FORMAT_FLOAT format: BUF_DATA_FORMAT_32 slc:1 glc:1
-    else
-		buffer_store_dword v0, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1
-		buffer_store_dword v1, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256
-		buffer_store_dword v2, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256*2
-		buffer_store_dword v3, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256*3
-	end
-
-    s_add_u32		m0, m0, 4														//next vgpr index
-	s_add_u32		s_save_mem_offset, s_save_mem_offset, 256*4						//every buffer_store_dword does 256 bytes
-	s_cmp_lt_u32 	m0,	s_save_alloc_size 											//scc = (m0 < s_save_alloc_size) ? 1 : 0
-	s_cbranch_scc1 	L_SAVE_VGPR_LOOP												//VGPR save is complete?
-	s_set_gpr_idx_off
-end
-	
-L_SAVE_VGPR_END:
-
-
-
-
-
-	
-	/*     S_PGM_END_SAVED  */    							//FIXME  graphics ONLY
-	if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_SAVE_NORMAL_EXIT))	
-		s_and_b32 s_save_pc_hi, s_save_pc_hi, 0x0000ffff    //pc[47:32]
-		s_add_u32 s_save_pc_lo, s_save_pc_lo, 4             //pc[31:0]+4
-		s_addc_u32 s_save_pc_hi, s_save_pc_hi, 0x0			//carry bit over
-		s_rfe_b64 s_save_pc_lo                              //Return to the main shader program
-	else
-	end
-
-// Save Done timestamp 
-if G8SR_DEBUG_TIMESTAMP
-        s_memrealtime	s_g8sr_ts_save_d
-        // SGPR SR memory offset : size(VGPR)	
-        get_vgpr_size_bytes(s_save_mem_offset)
-        s_add_u32 s_save_mem_offset, s_save_mem_offset, G8SR_DEBUG_TS_SAVE_D_OFFSET
-        s_waitcnt lgkmcnt(0)         //FIXME, will cause xnack??
-        // Need reset rsrc2??
-        s_mov_b32 m0, s_save_mem_offset
-        s_mov_b32 s_save_buf_rsrc2,  0x1000000                                  //NUM_RECORDS in bytes
-        s_buffer_store_dwordx2 s_g8sr_ts_save_d, s_save_buf_rsrc0, m0		glc:1	
-end
-
-	
-    s_branch	L_END_PGM
-	
-
-				
-/**************************************************************************/
-/*                     	restore routine						              */
-/**************************************************************************/
-
-L_RESTORE:
-    /*      Setup Resource Contants    */
-    if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_RESTORE_NORMAL))
-		//calculate wd_addr using absolute thread id
-		v_readlane_b32 s_restore_tmp, v9, 0
-		s_lshr_b32 s_restore_tmp, s_restore_tmp, 6
-		s_mul_i32 s_restore_tmp, s_restore_tmp, WAVE_SPACE
-		s_add_i32 s_restore_spi_init_lo, s_restore_tmp, WG_BASE_ADDR_LO
-		s_mov_b32 s_restore_spi_init_hi, WG_BASE_ADDR_HI
-		s_and_b32 s_restore_spi_init_hi, s_restore_spi_init_hi, CTX_RESTORE_CONTROL	
-	else
-	end
-
-if G8SR_DEBUG_TIMESTAMP
-        s_memrealtime	s_g8sr_ts_restore_s
-        s_waitcnt lgkmcnt(0)         //FIXME, will cause xnack??
-        // tma_lo/hi are sgpr 110, 111, which will not used for 112 SGPR allocated case...
-        s_mov_b32 s_restore_pc_lo, s_g8sr_ts_restore_s[0]
-        s_mov_b32 s_restore_pc_hi, s_g8sr_ts_restore_s[1]   //backup ts to ttmp0/1, sicne exec will be finally restored..
-end
-
-
-	
-    s_mov_b32		s_restore_buf_rsrc0, 	s_restore_spi_init_lo															//base_addr_lo
-	s_and_b32		s_restore_buf_rsrc1, 	s_restore_spi_init_hi, 0x0000FFFF												//base_addr_hi
-	s_or_b32		s_restore_buf_rsrc1, 	s_restore_buf_rsrc1,  S_RESTORE_BUF_RSRC_WORD1_STRIDE
-    s_mov_b32       s_restore_buf_rsrc2,   	0                                               								//NUM_RECORDS initial value = 0 (in bytes)
-	s_mov_b32		s_restore_buf_rsrc3, 	S_RESTORE_BUF_RSRC_WORD3_MISC
-	s_and_b32		s_restore_tmp,         	s_restore_spi_init_hi, S_RESTORE_SPI_INIT_ATC_MASK		
-	s_lshr_b32		s_restore_tmp,  		s_restore_tmp, (S_RESTORE_SPI_INIT_ATC_SHIFT-SQ_BUF_RSRC_WORD1_ATC_SHIFT)		//get ATC bit into position
-	s_or_b32		s_restore_buf_rsrc3, 	s_restore_buf_rsrc3,  s_restore_tmp												//or ATC
-	s_and_b32		s_restore_tmp,         	s_restore_spi_init_hi, S_RESTORE_SPI_INIT_MTYPE_MASK		
-	s_lshr_b32		s_restore_tmp,  		s_restore_tmp, (S_RESTORE_SPI_INIT_MTYPE_SHIFT-SQ_BUF_RSRC_WORD3_MTYPE_SHIFT)	//get MTYPE bits into position
-	s_or_b32		s_restore_buf_rsrc3, 	s_restore_buf_rsrc3,  s_restore_tmp												//or MTYPE
-	
-	/* 		global mem offset			*/
-//	s_mov_b32		s_restore_mem_offset, 0x0								//mem offset initial value = 0
-	
-	/*      the first wave in the threadgroup    */
-	s_and_b32		s_restore_tmp, s_restore_spi_init_hi, S_RESTORE_SPI_INIT_FIRST_WAVE_MASK			
-	s_cbranch_scc0	L_RESTORE_VGPR
-
-    /*      	restore LDS	    */
-	//////////////////////////////
-  L_RESTORE_LDS:
-
-	s_mov_b32		exec_lo, 0xFFFFFFFF 													//need every thread from now on   //be consistent with SAVE although can be moved ahead
-	s_mov_b32		exec_hi, 0xFFFFFFFF
-	
-	s_getreg_b32 	s_restore_alloc_size, hwreg(HW_REG_LDS_ALLOC,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE) 				//lds_size
-	s_and_b32		s_restore_alloc_size, s_restore_alloc_size, 0xFFFFFFFF					//lds_size is zero?
-	s_cbranch_scc0	L_RESTORE_VGPR															//no lds used? jump to L_RESTORE_VGPR
-	s_lshl_b32 		s_restore_alloc_size, s_restore_alloc_size, 6 							//LDS size in dwords = lds_size * 64dw
-	s_lshl_b32 		s_restore_alloc_size, s_restore_alloc_size, 2 							//LDS size in bytes
-	s_mov_b32		s_restore_buf_rsrc2,	s_restore_alloc_size							//NUM_RECORDS in bytes
-
-    // LDS at offset: size(VGPR)+SIZE(SGPR)+SIZE(HWREG)
-    // 
-    get_vgpr_size_bytes(s_restore_mem_offset)
-    get_sgpr_size_bytes(s_restore_tmp)
-    s_add_u32  s_restore_mem_offset, s_restore_mem_offset, s_restore_tmp
-    s_add_u32  s_restore_mem_offset, s_restore_mem_offset, get_hwreg_size_bytes()            //FIXME, Check if offset overflow???
-
-
-	if (SWIZZLE_EN)
-		s_add_u32		s_restore_buf_rsrc2, s_restore_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
-	else
-		s_mov_b32		s_restore_buf_rsrc2,  0x1000000										//NUM_RECORDS in bytes
-	end
-	s_mov_b32 		m0, 0x0 																//lds_offset initial value = 0
-
-  L_RESTORE_LDS_LOOP:									
-	if (SAVE_LDS)
-	    buffer_load_dword	v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset lds:1                    // first 64DW
-	    buffer_load_dword	v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset lds:1 offset:256         // second 64DW
-	end
-    s_add_u32		m0, m0, 256*2					                            // 128 DW
-	s_add_u32		s_restore_mem_offset, s_restore_mem_offset, 256*2           //mem offset increased by 128DW
-	s_cmp_lt_u32	m0, s_restore_alloc_size                                    //scc=(m0 < s_restore_alloc_size) ? 1 : 0
-	s_cbranch_scc1  L_RESTORE_LDS_LOOP														//LDS restore is complete?
-
-    
-    /*      	restore VGPRs	    */
-	//////////////////////////////
-  L_RESTORE_VGPR:
-        // VGPR SR memory offset : 0	
-	s_mov_b32		s_restore_mem_offset, 0x0
- 	s_mov_b32		exec_lo, 0xFFFFFFFF 													//need every thread from now on   //be consistent with SAVE although can be moved ahead
-	s_mov_b32		exec_hi, 0xFFFFFFFF
-	
-	s_getreg_b32 	s_restore_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE) 	//vpgr_size
-	s_add_u32 		s_restore_alloc_size, s_restore_alloc_size, 1
-	s_lshl_b32 		s_restore_alloc_size, s_restore_alloc_size, 2 							//Number of VGPRs = (vgpr_size + 1) * 4    (non-zero value)
-    s_lshl_b32		s_restore_buf_rsrc2,  s_restore_alloc_size, 8						    //NUM_RECORDS in bytes (64 threads*4)
-	if (SWIZZLE_EN)
-		s_add_u32		s_restore_buf_rsrc2, s_restore_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
-	else
-		s_mov_b32		s_restore_buf_rsrc2,  0x1000000										//NUM_RECORDS in bytes
-	end
-
-if G8SR_VGPR_SR_IN_DWX4
-     get_vgpr_size_bytes(s_restore_mem_offset)
-     s_sub_u32         s_restore_mem_offset, s_restore_mem_offset, 256*4
-
-     // the const stride for DWx4 is 4*4 bytes
-     s_and_b32 s_restore_buf_rsrc1, s_restore_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
-     s_or_b32  s_restore_buf_rsrc1, s_restore_buf_rsrc1, G8SR_RESTORE_BUF_RSRC_WORD1_STRIDE_DWx4  // const stride to 4*4 bytes
-
-     s_mov_b32         m0, s_restore_alloc_size
-     s_set_gpr_idx_on  m0, 0x8    // Note.. This will change m0
-
-L_RESTORE_VGPR_LOOP:
-     buffer_load_dwordx4 v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset slc:1 glc:1
-     s_waitcnt vmcnt(0)
-     s_sub_u32         m0, m0, 4
-     v_mov_b32         v0, v0   // v[0+m0] = v0
-     v_mov_b32         v1, v1
-     v_mov_b32         v2, v2
-     v_mov_b32         v3, v3
-     s_sub_u32         s_restore_mem_offset, s_restore_mem_offset, 256*4
-     s_cmp_eq_u32      m0, 0x8000
-     s_cbranch_scc0    L_RESTORE_VGPR_LOOP
-     s_set_gpr_idx_off
-
-     s_and_b32 s_restore_buf_rsrc1, s_restore_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
-     s_or_b32  s_restore_buf_rsrc1, s_restore_buf_rsrc1, S_RESTORE_BUF_RSRC_WORD1_STRIDE  // const stride to 4*4 bytes
-
-else
-	// VGPR load using dw burst
-	s_mov_b32		s_restore_mem_offset_save, s_restore_mem_offset		// restore start with v1, v0 will be the last
-	s_add_u32		s_restore_mem_offset, s_restore_mem_offset, 256*4
-    s_mov_b32 		m0, 4 			     				//VGPR initial index value = 1
-	s_set_gpr_idx_on  m0, 0x8						//M0[7:0] = M0[7:0] and M0[15:12] = 0x8
-    s_add_u32		s_restore_alloc_size, s_restore_alloc_size, 0x8000						//add 0x8000 since we compare m0 against it later	
-
-  L_RESTORE_VGPR_LOOP: 										
-    if(USE_MTBUF_INSTEAD_OF_MUBUF)       
-		tbuffer_load_format_x v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset format:BUF_NUM_FORMAT_FLOAT format: BUF_DATA_FORMAT_32 slc:1 glc:1
-    else
-		buffer_load_dword v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset	slc:1 glc:1	
-		buffer_load_dword v1, v0, s_restore_buf_rsrc0, s_restore_mem_offset	slc:1 glc:1	offset:256
-		buffer_load_dword v2, v0, s_restore_buf_rsrc0, s_restore_mem_offset	slc:1 glc:1	offset:256*2
-		buffer_load_dword v3, v0, s_restore_buf_rsrc0, s_restore_mem_offset	slc:1 glc:1	offset:256*3
-	end
-	s_waitcnt		vmcnt(0)																//ensure data ready
-	v_mov_b32		v0, v0																	//v[0+m0] = v0
-    v_mov_b32       v1, v1
-    v_mov_b32       v2, v2
-    v_mov_b32       v3, v3
-    s_add_u32		m0, m0, 4																//next vgpr index
-	s_add_u32		s_restore_mem_offset, s_restore_mem_offset, 256*4							//every buffer_load_dword does 256 bytes
-	s_cmp_lt_u32 	m0,	s_restore_alloc_size 												//scc = (m0 < s_restore_alloc_size) ? 1 : 0
-	s_cbranch_scc1 	L_RESTORE_VGPR_LOOP														//VGPR restore (except v0) is complete?
-	s_set_gpr_idx_off
-																							/* VGPR restore on v0 */
-    if(USE_MTBUF_INSTEAD_OF_MUBUF)       
-		tbuffer_load_format_x v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save format:BUF_NUM_FORMAT_FLOAT format: BUF_DATA_FORMAT_32 slc:1 glc:1
-    else
-		buffer_load_dword v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save	slc:1 glc:1	
-		buffer_load_dword v1, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save	slc:1 glc:1	offset:256
-		buffer_load_dword v2, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save	slc:1 glc:1	offset:256*2
-		buffer_load_dword v3, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save	slc:1 glc:1	offset:256*3
-	end
-
-end
-	
-    /*      	restore SGPRs	    */
-	//////////////////////////////
-
-    // SGPR SR memory offset : size(VGPR)	
-    get_vgpr_size_bytes(s_restore_mem_offset)
-    get_sgpr_size_bytes(s_restore_tmp)
-    s_add_u32 s_restore_mem_offset, s_restore_mem_offset, s_restore_tmp
-    s_sub_u32 s_restore_mem_offset, s_restore_mem_offset, 16*4     // restore SGPR from S[n] to S[0], by 16 sgprs group
-    // TODO, change RSRC word to rearrange memory layout for SGPRS
-
-	s_getreg_b32 	s_restore_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE) 				//spgr_size
-	s_add_u32 		s_restore_alloc_size, s_restore_alloc_size, 1
-	s_lshl_b32 		s_restore_alloc_size, s_restore_alloc_size, 4 							//Number of SGPRs = (sgpr_size + 1) * 16   (non-zero value)
-
-	if (SGPR_SAVE_USE_SQC)
-		s_lshl_b32		s_restore_buf_rsrc2,	s_restore_alloc_size, 2						//NUM_RECORDS in bytes 
-	else
-		s_lshl_b32		s_restore_buf_rsrc2,	s_restore_alloc_size, 8						//NUM_RECORDS in bytes (64 threads)
-	end
-	if (SWIZZLE_EN)
-		s_add_u32		s_restore_buf_rsrc2, s_restore_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
-	else
-		s_mov_b32		s_restore_buf_rsrc2,  0x1000000										//NUM_RECORDS in bytes
-	end
-
-    /* If 112 SGPRs ar allocated, 4 sgprs are not used TBA(108,109),TMA(110,111),
-       However, we are safe to restore these 4 SGPRs anyway, since TBA,TMA will later be restored by HWREG
-    */
-    s_mov_b32 m0, s_restore_alloc_size
-
- L_RESTORE_SGPR_LOOP: 
-	read_16sgpr_from_mem(s0, s_restore_buf_rsrc0, s_restore_mem_offset)  //PV: further performance improvement can be made
-	s_waitcnt		lgkmcnt(0)																//ensure data ready
-
-    s_sub_u32 m0, m0, 16    // Restore from S[n] to S[0]
-
-	s_movreld_b64 	s0, s0 		//s[0+m0] = s0
-    s_movreld_b64 	s2, s2
-    s_movreld_b64 	s4, s4
-    s_movreld_b64 	s6, s6
-    s_movreld_b64 	s8, s8
-    s_movreld_b64 	s10, s10
-    s_movreld_b64 	s12, s12
-    s_movreld_b64 	s14, s14
-
-	s_cmp_eq_u32 	m0, 0				//scc = (m0 < s_restore_alloc_size) ? 1 : 0
-	s_cbranch_scc0 	L_RESTORE_SGPR_LOOP             //SGPR restore (except s0) is complete?
-	
-    /* 		restore HW registers	*/
-	//////////////////////////////
-  L_RESTORE_HWREG:
-
-
-if G8SR_DEBUG_TIMESTAMP
-      s_mov_b32 s_g8sr_ts_restore_s[0], s_restore_pc_lo
-      s_mov_b32 s_g8sr_ts_restore_s[1], s_restore_pc_hi
-end
-
-    // HWREG SR memory offset : size(VGPR)+size(SGPR)
-    get_vgpr_size_bytes(s_restore_mem_offset)
-    get_sgpr_size_bytes(s_restore_tmp)
-    s_add_u32 s_restore_mem_offset, s_restore_mem_offset, s_restore_tmp
-
-
-    s_mov_b32		s_restore_buf_rsrc2, 0x4												//NUM_RECORDS	in bytes
-	if (SWIZZLE_EN)
-		s_add_u32		s_restore_buf_rsrc2, s_restore_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
-	else
-		s_mov_b32		s_restore_buf_rsrc2,  0x1000000										//NUM_RECORDS in bytes
-	end
-
-	read_hwreg_from_mem(s_restore_m0, s_restore_buf_rsrc0, s_restore_mem_offset)					//M0
-	read_hwreg_from_mem(s_restore_pc_lo, s_restore_buf_rsrc0, s_restore_mem_offset)				//PC
-	read_hwreg_from_mem(s_restore_pc_hi, s_restore_buf_rsrc0, s_restore_mem_offset)
-	read_hwreg_from_mem(s_restore_exec_lo, s_restore_buf_rsrc0, s_restore_mem_offset)				//EXEC
-	read_hwreg_from_mem(s_restore_exec_hi, s_restore_buf_rsrc0, s_restore_mem_offset)
-	read_hwreg_from_mem(s_restore_status, s_restore_buf_rsrc0, s_restore_mem_offset)				//STATUS
-	read_hwreg_from_mem(s_restore_trapsts, s_restore_buf_rsrc0, s_restore_mem_offset)				//TRAPSTS
-    read_hwreg_from_mem(xnack_mask_lo, s_restore_buf_rsrc0, s_restore_mem_offset)					//XNACK_MASK_LO
-	read_hwreg_from_mem(xnack_mask_hi, s_restore_buf_rsrc0, s_restore_mem_offset)					//XNACK_MASK_HI
-	read_hwreg_from_mem(s_restore_mode, s_restore_buf_rsrc0, s_restore_mem_offset)				//MODE
-	read_hwreg_from_mem(tba_lo, s_restore_buf_rsrc0, s_restore_mem_offset)						//TBA_LO
-	read_hwreg_from_mem(tba_hi, s_restore_buf_rsrc0, s_restore_mem_offset)						//TBA_HI
-	
-	s_waitcnt		lgkmcnt(0)																						//from now on, it is safe to restore STATUS and IB_STS
-
-	s_and_b32 s_restore_pc_hi, s_restore_pc_hi, 0x0000ffff    	//pc[47:32]        //Do it here in order not to affect STATUS
-
-	//for normal save & restore, the saved PC points to the next inst to execute, no adjustment needs to be made, otherwise:
-	if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_RESTORE_NORMAL))
-		s_add_u32 s_restore_pc_lo, s_restore_pc_lo, 8            //pc[31:0]+8	  //two back-to-back s_trap are used (first for save and second for restore)
-		s_addc_u32	s_restore_pc_hi, s_restore_pc_hi, 0x0		 //carry bit over
-	end	
-	if ((EMU_RUN_HACK) && (EMU_RUN_HACK_RESTORE_NORMAL))	      
-		s_add_u32 s_restore_pc_lo, s_restore_pc_lo, 4            //pc[31:0]+4     // save is hack through s_trap but restore is normal
-		s_addc_u32	s_restore_pc_hi, s_restore_pc_hi, 0x0		 //carry bit over
-	end
-	
-	s_mov_b32 		m0, 		s_restore_m0
-	s_mov_b32 		exec_lo, 	s_restore_exec_lo
-	s_mov_b32 		exec_hi, 	s_restore_exec_hi
-	
-	s_and_b32		s_restore_m0, SQ_WAVE_TRAPSTS_PRE_SAVECTX_MASK, s_restore_trapsts
-	s_setreg_b32	hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_PRE_SAVECTX_SHIFT, SQ_WAVE_TRAPSTS_PRE_SAVECTX_SIZE), s_restore_m0
-	s_and_b32		s_restore_m0, SQ_WAVE_TRAPSTS_POST_SAVECTX_MASK, s_restore_trapsts
-	s_lshr_b32		s_restore_m0, s_restore_m0, SQ_WAVE_TRAPSTS_POST_SAVECTX_SHIFT
-	s_setreg_b32	hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_POST_SAVECTX_SHIFT, SQ_WAVE_TRAPSTS_POST_SAVECTX_SIZE), s_restore_m0
-	//s_setreg_b32 	hwreg(HW_REG_TRAPSTS), 	s_restore_trapsts      //don't overwrite SAVECTX bit as it may be set through external SAVECTX during restore
-	s_setreg_b32 	hwreg(HW_REG_MODE), 	s_restore_mode
-	//reuse s_restore_m0 as a temp register
-	s_and_b32		s_restore_m0, s_restore_pc_hi, S_SAVE_PC_HI_RCNT_MASK
-	s_lshr_b32		s_restore_m0, s_restore_m0, S_SAVE_PC_HI_RCNT_SHIFT
-	s_lshl_b32		s_restore_m0, s_restore_m0, SQ_WAVE_IB_STS_RCNT_SHIFT
-	s_mov_b32		s_restore_tmp, 0x0																				//IB_STS is zero
-	s_or_b32		s_restore_tmp, s_restore_tmp, s_restore_m0
-	s_and_b32		s_restore_m0, s_restore_pc_hi, S_SAVE_PC_HI_FIRST_REPLAY_MASK
-	s_lshr_b32		s_restore_m0, s_restore_m0, S_SAVE_PC_HI_FIRST_REPLAY_SHIFT
-	s_lshl_b32		s_restore_m0, s_restore_m0, SQ_WAVE_IB_STS_FIRST_REPLAY_SHIFT
-	s_or_b32		s_restore_tmp, s_restore_tmp, s_restore_m0
-    s_and_b32       s_restore_m0, s_restore_status, SQ_WAVE_STATUS_INST_ATC_MASK 
-    s_lshr_b32		s_restore_m0, s_restore_m0, SQ_WAVE_STATUS_INST_ATC_SHIFT    
-    s_setreg_b32 	hwreg(HW_REG_IB_STS), 	s_restore_tmp
-
-	s_and_b64    exec, exec, exec  // Restore STATUS.EXECZ, not writable by s_setreg_b32
-	s_and_b64    vcc, vcc, vcc  // Restore STATUS.VCCZ, not writable by s_setreg_b32
-	s_setreg_b32 	hwreg(HW_REG_STATUS), 	s_restore_status     // SCC is included, which is changed by previous salu
-	
-	s_barrier													//barrier to ensure the readiness of LDS before access attemps from any other wave in the same TG //FIXME not performance-optimal at this time
-
-if G8SR_DEBUG_TIMESTAMP
-	s_memrealtime s_g8sr_ts_restore_d
-	s_waitcnt lgkmcnt(0)
-end	
-	
-//	s_rfe_b64 s_restore_pc_lo                              		//Return to the main shader program and resume execution
-    s_rfe_restore_b64  s_restore_pc_lo, s_restore_m0            // s_restore_m0[0] is used to set STATUS.inst_atc 
-
-
-/**************************************************************************/
-/*                     	the END								              */
-/**************************************************************************/	
-L_END_PGM:	
-	s_endpgm
-	
-end	
-
-
-/**************************************************************************/
-/*                     	the helper functions							  */
-/**************************************************************************/
-
-//Only for save hwreg to mem
-function write_hwreg_to_mem(s, s_rsrc, s_mem_offset)
-		s_mov_b32 exec_lo, m0					//assuming exec_lo is not needed anymore from this point on
-		s_mov_b32 m0, s_mem_offset
-		s_buffer_store_dword s, s_rsrc, m0		glc:1
-		s_add_u32		s_mem_offset, s_mem_offset, 4
-		s_mov_b32	m0, exec_lo
-end
-
-
-// HWREG are saved before SGPRs, so all HWREG could be use.
-function write_16sgpr_to_mem(s, s_rsrc, s_mem_offset)
-
-		s_buffer_store_dwordx4 s[0], s_rsrc, 0  glc:1
-		s_buffer_store_dwordx4 s[4], s_rsrc, 16  glc:1
-		s_buffer_store_dwordx4 s[8], s_rsrc, 32  glc:1
-		s_buffer_store_dwordx4 s[12], s_rsrc, 48 glc:1
-        s_add_u32       s_rsrc[0], s_rsrc[0], 4*16
-        s_addc_u32 		s_rsrc[1], s_rsrc[1], 0x0			  // +scc
-end
-
-
-function read_hwreg_from_mem(s, s_rsrc, s_mem_offset)
-	s_buffer_load_dword s, s_rsrc, s_mem_offset		glc:1
-	s_add_u32		s_mem_offset, s_mem_offset, 4
-end
-
-function read_16sgpr_from_mem(s, s_rsrc, s_mem_offset)
-	s_buffer_load_dwordx16 s, s_rsrc, s_mem_offset		glc:1
-	s_sub_u32		s_mem_offset, s_mem_offset, 4*16
-end
-
-
-
-function get_lds_size_bytes(s_lds_size_byte)
-    // SQ LDS granularity is 64DW, while PGM_RSRC2.lds_size is in granularity 128DW	 
-    s_getreg_b32   s_lds_size_byte, hwreg(HW_REG_LDS_ALLOC, SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT, SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE) 			// lds_size
-    s_lshl_b32     s_lds_size_byte, s_lds_size_byte, 8 						//LDS size in dwords = lds_size * 64 *4Bytes    // granularity 64DW
-end
-
-function get_vgpr_size_bytes(s_vgpr_size_byte)
-    s_getreg_b32   s_vgpr_size_byte, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE)  //vpgr_size
-    s_add_u32      s_vgpr_size_byte, s_vgpr_size_byte, 1
-    s_lshl_b32     s_vgpr_size_byte, s_vgpr_size_byte, (2+8) //Number of VGPRs = (vgpr_size + 1) * 4 * 64 * 4   (non-zero value)   //FIXME for GFX, zero is possible 
-end
-
-function get_sgpr_size_bytes(s_sgpr_size_byte)
-    s_getreg_b32   s_sgpr_size_byte, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE)  //spgr_size
-    s_add_u32      s_sgpr_size_byte, s_sgpr_size_byte, 1
-    s_lshl_b32     s_sgpr_size_byte, s_sgpr_size_byte, 6 //Number of SGPRs = (sgpr_size + 1) * 16 *4   (non-zero value) 
-end
-
-function get_hwreg_size_bytes
-    return 128 //HWREG size 128 bytes
-
-#endif
-
-static const uint32_t cwsr_trap_carrizo_hex[] = {
-	0xbf820001, 0xbf820124,
-	0xb8f4f802, 0x89748674,
-	0xb8f5f803, 0x8675ff75,
-	0x00000400, 0xbf850013,
-	0xc00a1e37, 0x00000000,
-	0xbf8c007f, 0x87777978,
-	0xbf840004, 0xbeee007a,
-	0xbeef007b, 0xb974f802,
-	0xbe801d78, 0xb8f5f803,
-	0x8675ff75, 0x000001ff,
-	0xbf850002, 0x80708470,
-	0x82718071, 0x8671ff71,
-	0x0000ffff, 0xb974f802,
-	0xbe801f70, 0xb8f5f803,
-	0x8675ff75, 0x00000100,
-	0xbf840006, 0xbefa0080,
-	0xb97a0203, 0x8671ff71,
-	0x0000ffff, 0x80f08870,
-	0x82f18071, 0xbefa0080,
-	0xb97a0283, 0xbef60068,
-	0xbef70069, 0xb8fa1c07,
-	0x8e7a9c7a, 0x87717a71,
-	0xb8fa03c7, 0x8e7a9b7a,
-	0x87717a71, 0xb8faf807,
-	0x867aff7a, 0x00007fff,
-	0xb97af807, 0xbef2007e,
-	0xbef3007f, 0xbefe0180,
-	0xbf900004, 0xbf8e0002,
-	0xbf88fffe, 0xbef8007e,
-	0x8679ff7f, 0x0000ffff,
-	0x8779ff79, 0x00040000,
-	0xbefa0080, 0xbefb00ff,
-	0x00807fac, 0x867aff7f,
-	0x08000000, 0x8f7a837a,
-	0x877b7a7b, 0x867aff7f,
-	0x70000000, 0x8f7a817a,
-	0x877b7a7b, 0xbeef007c,
-	0xbeee0080, 0xb8ee2a05,
-	0x806e816e, 0x8e6e8a6e,
-	0xb8fa1605, 0x807a817a,
-	0x8e7a867a, 0x806e7a6e,
-	0xbefa0084, 0xbefa00ff,
-	0x01000000, 0xbefe007c,
-	0xbefc006e, 0xc0611bfc,
-	0x0000007c, 0x806e846e,
-	0xbefc007e, 0xbefe007c,
-	0xbefc006e, 0xc0611c3c,
-	0x0000007c, 0x806e846e,
-	0xbefc007e, 0xbefe007c,
-	0xbefc006e, 0xc0611c7c,
-	0x0000007c, 0x806e846e,
-	0xbefc007e, 0xbefe007c,
-	0xbefc006e, 0xc0611cbc,
-	0x0000007c, 0x806e846e,
-	0xbefc007e, 0xbefe007c,
-	0xbefc006e, 0xc0611cfc,
-	0x0000007c, 0x806e846e,
-	0xbefc007e, 0xbefe007c,
-	0xbefc006e, 0xc0611d3c,
-	0x0000007c, 0x806e846e,
-	0xbefc007e, 0xb8f5f803,
-	0xbefe007c, 0xbefc006e,
-	0xc0611d7c, 0x0000007c,
-	0x806e846e, 0xbefc007e,
-	0xbefe007c, 0xbefc006e,
-	0xc0611dbc, 0x0000007c,
-	0x806e846e, 0xbefc007e,
-	0xbefe007c, 0xbefc006e,
-	0xc0611dfc, 0x0000007c,
-	0x806e846e, 0xbefc007e,
-	0xb8eff801, 0xbefe007c,
-	0xbefc006e, 0xc0611bfc,
-	0x0000007c, 0x806e846e,
-	0xbefc007e, 0xbefe007c,
-	0xbefc006e, 0xc0611b3c,
-	0x0000007c, 0x806e846e,
-	0xbefc007e, 0xbefe007c,
-	0xbefc006e, 0xc0611b7c,
-	0x0000007c, 0x806e846e,
-	0xbefc007e, 0x867aff7f,
-	0x04000000, 0xbef30080,
-	0x8773737a, 0xb8ee2a05,
-	0x806e816e, 0x8e6e8a6e,
-	0xb8f51605, 0x80758175,
-	0x8e758475, 0x8e7a8275,
-	0xbefa00ff, 0x01000000,
-	0xbef60178, 0x80786e78,
-	0xbefc0080, 0xbe802b00,
-	0xbe822b02, 0xbe842b04,
-	0xbe862b06, 0xbe882b08,
-	0xbe8a2b0a, 0xbe8c2b0c,
-	0xbe8e2b0e, 0xc06b003c,
-	0x00000000, 0xc06b013c,
-	0x00000010, 0xc06b023c,
-	0x00000020, 0xc06b033c,
-	0x00000030, 0x8078c078,
-	0x82798079, 0x807c907c,
-	0xbf0a757c, 0xbf85ffeb,
-	0xbef80176, 0xbeee0080,
-	0xbefe00c1, 0xbeff00c1,
-	0xbefa00ff, 0x01000000,
-	0xe0724000, 0x6e1e0000,
-	0xe0724100, 0x6e1e0100,
-	0xe0724200, 0x6e1e0200,
-	0xe0724300, 0x6e1e0300,
-	0xbefe00c1, 0xbeff00c1,
-	0xb8f54306, 0x8675c175,
-	0xbf84002c, 0xbf8a0000,
-	0x867aff73, 0x04000000,
-	0xbf840028, 0x8e758675,
-	0x8e758275, 0xbefa0075,
-	0xb8ee2a05, 0x806e816e,
-	0x8e6e8a6e, 0xb8fa1605,
-	0x807a817a, 0x8e7a867a,
-	0x806e7a6e, 0x806eff6e,
-	0x00000080, 0xbefa00ff,
-	0x01000000, 0xbefc0080,
-	0xd28c0002, 0x000100c1,
-	0xd28d0003, 0x000204c1,
-	0xd1060002, 0x00011103,
-	0x7e0602ff, 0x00000200,
-	0xbefc00ff, 0x00010000,
-	0xbe80007b, 0x867bff7b,
-	0xff7fffff, 0x877bff7b,
-	0x00058000, 0xd8ec0000,
-	0x00000002, 0xbf8c007f,
-	0xe0765000, 0x6e1e0002,
-	0x32040702, 0xd0c9006a,
-	0x0000eb02, 0xbf87fff7,
-	0xbefb0000, 0xbeee00ff,
-	0x00000400, 0xbefe00c1,
-	0xbeff00c1, 0xb8f52a05,
-	0x80758175, 0x8e758275,
-	0x8e7a8875, 0xbefa00ff,
-	0x01000000, 0xbefc0084,
-	0xbf0a757c, 0xbf840015,
-	0xbf11017c, 0x8075ff75,
-	0x00001000, 0x7e000300,
-	0x7e020301, 0x7e040302,
-	0x7e060303, 0xe0724000,
-	0x6e1e0000, 0xe0724100,
-	0x6e1e0100, 0xe0724200,
-	0x6e1e0200, 0xe0724300,
-	0x6e1e0300, 0x807c847c,
-	0x806eff6e, 0x00000400,
-	0xbf0a757c, 0xbf85ffef,
-	0xbf9c0000, 0xbf8200ca,
-	0xbef8007e, 0x8679ff7f,
-	0x0000ffff, 0x8779ff79,
-	0x00040000, 0xbefa0080,
-	0xbefb00ff, 0x00807fac,
-	0x8676ff7f, 0x08000000,
-	0x8f768376, 0x877b767b,
-	0x8676ff7f, 0x70000000,
-	0x8f768176, 0x877b767b,
-	0x8676ff7f, 0x04000000,
-	0xbf84001e, 0xbefe00c1,
-	0xbeff00c1, 0xb8f34306,
-	0x8673c173, 0xbf840019,
-	0x8e738673, 0x8e738273,
-	0xbefa0073, 0xb8f22a05,
-	0x80728172, 0x8e728a72,
-	0xb8f61605, 0x80768176,
-	0x8e768676, 0x80727672,
-	0x8072ff72, 0x00000080,
-	0xbefa00ff, 0x01000000,
-	0xbefc0080, 0xe0510000,
-	0x721e0000, 0xe0510100,
-	0x721e0000, 0x807cff7c,
-	0x00000200, 0x8072ff72,
-	0x00000200, 0xbf0a737c,
-	0xbf85fff6, 0xbef20080,
-	0xbefe00c1, 0xbeff00c1,
-	0xb8f32a05, 0x80738173,
-	0x8e738273, 0x8e7a8873,
-	0xbefa00ff, 0x01000000,
-	0xbef60072, 0x8072ff72,
-	0x00000400, 0xbefc0084,
-	0xbf11087c, 0x8073ff73,
-	0x00008000, 0xe0524000,
-	0x721e0000, 0xe0524100,
-	0x721e0100, 0xe0524200,
-	0x721e0200, 0xe0524300,
-	0x721e0300, 0xbf8c0f70,
-	0x7e000300, 0x7e020301,
-	0x7e040302, 0x7e060303,
-	0x807c847c, 0x8072ff72,
-	0x00000400, 0xbf0a737c,
-	0xbf85ffee, 0xbf9c0000,
-	0xe0524000, 0x761e0000,
-	0xe0524100, 0x761e0100,
-	0xe0524200, 0x761e0200,
-	0xe0524300, 0x761e0300,
-	0xb8f22a05, 0x80728172,
-	0x8e728a72, 0xb8f61605,
-	0x80768176, 0x8e768676,
-	0x80727672, 0x80f2c072,
-	0xb8f31605, 0x80738173,
-	0x8e738473, 0x8e7a8273,
-	0xbefa00ff, 0x01000000,
-	0xbefc0073, 0xc031003c,
-	0x00000072, 0x80f2c072,
-	0xbf8c007f, 0x80fc907c,
-	0xbe802d00, 0xbe822d02,
-	0xbe842d04, 0xbe862d06,
-	0xbe882d08, 0xbe8a2d0a,
-	0xbe8c2d0c, 0xbe8e2d0e,
-	0xbf06807c, 0xbf84fff1,
-	0xb8f22a05, 0x80728172,
-	0x8e728a72, 0xb8f61605,
-	0x80768176, 0x8e768676,
-	0x80727672, 0xbefa0084,
-	0xbefa00ff, 0x01000000,
-	0xc0211cfc, 0x00000072,
-	0x80728472, 0xc0211c3c,
-	0x00000072, 0x80728472,
-	0xc0211c7c, 0x00000072,
-	0x80728472, 0xc0211bbc,
-	0x00000072, 0x80728472,
-	0xc0211bfc, 0x00000072,
-	0x80728472, 0xc0211d3c,
-	0x00000072, 0x80728472,
-	0xc0211d7c, 0x00000072,
-	0x80728472, 0xc0211a3c,
-	0x00000072, 0x80728472,
-	0xc0211a7c, 0x00000072,
-	0x80728472, 0xc0211dfc,
-	0x00000072, 0x80728472,
-	0xc0211b3c, 0x00000072,
-	0x80728472, 0xc0211b7c,
-	0x00000072, 0x80728472,
-	0xbf8c007f, 0x8671ff71,
-	0x0000ffff, 0xbefc0073,
-	0xbefe006e, 0xbeff006f,
-	0x867375ff, 0x000003ff,
-	0xb9734803, 0x867375ff,
-	0xfffff800, 0x8f738b73,
-	0xb973a2c3, 0xb977f801,
-	0x8673ff71, 0xf0000000,
-	0x8f739c73, 0x8e739073,
-	0xbef60080, 0x87767376,
-	0x8673ff71, 0x08000000,
-	0x8f739b73, 0x8e738f73,
-	0x87767376, 0x8673ff74,
-	0x00800000, 0x8f739773,
-	0xb976f807, 0x86fe7e7e,
-	0x86ea6a6a, 0xb974f802,
-	0xbf8a0000, 0x95807370,
-	0xbf810000, 0x00000000,
-};
-
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
index 689b891..ee3e04e 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
@@ -31,22 +31,15 @@
 #include <uapi/linux/kfd_ioctl.h>
 #include <linux/time.h>
 #include <linux/mm.h>
-#include <uapi/asm-generic/mman-common.h>
+#include <linux/mman.h>
 #include <asm/processor.h>
-
 #include "kfd_priv.h"
 #include "kfd_device_queue_manager.h"
 #include "kfd_dbgmgr.h"
-#include "kfd_ipc.h"
-#include "cik_regs.h"
 
 static long kfd_ioctl(struct file *, unsigned int, unsigned long);
 static int kfd_open(struct inode *, struct file *);
 static int kfd_mmap(struct file *, struct vm_area_struct *);
-static uint32_t kfd_convert_user_mem_alloction_flags(
-		struct kfd_dev *dev,
-		uint32_t userspace_flags);
-static bool kfd_is_large_bar(struct kfd_dev *dev);
 
 static const char kfd_dev_name[] = "kfd";
 
@@ -62,14 +55,6 @@ static int kfd_char_dev_major = -1;
 static struct class *kfd_class;
 struct device *kfd_device;
 
-static char *kfd_devnode(struct device *dev, umode_t *mode)
-{
-	if (mode && dev->devt == MKDEV(kfd_char_dev_major, 0))
-		*mode = 0666;
-
-	return NULL;
-}
-
 int kfd_chardev_init(void)
 {
 	int err = 0;
@@ -84,8 +69,6 @@ int kfd_chardev_init(void)
 	if (IS_ERR(kfd_class))
 		goto err_class_create;
 
-	kfd_class->devnode = kfd_devnode;
-
 	kfd_device = device_create(kfd_class, NULL,
 					MKDEV(kfd_char_dev_major, 0),
 					NULL, kfd_dev_name);
@@ -124,7 +107,7 @@ static int kfd_open(struct inode *inode, struct file *filep)
 	if (iminor(inode) != 0)
 		return -ENODEV;
 
-	is_32bit_user_mode = is_compat_task();
+	is_32bit_user_mode = in_compat_syscall();
 
 	if (is_32bit_user_mode) {
 		dev_warn(kfd_device,
@@ -134,7 +117,7 @@ static int kfd_open(struct inode *inode, struct file *filep)
 		return -EPERM;
 	}
 
-	process = kfd_create_process(filep);
+	process = kfd_create_process(current);
 	if (IS_ERR(process))
 		return PTR_ERR(process);
 
@@ -223,7 +206,6 @@ static int set_queue_properties_from_user(struct queue_properties *q_properties,
 	q_properties->ctx_save_restore_area_address =
 			args->ctx_save_restore_address;
 	q_properties->ctx_save_restore_area_size = args->ctx_save_restore_size;
-	q_properties->ctl_stack_size = args->ctl_stack_size;
 	if (args->queue_type == KFD_IOC_QUEUE_TYPE_COMPUTE ||
 		args->queue_type == KFD_IOC_QUEUE_TYPE_COMPUTE_AQL)
 		q_properties->type = KFD_QUEUE_TYPE_COMPUTE;
@@ -288,7 +270,7 @@ static int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p,
 		return -EINVAL;
 	}
 
-	down_write(&p->lock);
+	mutex_lock(&p->mutex);
 
 	pdd = kfd_bind_process_to_device(dev, p);
 	if (IS_ERR(pdd)) {
@@ -300,7 +282,8 @@ static int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p,
 			p->pasid,
 			dev->id);
 
-	err = pqm_create_queue(&p->pqm, dev, filep, &q_properties, &queue_id);
+	err = pqm_create_queue(&p->pqm, dev, filep, &q_properties,
+				0, q_properties.type, &queue_id);
 	if (err != 0)
 		goto err_create_queue;
 
@@ -308,10 +291,10 @@ static int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p,
 
 
 	/* Return gpu_id as doorbell offset for mmap usage */
-	args->doorbell_offset = (KFD_MMAP_TYPE_DOORBELL | args->gpu_id);
+	args->doorbell_offset = (KFD_MMAP_DOORBELL_MASK | args->gpu_id);
 	args->doorbell_offset <<= PAGE_SHIFT;
 
-	up_write(&p->lock);
+	mutex_unlock(&p->mutex);
 
 	pr_debug("kfd: queue id %d was created successfully\n", args->queue_id);
 
@@ -328,7 +311,7 @@ static int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p,
 
 err_create_queue:
 err_bind_process:
-	up_write(&p->lock);
+	mutex_unlock(&p->mutex);
 	return err;
 }
 
@@ -342,11 +325,11 @@ static int kfd_ioctl_destroy_queue(struct file *filp, struct kfd_process *p,
 				args->queue_id,
 				p->pasid);
 
-	down_write(&p->lock);
+	mutex_lock(&p->mutex);
 
 	retval = pqm_destroy_queue(&p->pqm, args->queue_id);
 
-	up_write(&p->lock);
+	mutex_unlock(&p->mutex);
 	return retval;
 }
 
@@ -388,63 +371,11 @@ static int kfd_ioctl_update_queue(struct file *filp, struct kfd_process *p,
 	pr_debug("kfd: updating queue id %d for PASID %d\n",
 			args->queue_id, p->pasid);
 
-	down_write(&p->lock);
+	mutex_lock(&p->mutex);
 
 	retval = pqm_update_queue(&p->pqm, args->queue_id, &properties);
 
-	up_write(&p->lock);
-
-	return retval;
-}
-
-static int kfd_ioctl_set_cu_mask(struct file *filp, struct kfd_process *p,
-					void *data)
-{
-	int retval;
-	const int max_num_cus = 1024;
-	struct kfd_ioctl_set_cu_mask_args *args = data;
-	struct queue_properties properties;
-	uint32_t __user *cu_mask_ptr = (uint32_t __user *)args->cu_mask_ptr;
-	size_t cu_mask_size = sizeof(uint32_t) * (args->num_cu_mask / 32);
-
-	if ((args->num_cu_mask % 32) != 0) {
-		pr_debug("kfd: num_cu_mask (0x%x) must be a multiple of 32",
-				args->num_cu_mask);
-		return -EINVAL;
-	}
-
-	properties.cu_mask_count = args->num_cu_mask;
-	if (properties.cu_mask_count == 0) {
-		pr_debug("kfd: CU Mask cannot be 0");
-		return -EINVAL;
-	}
-
-	/* To prevent an unreasonably large CU mask size, set an arbitrary
-	 * limit of max_num_cus bits.  We can then just drop any CU mask bits
-	 * past max_num_cus bits and just use the first max_num_cus bits.
-	 */
-	if (properties.cu_mask_count > max_num_cus) {
-		pr_debug("kfd: CU mask cannot be greater than 1024 bits");
-		properties.cu_mask_count = max_num_cus;
-		cu_mask_size = sizeof(uint32_t) * (max_num_cus/32);
-	}
-
-	properties.cu_mask = kzalloc(cu_mask_size, GFP_KERNEL);
-	if (!properties.cu_mask)
-		return -ENOMEM;
-
-	retval = copy_from_user(properties.cu_mask, cu_mask_ptr, cu_mask_size);
-	if (retval) {
-		pr_debug("kfd: Could not copy cu mask from userspace");
-		kfree(properties.cu_mask);
-		return -EFAULT;
-	}
-
-	down_write(&p->lock);
-
-	retval = pqm_set_cu_mask(&p->pqm, args->queue_id, &properties);
-
-	up_write(&p->lock);
+	mutex_unlock(&p->mutex);
 
 	return retval;
 }
@@ -472,7 +403,7 @@ static int kfd_ioctl_set_memory_policy(struct file *filep,
 	if (dev == NULL)
 		return -EINVAL;
 
-	down_write(&p->lock);
+	mutex_lock(&p->mutex);
 
 	pdd = kfd_bind_process_to_device(dev, p);
 	if (IS_ERR(pdd)) {
@@ -496,75 +427,46 @@ static int kfd_ioctl_set_memory_policy(struct file *filep,
 		err = -EINVAL;
 
 out:
-	up_write(&p->lock);
+	mutex_unlock(&p->mutex);
 
 	return err;
 }
 
-static int kfd_ioctl_set_trap_handler(struct file *filep,
-					struct kfd_process *p, void *data)
+static int kfd_ioctl_dbg_register(struct file *filep,
+				struct kfd_process *p, void *data)
 {
-	struct kfd_ioctl_set_trap_handler_args *args = data;
+	struct kfd_ioctl_dbg_register_args *args = data;
 	struct kfd_dev *dev;
-	int err = 0;
+	struct kfd_dbgmgr *dbgmgr_ptr;
 	struct kfd_process_device *pdd;
+	bool create_ok;
+	long status = 0;
 
 	dev = kfd_device_by_id(args->gpu_id);
 	if (dev == NULL)
 		return -EINVAL;
 
-	down_write(&p->lock);
-
-	pdd = kfd_bind_process_to_device(dev, p);
-	if (IS_ERR(pdd)) {
-		err = -ESRCH;
-		goto out;
-	}
-
-	if (dev->dqm->ops.set_trap_handler(dev->dqm,
-					&pdd->qpd,
-					args->tba_addr,
-					args->tma_addr))
-		err = -EINVAL;
-
-out:
-	up_write(&p->lock);
-
-	return err;
-}
-
-static int
-kfd_ioctl_dbg_register(struct file *filep, struct kfd_process *p, void *data)
-{
-	long status = -EFAULT;
-	struct kfd_ioctl_dbg_register_args *args = data;
-	struct kfd_dev *dev;
-	struct kfd_dbgmgr *dbgmgr_ptr;
-	struct kfd_process_device *pdd;
-	bool create_ok = false;
-
-	pr_debug("kfd:dbg: %s\n", __func__);
-
-	dev = kfd_device_by_id(args->gpu_id);
-	if (!dev) {
-		dev_info(NULL, "Error! kfd: In func %s >> getting device by id failed\n", __func__);
-		return status;
+	if (dev->device_info->asic_family == CHIP_CARRIZO) {
+		pr_debug("kfd_ioctl_dbg_register not supported on CZ\n");
+		return -EINVAL;
 	}
 
-	down_write(&p->lock);
-	mutex_lock(get_dbgmgr_mutex());
+	mutex_lock(kfd_get_dbgmgr_mutex());
+	mutex_lock(&p->mutex);
 
-	/* make sure that we have pdd, if this the first queue created for this process */
+	/*
+	 * make sure that we have pdd, if this the first queue created for
+	 * this process
+	 */
 	pdd = kfd_bind_process_to_device(dev, p);
-	if (IS_ERR(pdd) < 0) {
-		mutex_unlock(get_dbgmgr_mutex());
-		up_write(&p->lock);
+	if (IS_ERR(pdd)) {
+		mutex_unlock(&p->mutex);
+		mutex_unlock(kfd_get_dbgmgr_mutex());
 		return PTR_ERR(pdd);
 	}
 
 	if (dev->dbgmgr == NULL) {
 		/* In case of a legal call, we have no dbgmgr yet */
-
 		create_ok = kfd_dbgmgr_create(&dbgmgr_ptr, dev);
 		if (create_ok) {
 			status = kfd_dbgmgr_register(dbgmgr_ptr, p);
@@ -573,32 +475,34 @@ kfd_ioctl_dbg_register(struct file *filep, struct kfd_process *p, void *data)
 			else
 				dev->dbgmgr = dbgmgr_ptr;
 		}
+	} else {
+		pr_debug("debugger already registered\n");
+		status = -EINVAL;
 	}
 
-	mutex_unlock(get_dbgmgr_mutex());
-	up_write(&p->lock);
+	mutex_unlock(&p->mutex);
+	mutex_unlock(kfd_get_dbgmgr_mutex());
 
 	return status;
 }
 
-/*
- * Unregister dbg IOCTL
- */
-
-static int
-kfd_ioctl_dbg_unregister(struct file *filep, struct kfd_process *p, void *data)
+static int kfd_ioctl_dbg_unrgesiter(struct file *filep,
+				struct kfd_process *p, void *data)
 {
-	long status = -EFAULT;
 	struct kfd_ioctl_dbg_unregister_args *args = data;
 	struct kfd_dev *dev;
+	long status;
 
 	dev = kfd_device_by_id(args->gpu_id);
-	if (!dev) {
-		dev_info(NULL, "Error! kfd: In func %s >> getting device by id failed\n", __func__);
-		return status;
+	if (dev == NULL)
+		return -EINVAL;
+
+	if (dev->device_info->asic_family == CHIP_CARRIZO) {
+		pr_debug("kfd_ioctl_dbg_unrgesiter not supported on CZ\n");
+		return -EINVAL;
 	}
 
-	mutex_lock(get_dbgmgr_mutex());
+	mutex_lock(kfd_get_dbgmgr_mutex());
 
 	status = kfd_dbgmgr_unregister(dev->dbgmgr, p);
 	if (status == 0) {
@@ -606,7 +510,7 @@ kfd_ioctl_dbg_unregister(struct file *filep, struct kfd_process *p, void *data)
 		dev->dbgmgr = NULL;
 	}
 
-	mutex_unlock(get_dbgmgr_mutex());
+	mutex_unlock(kfd_get_dbgmgr_mutex());
 
 	return status;
 }
@@ -615,140 +519,125 @@ kfd_ioctl_dbg_unregister(struct file *filep, struct kfd_process *p, void *data)
  * Parse and generate variable size data structure for address watch.
  * Total size of the buffer and # watch points is limited in order
  * to prevent kernel abuse. (no bearing to the much smaller HW limitation
- * which is enforced by dbgdev module.
+ * which is enforced by dbgdev module)
  * please also note that the watch address itself are not "copied from user",
  * since it be set into the HW in user mode values.
  *
  */
-
-static int
-kfd_ioctl_dbg_address_watch(struct file *filep,
-		struct kfd_process *p,
-		void *data)
+static int kfd_ioctl_dbg_address_watch(struct file *filep,
+					struct kfd_process *p, void *data)
 {
-	long status = -EFAULT;
 	struct kfd_ioctl_dbg_address_watch_args *args = data;
 	struct kfd_dev *dev;
 	struct dbg_address_watch_info aw_info;
-	unsigned char *args_buff = NULL;
-	unsigned int args_idx = 0;
+	unsigned char *args_buff;
+	long status;
 	void __user *cmd_from_user;
 	uint64_t watch_mask_value = 0;
+	unsigned int args_idx = 0;
 
 	memset((void *) &aw_info, 0, sizeof(struct dbg_address_watch_info));
 
-	do {
-		dev = kfd_device_by_id(args->gpu_id);
-		if (!dev) {
-			dev_info(NULL,
-			"Error! kfd: In func %s >> get device by id failed\n",
-			__func__);
-			break;
-		}
-
-		cmd_from_user = (void __user *) args->content_ptr;
-
-		if (args->buf_size_in_bytes > MAX_ALLOWED_AW_BUFF_SIZE) {
-			status = -EINVAL;
-			break;
-		}
-
-		if (args->buf_size_in_bytes <= sizeof(*args)) {
-			status = -EINVAL;
-			break;
-		}
-
-		/* this is the actual buffer to work with */
-
-		args_buff = memdup_user(cmd_from_user,
-					args->buf_size_in_bytes - sizeof(*args));
-		if (IS_ERR(args_buff))
-			return PTR_ERR(args_buff);
+	dev = kfd_device_by_id(args->gpu_id);
+	if (dev == NULL)
+		return -EINVAL;
 
-		aw_info.process = p;
+	if (dev->device_info->asic_family == CHIP_CARRIZO) {
+		pr_debug("kfd_ioctl_dbg_wave_control not supported on CZ\n");
+		return -EINVAL;
+	}
 
-		aw_info.num_watch_points = *((uint32_t *)(&args_buff[args_idx]));
-		args_idx += sizeof(aw_info.num_watch_points);
+	cmd_from_user = (void __user *) args->content_ptr;
 
-		aw_info.watch_mode = (HSA_DBG_WATCH_MODE *) &args_buff[args_idx];
-		args_idx += sizeof(HSA_DBG_WATCH_MODE) * aw_info.num_watch_points;
+	/* Validate arguments */
 
-		/* set watch address base pointer to point on the array base within args_buff */
+	if ((args->buf_size_in_bytes > MAX_ALLOWED_AW_BUFF_SIZE) ||
+		(args->buf_size_in_bytes <= sizeof(*args) + sizeof(int) * 2) ||
+		(cmd_from_user == NULL))
+		return -EINVAL;
 
-		aw_info.watch_address = (uint64_t *) &args_buff[args_idx];
+	/* this is the actual buffer to work with */
+	args_buff = memdup_user(cmd_from_user,
+				args->buf_size_in_bytes - sizeof(*args));
+	if (IS_ERR(args_buff))
+		return PTR_ERR(args_buff);
 
-		/*skip over the addresses buffer */
-		args_idx += sizeof(aw_info.watch_address) * aw_info.num_watch_points;
+	aw_info.process = p;
 
-		if (args_idx >= args->buf_size_in_bytes) {
-			status = -EINVAL;
-			break;
-		}
+	aw_info.num_watch_points = *((uint32_t *)(&args_buff[args_idx]));
+	args_idx += sizeof(aw_info.num_watch_points);
 
-		watch_mask_value = (uint64_t) args_buff[args_idx];
+	aw_info.watch_mode = (enum HSA_DBG_WATCH_MODE *) &args_buff[args_idx];
+	args_idx += sizeof(enum HSA_DBG_WATCH_MODE) * aw_info.num_watch_points;
 
-		if (watch_mask_value > 0) {
-			/* there is an array of masks */
+	/*
+	 * set watch address base pointer to point on the array base
+	 * within args_buff
+	 */
+	aw_info.watch_address = (uint64_t *) &args_buff[args_idx];
 
-			/* set watch mask base pointer to point on the array base within args_buff */
-			aw_info.watch_mask = (uint64_t *) &args_buff[args_idx];
+	/* skip over the addresses buffer */
+	args_idx += sizeof(aw_info.watch_address) * aw_info.num_watch_points;
 
-			/*skip over the masks buffer */
-			args_idx += sizeof(aw_info.watch_mask) * aw_info.num_watch_points;
-		}
+	if (args_idx >= args->buf_size_in_bytes - sizeof(*args)) {
+		kfree(args_buff);
+		return -EINVAL;
+	}
 
-		else
-			/* just the NULL mask, set to NULL and skip over it */
-		{
-			aw_info.watch_mask = NULL;
-			args_idx += sizeof(aw_info.watch_mask);
-		}
+	watch_mask_value = (uint64_t) args_buff[args_idx];
 
-		if (args_idx > args->buf_size_in_bytes) {
-			status = -EINVAL;
-			break;
-		}
+	if (watch_mask_value > 0) {
+		/*
+		 * There is an array of masks.
+		 * set watch mask base pointer to point on the array base
+		 * within args_buff
+		 */
+		aw_info.watch_mask = (uint64_t *) &args_buff[args_idx];
 
-		aw_info.watch_event = NULL;	/* Currently HSA Event is not supported for DBG */
-		status = 0;
+		/* skip over the masks buffer */
+		args_idx += sizeof(aw_info.watch_mask) *
+				aw_info.num_watch_points;
+	} else {
+		/* just the NULL mask, set to NULL and skip over it */
+		aw_info.watch_mask = NULL;
+		args_idx += sizeof(aw_info.watch_mask);
+	}
 
-	} while (0);
+	if (args_idx >= args->buf_size_in_bytes - sizeof(args)) {
+		kfree(args_buff);
+		return -EINVAL;
+	}
 
-	if (status == 0) {
-		mutex_lock(get_dbgmgr_mutex());
+	/* Currently HSA Event is not supported for DBG */
+	aw_info.watch_event = NULL;
 
-		status = kfd_dbgmgr_address_watch(dev->dbgmgr, &aw_info);
+	mutex_lock(kfd_get_dbgmgr_mutex());
 
-		mutex_unlock(get_dbgmgr_mutex());
+	status = kfd_dbgmgr_address_watch(dev->dbgmgr, &aw_info);
 
-	}
+	mutex_unlock(kfd_get_dbgmgr_mutex());
 
 	kfree(args_buff);
 
 	return status;
 }
 
-/*
- * Parse and generate fixed size data structure for wave control.
- * Buffer is generated in a "packed" form, for avoiding structure packing/pending dependencies.
- */
-
-static int
-kfd_ioctl_dbg_wave_control(struct file *filep, struct kfd_process *p, void *data)
+/* Parse and generate fixed size data structure for wave control */
+static int kfd_ioctl_dbg_wave_control(struct file *filep,
+					struct kfd_process *p, void *data)
 {
-	long status = -EFAULT;
 	struct kfd_ioctl_dbg_wave_control_args *args = data;
 	struct kfd_dev *dev;
 	struct dbg_wave_control_info wac_info;
-	unsigned char *args_buff = NULL;
-	unsigned int args_idx = 0;
-	void __user *cmd_from_user;
+	unsigned char *args_buff;
 	uint32_t computed_buff_size;
+	long status;
+	void __user *cmd_from_user;
+	unsigned int args_idx = 0;
 
 	memset((void *) &wac_info, 0, sizeof(struct dbg_wave_control_info));
 
 	/* we use compact form, independent of the packing attribute value */
-
 	computed_buff_size = sizeof(*args) +
 				sizeof(wac_info.mode) +
 				sizeof(wac_info.operand) +
@@ -756,80 +645,62 @@ kfd_ioctl_dbg_wave_control(struct file *filep, struct kfd_process *p, void *data
 				sizeof(wac_info.dbgWave_msg.MemoryVA) +
 				sizeof(wac_info.trapId);
 
+	dev = kfd_device_by_id(args->gpu_id);
+	if (dev == NULL)
+		return -EINVAL;
 
-	dev_info(NULL, "kfd: In func %s - start\n", __func__);
-
-	do {
-		dev = kfd_device_by_id(args->gpu_id);
-		if (!dev) {
-			dev_info(NULL, "Error! kfd: In func %s >> getting device by id failed\n", __func__);
-			break;
-		}
+	if (dev->device_info->asic_family == CHIP_CARRIZO) {
+		pr_debug("kfd_ioctl_dbg_wave_control not supported on CZ\n");
+		return -EINVAL;
+	}
 
-		/* input size must match the computed "compact" size */
+	/* input size must match the computed "compact" size */
+	if (args->buf_size_in_bytes != computed_buff_size) {
+		pr_debug("size mismatch, computed : actual %u : %u\n",
+				args->buf_size_in_bytes, computed_buff_size);
+		return -EINVAL;
+	}
 
-		if (args->buf_size_in_bytes != computed_buff_size) {
-			dev_info(NULL,
-					 "Error! kfd: In func %s >> size mismatch, computed : actual %u : %u\n",
-					__func__, args->buf_size_in_bytes, computed_buff_size);
-			status = -EINVAL;
-			break;
-		}
+	cmd_from_user = (void __user *) args->content_ptr;
 
-		cmd_from_user = (void __user *) args->content_ptr;
+	if (cmd_from_user == NULL)
+		return -EINVAL;
 
-		/* copy the entire buffer from user */
+	/* copy the entire buffer from user */
 
-		args_buff = memdup_user(cmd_from_user,
+	args_buff = memdup_user(cmd_from_user,
 				args->buf_size_in_bytes - sizeof(*args));
-		if (IS_ERR(args_buff))
-			return PTR_ERR(args_buff);
-
-		if (copy_from_user(args_buff,
-				(void __user *) args->content_ptr,
-				args->buf_size_in_bytes - sizeof(*args))) {
-			dev_info(NULL,
-			"Error! kfd: In func %s >> copy_from_user failed\n",
-			 __func__);
-			break;
-		}
-
-		/* move ptr to the start of the "pay-load" area */
-
-
-		wac_info.process = p;
+	if (IS_ERR(args_buff))
+		return PTR_ERR(args_buff);
 
-		wac_info.operand = (HSA_DBG_WAVEOP) *((HSA_DBG_WAVEOP *)(&args_buff[args_idx]));
-		args_idx += sizeof(wac_info.operand);
+	/* move ptr to the start of the "pay-load" area */
+	wac_info.process = p;
 
-		wac_info.mode = (HSA_DBG_WAVEMODE) *((HSA_DBG_WAVEMODE *)(&args_buff[args_idx]));
-		args_idx += sizeof(wac_info.mode);
+	wac_info.operand = *((enum HSA_DBG_WAVEOP *)(&args_buff[args_idx]));
+	args_idx += sizeof(wac_info.operand);
 
-		wac_info.trapId = (uint32_t) *((uint32_t *)(&args_buff[args_idx]));
-		args_idx += sizeof(wac_info.trapId);
+	wac_info.mode = *((enum HSA_DBG_WAVEMODE *)(&args_buff[args_idx]));
+	args_idx += sizeof(wac_info.mode);
 
-		wac_info.dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value = *((uint32_t *)(&args_buff[args_idx]));
-		wac_info.dbgWave_msg.MemoryVA = NULL;
+	wac_info.trapId = *((uint32_t *)(&args_buff[args_idx]));
+	args_idx += sizeof(wac_info.trapId);
 
+	wac_info.dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value =
+					*((uint32_t *)(&args_buff[args_idx]));
+	wac_info.dbgWave_msg.MemoryVA = NULL;
 
-		status = 0;
+	mutex_lock(kfd_get_dbgmgr_mutex());
 
-	} while (0);
-	if (status == 0) {
-		mutex_lock(get_dbgmgr_mutex());
-
-		dev_info(NULL,
-				"kfd: In func %s >> calling dbg manager process %p, operand %u, mode %u, trapId %u, message %u\n",
-				__func__, wac_info.process, wac_info.operand, wac_info.mode, wac_info.trapId,
-				wac_info.dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value);
+	pr_debug("Calling dbg manager process %p, operand %u, mode %u, trapId %u, message %u\n",
+			wac_info.process, wac_info.operand,
+			wac_info.mode, wac_info.trapId,
+			wac_info.dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value);
 
-		status = kfd_dbgmgr_wave_control(dev->dbgmgr, &wac_info);
+	status = kfd_dbgmgr_wave_control(dev->dbgmgr, &wac_info);
 
-		dev_info(NULL, "kfd: In func %s >> returned status of dbg manager is %ld\n", __func__, status);
+	pr_debug("Returned status of dbg manager is %ld\n", status);
 
-		mutex_unlock(get_dbgmgr_mutex());
-
-	}
+	mutex_unlock(kfd_get_dbgmgr_mutex());
 
 	kfree(args_buff);
 
@@ -844,13 +715,12 @@ static int kfd_ioctl_get_clock_counters(struct file *filep,
 	struct timespec64 time;
 
 	dev = kfd_device_by_id(args->gpu_id);
-	if (dev)
-		/* Reading GPU clock counter from KGD */
-		args->gpu_clock_counter =
-			dev->kfd2kgd->get_gpu_clock_counter(dev->kgd);
-	else
-		/* Node without GPU resource */
-		args->gpu_clock_counter = 0;
+	if (dev == NULL)
+		return -EINVAL;
+
+	/* Reading GPU clock counter from KGD */
+	args->gpu_clock_counter =
+		dev->kfd2kgd->get_gpu_clock_counter(dev->kgd);
 
 	/* No access to rdtsc. Using raw monotonic time */
 	getrawmonotonic64(&time);
@@ -877,7 +747,7 @@ static int kfd_ioctl_get_process_apertures(struct file *filp,
 
 	args->num_of_nodes = 0;
 
-	down_write(&p->lock);
+	mutex_lock(&p->mutex);
 
 	/*if the process-device list isn't empty*/
 	if (kfd_has_process_device_data(p)) {
@@ -916,180 +786,52 @@ static int kfd_ioctl_get_process_apertures(struct file *filp,
 				(args->num_of_nodes < NUM_OF_SUPPORTED_GPUS));
 	}
 
-	up_write(&p->lock);
+	mutex_unlock(&p->mutex);
 
 	return 0;
 }
 
-static int kfd_ioctl_get_process_apertures_new(struct file *filp,
-				struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_get_process_apertures_new_args *args = data;
-	struct kfd_process_device_apertures *pa;
-	struct kfd_process_device *pdd;
-	uint32_t nodes = 0;
-	int ret;
-
-	dev_dbg(kfd_device, "get apertures for PASID %d", p->pasid);
-
-	if (args->num_of_nodes == 0) {
-		/* Return number of nodes, so that user space can alloacate
-		* sufficient memory */
-		down_write(&p->lock);
-
-		if (!kfd_has_process_device_data(p)) {
-			up_write(&p->lock);
-			return 0;
-		}
-
-		/* Run over all pdd of the process */
-		pdd = kfd_get_first_process_device_data(p);
-		do {
-			args->num_of_nodes++;
-		} while ((pdd =
-			kfd_get_next_process_device_data(p, pdd)) != NULL);
-
-		up_write(&p->lock);
-		return 0;
-	}
-
-	/* Fill in process-aperture information for all available
-	 * nodes, but not more than args->num_of_nodes as that is
-	 * the amount of memory allocated by user */
-	pa = kzalloc((sizeof(struct kfd_process_device_apertures) *
-				args->num_of_nodes), GFP_KERNEL);
-	if (!pa)
-		return -ENOMEM;
-
-	down_write(&p->lock);
-
-	if (!kfd_has_process_device_data(p)) {
-		up_write(&p->lock);
-		args->num_of_nodes = 0;
-		kfree(pa);
-		return 0;
-	}
-
-	/* Run over all pdd of the process */
-	pdd = kfd_get_first_process_device_data(p);
-	do {
-		pa[nodes].gpu_id = pdd->dev->id;
-		pa[nodes].lds_base = pdd->lds_base;
-		pa[nodes].lds_limit = pdd->lds_limit;
-		pa[nodes].gpuvm_base = pdd->gpuvm_base;
-		pa[nodes].gpuvm_limit = pdd->gpuvm_limit;
-		pa[nodes].scratch_base = pdd->scratch_base;
-		pa[nodes].scratch_limit = pdd->scratch_limit;
-
-		dev_dbg(kfd_device,
-			"gpu id %u\n", pdd->dev->id);
-		dev_dbg(kfd_device,
-			"lds_base %llX\n", pdd->lds_base);
-		dev_dbg(kfd_device,
-			"lds_limit %llX\n", pdd->lds_limit);
-		dev_dbg(kfd_device,
-			"gpuvm_base %llX\n", pdd->gpuvm_base);
-		dev_dbg(kfd_device,
-			"gpuvm_limit %llX\n", pdd->gpuvm_limit);
-		dev_dbg(kfd_device,
-			"scratch_base %llX\n", pdd->scratch_base);
-		dev_dbg(kfd_device,
-			"scratch_limit %llX\n", pdd->scratch_limit);
-		nodes++;
-	} while (
-		(pdd = kfd_get_next_process_device_data(p, pdd)) != NULL &&
-		(nodes < args->num_of_nodes));
-	up_write(&p->lock);
-
-	args->num_of_nodes = nodes;
-	ret = copy_to_user(
-			(void __user *)args->kfd_process_device_apertures_ptr,
-			pa,
-			(nodes * sizeof(struct kfd_process_device_apertures)));
-	kfree(pa);
-	return ret ? -EFAULT : 0;
-}
-
-static int
-kfd_ioctl_create_event(struct file *filp, struct kfd_process *p, void *data)
+static int kfd_ioctl_create_event(struct file *filp, struct kfd_process *p,
+					void *data)
 {
 	struct kfd_ioctl_create_event_args *args = data;
-	struct kfd_dev *kfd;
-	struct kfd_process_device *pdd;
-	int err = -EINVAL;
-	void *mem, *kern_addr = NULL;
-
-	pr_debug("amdkfd: Event page offset 0x%llx\n", args->event_page_offset);
-
-	if (args->event_page_offset) {
-		kfd = kfd_device_by_id(GET_GPU_ID(args->event_page_offset));
-		if (!kfd) {
-			pr_err("amdkfd: can't find kfd device\n");
-			return -EFAULT;
-		}
-		if (KFD_IS_DGPU(kfd->device_info->asic_family)) {
-			down_write(&p->lock);
-			pdd = kfd_bind_process_to_device(kfd, p);
-			if (IS_ERR(pdd) < 0) {
-				err = PTR_ERR(pdd);
-				up_write(&p->lock);
-				return -EFAULT;
-			}
-			mem = kfd_process_device_translate_handle(pdd,
-				GET_IDR_HANDLE(args->event_page_offset));
-			if (!mem) {
-				pr_err("amdkfd: can't find BO offset is 0x%llx\n",
-						args->event_page_offset);
-				up_write(&p->lock);
-				return -EFAULT;
-			}
-			up_write(&p->lock);
-
-			/* Map dGPU gtt BO to kernel */
-			kfd->kfd2kgd->map_gtt_bo_to_kernel(kfd->kgd,
-					mem, &kern_addr);
-		}
-	}
+	int err;
 
-	err = kfd_event_create(filp, p,
-			args->event_type,
-			args->auto_reset != 0,
-			args->node_id,
-			&args->event_id,
-			&args->event_trigger_data,
-			&args->event_page_offset,
-			&args->event_slot_index,
-			kern_addr);
+	err = kfd_event_create(filp, p, args->event_type,
+				args->auto_reset != 0, args->node_id,
+				&args->event_id, &args->event_trigger_data,
+				&args->event_page_offset,
+				&args->event_slot_index);
 
 	return err;
 }
 
-static int
-kfd_ioctl_destroy_event(struct file *filp, struct kfd_process *p, void *data)
+static int kfd_ioctl_destroy_event(struct file *filp, struct kfd_process *p,
+					void *data)
 {
 	struct kfd_ioctl_destroy_event_args *args = data;
 
 	return kfd_event_destroy(p, args->event_id);
 }
 
-static int
-kfd_ioctl_set_event(struct file *filp, struct kfd_process *p, void *data)
+static int kfd_ioctl_set_event(struct file *filp, struct kfd_process *p,
+				void *data)
 {
 	struct kfd_ioctl_set_event_args *args = data;
 
 	return kfd_set_event(p, args->event_id);
 }
 
-static int
-kfd_ioctl_reset_event(struct file *filp, struct kfd_process *p, void *data)
+static int kfd_ioctl_reset_event(struct file *filp, struct kfd_process *p,
+				void *data)
 {
 	struct kfd_ioctl_reset_event_args *args = data;
 
 	return kfd_reset_event(p, args->event_id);
 }
 
-static int
-kfd_ioctl_wait_events(struct file *filp, struct kfd_process *p, void *data)
+static int kfd_ioctl_wait_events(struct file *filp, struct kfd_process *p,
+				void *data)
 {
 	struct kfd_ioctl_wait_events_args *args = data;
 	enum kfd_event_wait_result wait_result;
@@ -1104,777 +846,6 @@ kfd_ioctl_wait_events(struct file *filp, struct kfd_process *p, void *data)
 
 	return err;
 }
-static int kfd_ioctl_alloc_scratch_memory(struct file *filep,
-					struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_alloc_memory_of_gpu_args *args =
-			(struct kfd_ioctl_alloc_memory_of_gpu_args *)data;
-	struct kfd_process_device *pdd;
-	struct kfd_dev *dev;
-	long err;
-
-	if (args->size == 0)
-		return -EINVAL;
-
-	dev = kfd_device_by_id(args->gpu_id);
-	if (dev == NULL)
-		return -EINVAL;
-
-	down_write(&p->lock);
-
-	pdd = kfd_bind_process_to_device(dev, p);
-	if (IS_ERR(pdd) < 0) {
-		err = PTR_ERR(pdd);
-		goto bind_process_to_device_fail;
-	}
-
-	pdd->sh_hidden_private_base_vmid = args->va_addr;
-	pdd->qpd.sh_hidden_private_base = args->va_addr;
-
-	up_write(&p->lock);
-
-	if (dev->dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS &&
-	    pdd->qpd.vmid != 0) {
-		err = dev->kfd2kgd->alloc_memory_of_scratch(
-			dev->kgd, args->va_addr, pdd->qpd.vmid);
-		if (err != 0)
-			goto alloc_memory_of_scratch_failed;
-	}
-
-	return 0;
-
-bind_process_to_device_fail:
-	up_write(&p->lock);
-alloc_memory_of_scratch_failed:
-	return -EFAULT;
-}
-
-static int kfd_ioctl_alloc_memory_of_gpu(struct file *filep,
-					struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_alloc_memory_of_gpu_args *args = data;
-	struct kfd_process_device *pdd;
-	void *mem;
-	struct kfd_dev *dev;
-	int idr_handle;
-	long err;
-
-	if (args->size == 0)
-		return -EINVAL;
-
-	dev = kfd_device_by_id(args->gpu_id);
-	if (dev == NULL)
-		return -EINVAL;
-
-	down_write(&p->lock);
-	pdd = kfd_bind_process_to_device(dev, p);
-	up_write(&p->lock);
-	if (IS_ERR(pdd) < 0)
-		return PTR_ERR(pdd);
-
-	err = dev->kfd2kgd->alloc_memory_of_gpu(
-		dev->kgd, args->va_addr, args->size,
-		pdd->vm, (struct kgd_mem **) &mem, NULL, NULL, 0);
-
-	if (err != 0)
-		return err;
-
-	down_write(&p->lock);
-	idr_handle = kfd_process_device_create_obj_handle(pdd, mem,
-			args->va_addr, args->size, NULL);
-	up_write(&p->lock);
-	if (idr_handle < 0) {
-		dev->kfd2kgd->free_memory_of_gpu(dev->kgd,
-						 (struct kgd_mem *) mem,
-						 pdd->vm);
-		return -EFAULT;
-	}
-
-	args->handle = MAKE_HANDLE(args->gpu_id, idr_handle);
-
-	return 0;
-}
-
-bool kfd_is_large_bar(struct kfd_dev *dev)
-{
-	struct kfd_local_mem_info mem_info;
-
-	if (debug_largebar) {
-		pr_debug("amdkfd: simulate large-bar allocation on non large-bar machine\n");
-		return true;
-	}
-
-	if (!KFD_IS_DGPU(dev->device_info->asic_family))
-		return false;
-
-	dev->kfd2kgd->get_local_mem_info(dev->kgd, &mem_info);
-	if (mem_info.local_mem_size_private == 0 &&
-			mem_info.local_mem_size_public > 0)
-		return true;
-	return false;
-}
-
-static uint32_t kfd_convert_user_mem_alloction_flags(
-		struct kfd_dev *dev,
-		uint32_t userspace_flags)
-{
-	uint32_t kernel_allocation_flags;
-
-	kernel_allocation_flags = 0;
-
-	/* Allocate VRAM bo */
-	if ((userspace_flags & KFD_IOC_ALLOC_MEM_FLAGS_DGPU_DEVICE) ||
-		(userspace_flags & KFD_IOC_ALLOC_MEM_FLAGS_APU_DEVICE)) {
-		kernel_allocation_flags = ALLOC_MEM_FLAGS_VRAM;
-		if ((userspace_flags & KFD_IOC_ALLOC_MEM_FLAGS_DGPU_DEVICE) &&
-				kfd_is_large_bar(dev))
-			kernel_allocation_flags |= ALLOC_MEM_FLAGS_PUBLIC;
-		goto out;
-	}
-	/*
-	 * Since currently user space library doesn't uses scratch
-	 * allocation flag I route it to VRAM
-	 */
-	if ((userspace_flags & KFD_IOC_ALLOC_MEM_FLAGS_DGPU_SCRATCH) ||
-		(userspace_flags & KFD_IOC_ALLOC_MEM_FLAGS_APU_SCRATCH)) {
-		kernel_allocation_flags = ALLOC_MEM_FLAGS_VRAM;
-		goto out;
-	}
-	/*
-	 * The current usage for *_HOST allocation flags are for GTT memory
-	 * Need to verify if we're node zero or we want to allocate bo on
-	 * public domain for P2P buffers.
-	 */
-	if (userspace_flags & KFD_IOC_ALLOC_MEM_FLAGS_DGPU_HOST) {
-		kernel_allocation_flags = ALLOC_MEM_FLAGS_GTT;
-		goto out;
-	}
-	/* Allocate userptr BO */
-	if (userspace_flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {
-		kernel_allocation_flags = ALLOC_MEM_FLAGS_USERPTR;
-		goto out;
-	}
-	/* Allocate doorbell BO */
-	if (userspace_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) {
-		kernel_allocation_flags = ALLOC_MEM_FLAGS_DOORBELL;
-		goto out;
-	}
-
-out:
-	if (userspace_flags & KFD_IOC_ALLOC_MEM_FLAGS_DGPU_AQL_QUEUE_MEM)
-		kernel_allocation_flags |= ALLOC_MEM_FLAGS_AQL_QUEUE_MEM;
-	/* Current HW doesn't support non paged memory */
-	kernel_allocation_flags |= ALLOC_MEM_FLAGS_NONPAGED;
-	/*
-	 *  Set by default execute access as this buffer might be allocated
-	 * for CP's ring buffer
-	 */
-	kernel_allocation_flags |= ALLOC_MEM_FLAGS_EXECUTE_ACCESS;
-	kernel_allocation_flags |= ALLOC_MEM_FLAGS_NO_SUBSTITUTE;
-
-	pr_debug("amdkfd: user allocation flags 0x%x kernel allocation flags: 0x%x\n",
-			userspace_flags, kernel_allocation_flags);
-
-	return kernel_allocation_flags;
-}
-
-static int kfd_ioctl_alloc_memory_of_gpu_new(struct file *filep,
-					struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_alloc_memory_of_gpu_new_args *args = data;
-	struct kfd_process_device *pdd;
-	void *mem;
-	struct kfd_dev *dev;
-	int idr_handle;
-	long err;
-	uint64_t offset;
-	uint32_t alloc_flags = 0;
-
-	if (args->size == 0)
-		return -EINVAL;
-
-	dev = kfd_device_by_id(args->gpu_id);
-	if (dev == NULL)
-		return -EINVAL;
-
-	down_write(&p->lock);
-	pdd = kfd_bind_process_to_device(dev, p);
-	up_write(&p->lock);
-	if (IS_ERR(pdd) < 0)
-		return PTR_ERR(pdd);
-
-	if (args->flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) {
-		if (args->size != kfd_doorbell_process_slice())
-			return -EINVAL;
-		offset = kfd_get_process_doorbells(dev, p);
-	} else
-		offset = args->mmap_offset;
-
-	alloc_flags = kfd_convert_user_mem_alloction_flags(dev, args->flags);
-
-	err = dev->kfd2kgd->alloc_memory_of_gpu(
-		dev->kgd, args->va_addr, args->size,
-		pdd->vm, (struct kgd_mem **) &mem, &offset,
-		NULL, alloc_flags);
-
-	if (err != 0)
-		return err;
-
-	down_write(&p->lock);
-	idr_handle = kfd_process_device_create_obj_handle(pdd, mem,
-			args->va_addr, args->size, NULL);
-	up_write(&p->lock);
-	if (idr_handle < 0) {
-		dev->kfd2kgd->free_memory_of_gpu(dev->kgd,
-						 (struct kgd_mem *) mem,
-						 pdd->vm);
-		return -EFAULT;
-	}
-
-	args->handle = MAKE_HANDLE(args->gpu_id, idr_handle);
-	if ((args->flags & KFD_IOC_ALLOC_MEM_FLAGS_DGPU_DEVICE) != 0 &&
-			!kfd_is_large_bar(dev)) {
-		args->mmap_offset = 0;
-	} else {
-		args->mmap_offset = KFD_MMAP_TYPE_MAP_BO;
-		args->mmap_offset |= KFD_MMAP_GPU_ID(args->gpu_id);
-		args->mmap_offset <<= PAGE_SHIFT;
-		args->mmap_offset |= offset;
-	}
-
-	return 0;
-}
-
-static int kfd_ioctl_free_memory_of_gpu(struct file *filep,
-					struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_free_memory_of_gpu_args *args = data;
-	struct kfd_process_device *pdd;
-	struct kfd_bo *buf_obj;
-	struct kfd_dev *dev;
-	int ret;
-
-	dev = kfd_device_by_id(GET_GPU_ID(args->handle));
-	if (dev == NULL)
-		return -EINVAL;
-
-	down_write(&p->lock);
-
-	pdd = kfd_get_process_device_data(dev, p);
-	if (!pdd) {
-		pr_err("Process device data doesn't exist\n");
-		ret = -EINVAL;
-		goto err_unlock;
-	}
-
-	buf_obj = kfd_process_device_find_bo(pdd,
-					GET_IDR_HANDLE(args->handle));
-	if (buf_obj == NULL) {
-		ret = -EINVAL;
-		goto err_unlock;
-	}
-	run_rdma_free_callback(buf_obj);
-
-	up_write(&p->lock);
-
-	ret = dev->kfd2kgd->free_memory_of_gpu(dev->kgd, buf_obj->mem,
-					       pdd->vm);
-
-	/* If freeing the buffer failed, leave the handle in place for
-	 * clean-up during process tear-down. */
-	if (ret == 0) {
-		down_write(&p->lock);
-		kfd_process_device_remove_obj_handle(
-			pdd, GET_IDR_HANDLE(args->handle));
-		up_write(&p->lock);
-	}
-
-	return ret;
-
-err_unlock:
-	up_write(&p->lock);
-	return ret;
-}
-
-int kfd_map_memory_to_gpu(void *mem, struct kfd_process_device *pdd)
-{
-	int err;
-	struct kfd_dev *dev = pdd->dev;
-
-	err = dev->kfd2kgd->map_memory_to_gpu(
-		dev->kgd, (struct kgd_mem *) mem, pdd->vm);
-
-	/* Theoretically we don't need this flush. However, as there are
-	 * some bugs in our PTE handling for mapping and unmapping, we
-	 * need this flush to pass all the tests.
-	 */
-	kfd_flush_tlb(dev, pdd->process->pasid);
-
-	return err;
-}
-
-static int kfd_ioctl_map_memory_to_gpu(struct file *filep,
-					struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_map_memory_to_gpu_new_args *args = data;
-	struct kfd_process_device *pdd, *peer_pdd;
-	void *mem;
-	struct kfd_dev *dev, *peer;
-	long err = 0;
-	int i, num_dev;
-	uint32_t *devices_arr = NULL;
-
-	dev = kfd_device_by_id(GET_GPU_ID(args->handle));
-	if (dev == NULL)
-		return -EINVAL;
-
-	if (args->device_ids_array_size > 0 &&
-			(args->device_ids_array_size < sizeof(uint32_t))) {
-		pr_err("amdkfd: err node IDs array size %u\n",
-				args->device_ids_array_size);
-		return -EFAULT;
-	}
-
-	if (args->device_ids_array_size > 0) {
-		devices_arr = kmalloc(args->device_ids_array_size, GFP_KERNEL);
-		if (!devices_arr)
-			return -ENOMEM;
-
-		err = copy_from_user(devices_arr,
-				(void __user *)args->device_ids_array,
-				args->device_ids_array_size);
-		if (err != 0) {
-			err = -EFAULT;
-			goto copy_from_user_failed;
-		}
-	}
-
-	down_write(&p->lock);
-
-	pdd = kfd_bind_process_to_device(dev, p);
-	if (IS_ERR(pdd) < 0) {
-		err = PTR_ERR(pdd);
-		goto bind_process_to_device_failed;
-	}
-
-	mem = kfd_process_device_translate_handle(pdd,
-						GET_IDR_HANDLE(args->handle));
-	up_write(&p->lock);
-
-	if (mem == NULL) {
-		err = PTR_ERR(mem);
-		goto get_mem_obj_from_handle_failed;
-	}
-
-	if (args->device_ids_array_size > 0) {
-		num_dev = args->device_ids_array_size / sizeof(uint32_t);
-		for (i = 0 ; i < num_dev; i++) {
-			peer = kfd_device_by_id(devices_arr[i]);
-			if (!peer) {
-				pr_err("amdkfd: didn't found kfd-dev for 0x%x\n",
-						devices_arr[i]);
-				err = -EFAULT;
-				goto get_mem_obj_from_handle_failed;
-			}
-			down_write(&p->lock);
-			peer_pdd = kfd_bind_process_to_device(peer, p);
-			up_write(&p->lock);
-			if (!peer_pdd) {
-				err = -EFAULT;
-				goto get_mem_obj_from_handle_failed;
-			}
-			err = kfd_map_memory_to_gpu(mem, peer_pdd);
-			if (err != 0)
-				pr_err("amdkfd: failed to map\n");
-		}
-	} else {
-		err = kfd_map_memory_to_gpu(mem, pdd);
-		if (err != 0)
-			pr_err("amdkfd: failed to map\n");
-	}
-
-	if (args->device_ids_array_size > 0 && devices_arr)
-		kfree(devices_arr);
-
-	return err;
-
-bind_process_to_device_failed:
-	up_write(&p->lock);
-get_mem_obj_from_handle_failed:
-copy_from_user_failed:
-	kfree(devices_arr);
-	return err;
-}
-
-static int kfd_ioctl_map_memory_to_gpu_wrapper(struct file *filep,
-					struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_map_memory_to_gpu_args *args = data;
-	struct kfd_ioctl_map_memory_to_gpu_new_args new_args;
-
-	new_args.handle = args->handle;
-	new_args.device_ids_array = NULL;
-	new_args.device_ids_array_size = 0;
-
-	return kfd_ioctl_map_memory_to_gpu(filep, p, &new_args);
-}
-
-int kfd_unmap_memory_from_gpu(void *mem, struct kfd_process_device *pdd)
-{
-	int err;
-	struct kfd_dev *dev = pdd->dev;
-
-	err = dev->kfd2kgd->unmap_memory_to_gpu(
-		dev->kgd, (struct kgd_mem *) mem, pdd->vm);
-
-	if (err != 0)
-		return err;
-
-	kfd_flush_tlb(dev, pdd->process->pasid);
-
-	return 0;
-}
-
-static int kfd_ioctl_unmap_memory_from_gpu(struct file *filep,
-					struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_unmap_memory_from_gpu_new_args *args = data;
-	struct kfd_process_device *pdd, *peer_pdd;
-	void *mem;
-	struct kfd_dev *dev, *peer;
-	long err = 0;
-	uint32_t *devices_arr = NULL, num_dev, i;
-
-	dev = kfd_device_by_id(GET_GPU_ID(args->handle));
-	if (dev == NULL)
-		return -EINVAL;
-
-	if (args->device_ids_array_size > 0 &&
-			(args->device_ids_array_size < sizeof(uint32_t))) {
-		pr_err("amdkfd: err node IDs array size %u\n",
-				args->device_ids_array_size);
-		return -EFAULT;
-	}
-
-	if (args->device_ids_array_size > 0) {
-		devices_arr = kmalloc(args->device_ids_array_size, GFP_KERNEL);
-		if (!devices_arr)
-			return -ENOMEM;
-
-		err = copy_from_user(devices_arr,
-				(void __user *)args->device_ids_array,
-				args->device_ids_array_size);
-		if (err != 0) {
-			err = -EFAULT;
-			goto copy_from_user_failed;
-		}
-	}
-
-	down_write(&p->lock);
-
-	pdd = kfd_get_process_device_data(dev, p);
-	if (!pdd) {
-		pr_err("Process device data doesn't exist\n");
-		err = PTR_ERR(pdd);
-		goto bind_process_to_device_failed;
-	}
-
-	mem = kfd_process_device_translate_handle(pdd,
-						GET_IDR_HANDLE(args->handle));
-	up_write(&p->lock);
-
-	if (mem == NULL) {
-		err = PTR_ERR(mem);
-		goto get_mem_obj_from_handle_failed;
-	}
-
-	if (args->device_ids_array_size > 0) {
-		num_dev = args->device_ids_array_size / sizeof(uint32_t);
-		for (i = 0 ; i < num_dev; i++) {
-			peer = kfd_device_by_id(devices_arr[i]);
-			if (!peer) {
-				err = -EFAULT;
-				goto get_mem_obj_from_handle_failed;
-			}
-			down_write(&p->lock);
-			peer_pdd = kfd_get_process_device_data(peer, p);
-			up_write(&p->lock);
-			if (!peer_pdd) {
-				err = -EFAULT;
-				goto get_mem_obj_from_handle_failed;
-			}
-			kfd_unmap_memory_from_gpu(mem, peer_pdd);
-		}
-		kfree(devices_arr);
-	} else
-		kfd_unmap_memory_from_gpu(mem, pdd);
-
-	return 0;
-
-bind_process_to_device_failed:
-	up_write(&p->lock);
-get_mem_obj_from_handle_failed:
-copy_from_user_failed:
-	kfree(devices_arr);
-	return err;
-}
-
-static int kfd_ioctl_unmap_memory_from_gpu_wrapper(struct file *filep,
-					struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_unmap_memory_from_gpu_args *args = data;
-	struct kfd_ioctl_unmap_memory_from_gpu_new_args new_args;
-
-	new_args.handle = args->handle;
-	new_args.device_ids_array = NULL;
-	new_args.device_ids_array_size = 0;
-
-	return kfd_ioctl_unmap_memory_from_gpu(filep, p, &new_args);
-}
-
-static int kfd_ioctl_open_graphic_handle(struct file *filep,
-					struct kfd_process *p,
-					void *data)
-{
-	struct kfd_ioctl_open_graphic_handle_args *args = data;
-	struct kfd_dev *dev;
-	struct kfd_process_device *pdd;
-	void *mem;
-	int idr_handle;
-	long err;
-
-	dev = kfd_device_by_id(args->gpu_id);
-	if (dev == NULL)
-		return -EINVAL;
-
-	if (dev->device_info->asic_family != CHIP_KAVERI) {
-		pr_debug("kfd_ioctl_open_graphic_handle only supported on KV\n");
-		return -EINVAL;
-	}
-
-	down_write(&p->lock);
-	pdd = kfd_bind_process_to_device(dev, p);
-	up_write(&p->lock);
-	if (IS_ERR(pdd) < 0)
-		return PTR_ERR(pdd);
-
-	err = dev->kfd2kgd->open_graphic_handle(dev->kgd,
-			args->va_addr,
-			(struct kgd_vm *) pdd->vm,
-			args->graphic_device_fd,
-			args->graphic_handle,
-			(struct kgd_mem **) &mem);
-
-	if (err != 0)
-		return err;
-
-	down_write(&p->lock);
-	/*TODO: When open_graphic_handle is implemented, we need to create
-	* the corresponding interval tree. We need to know the size of
-	* the buffer through open_graphic_handle(). We use 1 for now.*/
-	idr_handle = kfd_process_device_create_obj_handle(pdd, mem,
-			args->va_addr, 1, NULL);
-	up_write(&p->lock);
-	if (idr_handle < 0) {
-		/* FIXME: destroy_process_gpumem doesn't seem to be
-		 * implemented anywhere */
-		dev->kfd2kgd->destroy_process_gpumem(dev->kgd, mem);
-		return -EFAULT;
-	}
-
-	args->handle = MAKE_HANDLE(args->gpu_id, idr_handle);
-
-	return 0;
-}
-
-static int kfd_ioctl_set_process_dgpu_aperture(struct file *filep,
-		struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_set_process_dgpu_aperture_args *args = data;
-	struct kfd_dev *dev;
-	struct kfd_process_device *pdd;
-	long err;
-
-	dev = kfd_device_by_id(args->gpu_id);
-	if (dev == NULL)
-		return -EINVAL;
-
-	down_write(&p->lock);
-
-	pdd = kfd_bind_process_to_device(dev, p);
-	if (IS_ERR(pdd) < 0) {
-		err = PTR_ERR(pdd);
-		goto exit;
-	}
-
-	err = kfd_set_process_dgpu_aperture(pdd, args->dgpu_base,
-			args->dgpu_limit);
-
-exit:
-	up_write(&p->lock);
-	return err;
-}
-
-static int kfd_ioctl_get_dmabuf_info(struct file *filep,
-		struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_get_dmabuf_info_args *args = data;
-	struct kfd_dev *dev = NULL;
-	struct kgd_dev *dma_buf_kgd;
-	void *metadata_buffer = NULL;
-	uint32_t flags;
-	unsigned i;
-	int r;
-
-	/* Find a KFD GPU device that supports the get_dmabuf_info query */
-	for (i = 0; kfd_topology_enum_kfd_devices(i, &dev) == 0; i++)
-		if (dev && dev->kfd2kgd->get_dmabuf_info)
-			break;
-	if (!dev)
-		return -EINVAL;
-
-	if (args->metadata_ptr) {
-		metadata_buffer = kzalloc(args->metadata_size, GFP_KERNEL);
-		if (!metadata_buffer)
-			return -ENOMEM;
-	}
-
-	/* Get dmabuf info from KGD */
-	r = dev->kfd2kgd->get_dmabuf_info(dev->kgd, args->dmabuf_fd,
-					  &dma_buf_kgd, &args->size,
-					  metadata_buffer, args->metadata_size,
-					  &args->metadata_size, &flags);
-	if (r)
-		goto exit;
-
-	/* Reverse-lookup gpu_id from kgd pointer */
-	dev = kfd_device_by_kgd(dma_buf_kgd);
-	if (!dev) {
-		r = -EINVAL;
-		goto exit;
-	}
-	args->gpu_id = kfd_get_gpu_id(dev);
-
-	/* Translate flags */
-	if (flags & ALLOC_MEM_FLAGS_VRAM) {
-		args->flags = KFD_IS_DGPU(dev->device_info->asic_family) ?
-			KFD_IOC_ALLOC_MEM_FLAGS_DGPU_DEVICE :
-			KFD_IOC_ALLOC_MEM_FLAGS_APU_DEVICE;
-	} else
-		args->flags = KFD_IOC_ALLOC_MEM_FLAGS_DGPU_HOST;
-
-	/* Copy metadata buffer to user mode */
-	if (metadata_buffer) {
-		r = copy_to_user((void __user *)args->metadata_ptr,
-				 metadata_buffer, args->metadata_size);
-		if (r != 0)
-			r = -EFAULT;
-	}
-
-exit:
-	kfree(metadata_buffer);
-
-	return r;
-}
-
-static int kfd_ioctl_import_dmabuf(struct file *filep,
-				   struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_import_dmabuf_args *args = data;
-	struct kfd_dev *dev;
-	int r;
-
-	dev = kfd_device_by_id(args->gpu_id);
-	if (!dev)
-		return -EINVAL;
-
-	r = kfd_ipc_import_dmabuf(dev, p, args->gpu_id, args->dmabuf_fd,
-				  args->va_addr, &args->handle, NULL);
-	if (r)
-		dev_err(kfd_device, "Failed to import dmabuf\n");
-
-	return r;
-}
-
-static int kfd_ioctl_ipc_export_handle(struct file *filep,
-				       struct kfd_process *p,
-				       void *data)
-{
-	struct kfd_ioctl_ipc_export_handle_args *args = data;
-	struct kfd_dev *dev;
-	int r;
-
-	dev = kfd_device_by_id(args->gpu_id);
-	if (!dev)
-		return -EINVAL;
-
-	r = kfd_ipc_export_as_handle(dev, p, args->handle, args->share_handle);
-	if (r)
-		dev_err(kfd_device, "Failed to export IPC handle\n");
-
-	return r;
-}
-
-static int kfd_ioctl_ipc_import_handle(struct file *filep,
-				       struct kfd_process *p,
-				       void *data)
-{
-	struct kfd_ioctl_ipc_import_handle_args *args = data;
-	struct kfd_dev *dev = NULL;
-	int r;
-
-	dev = kfd_device_by_id(args->gpu_id);
-	if (!dev)
-		return -EINVAL;
-
-	r = kfd_ipc_import_handle(dev, p, args->gpu_id, args->share_handle,
-				  args->va_addr, &args->handle,
-				  &args->mmap_offset);
-	if (r)
-		dev_err(kfd_device, "Failed to import IPC handle\n");
-
-	return r;
-}
-
-static int kfd_ioctl_get_tile_config(struct file *filep,
-		struct kfd_process *p, void *data)
-{
-	struct kfd_ioctl_get_tile_config_args *args = data;
-	struct kfd_dev *dev;
-	struct tile_config config;
-	int err = 0;
-
-	dev = kfd_device_by_id(args->gpu_id);
-
-	dev->kfd2kgd->get_tile_config(dev->kgd, &config);
-
-	args->gb_addr_config = config.gb_addr_config;
-	args->num_banks = config.num_banks;
-	args->num_ranks = config.num_ranks;
-
-	if (args->num_tile_configs > config.num_tile_configs)
-		args->num_tile_configs = config.num_tile_configs;
-	err = copy_to_user((void __user *)args->tile_config_ptr,
-			config.tile_config_ptr,
-			args->num_tile_configs * sizeof(uint32_t));
-	if (err) {
-		args->num_tile_configs = 0;
-		return -EFAULT;
-	}
-
-	if (args->num_macro_tile_configs > config.num_macro_tile_configs)
-		args->num_macro_tile_configs =
-				config.num_macro_tile_configs;
-	err = copy_to_user((void __user *)args->macro_tile_config_ptr,
-			config.macro_tile_config_ptr,
-			args->num_macro_tile_configs * sizeof(uint32_t));
-	if (err) {
-		args->num_macro_tile_configs = 0;
-		return -EFAULT;
-	}
-
-	return 0;
-}
 
 #define AMDKFD_IOCTL_DEF(ioctl, _func, _flags) \
 	[_IOC_NR(ioctl)] = {.cmd = ioctl, .func = _func, .flags = _flags, .cmd_drv = 0, .name = #ioctl}
@@ -1921,68 +892,13 @@ static const struct amdkfd_ioctl_desc amdkfd_ioctls[] = {
 			kfd_ioctl_dbg_register, 0),
 
 	AMDKFD_IOCTL_DEF(AMDKFD_IOC_DBG_UNREGISTER,
-			kfd_ioctl_dbg_unregister, 0),
+			kfd_ioctl_dbg_unrgesiter, 0),
 
 	AMDKFD_IOCTL_DEF(AMDKFD_IOC_DBG_ADDRESS_WATCH,
 			kfd_ioctl_dbg_address_watch, 0),
 
 	AMDKFD_IOCTL_DEF(AMDKFD_IOC_DBG_WAVE_CONTROL,
 			kfd_ioctl_dbg_wave_control, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_ALLOC_MEMORY_OF_GPU,
-			kfd_ioctl_alloc_memory_of_gpu, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_FREE_MEMORY_OF_GPU,
-			kfd_ioctl_free_memory_of_gpu, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_MAP_MEMORY_TO_GPU,
-			kfd_ioctl_map_memory_to_gpu_wrapper, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_UNMAP_MEMORY_FROM_GPU,
-			kfd_ioctl_unmap_memory_from_gpu_wrapper, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_OPEN_GRAPHIC_HANDLE,
-			kfd_ioctl_open_graphic_handle, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_ALLOC_MEMORY_OF_SCRATCH,
-			kfd_ioctl_alloc_scratch_memory, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_SET_CU_MASK,
-			kfd_ioctl_set_cu_mask, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_SET_PROCESS_DGPU_APERTURE,
-			kfd_ioctl_set_process_dgpu_aperture, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_SET_TRAP_HANDLER,
-			kfd_ioctl_set_trap_handler, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_ALLOC_MEMORY_OF_GPU_NEW,
-				kfd_ioctl_alloc_memory_of_gpu_new, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_MAP_MEMORY_TO_GPU_NEW,
-				kfd_ioctl_map_memory_to_gpu, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_UNMAP_MEMORY_FROM_GPU_NEW,
-				kfd_ioctl_unmap_memory_from_gpu, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_PROCESS_APERTURES_NEW,
-				kfd_ioctl_get_process_apertures_new, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_DMABUF_INFO,
-				kfd_ioctl_get_dmabuf_info, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_IMPORT_DMABUF,
-				kfd_ioctl_import_dmabuf, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_TILE_CONFIG,
-				kfd_ioctl_get_tile_config, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_IPC_IMPORT_HANDLE,
-				kfd_ioctl_ipc_import_handle, 0),
-
-	AMDKFD_IOCTL_DEF(AMDKFD_IOC_IPC_EXPORT_HANDLE,
-				kfd_ioctl_ipc_export_handle, 0)
-
 };
 
 #define AMDKFD_CORE_IOCTL_COUNT	ARRAY_SIZE(amdkfd_ioctls)
@@ -2015,8 +931,7 @@ static long kfd_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)
 	} else
 		goto err_i1;
 
-	dev_dbg(kfd_device, "ioctl cmd (#0x%x), arg 0x%lx\n",
-				nr, arg);
+	dev_dbg(kfd_device, "ioctl cmd 0x%x (#%d), arg 0x%lx\n", cmd, nr, arg);
 
 	process = kfd_get_process(current);
 	if (IS_ERR(process)) {
@@ -2071,8 +986,7 @@ err_i1:
 		kfree(kdata);
 
 	if (retcode)
-		dev_dbg(kfd_device, "ioctl cmd (#0x%x), arg 0x%lx, failed %d\n",
-				nr, arg, retcode);
+		dev_dbg(kfd_device, "ret = %d\n", retcode);
 
 	return retcode;
 }
@@ -2080,37 +994,20 @@ err_i1:
 static int kfd_mmap(struct file *filp, struct vm_area_struct *vma)
 {
 	struct kfd_process *process;
-	struct kfd_dev *kfd;
-	unsigned long vm_pgoff;
-	int retval;
 
 	process = kfd_get_process(current);
 	if (IS_ERR(process))
 		return PTR_ERR(process);
 
-	vm_pgoff = vma->vm_pgoff;
-	vma->vm_pgoff = KFD_MMAP_OFFSET_VALUE_GET(vma->vm_pgoff);
-
-	switch (vm_pgoff & KFD_MMAP_TYPE_MASK) {
-	case KFD_MMAP_TYPE_DOORBELL:
+	if ((vma->vm_pgoff & KFD_MMAP_DOORBELL_MASK) ==
+			KFD_MMAP_DOORBELL_MASK) {
+		vma->vm_pgoff = vma->vm_pgoff ^ KFD_MMAP_DOORBELL_MASK;
 		return kfd_doorbell_mmap(process, vma);
-
-	case KFD_MMAP_TYPE_EVENTS:
+	} else if ((vma->vm_pgoff & KFD_MMAP_EVENTS_MASK) ==
+			KFD_MMAP_EVENTS_MASK) {
+		vma->vm_pgoff = vma->vm_pgoff ^ KFD_MMAP_EVENTS_MASK;
 		return kfd_event_mmap(process, vma);
-
-	case KFD_MMAP_TYPE_MAP_BO:
-		kfd = kfd_device_by_id(KFD_MMAP_GPU_ID_GET(vm_pgoff));
-		if (!kfd)
-			return -EFAULT;
-		retval = kfd->kfd2kgd->mmap_bo(kfd->kgd, vma);
-		return retval;
-
-	case KFD_MMAP_TYPE_RESERVED_MEM:
-		return kfd_reserved_mem_mmap(process, vma);
-
 	}
 
 	return -EFAULT;
 }
-
-
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_crat.c b/drivers/gpu/drm/amd/amdkfd/kfd_crat.c
deleted file mode 100644
index 3681fcf..0000000
--- a/drivers/gpu/drm/amd/amdkfd/kfd_crat.c
+++ /dev/null
@@ -1,1184 +0,0 @@
-#include <linux/kernel.h>
-#include <linux/acpi.h>
-#include <linux/mm.h>
-#include <linux/amd-iommu.h>
-#include <linux/pci.h>
-#include "kfd_crat.h"
-#include "kfd_priv.h"
-#include "kfd_topology.h"
-
-/* GPU Processor ID base for dGPUs for which VCRAT needs to be created.
- * GPU processor ID are expressed with Bit[31]=1.
- * The base is set to 0x8000_0000 + 0x1000 to avoid collision with GPU IDs
- * used in the CRAT. */
-static uint32_t gpu_processor_id_low = 0x80001000;
-
-/* Return the next available gpu_processor_id and increment it for next GPU
- * @total_cu_count - Total CUs present in the GPU including ones masked off
- */
-static inline unsigned int get_and_inc_gpu_processor_id(
-				unsigned int total_cu_count)
-{
-	int current_id = gpu_processor_id_low;
-
-	gpu_processor_id_low += total_cu_count;
-	return current_id;
-}
-
-/* Static table to describe GPU Cache information */
-struct kfd_gpu_cache_info {
-	uint32_t	cache_size;
-	uint32_t	cache_level;
-	uint32_t	flags;
-	/* Indicates how many Compute Units share this cache
-	 * Value = 1 indicates the cache is not shared */
-	uint32_t	num_cu_shared;
-};
-
-static struct kfd_gpu_cache_info kaveri_cache_info[] = {
-	{
-		/* TCP L1 Cache per CU */
-		.cache_size = 16,
-		.cache_level = 1,
-		.flags = (CRAT_CACHE_FLAGS_ENABLED |
-				CRAT_CACHE_FLAGS_DATA_CACHE |
-				CRAT_CACHE_FLAGS_SIMD_CACHE),
-		.num_cu_shared = 1,
-
-	},
-	{
-		/* Scalar L1 Instruction Cache (in SQC module) per bank */
-		.cache_size = 16,
-		.cache_level = 1,
-		.flags = (CRAT_CACHE_FLAGS_ENABLED |
-				CRAT_CACHE_FLAGS_INST_CACHE |
-				CRAT_CACHE_FLAGS_SIMD_CACHE),
-		.num_cu_shared = 2,
-	},
-	{
-		/* Scalar L1 Data Cache (in SQC module) per bank */
-		.cache_size = 8,
-		.cache_level = 1,
-		.flags = (CRAT_CACHE_FLAGS_ENABLED |
-				CRAT_CACHE_FLAGS_DATA_CACHE |
-				CRAT_CACHE_FLAGS_SIMD_CACHE),
-		.num_cu_shared = 2,
-	},
-
-	/* TODO: Add L2 Cache information */
-};
-
-
-static struct kfd_gpu_cache_info carrizo_cache_info[] = {
-	{
-		/* TCP L1 Cache per CU */
-		.cache_size = 16,
-		.cache_level = 1,
-		.flags = (CRAT_CACHE_FLAGS_ENABLED |
-				CRAT_CACHE_FLAGS_DATA_CACHE |
-				CRAT_CACHE_FLAGS_SIMD_CACHE),
-		.num_cu_shared = 1,
-	},
-	{
-		/* Scalar L1 Instruction Cache (in SQC module) per bank */
-		.cache_size = 8,
-		.cache_level = 1,
-		.flags = (CRAT_CACHE_FLAGS_ENABLED |
-				CRAT_CACHE_FLAGS_INST_CACHE |
-				CRAT_CACHE_FLAGS_SIMD_CACHE),
-		.num_cu_shared = 4,
-	},
-	{
-		/* Scalar L1 Data Cache (in SQC module) per bank. */
-		.cache_size = 4,
-		.cache_level = 1,
-		.flags = (CRAT_CACHE_FLAGS_ENABLED |
-				CRAT_CACHE_FLAGS_DATA_CACHE |
-				CRAT_CACHE_FLAGS_SIMD_CACHE),
-		.num_cu_shared = 4,
-	},
-
-	/* TODO: Add L2 Cache information */
-};
-
-/* NOTE: In future if more information is added to struct kfd_gpu_cache_info
- *	the following ASICs may need a separate table. */
-#define hawaii_cache_info kaveri_cache_info
-#define tonga_cache_info carrizo_cache_info
-#define fiji_cache_info  carrizo_cache_info
-#define polaris10_cache_info carrizo_cache_info
-#define polaris11_cache_info carrizo_cache_info
-
-static void kfd_populated_cu_info_cpu(struct kfd_topology_device *dev,
-		struct crat_subtype_computeunit *cu)
-{
-	BUG_ON(!dev);
-	BUG_ON(!cu);
-
-	dev->node_props.cpu_cores_count = cu->num_cpu_cores;
-	dev->node_props.cpu_core_id_base = cu->processor_id_low;
-	if (cu->hsa_capability & CRAT_CU_FLAGS_IOMMU_PRESENT)
-		dev->node_props.capability |= HSA_CAP_ATS_PRESENT;
-
-	pr_debug("CU CPU: cores=%d id_base=%d\n", cu->num_cpu_cores,
-			cu->processor_id_low);
-}
-
-static void kfd_populated_cu_info_gpu(struct kfd_topology_device *dev,
-		struct crat_subtype_computeunit *cu)
-{
-	BUG_ON(!dev);
-	BUG_ON(!cu);
-
-	dev->node_props.simd_id_base = cu->processor_id_low;
-	dev->node_props.simd_count = cu->num_simd_cores;
-	dev->node_props.lds_size_in_kb = cu->lds_size_in_kb;
-	dev->node_props.max_waves_per_simd = cu->max_waves_simd;
-	dev->node_props.wave_front_size = cu->wave_front_size;
-	dev->node_props.array_count = cu->array_count;
-	dev->node_props.cu_per_simd_array = cu->num_cu_per_array;
-	dev->node_props.simd_per_cu = cu->num_simd_per_cu;
-	dev->node_props.max_slots_scratch_cu = cu->max_slots_scatch_cu;
-	if (cu->hsa_capability & CRAT_CU_FLAGS_HOT_PLUGGABLE)
-		dev->node_props.capability |= HSA_CAP_HOT_PLUGGABLE;
-	pr_debug("CU GPU: id_base=%d\n", cu->processor_id_low);
-}
-
-/* kfd_parse_subtype_cu - parse compute unit subtypes and attach it to correct
- *		topology device present in the device_list
- */
-static int kfd_parse_subtype_cu(struct crat_subtype_computeunit *cu,
-				struct list_head *device_list)
-{
-	struct kfd_topology_device *dev;
-
-	BUG_ON(!cu);
-
-	pr_debug("Found CU entry in CRAT table with proximity_domain=%d caps=%x\n",
-			cu->proximity_domain, cu->hsa_capability);
-	list_for_each_entry(dev, device_list, list) {
-		if (cu->proximity_domain == dev->proximity_domain) {
-			if (cu->flags & CRAT_CU_FLAGS_CPU_PRESENT)
-				kfd_populated_cu_info_cpu(dev, cu);
-
-			if (cu->flags & CRAT_CU_FLAGS_GPU_PRESENT)
-				kfd_populated_cu_info_gpu(dev, cu);
-			break;
-		}
-	}
-
-	return 0;
-}
-
-/* kfd_parse_subtype_mem - parse memory subtypes and attach it to correct
- *		topology device present in the device_list
- */
-static int kfd_parse_subtype_mem(struct crat_subtype_memory *mem,
-				struct list_head *device_list)
-{
-	struct kfd_mem_properties *props;
-	struct kfd_topology_device *dev;
-
-	BUG_ON(!mem);
-
-	pr_debug("Found memory entry in CRAT table with proximity_domain=%d\n",
-			mem->proximity_domain);
-	list_for_each_entry(dev, device_list, list) {
-		if (mem->proximity_domain == dev->proximity_domain) {
-			props = kfd_alloc_struct(props);
-			if (props == NULL)
-				return -ENOMEM;
-
-			/*
-			 * We're on GPU node
-			 */
-			if (dev->node_props.cpu_cores_count == 0) {
-				/* APU */
-				if (mem->visibility_type == 0)
-					props->heap_type =
-						HSA_MEM_HEAP_TYPE_FB_PRIVATE;
-				/* dGPU */
-				else
-					props->heap_type = mem->visibility_type;
-			}
-			else
-				props->heap_type = HSA_MEM_HEAP_TYPE_SYSTEM;
-
-			if (mem->flags & CRAT_MEM_FLAGS_HOT_PLUGGABLE)
-				props->flags |= HSA_MEM_FLAGS_HOT_PLUGGABLE;
-			if (mem->flags & CRAT_MEM_FLAGS_NON_VOLATILE)
-				props->flags |= HSA_MEM_FLAGS_NON_VOLATILE;
-
-			props->size_in_bytes =
-				((uint64_t)mem->length_high << 32) +
-							mem->length_low;
-			props->width = mem->width;
-
-			dev->node_props.mem_banks_count++;
-			list_add_tail(&props->list, &dev->mem_props);
-
-			break;
-		}
-	}
-
-	return 0;
-}
-
-/* kfd_parse_subtype_cache - parse cache subtypes and attach it to correct
- *		topology device present in the device_list
- */
-static int kfd_parse_subtype_cache(struct crat_subtype_cache *cache,
-			struct list_head *device_list)
-{
-	struct kfd_cache_properties *props;
-	struct kfd_topology_device *dev;
-	uint32_t id;
-	uint32_t total_num_of_cu;
-
-	BUG_ON(!cache);
-
-	id = cache->processor_id_low;
-
-	list_for_each_entry(dev, device_list, list) {
-		total_num_of_cu = (dev->node_props.array_count *
-					dev->node_props.cu_per_simd_array);
-
-		/* Cache infomration in CRAT doesn't have proximity_domain
-		 * information as it is associated with a CPU core or GPU
-		 * Compute Unit. So map the cache using CPU core Id or SIMD
-		 * (GPU) ID.
-		 * TODO: This works because currently we can safely assume that
-		 *  Compute Units are parsed before caches are parsed. In future
-		 *  remove this dependency
-		 */
-		if ((id >= dev->node_props.cpu_core_id_base &&
-			id <= dev->node_props.cpu_core_id_base +
-				dev->node_props.cpu_cores_count) ||
-			(id >= dev->node_props.simd_id_base &&
-			id < dev->node_props.simd_id_base +
-				total_num_of_cu)) {
-			props = kfd_alloc_struct(props);
-			if (props == NULL)
-				return -ENOMEM;
-
-			props->processor_id_low = id;
-			props->cache_level = cache->cache_level;
-			props->cache_size = cache->cache_size;
-			props->cacheline_size = cache->cache_line_size;
-			props->cachelines_per_tag = cache->lines_per_tag;
-			props->cache_assoc = cache->associativity;
-			props->cache_latency = cache->cache_latency;
-			memcpy(props->sibling_map, cache->sibling_map,
-					sizeof(props->sibling_map));
-
-			if (cache->flags & CRAT_CACHE_FLAGS_DATA_CACHE)
-				props->cache_type |= HSA_CACHE_TYPE_DATA;
-			if (cache->flags & CRAT_CACHE_FLAGS_INST_CACHE)
-				props->cache_type |= HSA_CACHE_TYPE_INSTRUCTION;
-			if (cache->flags & CRAT_CACHE_FLAGS_CPU_CACHE)
-				props->cache_type |= HSA_CACHE_TYPE_CPU;
-			if (cache->flags & CRAT_CACHE_FLAGS_SIMD_CACHE)
-				props->cache_type |= HSA_CACHE_TYPE_HSACU;
-
-			dev->cache_count++;
-			dev->node_props.caches_count++;
-			list_add_tail(&props->list, &dev->cache_props);
-
-			break;
-		}
-	}
-
-	return 0;
-}
-
-/* kfd_parse_subtype_iolink - parse iolink subtypes and attach it to correct
- *		topology device present in the device_list
- */
-static int kfd_parse_subtype_iolink(struct crat_subtype_iolink *iolink,
-					struct list_head *device_list)
-{
-	struct kfd_iolink_properties *props;
-	struct kfd_topology_device *dev;
-	uint32_t i = 0;
-	uint32_t id_from;
-	uint32_t id_to;
-
-	BUG_ON(!iolink);
-
-	id_from = iolink->proximity_domain_from;
-	id_to = iolink->proximity_domain_to;
-
-	pr_debug("Found IO link entry in CRAT table with id_from=%d\n", id_from);
-	list_for_each_entry(dev, device_list, list) {
-		if (id_from == dev->proximity_domain) {
-			props = kfd_alloc_struct(props);
-			if (props == NULL)
-				return -ENOMEM;
-
-			props->node_from = id_from;
-			props->node_to = id_to;
-			props->ver_maj = iolink->version_major;
-			props->ver_min = iolink->version_minor;
-			props->iolink_type = iolink->io_interface_type;
-
-			/*
-			 * weight factor (derived from CDIR), currently always 1
-			 */
-			props->weight = 1;
-
-			props->min_latency = iolink->minimum_latency;
-			props->max_latency = iolink->maximum_latency;
-			props->min_bandwidth = iolink->minimum_bandwidth_mbs;
-			props->max_bandwidth = iolink->maximum_bandwidth_mbs;
-			props->rec_transfer_size =
-					iolink->recommended_transfer_size;
-
-			dev->io_link_count++;
-			dev->node_props.io_links_count++;
-			list_add_tail(&props->list, &dev->io_link_props);
-
-			break;
-		}
-		i++;
-	}
-
-	return 0;
-}
-
-/* kfd_parse_subtype - parse subtypes and attach it to correct topology device
- *		present in the device_list
- *	@sub_type_hdr - subtype section of crat_image
- *	@device_list - list of topology devices present in this crat_image
- */
-static int kfd_parse_subtype(struct crat_subtype_generic *sub_type_hdr,
-				struct list_head *device_list)
-{
-	struct crat_subtype_computeunit *cu;
-	struct crat_subtype_memory *mem;
-	struct crat_subtype_cache *cache;
-	struct crat_subtype_iolink *iolink;
-	int ret = 0;
-
-	BUG_ON(!sub_type_hdr);
-
-	switch (sub_type_hdr->type) {
-	case CRAT_SUBTYPE_COMPUTEUNIT_AFFINITY:
-		cu = (struct crat_subtype_computeunit *)sub_type_hdr;
-		ret = kfd_parse_subtype_cu(cu, device_list);
-		break;
-	case CRAT_SUBTYPE_MEMORY_AFFINITY:
-		mem = (struct crat_subtype_memory *)sub_type_hdr;
-		ret = kfd_parse_subtype_mem(mem, device_list);
-		break;
-	case CRAT_SUBTYPE_CACHE_AFFINITY:
-		cache = (struct crat_subtype_cache *)sub_type_hdr;
-		ret = kfd_parse_subtype_cache(cache, device_list);
-		break;
-	case CRAT_SUBTYPE_TLB_AFFINITY:
-		/*
-		 * For now, nothing to do here
-		 */
-		pr_debug("Found TLB entry in CRAT table (not processing)\n");
-		break;
-	case CRAT_SUBTYPE_CCOMPUTE_AFFINITY:
-		/*
-		 * For now, nothing to do here
-		 */
-		pr_debug("Found CCOMPUTE entry in CRAT table (not processing)\n");
-		break;
-	case CRAT_SUBTYPE_IOLINK_AFFINITY:
-		iolink = (struct crat_subtype_iolink *)sub_type_hdr;
-		ret = kfd_parse_subtype_iolink(iolink, device_list);
-		break;
-	default:
-		pr_warn("Unknown subtype (%d) in CRAT\n",
-				sub_type_hdr->type);
-	}
-
-	return ret;
-}
-
-/* kfd_parse_crat_table - parse CRAT table. For each node present in CRAT
- *		create a kfd_topology_device and add in to device_list. Also parse
- *		CRAT subtypes and attach it to appropriate kfd_topology_device
- *	@crat_image - input image containing CRAT
- *	@device_list - [OUT] list of kfd_topology_device generated after parsing
- *				   crat_image
- *	@proximity_domain - Proximity domain of the first device in the table
- *	Return - 0 if successful else -ve value
- */
-int kfd_parse_crat_table(void *crat_image,
-				struct list_head *device_list,
-				uint32_t proximity_domain)
-{
-	struct kfd_topology_device *top_dev = NULL;
-	struct crat_subtype_generic *sub_type_hdr;
-	uint16_t node_id;
-	int ret;
-	struct crat_header *crat_table = (struct crat_header *)crat_image;
-	uint16_t num_nodes;
-	uint32_t image_len;
-	uint32_t last_header_type, last_header_length;
-
-	if (!crat_image)
-		return -EINVAL;
-
-	if (!list_empty(device_list)) {
-		pr_warn("Error device list should be empty\n");
-	}
-
-	num_nodes = crat_table->num_domains;
-	image_len = crat_table->length;
-
-	pr_info("Parsing CRAT table with %d nodes\n", num_nodes);
-
-	for (node_id = 0; node_id < num_nodes; node_id++) {
-		top_dev = kfd_create_topology_device(device_list);
-		if (!top_dev)
-			break;
-		top_dev->proximity_domain = proximity_domain++;
-	}
-
-	if (!top_dev)
-		return -ENOMEM;
-
-	memcpy(top_dev->oem_id, crat_table->oem_id, CRAT_OEMID_LENGTH);
-	memcpy(top_dev->oem_table_id, crat_table->oem_table_id, CRAT_OEMTABLEID_LENGTH);
-	top_dev->oem_revision = crat_table->oem_revision;
-
-	last_header_type = last_header_length = 0;
-	sub_type_hdr = (struct crat_subtype_generic *)(crat_table+1);
-	while ((char *)sub_type_hdr + sizeof(struct crat_subtype_generic) <
-			((char *)crat_image) + image_len) {
-		pr_debug("kfd parsing crat sub type header %p enabled: %s type: 0x%x length %d\n",
-				sub_type_hdr,
-				(sub_type_hdr->flags &
-					CRAT_SUBTYPE_FLAGS_ENABLED)
-					? "true" : "false",
-				sub_type_hdr->type,
-				sub_type_hdr->length);
-
-		if (sub_type_hdr->length == 0) {
-			pr_err("amdkfd: Parsing wrong CRAT's sub header last header type: %d last header len %d\n",
-				last_header_type, last_header_type);
-			pr_err("amdkfd: Current header type %d length %d\n",
-				sub_type_hdr->type, sub_type_hdr->length);
-			break;
-		}
-
-		if (sub_type_hdr->flags & CRAT_SUBTYPE_FLAGS_ENABLED) {
-			ret = kfd_parse_subtype(sub_type_hdr, device_list);
-			if (ret != 0)
-				return ret;
-		}
-
-		last_header_type = sub_type_hdr->type;
-		last_header_length = sub_type_hdr->length;
-		sub_type_hdr = (typeof(sub_type_hdr))((char *)sub_type_hdr +
-				sub_type_hdr->length);
-	}
-
-	return 0;
-}
-
-/* Helper function. See kfd_fill_gpu_cache_info for parameter description */
-static int fill_in_pcache(struct crat_subtype_cache *pcache,
-				struct kfd_gpu_cache_info *pcache_info,
-				struct kfd_cu_info *cu_info,
-				int mem_available,
-				int cu_bitmask,
-				int cache_type, unsigned int cu_processor_id,
-				int cu_block)
-{
-	unsigned int cu_sibling_map_mask;
-	int first_active_cu;
-
-	/* First check if enough memory is available */
-	if (mem_available - sizeof(struct crat_subtype_cache) < 0)
-		return -ENOMEM;
-
-	cu_sibling_map_mask = cu_bitmask;
-	cu_sibling_map_mask >>= cu_block;
-	cu_sibling_map_mask &=
-		((1 << pcache_info[cache_type].num_cu_shared) - 1);
-	first_active_cu = ffs(cu_sibling_map_mask);
-
-	/* CU could be inactive. In case of shared cache find the first active
-	 * CU. and incase of non-shared cache check if the CU is inactive. If
-	 * inactive active skip it*/
-	if (first_active_cu) {
-		memset(pcache, 0, sizeof(struct crat_subtype_cache));
-		pcache->type = CRAT_SUBTYPE_CACHE_AFFINITY;
-		pcache->length = sizeof(struct crat_subtype_cache);
-		pcache->flags = pcache_info[cache_type].flags;
-		pcache->processor_id_low = cu_processor_id
-					 + (first_active_cu - 1);
-		pcache->cache_level = pcache_info[cache_type].cache_level;
-		pcache->cache_size = pcache_info[cache_type].cache_size;
-
-		/* Sibling map is w.r.t processor_id_low, so shift out
-		 * inactive CU */
-		cu_sibling_map_mask =
-			cu_sibling_map_mask >> (first_active_cu - 1);
-
-		pcache->sibling_map[0] = (uint8_t)(cu_sibling_map_mask & 0xFF);
-		pcache->sibling_map[1] =
-				(uint8_t)((cu_sibling_map_mask >> 8) & 0xFF);
-		pcache->sibling_map[2] =
-				(uint8_t)((cu_sibling_map_mask >> 16) & 0xFF);
-		pcache->sibling_map[3] =
-				(uint8_t)((cu_sibling_map_mask >> 24) & 0xFF);
-		return 0;
-	}
-	return 1;
-}
-
-/* kfd_fill_gpu_cache_info - Fill GPU cache info using kfd_gpu_cache_info tables
- *	@kdev - [IN] GPU device
- *	@gpu_processor_id - [IN] GPU processor ID to which these caches associate
- *	@available_size - [IN] Amount of memory available in pcache
- *	@cu_info - [IN] Compute Unit info obtained from KGD
- *	@pcache - [OUT] memory into which cache data is to be filled in.
- *	@size_filled - [OUT] amount of data used up in pcache.
- *	@num_of_entries - [OUT] number of caches added
- */
-static int kfd_fill_gpu_cache_info(struct kfd_dev *kdev,
-			int gpu_processor_id,
-			int available_size,
-			struct kfd_cu_info *cu_info,
-			struct crat_subtype_cache *pcache,
-			int *size_filled,
-			int *num_of_entries)
-{
-	struct kfd_gpu_cache_info *pcache_info;
-	int num_of_cache_types = 0;
-	int i, j, k;
-	int ct = 0;
-	int mem_available = available_size;
-	unsigned int cu_processor_id;
-	int ret;
-
-	switch (kdev->device_info->asic_family) {
-	case CHIP_KAVERI:
-		pcache_info = kaveri_cache_info;
-		num_of_cache_types = ARRAY_SIZE(kaveri_cache_info);
-		break;
-	case CHIP_HAWAII:
-		pcache_info = hawaii_cache_info;
-		num_of_cache_types = ARRAY_SIZE(hawaii_cache_info);
-		break;
-	case CHIP_CARRIZO:
-		pcache_info = carrizo_cache_info;
-		num_of_cache_types = ARRAY_SIZE(carrizo_cache_info);
-		break;
-	case CHIP_TONGA:
-		pcache_info = tonga_cache_info;
-		num_of_cache_types = ARRAY_SIZE(tonga_cache_info);
-		break;
-	case CHIP_FIJI:
-		pcache_info = fiji_cache_info;
-		num_of_cache_types = ARRAY_SIZE(fiji_cache_info);
-		break;
-	case CHIP_POLARIS10:
-		pcache_info = polaris10_cache_info;
-		num_of_cache_types = ARRAY_SIZE(polaris10_cache_info);
-		break;
-	case CHIP_POLARIS11:
-		pcache_info = polaris11_cache_info;
-		num_of_cache_types = ARRAY_SIZE(polaris11_cache_info);
-		break;
-	default:
-		return -EINVAL;
-	}
-
-	*size_filled = 0;
-	*num_of_entries = 0;
-
-	/* For each type of cache listed in the kfd_gpu_cache_info table,
-	 * go through all available Compute Units.
-	 * The [i,j,k] loop will
-	 *		if kfd_gpu_cache_info.num_cu_shared = 1
-	 *			will parse through all available CU
-	 *		If (kfd_gpu_cache_info.num_cu_shared != 1)
-	 *			then it will consider only one CU from
-	 *			the shared unit
-	 */
-
-	for (ct = 0; ct < num_of_cache_types; ct++) {
-		cu_processor_id = gpu_processor_id;
-		for (i = 0; i < cu_info->num_shader_engines; i++) {
-			for (j = 0; j < cu_info->num_shader_arrays_per_engine;
-				j++) {
-				for (k = 0; k < cu_info->num_cu_per_sh;
-					k += pcache_info[ct].num_cu_shared) {
-
-					ret = fill_in_pcache(pcache,
-						pcache_info,
-						cu_info,
-						mem_available,
-						cu_info->cu_bitmap[i][j],
-						ct,
-						cu_processor_id,
-						k);
-
-					if (ret < 0)
-						break;
-
-					if (!ret) {
-						pcache++;
-						(*num_of_entries)++;
-						mem_available -=
-							sizeof(*pcache);
-						(*size_filled) +=
-							sizeof(*pcache);
-					}
-
-					/* Move to next CU block */
-					cu_processor_id +=
-						pcache_info[ct].num_cu_shared;
-				}
-			}
-		}
-	}
-
-	pr_debug("Added [%d] GPU cache entries\n", *num_of_entries);
-
-	return 0;
-}
-
-/*
- * kfd_create_crat_image_acpi - Allocates memory for CRAT image and
- *		copies CRAT from ACPI (if available).
- *
- * NOTE: Call kfd_destroy_crat_image to free CRAT image memory
- *
- * @crat_image: CRAT read from ACPI. If no CRAT in ACPI then
- *				*crat_image will be NULL
- * @size: [OUT] size of crat_image
- *
- * Return 0 if successful else return -ve value
- */
-int kfd_create_crat_image_acpi(void **crat_image, size_t *size)
-{
-	struct acpi_table_header *crat_table;
-	acpi_status status;
-	void *pcrat_image;
-
-	if (!crat_image)
-		return -EINVAL;
-
-	*crat_image = NULL;
-
-	/*
-	 * Fetch the CRAT table from ACPI
-	 */
-	status = acpi_get_table(CRAT_SIGNATURE, 0, &crat_table);
-	if (status == AE_NOT_FOUND) {
-		pr_warn("CRAT table not found\n");
-		return -ENODATA;
-	} else if (ACPI_FAILURE(status)) {
-		const char *err = acpi_format_exception(status);
-		pr_err("CRAT table error: %s\n", err);
-		return -EINVAL;
-	}
-
-	pcrat_image = kmalloc(crat_table->length, GFP_KERNEL);
-	if (!pcrat_image) {
-		pr_err("No memory for allocating CRAT image\n");
-		return -ENOMEM;
-	}
-
-	memcpy(pcrat_image, crat_table, crat_table->length);
-
-	*crat_image = pcrat_image;
-	*size = crat_table->length;
-
-	return 0;
-}
-
-/* Memory required to create Virtual CRAT.
- * Since there is no easy way to predict the amount of memory required, the
- * following amount are allocated for CPU and GPU Virtual CRAT. This is
- * expected to cover all known conditions. But to be safe additional check
- * is put in the code to ensure we don't overwrite.
- */
-#define VCRAT_SIZE_FOR_CPU	PAGE_SIZE
-#define VCRAT_SIZE_FOR_GPU	(3 * PAGE_SIZE)
-
-/* kfd_fill_cu_for_cpu - Fill in Compute info for the given CPU NUMA node
- *
- *  @numa_node_id: CPU NUMA node id
- *  @avail_size: Available size in the memory
- *  @sub_type_hdr: Memory into which compute info will be filled in
- *
- *  Return 0 if successful else return -ve value
- */
-static int kfd_fill_cu_for_cpu(int numa_node_id, int *avail_size,
-				int proximity_domain,
-				struct crat_subtype_computeunit *sub_type_hdr)
-{
-	const struct cpumask *cpumask;
-
-	*avail_size -= sizeof(struct crat_subtype_computeunit);
-	if (*avail_size < 0)
-		return -ENOMEM;
-
-	memset(sub_type_hdr, 0, sizeof(struct crat_subtype_computeunit));
-
-	/* Fill in subtype header data */
-	sub_type_hdr->type = CRAT_SUBTYPE_COMPUTEUNIT_AFFINITY;
-	sub_type_hdr->length = sizeof(struct crat_subtype_computeunit);
-	sub_type_hdr->flags = CRAT_SUBTYPE_FLAGS_ENABLED;
-
-	cpumask = cpumask_of_node(numa_node_id);
-
-	/* Fill in CU data */
-	sub_type_hdr->flags |= CRAT_CU_FLAGS_CPU_PRESENT;
-	sub_type_hdr->proximity_domain = proximity_domain;
-	sub_type_hdr->processor_id_low = kfd_numa_node_to_apic_id(numa_node_id);
-	if (sub_type_hdr->processor_id_low == -1)
-		return -EINVAL;
-
-	sub_type_hdr->num_cpu_cores = cpumask_weight(cpumask);
-
-	return 0;
-}
-
-/* kfd_fill_mem_info_for_cpu - Fill in Memory info for the given CPU NUMA node
- *
- *  @numa_node_id: CPU NUMA node id
- *  @avail_size: Available size in the memory
- *  @sub_type_hdr: Memory into which compute info will be filled in
- *
- *  Return 0 if successful else return -ve value
- */
-static int kfd_fill_mem_info_for_cpu(int numa_node_id, int *avail_size,
-			int proximity_domain,
-			struct crat_subtype_memory *sub_type_hdr)
-{
-	uint64_t mem_in_bytes = 0;
-	pg_data_t *pgdat;
-	int zone_type;
-
-	*avail_size -= sizeof(struct crat_subtype_computeunit);
-	if (*avail_size < 0)
-		return -ENOMEM;
-
-	memset(sub_type_hdr, 0, sizeof(struct crat_subtype_memory));
-
-	/* Fill in subtype header data */
-	sub_type_hdr->type = CRAT_SUBTYPE_MEMORY_AFFINITY;
-	sub_type_hdr->length = sizeof(struct crat_subtype_memory);
-	sub_type_hdr->flags = CRAT_SUBTYPE_FLAGS_ENABLED;
-
-	/* Fill in Memory Subunit data */
-
-	/* Unlike si_meminfo, si_meminfo_node is not exported. So
-	 * the following lines are duplicated from si_meminfo_node
-	 * function */
-	pgdat = NODE_DATA(numa_node_id);
-	for (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)
-		mem_in_bytes += pgdat->node_zones[zone_type].managed_pages;
-	mem_in_bytes <<= PAGE_SHIFT;
-
-	sub_type_hdr->length_low = lower_32_bits(mem_in_bytes);
-	sub_type_hdr->length_high = upper_32_bits(mem_in_bytes);
-	sub_type_hdr->proximity_domain = proximity_domain;
-
-	return 0;
-}
-
-/* kfd_create_vcrat_image_cpu - Create Virtual CRAT for CPU
- *
- *	@pcrat_image: Fill in VCRAT for CPU
- *	@size:	[IN] allocated size of crat_image.
- *			[OUT] actual size of data filled in crat_image
- */
-static int kfd_create_vcrat_image_cpu(void *pcrat_image, size_t *size)
-{
-	struct crat_header *crat_table = (struct crat_header *)pcrat_image;
-	struct acpi_table_header *acpi_table;
-	acpi_status status;
-	struct crat_subtype_generic *sub_type_hdr;
-	int avail_size = *size;
-	int numa_node_id;
-	int ret = 0;
-
-	if (pcrat_image == NULL || avail_size < VCRAT_SIZE_FOR_CPU)
-		return -EINVAL;
-
-	/* Fill in CRAT Header.
-	 * Modify length and total_entries as subunits are added.
-	 */
-	avail_size -= sizeof(struct crat_header);
-	if (avail_size < 0)
-		return -ENOMEM;
-
-	memset(crat_table, 0, sizeof(struct crat_header));
-	memcpy(&crat_table->signature, CRAT_SIGNATURE, sizeof(crat_table->signature));
-	crat_table->length = sizeof(struct crat_header);
-
-	status = acpi_get_table("DSDT", 0, &acpi_table);
-	if (status == AE_NOT_FOUND)
-		pr_warn("DSDT table not found for OEM information\n");
-	else {
-		crat_table->oem_revision = acpi_table->revision;
-		memcpy(crat_table->oem_id, acpi_table->oem_id, CRAT_OEMID_LENGTH);
-		memcpy(crat_table->oem_table_id, acpi_table->oem_table_id, CRAT_OEMTABLEID_LENGTH);
-	}
-	crat_table->total_entries = 0;
-	crat_table->num_domains = 0;
-
-	sub_type_hdr = (struct crat_subtype_generic *)(crat_table+1);
-
-	for_each_online_node(numa_node_id) {
-		if (kfd_numa_node_to_apic_id(numa_node_id) == -1)
-			continue;
-
-		/* Fill in Subtype: Compute Unit */
-		ret = kfd_fill_cu_for_cpu(numa_node_id, &avail_size,
-			crat_table->num_domains,
-			(struct crat_subtype_computeunit *)sub_type_hdr);
-		if (ret < 0)
-			return ret;
-		crat_table->length += sub_type_hdr->length;
-		crat_table->total_entries++;
-
-		sub_type_hdr = (typeof(sub_type_hdr))((char *)sub_type_hdr +
-			sub_type_hdr->length);
-
-		/* Fill in Subtype: Memory */
-		ret = kfd_fill_mem_info_for_cpu(numa_node_id, &avail_size,
-			crat_table->num_domains,
-			(struct crat_subtype_memory *)sub_type_hdr);
-		if (ret < 0)
-			return ret;
-		crat_table->length += sub_type_hdr->length;
-		crat_table->total_entries++;
-
-		sub_type_hdr = (typeof(sub_type_hdr))((char *)sub_type_hdr +
-			sub_type_hdr->length);
-
-		crat_table->num_domains++;
-	}
-
-	/* TODO: Add cache Subtype for CPU.
-	 * Currently, CPU cache information is available in function
-	 * detect_cache_attributes(cpu) defined in the file
-	 * ./arch/x86/kernel/cpu/intel_cacheinfo.c. This function is not exported
-	 * and to get the same information the code needs to be duplicated.
-	 */
-
-	*size = crat_table->length;
-	pr_info("Virtual CRAT table created for CPU\n");
-
-	return 0;
-}
-
-static int kfd_fill_gpu_memory_affinity(int *avail_size,
-		struct kfd_dev *kdev, uint8_t type, uint64_t size,
-		struct crat_subtype_memory *sub_type_hdr,
-		uint32_t proximity_domain,
-		const struct kfd_local_mem_info *local_mem_info)
-{
-	*avail_size -= sizeof(struct crat_subtype_memory);
-	if (*avail_size < 0)
-		return -ENOMEM;
-
-	memset((void *)sub_type_hdr, 0, sizeof(struct crat_subtype_memory));
-	sub_type_hdr->type = CRAT_SUBTYPE_MEMORY_AFFINITY;
-	sub_type_hdr->length = sizeof(struct crat_subtype_memory);
-	sub_type_hdr->flags |= CRAT_SUBTYPE_FLAGS_ENABLED;
-
-	sub_type_hdr->proximity_domain = proximity_domain;
-
-	pr_debug("amdkfd: fill gpu memory affinity - type 0x%x size 0x%llx\n",
-			type, size);
-
-	sub_type_hdr->length_low = lower_32_bits(size);
-	sub_type_hdr->length_high = upper_32_bits(size);
-
-	sub_type_hdr->width = local_mem_info->vram_width;
-	sub_type_hdr->visibility_type = type;
-
-	return 0;
-}
-
-/* kfd_fill_gpu_direct_io_link - Fill in direct io link from GPU
- *	to its NUMA node
- *
- *  @avail_size: Available size in the memory
- *  @kdev - [IN] GPU device
- *  @sub_type_hdr: Memory into which io link info will be filled in
- *  @proximity_domain - proximity domain of the GPU node
- *
- *  Return 0 if successful else return -ve value
- */
-static int kfd_fill_gpu_direct_io_link(int *avail_size,
-			struct kfd_dev *kdev,
-			struct crat_subtype_iolink *sub_type_hdr,
-			uint32_t proximity_domain)
-{
-	int proximity_domain_to;
-	*avail_size -= sizeof(struct crat_subtype_iolink);
-	if (*avail_size < 0)
-		return -ENOMEM;
-
-	memset((void *)sub_type_hdr, 0, sizeof(struct crat_subtype_iolink));
-
-	/* Fill in subtype header data */
-	sub_type_hdr->type = CRAT_SUBTYPE_IOLINK_AFFINITY;
-	sub_type_hdr->length = sizeof(struct crat_subtype_iolink);
-	sub_type_hdr->flags |= CRAT_SUBTYPE_FLAGS_ENABLED;
-
-	/* Fill in IOLINK subtype.
-	 * TODO: Fill-in other fields of iolink subtype */
-	sub_type_hdr->io_interface_type = CRAT_IOLINK_TYPE_PCIEXPRESS;
-	sub_type_hdr->proximity_domain_from = proximity_domain;
-	proximity_domain_to =
-		kfd_get_proximity_domain(kdev->pdev->bus);
-	if (proximity_domain_to == -1)
-		return -EINVAL;
-
-	sub_type_hdr->proximity_domain_to = proximity_domain_to;
-	return 0;
-}
-
-/* kfd_create_vcrat_image_gpu - Create Virtual CRAT for CPU
- *
- *	@pcrat_image: Fill in VCRAT for GPU
- *	@size:	[IN] allocated size of crat_image.
- *		[OUT] actual size of data filled in crat_image
- */
-static int kfd_create_vcrat_image_gpu(void *pcrat_image,
-			size_t *size, struct kfd_dev *kdev,
-			uint32_t proximity_domain)
-{
-	struct crat_header *crat_table = (struct crat_header *)pcrat_image;
-	struct crat_subtype_generic *sub_type_hdr;
-	struct crat_subtype_computeunit *cu;
-	struct kfd_cu_info cu_info;
-	struct amd_iommu_device_info iommu_info;
-	int avail_size = *size;
-	uint32_t total_num_of_cu;
-	int num_of_cache_entries = 0;
-	int cache_mem_filled = 0;
-	int ret = 0;
-	const u32 required_iommu_flags = AMD_IOMMU_DEVICE_FLAG_ATS_SUP |
-								AMD_IOMMU_DEVICE_FLAG_PRI_SUP |
-								AMD_IOMMU_DEVICE_FLAG_PASID_SUP;
-	struct kfd_local_mem_info local_mem_info;
-
-	if (pcrat_image == NULL || avail_size < VCRAT_SIZE_FOR_GPU)
-		return -EINVAL;
-
-	/* Fill the CRAT Header.
-	 * Modify length and total_entries as subunits are added.
-	 */
-	avail_size -= sizeof(struct crat_header);
-	if (avail_size < 0)
-		return -ENOMEM;
-
-	memset(crat_table, 0, sizeof(struct crat_header));
-
-	memcpy(&crat_table->signature, CRAT_SIGNATURE, sizeof(crat_table->signature));
-	crat_table->length = sizeof(struct crat_header); /* Change length as we add more subtypes*/
-	crat_table->num_domains = 1;
-	crat_table->total_entries = 0;
-
-	/* Fill in Subtype: Compute Unit
-	 * First fill in the sub type header and then sub type data
-	 */
-	avail_size -= sizeof(struct crat_subtype_computeunit);
-	if (avail_size < 0)
-		return -ENOMEM;
-
-	sub_type_hdr = (struct crat_subtype_generic *)(crat_table + 1);
-	memset(sub_type_hdr, 0, sizeof(struct crat_subtype_computeunit));
-
-	sub_type_hdr->type = CRAT_SUBTYPE_COMPUTEUNIT_AFFINITY;
-	sub_type_hdr->length = sizeof(struct crat_subtype_computeunit);
-	sub_type_hdr->flags = CRAT_SUBTYPE_FLAGS_ENABLED;
-
-	/* Fill CU subtype data */
-	cu = (struct crat_subtype_computeunit *)sub_type_hdr;
-	cu->flags |= CRAT_CU_FLAGS_GPU_PRESENT;
-	cu->proximity_domain = proximity_domain;
-
-	kdev->kfd2kgd->get_cu_info(kdev->kgd, &cu_info);
-	cu->num_simd_per_cu = cu_info.simd_per_cu;
-	cu->num_simd_cores = cu_info.simd_per_cu * cu_info.cu_active_number;
-	cu->max_waves_simd = cu_info.max_waves_per_simd;
-
-	cu->wave_front_size = cu_info.wave_front_size;
-	cu->array_count = cu_info.num_shader_arrays_per_engine *
-		cu_info.num_shader_engines;
-	total_num_of_cu = (cu->array_count * cu_info.num_cu_per_sh);
-	cu->processor_id_low = get_and_inc_gpu_processor_id(total_num_of_cu);
-	cu->num_cu_per_array = cu_info.num_cu_per_sh;
-	cu->max_slots_scatch_cu = cu_info.max_scratch_slots_per_cu;
-	cu->num_banks = cu_info.num_shader_engines;
-	cu->lds_size_in_kb = cu_info.lds_size;
-
-	cu->hsa_capability = 0;
-
-	/* Check if this node supports IOMMU. During parsing this flag will
-	 * translate to HSA_CAP_ATS_PRESENT */
-	iommu_info.flags = 0;
-	if (0 == amd_iommu_device_info(kdev->pdev, &iommu_info)) {
-		if ((iommu_info.flags & required_iommu_flags) == required_iommu_flags)
-			cu->hsa_capability |= CRAT_CU_FLAGS_IOMMU_PRESENT;
-	}
-
-	crat_table->length += sub_type_hdr->length;
-	crat_table->total_entries++;
-
-	/* Fill in Subtype: Memory. Only on systems with large BAR (no
-	 * private FB), report memory as public. On other systems
-	 * report the total FB size (public+private) as a single
-	 * private heap. */
-	kdev->kfd2kgd->get_local_mem_info(kdev->kgd, &local_mem_info);
-	sub_type_hdr = (typeof(sub_type_hdr))((char *)sub_type_hdr +
-			sub_type_hdr->length);
-
-	if (debug_largebar)
-		local_mem_info.local_mem_size_private = 0;
-
-	if (local_mem_info.local_mem_size_private == 0)
-		ret = kfd_fill_gpu_memory_affinity(&avail_size,
-				kdev, HSA_MEM_HEAP_TYPE_FB_PUBLIC,
-				local_mem_info.local_mem_size_public,
-				(struct crat_subtype_memory *)sub_type_hdr,
-				proximity_domain,
-				&local_mem_info);
-	else
-		ret = kfd_fill_gpu_memory_affinity(&avail_size,
-				kdev, HSA_MEM_HEAP_TYPE_FB_PRIVATE,
-				local_mem_info.local_mem_size_public +
-				local_mem_info.local_mem_size_private,
-				(struct crat_subtype_memory *)sub_type_hdr,
-				proximity_domain,
-				&local_mem_info);
-	if (ret < 0)
-		return ret;
-
-	crat_table->length += sizeof(struct crat_subtype_memory);
-	crat_table->total_entries++;
-
-	/* TODO: Fill in cache information. This information is NOT readily
-	 * available in KGD */
-	sub_type_hdr = (typeof(sub_type_hdr))((char *)sub_type_hdr +
-		sub_type_hdr->length);
-	ret = kfd_fill_gpu_cache_info(kdev, cu->processor_id_low,
-				avail_size,
-				&cu_info,
-				(struct crat_subtype_cache *)sub_type_hdr,
-				&cache_mem_filled,
-				&num_of_cache_entries);
-
-	if (ret < 0)
-		return ret;
-
-	crat_table->length += cache_mem_filled;
-	crat_table->total_entries += num_of_cache_entries;
-	avail_size -= cache_mem_filled;
-
-	/* Fill in Subtype: IO_LINKS
-	 *  Only direct links are added here which is Link from GPU to
-	 *  to its NUMA node. Indirect links are added by userspace.
-	 */
-	sub_type_hdr = (typeof(sub_type_hdr))((char *)sub_type_hdr +
-		cache_mem_filled);
-	ret = kfd_fill_gpu_direct_io_link(&avail_size, kdev,
-		(struct crat_subtype_iolink *)sub_type_hdr, proximity_domain);
-
-	if (ret < 0)
-		return ret;
-
-	crat_table->length += sub_type_hdr->length;
-	crat_table->total_entries++;
-
-	*size = crat_table->length;
-	pr_info("Virtual CRAT table created for GPU\n");
-
-	return ret;
-}
-
-/* kfd_create_crat_image_virtual - Allocates memory for CRAT image and
- *		creates a Virtual CRAT (VCRAT) image
- *
- * NOTE: Call kfd_destroy_crat_image to free CRAT image memory
- *
- *	@crat_image: VCRAT image created because ACPI does not have a
- *		CRAT for this device
- *	@size: [OUT] size of virtual crat_image
- *	@flags:	COMPUTE_UNIT_CPU - Create VCRAT for CPU device
- *			COMPUTE_UNIT_GPU - Create VCRAT for GPU
- *			(COMPUTE_UNIT_CPU | COMPUTE_UNIT_GPU) - Create VCRAT for APU
- *				-- this option is not currently implemented. The assumption
- *				   is that all AMD APUs will have CRAT
- *	@kdev: Valid kfd_device required if flags contain COMPUTE_UNIT_GPU
- *
- * Return 0 if successful else return -ve value
-*/
-int kfd_create_crat_image_virtual(void **crat_image, size_t *size,
-		int flags, struct kfd_dev *kdev, uint32_t proximity_domain)
-{
-	void *pcrat_image;
-	int ret = 0;
-
-	if (!crat_image)
-		return -EINVAL;
-
-	*crat_image = NULL;
-
-	/* Allocate one VCRAT_SIZE_FOR_CPU for CPU virtual CRAT image and
-	 * VCRAT_SIZE_FOR_GPU for GPU virtual CRAT image. This should cover
-	 * all the current conditions. A check is put not to overwrite beyond
-	 * allocated size
-	 */
-	switch (flags) {
-	case COMPUTE_UNIT_CPU:
-		pcrat_image = kmalloc(VCRAT_SIZE_FOR_CPU, GFP_KERNEL);
-		if (!pcrat_image)
-			return -ENOMEM;
-		*size = VCRAT_SIZE_FOR_CPU;
-		ret = kfd_create_vcrat_image_cpu(pcrat_image, size);
-		break;
-	case COMPUTE_UNIT_GPU:
-		if (!kdev)
-			return -EINVAL;
-		pcrat_image = kmalloc(VCRAT_SIZE_FOR_GPU, GFP_KERNEL);
-		if (!pcrat_image)
-			return -ENOMEM;
-		*size = VCRAT_SIZE_FOR_GPU;
-		ret = kfd_create_vcrat_image_gpu(pcrat_image, size,
-				kdev, proximity_domain);
-		break;
-	case (COMPUTE_UNIT_CPU | COMPUTE_UNIT_GPU) :
-		/*TODO:*/
-		ret = -EINVAL;
-		pr_err("VCRAT not implemented for APU\n");
-		break;
-	default:
-		ret = -EINVAL;
-	}
-
-	if (ret == 0)
-		*crat_image = pcrat_image;
-
-	return ret;
-}
-
-
-/* kfd_destroy_crat_image
- *
- * @crat_image: [IN] - crat_image from kfd_create_crat_image_xxx(..)
- *
- */
-void kfd_destroy_crat_image(void *crat_image)
-{
-	if (crat_image)
-		kfree(crat_image);
-	return;
-}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_crat.h b/drivers/gpu/drm/amd/amdkfd/kfd_crat.h
index 9af3745..a374fa3 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_crat.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_crat.h
@@ -24,7 +24,6 @@
 #define KFD_CRAT_H_INCLUDED
 
 #include <linux/types.h>
-#include "kfd_priv.h"
 
 #pragma pack(1)
 
@@ -45,10 +44,6 @@
 
 #define CRAT_OEMID_64BIT_MASK ((1ULL << (CRAT_OEMID_LENGTH * 8)) - 1)
 
-/* Compute Unit flags */
-#define COMPUTE_UNIT_CPU	(1 << 0)	/* Create Virtual CRAT for CPU */
-#define COMPUTE_UNIT_GPU	(1 << 1)	/* Create Virtual CRAT for GPU */
-
 struct crat_header {
 	uint32_t	signature;
 	uint32_t	length;
@@ -110,7 +105,7 @@ struct crat_subtype_computeunit {
 	uint8_t		wave_front_size;
 	uint8_t		num_banks;
 	uint16_t	micro_engine_id;
-	uint8_t		array_count;
+	uint8_t		num_arrays;
 	uint8_t		num_cu_per_array;
 	uint8_t		num_simd_per_cu;
 	uint8_t		max_slots_scatch_cu;
@@ -132,14 +127,13 @@ struct crat_subtype_memory {
 	uint8_t		length;
 	uint16_t	reserved;
 	uint32_t	flags;
-	uint32_t	proximity_domain;
+	uint32_t	promixity_domain;
 	uint32_t	base_addr_low;
 	uint32_t	base_addr_high;
 	uint32_t	length_low;
 	uint32_t	length_high;
 	uint32_t	width;
-	uint8_t		visibility_type; /* for virtual (dGPU) CRAT */
-	uint8_t		reserved2[CRAT_MEMORY_RESERVED_LENGTH - 1];
+	uint8_t		reserved2[CRAT_MEMORY_RESERVED_LENGTH];
 };
 
 /*
@@ -228,12 +222,9 @@ struct crat_subtype_ccompute {
 /*
  * HSA IO Link Affinity structure and definitions
  */
-#define CRAT_IOLINK_FLAGS_ENABLED	(1 << 0)
-#define CRAT_IOLINK_FLAGS_NON_COHERENT	(1 << 1)
-#define CRAT_IOLINK_FLAGS_NO_ATOMICS_32_BIT (1 << 2)
-#define CRAT_IOLINK_FLAGS_NO_ATOMICS_64_BIT (1 << 3)
-#define CRAT_IOLINK_FLAGS_NO_PEER_TO_PEER_DMA (1 << 4)
-#define CRAT_IOLINK_FLAGS_RESERVED_MASK 0xffffffe0
+#define CRAT_IOLINK_FLAGS_ENABLED	0x00000001
+#define CRAT_IOLINK_FLAGS_COHERENCY	0x00000002
+#define CRAT_IOLINK_FLAGS_RESERVED	0xfffffffc
 
 /*
  * IO interface types
@@ -241,16 +232,8 @@ struct crat_subtype_ccompute {
 #define CRAT_IOLINK_TYPE_UNDEFINED	0
 #define CRAT_IOLINK_TYPE_HYPERTRANSPORT	1
 #define CRAT_IOLINK_TYPE_PCIEXPRESS	2
-#define CRAT_IOLINK_TYPE_AMBA 3
-#define CRAT_IOLINK_TYPE_MIPI 4
-#define CRAT_IOLINK_TYPE_QPI_1_1 5
-#define CRAT_IOLINK_TYPE_RESERVED1 6
-#define CRAT_IOLINK_TYPE_RESERVED2 7
-#define CRAT_IOLINK_TYPE_RAPID_IO 8
-#define CRAT_IOLINK_TYPE_INFINIBAND 9
-#define CRAT_IOLINK_TYPE_RESERVED3 10
-#define CRAT_IOLINK_TYPE_OTHER 11
-#define CRAT_IOLINK_TYPE_MAX 255
+#define CRAT_IOLINK_TYPE_OTHER		3
+#define CRAT_IOLINK_TYPE_MAX		255
 
 #define CRAT_IOLINK_RESERVED_LENGTH 24
 
@@ -308,11 +291,4 @@ struct cdit_header {
 
 #pragma pack()
 
-int kfd_create_crat_image_acpi(void **crat_image, size_t *size);
-void kfd_destroy_crat_image(void *crat_image);
-int kfd_parse_crat_table(void *crat_image,
-		struct list_head *device_list,
-		uint32_t proximity_domain);
-int kfd_create_crat_image_virtual(void **crat_image, size_t *size,
-		int flags, struct kfd_dev *kdev, uint32_t proximity_domain);
 #endif /* KFD_CRAT_H_INCLUDED */
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
index 9de73ce..d5e19b5 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
@@ -42,135 +42,138 @@
 
 static void dbgdev_address_watch_disable_nodiq(struct kfd_dev *dev)
 {
+	BUG_ON(!dev || !dev->kfd2kgd);
+
 	dev->kfd2kgd->address_watch_disable(dev->kgd);
 }
 
 static int dbgdev_diq_submit_ib(struct kfd_dbgdev *dbgdev,
 				unsigned int pasid, uint64_t vmid0_address,
-				uint32_t *packet_buff, size_t size_in_bytes,
-				bool sync)
+				uint32_t *packet_buff, size_t size_in_bytes)
 {
-	int status = 0;
-	unsigned int *ib_packet_buff = NULL;
 	struct pm4__release_mem *rm_packet;
 	struct pm4__indirect_buffer_pasid *ib_packet;
-	struct kernel_queue *kq = dbgdev->kq;
-	size_t pq_packets_size_in_bytes =
-		sizeof(struct pm4__indirect_buffer_pasid);
 	struct kfd_mem_obj *mem_obj;
-
-	uint64_t *rm_state = NULL;
-
+	size_t pq_packets_size_in_bytes;
 	union ULARGE_INTEGER *largep;
 	union ULARGE_INTEGER addr;
+	struct kernel_queue *kq;
+	uint64_t *rm_state;
+	unsigned int *ib_packet_buff;
+	int status;
+
+	BUG_ON(!dbgdev || !dbgdev->kq || !packet_buff || !size_in_bytes);
+
+	kq = dbgdev->kq;
+
+	pq_packets_size_in_bytes = sizeof(struct pm4__release_mem) +
+				sizeof(struct pm4__indirect_buffer_pasid);
+
+	/*
+	 * We acquire a buffer from DIQ
+	 * The receive packet buff will be sitting on the Indirect Buffer
+	 * and in the PQ we put the IB packet + sync packet(s).
+	 */
+	status = kq->ops.acquire_packet_buffer(kq,
+				pq_packets_size_in_bytes / sizeof(uint32_t),
+				&ib_packet_buff);
+	if (status != 0) {
+		pr_err("amdkfd: acquire_packet_buffer failed\n");
+		return status;
+	}
 
-	do {
-		if ((kq == NULL) || (packet_buff == NULL) || (size_in_bytes == 0)) {
-			pr_debug("Error! kfd: In func %s >> Illegal packet parameters\n", __func__);
-			status = -EINVAL;
-			break;
-		}
-		/* todo - enter proper locking to be multithreaded safe */
-
-		/* We acquire a buffer from DIQ
-		 * The receive packet buff will be sitting on the Indirect Buffer
-		 * and in the PQ we put the IB packet + sync packet(s).
-		 */
-		if (sync)
-			pq_packets_size_in_bytes +=
-				sizeof(struct pm4__release_mem);
-		status = kq->ops.acquire_packet_buffer(kq, pq_packets_size_in_bytes / sizeof(uint32_t), &ib_packet_buff);
-		if (status != 0) {
-			pr_debug("Error! kfd: In func %s >> acquire_packet_buffer failed\n", __func__);
-			break;
-		}
-
-		memset(ib_packet_buff, 0, pq_packets_size_in_bytes);
+	memset(ib_packet_buff, 0, pq_packets_size_in_bytes);
 
-		ib_packet = (struct pm4__indirect_buffer_pasid *) (ib_packet_buff);
+	ib_packet = (struct pm4__indirect_buffer_pasid *) (ib_packet_buff);
 
-		ib_packet->header.count = 3;
-		ib_packet->header.opcode = IT_INDIRECT_BUFFER_PASID;
-		ib_packet->header.type = PM4_TYPE_3;
+	ib_packet->header.count = 3;
+	ib_packet->header.opcode = IT_INDIRECT_BUFFER_PASID;
+	ib_packet->header.type = PM4_TYPE_3;
 
-		largep = (union ULARGE_INTEGER *) &vmid0_address;
+	largep = (union ULARGE_INTEGER *) &vmid0_address;
 
-		ib_packet->bitfields2.ib_base_lo = largep->u.low_part >> 2;
-		ib_packet->bitfields3.ib_base_hi = largep->u.high_part;
+	ib_packet->bitfields2.ib_base_lo = largep->u.low_part >> 2;
+	ib_packet->bitfields3.ib_base_hi = largep->u.high_part;
 
-		ib_packet->control = (1 << 23) | (1 << 31) |
-				((size_in_bytes / sizeof(uint32_t)) & 0xfffff);
+	ib_packet->control = (1 << 23) | (1 << 31) |
+			((size_in_bytes / sizeof(uint32_t)) & 0xfffff);
 
-		ib_packet->bitfields5.pasid = pasid;
+	ib_packet->bitfields5.pasid = pasid;
 
-		if (!sync) {
-			kq->ops.submit_packet(kq);
-			break;
-		}
+	/*
+	 * for now we use release mem for GPU-CPU synchronization
+	 * Consider WaitRegMem + WriteData as a better alternative
+	 * we get a GART allocations ( gpu/cpu mapping),
+	 * for the sync variable, and wait until:
+	 * (a) Sync with HW
+	 * (b) Sync var is written by CP to mem.
+	 */
+	rm_packet = (struct pm4__release_mem *) (ib_packet_buff +
+			(sizeof(struct pm4__indirect_buffer_pasid) /
+					sizeof(unsigned int)));
 
-		/*
-		 * for now we use release mem for GPU-CPU synchronization
-		 * Consider WaitRegMem + WriteData as a better alternative
-		 * we get a GART allocations ( gpu/cpu mapping),
-		 * for the sync variable, and wait until:
-		 * (a) Sync with HW
-		 * (b) Sync var is written by CP to mem.
-		 */
-		rm_packet = (struct pm4__release_mem *) (ib_packet_buff +
-				(sizeof(struct pm4__indirect_buffer_pasid) / sizeof(unsigned int)));
+	status = kfd_gtt_sa_allocate(dbgdev->dev, sizeof(uint64_t),
+					&mem_obj);
 
-		status = kfd_gtt_sa_allocate(dbgdev->dev, sizeof(uint64_t),
-						&mem_obj);
+	if (status != 0) {
+		pr_err("amdkfd: Failed to allocate GART memory\n");
+		kq->ops.rollback_packet(kq);
+		return status;
+	}
 
-		if (status == 0) {
+	rm_state = (uint64_t *) mem_obj->cpu_ptr;
 
-			rm_state = (uint64_t *) mem_obj->cpu_ptr;
+	*rm_state = QUEUESTATE__ACTIVE_COMPLETION_PENDING;
 
-			*rm_state = QUEUESTATE__ACTIVE_COMPLETION_PENDING;
+	rm_packet->header.opcode = IT_RELEASE_MEM;
+	rm_packet->header.type = PM4_TYPE_3;
+	rm_packet->header.count = sizeof(struct pm4__release_mem) /
+					sizeof(unsigned int) - 2;
 
-			rm_packet->header.opcode = IT_RELEASE_MEM;
-			rm_packet->header.type = PM4_TYPE_3;
-			rm_packet->header.count = sizeof(struct pm4__release_mem) / sizeof(unsigned int) - 2;
+	rm_packet->bitfields2.event_type = CACHE_FLUSH_AND_INV_TS_EVENT;
+	rm_packet->bitfields2.event_index =
+				event_index___release_mem__end_of_pipe;
 
-			rm_packet->bitfields2.event_type = CACHE_FLUSH_AND_INV_TS_EVENT;
-			rm_packet->bitfields2.event_index = event_index___release_mem__end_of_pipe;
-			rm_packet->bitfields2.cache_policy = cache_policy___release_mem__lru;
-			rm_packet->bitfields2.atc = 0;
-			rm_packet->bitfields2.tc_wb_action_ena = 1;
+	rm_packet->bitfields2.cache_policy = cache_policy___release_mem__lru;
+	rm_packet->bitfields2.atc = 0;
+	rm_packet->bitfields2.tc_wb_action_ena = 1;
 
-			addr.quad_part = mem_obj->gpu_addr;
+	addr.quad_part = mem_obj->gpu_addr;
 
-			rm_packet->bitfields4.address_lo_32b = addr.u.low_part >> 2;
-			rm_packet->address_hi = addr.u.high_part;
+	rm_packet->bitfields4.address_lo_32b = addr.u.low_part >> 2;
+	rm_packet->address_hi = addr.u.high_part;
 
-			rm_packet->bitfields3.data_sel = data_sel___release_mem__send_64_bit_data;
-			rm_packet->bitfields3.int_sel = int_sel___release_mem__send_data_after_write_confirm;
-			rm_packet->bitfields3.dst_sel = dst_sel___release_mem__memory_controller;
+	rm_packet->bitfields3.data_sel =
+				data_sel___release_mem__send_64_bit_data;
 
-			rm_packet->data_lo = QUEUESTATE__ACTIVE;
+	rm_packet->bitfields3.int_sel =
+			int_sel___release_mem__send_data_after_write_confirm;
 
-			kq->ops.submit_packet(kq);
+	rm_packet->bitfields3.dst_sel =
+			dst_sel___release_mem__memory_controller;
 
-			/* Wait till CP writes sync code: */
+	rm_packet->data_lo = QUEUESTATE__ACTIVE;
 
-			status = amdkfd_fence_wait_timeout(
-					(unsigned int *) rm_state,
-					QUEUESTATE__ACTIVE, 1500);
+	kq->ops.submit_packet(kq);
 
-		} else {
-			pr_debug("Error! kfd: In func %s >> failed to allocate GART memory\n", __func__);
-		}
-	} while (false);
+	/* Wait till CP writes sync code: */
+	status = amdkfd_fence_wait_timeout(
+			(unsigned int *) rm_state,
+			QUEUESTATE__ACTIVE, 1500);
 
-	if (rm_state != NULL)
-		kfd_gtt_sa_free(dbgdev->dev, mem_obj);
+	kfd_gtt_sa_free(dbgdev->dev, mem_obj);
 
 	return status;
 }
 
 static int dbgdev_register_nodiq(struct kfd_dbgdev *dbgdev)
 {
-	/* no action is needed in this case, just make sure diq will not be used */
+	BUG_ON(!dbgdev);
+
+	/*
+	 * no action is needed in this case,
+	 * just make sure diq will not be used
+	 */
 
 	dbgdev->kq = NULL;
 
@@ -179,68 +182,57 @@ static int dbgdev_register_nodiq(struct kfd_dbgdev *dbgdev)
 
 static int dbgdev_register_diq(struct kfd_dbgdev *dbgdev)
 {
-
-	int status = 0;
-	struct kernel_queue *kq = NULL;
 	struct queue_properties properties;
 	unsigned int qid;
-	struct process_queue_manager *pqm = dbgdev->pqm;
-
-	do {
-
-		if (!pqm) {
-			pr_debug("Error! kfd: In func %s >> No PQM\n", __func__);
-			status = -EFAULT;
-			break;
-		}
-
-		properties.type = KFD_QUEUE_TYPE_DIQ;
+	struct kernel_queue *kq = NULL;
+	int status;
 
-		status = pqm_create_queue(dbgdev->pqm, dbgdev->dev, NULL,
-				&properties, &qid);
+	BUG_ON(!dbgdev || !dbgdev->pqm || !dbgdev->dev);
 
-		if (status != 0) {
-			pr_debug("Error! kfd: In func %s >> Create Queue failed\n", __func__);
-			break;
-		}
+	status = pqm_create_queue(dbgdev->pqm, dbgdev->dev, NULL,
+				&properties, 0, KFD_QUEUE_TYPE_DIQ,
+				&qid);
 
-		pr_debug("kfd: DIQ Created with queue id: %d\n", qid);
+	if (status) {
+		pr_err("amdkfd: Failed to create DIQ\n");
+		return status;
+	}
 
-		kq = pqm_get_kernel_queue(dbgdev->pqm, qid);
+	pr_debug("DIQ Created with queue id: %d\n", qid);
 
-		if (kq == NULL) {
-			pr_debug("Error! kfd: In func %s >> Error getting Kernel Queue\n", __func__);
-			status = -ENOMEM;
-			break;
-		}
+	kq = pqm_get_kernel_queue(dbgdev->pqm, qid);
 
-		dbgdev->kq = kq;
+	if (kq == NULL) {
+		pr_err("amdkfd: Error getting DIQ\n");
+		pqm_destroy_queue(dbgdev->pqm, qid);
+		return -EFAULT;
+	}
 
-	} while (false);
+	dbgdev->kq = kq;
 
 	return status;
 }
 
 static int dbgdev_unregister_nodiq(struct kfd_dbgdev *dbgdev)
 {
-	/* disable watch address */
+	BUG_ON(!dbgdev || !dbgdev->dev);
 
+	/* disable watch address */
 	dbgdev_address_watch_disable_nodiq(dbgdev->dev);
 	return 0;
 }
 
 static int dbgdev_unregister_diq(struct kfd_dbgdev *dbgdev)
 {
-	/* todo - if needed, kill wavefronts and disable watch */
-	int status = 0;
-	if ((dbgdev == NULL) || (dbgdev->pqm == NULL) || (dbgdev->kq == NULL)) {
-		pr_debug("kfd Err:In func %s >> can't destroy diq\n", __func__);
-		status = -EFAULT;
-	} else {
-		pqm_destroy_queue(dbgdev->pqm,
-				dbgdev->kq->queue->properties.queue_id);
-		dbgdev->kq = NULL;
-	}
+	/* todo - disable address watch */
+	int status;
+
+	BUG_ON(!dbgdev || !dbgdev->pqm || !dbgdev->kq);
+
+	status = pqm_destroy_queue(dbgdev->pqm,
+			dbgdev->kq->queue->properties.queue_id);
+	dbgdev->kq = NULL;
+
 	return status;
 }
 
@@ -249,348 +241,341 @@ static void dbgdev_address_watch_set_registers(
 			union TCP_WATCH_ADDR_H_BITS *addrHi,
 			union TCP_WATCH_ADDR_L_BITS *addrLo,
 			union TCP_WATCH_CNTL_BITS *cntl,
-			unsigned int index, unsigned int vmid,
-			unsigned int asic_family)
+			unsigned int index, unsigned int vmid)
 {
 	union ULARGE_INTEGER addr;
 
+	BUG_ON(!adw_info || !addrHi || !addrLo || !cntl);
+
 	addr.quad_part = 0;
 	addrHi->u32All = 0;
 	addrLo->u32All = 0;
 	cntl->u32All = 0;
 
 	if (adw_info->watch_mask != NULL)
-		cntl->bitfields.mask = (uint32_t) (adw_info->watch_mask[index] & ADDRESS_WATCH_REG_CNTL_DEFAULT_MASK);
+		cntl->bitfields.mask =
+			(uint32_t) (adw_info->watch_mask[index] &
+					ADDRESS_WATCH_REG_CNTL_DEFAULT_MASK);
 	else
 		cntl->bitfields.mask = ADDRESS_WATCH_REG_CNTL_DEFAULT_MASK;
 
 	addr.quad_part = (unsigned long long) adw_info->watch_address[index];
 
-	addrHi->bitfields.addr = addr.u.high_part & ADDRESS_WATCH_REG_ADDHIGH_MASK;
+	addrHi->bitfields.addr = addr.u.high_part &
+					ADDRESS_WATCH_REG_ADDHIGH_MASK;
 	addrLo->bitfields.addr =
 			(addr.u.low_part >> ADDRESS_WATCH_REG_ADDLOW_SHIFT);
 
 	cntl->bitfields.mode = adw_info->watch_mode[index];
 	cntl->bitfields.vmid = (uint32_t) vmid;
-	/*  for APU assume it is an ATC address.  */
-	if (KFD_IS_DGPU(asic_family) == false)
-		cntl->u32All |= ADDRESS_WATCH_REG_CNTL_ATC_BIT;
-	pr_debug("\t\t%20s %08x\n", "set reg mask :", cntl->bitfields.mask);
-	pr_debug("\t\t%20s %08x\n", "set reg add high :", addrHi->bitfields.addr);
-	pr_debug("\t\t%20s %08x\n", "set reg add low :", addrLo->bitfields.addr);
+	/* for now assume it is an ATC address */
+	cntl->u32All |= ADDRESS_WATCH_REG_CNTL_ATC_BIT;
 
+	pr_debug("\t\t%20s %08x\n", "set reg mask :", cntl->bitfields.mask);
+	pr_debug("\t\t%20s %08x\n", "set reg add high :",
+			addrHi->bitfields.addr);
+	pr_debug("\t\t%20s %08x\n", "set reg add low :",
+			addrLo->bitfields.addr);
 }
 
 static int dbgdev_address_watch_nodiq(struct kfd_dbgdev *dbgdev,
 					struct dbg_address_watch_info *adw_info)
 {
-
-	int status = 0;
-
 	union TCP_WATCH_ADDR_H_BITS addrHi;
 	union TCP_WATCH_ADDR_L_BITS addrLo;
 	union TCP_WATCH_CNTL_BITS cntl;
-
-	unsigned int vmid;
-	unsigned int i;
-
 	struct kfd_process_device *pdd;
+	unsigned int i;
 
-	do {
-		/* taking the vmid for that process on the safe way using pdd */
-		pdd = kfd_get_process_device_data(dbgdev->dev,
-						adw_info->process);
-		if (!pdd) {
-			pr_debug("Error! kfd: In func %s >> no PDD available\n", __func__);
-			status = -EFAULT;
-			break;
-		}
-
-		addrHi.u32All = 0;
-		addrLo.u32All = 0;
-		cntl.u32All = 0;
+	BUG_ON(!dbgdev || !dbgdev->dev || !adw_info);
 
-		vmid = pdd->qpd.vmid;
+	/* taking the vmid for that process on the safe way using pdd */
+	pdd = kfd_get_process_device_data(dbgdev->dev,
+					adw_info->process);
+	if (!pdd) {
+		pr_err("amdkfd: Failed to get pdd for wave control no DIQ\n");
+		return -EFAULT;
+	}
 
-		if ((adw_info->num_watch_points > MAX_WATCH_ADDRESSES)
-		    || (adw_info->num_watch_points == 0)) {
-			status = -EINVAL;
-			break;
-		}
+	addrHi.u32All = 0;
+	addrLo.u32All = 0;
+	cntl.u32All = 0;
 
-		if ((adw_info->watch_mode == NULL) || (adw_info->watch_address == NULL)) {
-			status = -EINVAL;
-			break;
-		}
+	if ((adw_info->num_watch_points > MAX_WATCH_ADDRESSES) ||
+			(adw_info->num_watch_points == 0)) {
+		pr_err("amdkfd: num_watch_points is invalid\n");
+		return -EINVAL;
+	}
 
-		for (i = 0; i < adw_info->num_watch_points; i++) {
-
-			dbgdev_address_watch_set_registers(
-				adw_info,
-				&addrHi,
-				&addrLo,
-				&cntl,
-				i,
-				vmid,
-				dbgdev->dev->device_info->asic_family
-				);
-
-			pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
-			pr_debug("\t\t%20s %08x\n", "register index :", i);
-			pr_debug("\t\t%20s %08x\n", "vmid is :", vmid);
-			pr_debug("\t\t%20s %08x\n", "Address Low is :", addrLo.bitfields.addr);
-			pr_debug("\t\t%20s %08x\n", "Address high is :", addrHi.bitfields.addr);
-			pr_debug("\t\t%20s %08x\n", "Address high is :", addrHi.bitfields.addr);
-			pr_debug("\t\t%20s %08x\n", "Control Mask is :", cntl.bitfields.mask);
-			pr_debug("\t\t%20s %08x\n", "Control Mode is :", cntl.bitfields.mode);
-			pr_debug("\t\t%20s %08x\n", "Control Vmid is :", cntl.bitfields.vmid);
-			pr_debug("\t\t%20s %08x\n", "Control atc  is :", cntl.bitfields.atc);
-			pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
-
-			pdd->dev->kfd2kgd->address_watch_execute(
-							dbgdev->dev->kgd,
-							i,
-							cntl.u32All,
-							addrHi.u32All,
-							addrLo.u32All);
-		}
+	if ((adw_info->watch_mode == NULL) ||
+		(adw_info->watch_address == NULL)) {
+		pr_err("amdkfd: adw_info fields are not valid\n");
+		return -EINVAL;
+	}
 
-	} while (false);
+	for (i = 0 ; i < adw_info->num_watch_points ; i++) {
+		dbgdev_address_watch_set_registers(adw_info, &addrHi, &addrLo,
+						&cntl, i, pdd->qpd.vmid);
+
+		pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
+		pr_debug("\t\t%20s %08x\n", "register index :", i);
+		pr_debug("\t\t%20s %08x\n", "vmid is :", pdd->qpd.vmid);
+		pr_debug("\t\t%20s %08x\n", "Address Low is :",
+				addrLo.bitfields.addr);
+		pr_debug("\t\t%20s %08x\n", "Address high is :",
+				addrHi.bitfields.addr);
+		pr_debug("\t\t%20s %08x\n", "Address high is :",
+				addrHi.bitfields.addr);
+		pr_debug("\t\t%20s %08x\n", "Control Mask is :",
+				cntl.bitfields.mask);
+		pr_debug("\t\t%20s %08x\n", "Control Mode is :",
+				cntl.bitfields.mode);
+		pr_debug("\t\t%20s %08x\n", "Control Vmid is :",
+				cntl.bitfields.vmid);
+		pr_debug("\t\t%20s %08x\n", "Control atc  is :",
+				cntl.bitfields.atc);
+		pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
+
+		pdd->dev->kfd2kgd->address_watch_execute(
+						dbgdev->dev->kgd,
+						i,
+						cntl.u32All,
+						addrHi.u32All,
+						addrLo.u32All);
+	}
 
-	return status;
+	return 0;
 }
 
 static int dbgdev_address_watch_diq(struct kfd_dbgdev *dbgdev,
 					struct dbg_address_watch_info *adw_info)
 {
-
-	int status = 0;
-	unsigned int i = 0;
+	struct pm4__set_config_reg *packets_vec;
 	union TCP_WATCH_ADDR_H_BITS addrHi;
 	union TCP_WATCH_ADDR_L_BITS addrLo;
 	union TCP_WATCH_CNTL_BITS cntl;
-
+	struct kfd_mem_obj *mem_obj;
+	unsigned int aw_reg_add_dword;
+	uint32_t *packet_buff_uint;
+	unsigned int i;
+	int status;
+	size_t ib_size = sizeof(struct pm4__set_config_reg) * 4;
 	/* we do not control the vmid in DIQ mode, just a place holder */
 	unsigned int vmid = 0;
 
-	uint32_t *packet_buff_uint = NULL;
-	uint64_t packet_buff_gpu_addr = 0;
-
-	struct pm4__set_config_reg *packets_vec = NULL;
-
-	size_t ib_size = sizeof(struct pm4__set_config_reg) * 4;
-
-	unsigned int aw_reg_add_dword;
+	BUG_ON(!dbgdev || !dbgdev->dev || !adw_info);
 
 	addrHi.u32All = 0;
 	addrLo.u32All = 0;
 	cntl.u32All = 0;
 
-	do {
-
-		if ((adw_info->num_watch_points > MAX_WATCH_ADDRESSES) || (adw_info->num_watch_points == 0)) {
-			status = -EINVAL;
-			break;
-		}
+	if ((adw_info->num_watch_points > MAX_WATCH_ADDRESSES) ||
+			(adw_info->num_watch_points == 0)) {
+		pr_err("amdkfd: num_watch_points is invalid\n");
+		return -EINVAL;
+	}
 
-		if ((NULL == adw_info->watch_mode) || (NULL == adw_info->watch_address)) {
-			status = -EINVAL;
-			break;
-		}
+	if ((NULL == adw_info->watch_mode) ||
+			(NULL == adw_info->watch_address)) {
+		pr_err("amdkfd: adw_info fields are not valid\n");
+		return -EINVAL;
+	}
 
-		status = dbgdev->kq->ops.acquire_inline_ib(dbgdev->kq,
-				ib_size/sizeof(uint32_t),
-				&packet_buff_uint, &packet_buff_gpu_addr);
+	status = kfd_gtt_sa_allocate(dbgdev->dev, ib_size, &mem_obj);
 
-		if (status != 0)
-			break;
+	if (status != 0) {
+		pr_err("amdkfd: Failed to allocate GART memory\n");
+		return status;
+	}
 
-		memset(packet_buff_uint, 0, ib_size);
-
-		packets_vec = (struct pm4__set_config_reg *) (packet_buff_uint);
-
-		packets_vec[0].header.count = 1;
-		packets_vec[0].header.opcode = IT_SET_CONFIG_REG;
-		packets_vec[0].header.type = PM4_TYPE_3;
-		packets_vec[0].bitfields2.vmid_shift = ADDRESS_WATCH_CNTL_OFFSET;
-		packets_vec[0].bitfields2.insert_vmid = 1;
-		packets_vec[1].ordinal1 = packets_vec[0].ordinal1;
-		packets_vec[1].bitfields2.insert_vmid = 0;
-		packets_vec[2].ordinal1 = packets_vec[0].ordinal1;
-		packets_vec[2].bitfields2.insert_vmid = 0;
-		packets_vec[3].ordinal1 = packets_vec[0].ordinal1;
-		packets_vec[3].bitfields2.vmid_shift = ADDRESS_WATCH_CNTL_OFFSET;
-		packets_vec[3].bitfields2.insert_vmid = 1;
-
-		for (i = 0; i < adw_info->num_watch_points; i++) {
-
-			dbgdev_address_watch_set_registers(
-					adw_info,
-					&addrHi,
-					&addrLo,
-					&cntl,
-					i,
-					vmid,
-					dbgdev->dev->device_info->asic_family
-					);
-
-			pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
-			pr_debug("\t\t%20s %08x\n", "register index :", i);
-			pr_debug("\t\t%20s %08x\n", "vmid is :", vmid);
-			pr_debug("\t\t%20s %p\n", "Add ptr is :", adw_info->watch_address);
-			pr_debug("\t\t%20s %08llx\n", "Add     is :", adw_info->watch_address[i]);
-			pr_debug("\t\t%20s %08x\n", "Address Low is :", addrLo.bitfields.addr);
-			pr_debug("\t\t%20s %08x\n", "Address high is :", addrHi.bitfields.addr);
-			pr_debug("\t\t%20s %08x\n", "Control Mask is :", cntl.bitfields.mask);
-			pr_debug("\t\t%20s %08x\n", "Control Mode is :", cntl.bitfields.mode);
-			pr_debug("\t\t%20s %08x\n", "Control Vmid is :", cntl.bitfields.vmid);
-			pr_debug("\t\t%20s %08x\n", "Control atc  is :", cntl.bitfields.atc);
-			pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
-
-			aw_reg_add_dword =
-					dbgdev->dev->kfd2kgd
-					->address_watch_get_offset(
-						dbgdev->dev->kgd,
+	packet_buff_uint = mem_obj->cpu_ptr;
+
+	memset(packet_buff_uint, 0, ib_size);
+
+	packets_vec = (struct pm4__set_config_reg *) (packet_buff_uint);
+
+	packets_vec[0].header.count = 1;
+	packets_vec[0].header.opcode = IT_SET_CONFIG_REG;
+	packets_vec[0].header.type = PM4_TYPE_3;
+	packets_vec[0].bitfields2.vmid_shift = ADDRESS_WATCH_CNTL_OFFSET;
+	packets_vec[0].bitfields2.insert_vmid = 1;
+	packets_vec[1].ordinal1 = packets_vec[0].ordinal1;
+	packets_vec[1].bitfields2.insert_vmid = 0;
+	packets_vec[2].ordinal1 = packets_vec[0].ordinal1;
+	packets_vec[2].bitfields2.insert_vmid = 0;
+	packets_vec[3].ordinal1 = packets_vec[0].ordinal1;
+	packets_vec[3].bitfields2.vmid_shift = ADDRESS_WATCH_CNTL_OFFSET;
+	packets_vec[3].bitfields2.insert_vmid = 1;
+
+	for (i = 0; i < adw_info->num_watch_points; i++) {
+		dbgdev_address_watch_set_registers(adw_info,
+						&addrHi,
+						&addrLo,
+						&cntl,
 						i,
-						ADDRESS_WATCH_REG_CNTL);
+						vmid);
+
+		pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
+		pr_debug("\t\t%20s %08x\n", "register index :", i);
+		pr_debug("\t\t%20s %08x\n", "vmid is :", vmid);
+		pr_debug("\t\t%20s %p\n", "Add ptr is :",
+				adw_info->watch_address);
+		pr_debug("\t\t%20s %08llx\n", "Add     is :",
+				adw_info->watch_address[i]);
+		pr_debug("\t\t%20s %08x\n", "Address Low is :",
+				addrLo.bitfields.addr);
+		pr_debug("\t\t%20s %08x\n", "Address high is :",
+				addrHi.bitfields.addr);
+		pr_debug("\t\t%20s %08x\n", "Control Mask is :",
+				cntl.bitfields.mask);
+		pr_debug("\t\t%20s %08x\n", "Control Mode is :",
+				cntl.bitfields.mode);
+		pr_debug("\t\t%20s %08x\n", "Control Vmid is :",
+				cntl.bitfields.vmid);
+		pr_debug("\t\t%20s %08x\n", "Control atc  is :",
+				cntl.bitfields.atc);
+		pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
+
+		aw_reg_add_dword =
+				dbgdev->dev->kfd2kgd->address_watch_get_offset(
+					dbgdev->dev->kgd,
+					i,
+					ADDRESS_WATCH_REG_CNTL);
 
-			packets_vec[0].bitfields2.reg_offset = aw_reg_add_dword - CONFIG_REG_BASE;
-			packets_vec[0].reg_data[0] = cntl.u32All;
+		aw_reg_add_dword /= sizeof(uint32_t);
 
-			aw_reg_add_dword =
-					dbgdev->dev->kfd2kgd
-					->address_watch_get_offset(
-						dbgdev->dev->kgd,
-						i,
-						ADDRESS_WATCH_REG_ADDR_HI);
+		packets_vec[0].bitfields2.reg_offset =
+					aw_reg_add_dword - AMD_CONFIG_REG_BASE;
 
+		packets_vec[0].reg_data[0] = cntl.u32All;
 
-			packets_vec[1].bitfields2.reg_offset = aw_reg_add_dword - CONFIG_REG_BASE;
-			packets_vec[1].reg_data[0] = addrHi.u32All;
+		aw_reg_add_dword =
+				dbgdev->dev->kfd2kgd->address_watch_get_offset(
+					dbgdev->dev->kgd,
+					i,
+					ADDRESS_WATCH_REG_ADDR_HI);
 
-			aw_reg_add_dword =
-					dbgdev->dev->kfd2kgd
-					->address_watch_get_offset(
-						dbgdev->dev->kgd,
-						i,
-						ADDRESS_WATCH_REG_ADDR_LO);
+		aw_reg_add_dword /= sizeof(uint32_t);
 
+		packets_vec[1].bitfields2.reg_offset =
+					aw_reg_add_dword - AMD_CONFIG_REG_BASE;
+		packets_vec[1].reg_data[0] = addrHi.u32All;
 
-			packets_vec[2].bitfields2.reg_offset = aw_reg_add_dword - CONFIG_REG_BASE;
-			packets_vec[2].reg_data[0] = addrLo.u32All;
+		aw_reg_add_dword =
+				dbgdev->dev->kfd2kgd->address_watch_get_offset(
+					dbgdev->dev->kgd,
+					i,
+					ADDRESS_WATCH_REG_ADDR_LO);
 
-			/* enable watch flag if address is not zero*/
-			if (adw_info->watch_address[i] > 0)
-				cntl.bitfields.valid = 1;
-			else
-				cntl.bitfields.valid = 0;
+		aw_reg_add_dword /= sizeof(uint32_t);
 
-			aw_reg_add_dword =
-					dbgdev->dev->kfd2kgd
-					->address_watch_get_offset(
-						dbgdev->dev->kgd,
-						i,
-						ADDRESS_WATCH_REG_CNTL);
+		packets_vec[2].bitfields2.reg_offset =
+				aw_reg_add_dword - AMD_CONFIG_REG_BASE;
+		packets_vec[2].reg_data[0] = addrLo.u32All;
 
+		/* enable watch flag if address is not zero*/
+		if (adw_info->watch_address[i] > 0)
+			cntl.bitfields.valid = 1;
+		else
+			cntl.bitfields.valid = 0;
 
-			packets_vec[3].bitfields2.reg_offset = aw_reg_add_dword - CONFIG_REG_BASE;
-			packets_vec[3].reg_data[0] = cntl.u32All;
+		aw_reg_add_dword =
+				dbgdev->dev->kfd2kgd->address_watch_get_offset(
+					dbgdev->dev->kgd,
+					i,
+					ADDRESS_WATCH_REG_CNTL);
 
-			status = dbgdev_diq_submit_ib(
-						dbgdev,
-						adw_info->process->pasid,
-						packet_buff_gpu_addr,
-						packet_buff_uint,
-						ib_size, true);
+		aw_reg_add_dword /= sizeof(uint32_t);
 
-			if (status != 0) {
-				pr_debug("Error! kfd: In func %s >> failed to submit DIQ packet\n", __func__);
-				break;
-			}
+		packets_vec[3].bitfields2.reg_offset =
+					aw_reg_add_dword - AMD_CONFIG_REG_BASE;
+		packets_vec[3].reg_data[0] = cntl.u32All;
 
-		}
+		status = dbgdev_diq_submit_ib(
+					dbgdev,
+					adw_info->process->pasid,
+					mem_obj->gpu_addr,
+					packet_buff_uint,
+					ib_size);
 
-	} while (false);
+		if (status != 0) {
+			pr_err("amdkfd: Failed to submit IB to DIQ\n");
+			break;
+		}
+	}
 
+	kfd_gtt_sa_free(dbgdev->dev, mem_obj);
 	return status;
-
 }
 
 static int dbgdev_wave_control_set_registers(
 				struct dbg_wave_control_info *wac_info,
 				union SQ_CMD_BITS *in_reg_sq_cmd,
-				union GRBM_GFX_INDEX_BITS *in_reg_gfx_index,
-				unsigned int asic_family)
+				union GRBM_GFX_INDEX_BITS *in_reg_gfx_index)
 {
 	int status = 0;
 	union SQ_CMD_BITS reg_sq_cmd;
 	union GRBM_GFX_INDEX_BITS reg_gfx_index;
+	struct HsaDbgWaveMsgAMDGen2 *pMsg;
 
-	reg_sq_cmd.u32All = 0;
+	BUG_ON(!wac_info || !in_reg_sq_cmd || !in_reg_gfx_index);
 
+	reg_sq_cmd.u32All = 0;
 	reg_gfx_index.u32All = 0;
+	pMsg = &wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2;
 
 	switch (wac_info->mode) {
-	case HSA_DBG_WAVEMODE_SINGLE:	/*  Send command to single wave  */
-		/*limit access to the process waves only,by setting vmid check */
+	/* Send command to single wave */
+	case HSA_DBG_WAVEMODE_SINGLE:
+		/*
+		 * Limit access to the process waves only,
+		 * by setting vmid check
+		 */
 		reg_sq_cmd.bits.check_vmid = 1;
-		reg_sq_cmd.bits.simd_id = wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.ui32.SIMD;
-		reg_sq_cmd.bits.wave_id = wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.ui32.WaveId;
+		reg_sq_cmd.bits.simd_id = pMsg->ui32.SIMD;
+		reg_sq_cmd.bits.wave_id = pMsg->ui32.WaveId;
 		reg_sq_cmd.bits.mode = SQ_IND_CMD_MODE_SINGLE;
 
-		reg_gfx_index.bits.sh_index = wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.ui32.ShaderArray;
-		reg_gfx_index.bits.se_index = wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.ui32.ShaderEngine;
-		reg_gfx_index.bits.instance_index = wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.ui32.HSACU;
+		reg_gfx_index.bits.sh_index = pMsg->ui32.ShaderArray;
+		reg_gfx_index.bits.se_index = pMsg->ui32.ShaderEngine;
+		reg_gfx_index.bits.instance_index = pMsg->ui32.HSACU;
 
 		break;
 
-	case HSA_DBG_WAVEMODE_BROADCAST_PROCESS:	/*  Send command to all waves with matching VMID  */
-
+	/* Send command to all waves with matching VMID */
+	case HSA_DBG_WAVEMODE_BROADCAST_PROCESS:
 
 		reg_gfx_index.bits.sh_broadcast_writes = 1;
 		reg_gfx_index.bits.se_broadcast_writes = 1;
 		reg_gfx_index.bits.instance_broadcast_writes = 1;
 
 		reg_sq_cmd.bits.mode = SQ_IND_CMD_MODE_BROADCAST;
+
 		break;
 
-	case HSA_DBG_WAVEMODE_BROADCAST_PROCESS_CU:	/*  Send command to all CU waves with matching VMID  */
+	/* Send command to all CU waves with matching VMID */
+	case HSA_DBG_WAVEMODE_BROADCAST_PROCESS_CU:
 
 		reg_sq_cmd.bits.check_vmid = 1;
 		reg_sq_cmd.bits.mode = SQ_IND_CMD_MODE_BROADCAST;
 
-		reg_gfx_index.bits.sh_index = wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.ui32.ShaderArray;
-		reg_gfx_index.bits.se_index = wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.ui32.ShaderEngine;
-		reg_gfx_index.bits.instance_index = wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.ui32.HSACU;
+		reg_gfx_index.bits.sh_index = pMsg->ui32.ShaderArray;
+		reg_gfx_index.bits.se_index = pMsg->ui32.ShaderEngine;
+		reg_gfx_index.bits.instance_index = pMsg->ui32.HSACU;
 
 		break;
 
 	default:
-		status = -EINVAL;
-		break;
+		return -EINVAL;
 	}
 
 	switch (wac_info->operand) {
 	case HSA_DBG_WAVEOP_HALT:
-		if (asic_family == CHIP_KAVERI) {
-			reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_HALT;
-			pr_debug("kfd:dbgdev: halting KV\n");
-		} else {
-			reg_sq_cmd.bits_sethalt.cmd  = SQ_IND_CMD_NEW_SETHALT;
-			reg_sq_cmd.bits_sethalt.data = SQ_IND_CMD_DATA_HALT;
-			pr_debug("kfd:dbgdev: halting CZ\n");
-		}
+		reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_HALT;
 		break;
 
 	case HSA_DBG_WAVEOP_RESUME:
-		if (asic_family == CHIP_KAVERI) {
-			reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_RESUME;
-			pr_debug("kfd:dbgdev: resuming KV\n");
-		} else {
-			reg_sq_cmd.bits_sethalt.cmd  = SQ_IND_CMD_NEW_SETHALT;
-			reg_sq_cmd.bits_sethalt.data = SQ_IND_CMD_DATA_RESUME;
-			pr_debug("kfd:dbgdev: resuming CZ\n");
-		}
+		reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_RESUME;
 		break;
 
 	case HSA_DBG_WAVEOP_KILL:
@@ -616,111 +601,128 @@ static int dbgdev_wave_control_set_registers(
 	}
 
 	if (status == 0) {
-		*in_reg_sq_cmd    = reg_sq_cmd;
+		*in_reg_sq_cmd = reg_sq_cmd;
 		*in_reg_gfx_index = reg_gfx_index;
 	}
-	return status;
 
+	return status;
 }
 
 static int dbgdev_wave_control_diq(struct kfd_dbgdev *dbgdev,
 					struct dbg_wave_control_info *wac_info)
 {
 
-	int status = 0;
+	int status;
 	union SQ_CMD_BITS reg_sq_cmd;
 	union GRBM_GFX_INDEX_BITS reg_gfx_index;
-	uint32_t *packet_buff_uint = NULL;
-	uint64_t packet_buff_gpu_addr = 0;
-	struct pm4__set_config_reg *packets_vec = NULL;
+	struct kfd_mem_obj *mem_obj;
+	uint32_t *packet_buff_uint;
+	struct pm4__set_config_reg *packets_vec;
 	size_t ib_size = sizeof(struct pm4__set_config_reg) * 3;
 
+	BUG_ON(!dbgdev || !wac_info);
+
 	reg_sq_cmd.u32All = 0;
-	do {
 
-		status = dbgdev_wave_control_set_registers(wac_info,
-				&reg_sq_cmd,
-				&reg_gfx_index,
-				dbgdev->dev->device_info->asic_family);
+	status = dbgdev_wave_control_set_registers(wac_info, &reg_sq_cmd,
+							&reg_gfx_index);
+	if (status) {
+		pr_err("amdkfd: Failed to set wave control registers\n");
+		return status;
+	}
 
-		/* we do not control the VMID in DIQ,so reset it to a known value */
-		reg_sq_cmd.bits.vm_id = 0;
-		if (status != 0)
-			break;
-		pr_debug("\t\t %30s\n", "* * * * * * * * * * * * * * * * * *");
-
-		pr_debug("\t\t mode      is: %u\n", wac_info->mode);
-		pr_debug("\t\t operand   is: %u\n", wac_info->operand);
-		pr_debug("\t\t trap id   is: %u\n", wac_info->trapId);
-		pr_debug("\t\t msg value is: %u\n", wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value);
-		pr_debug("\t\t vmid      is: N/A\n");
-
-		pr_debug("\t\t chk_vmid  is : %u\n", reg_sq_cmd.bitfields.check_vmid);
-		pr_debug("\t\t command   is : %u\n", reg_sq_cmd.bitfields.cmd);
-		pr_debug("\t\t queue id  is : %u\n", reg_sq_cmd.bitfields.queue_id);
-		pr_debug("\t\t simd id   is : %u\n", reg_sq_cmd.bitfields.simd_id);
-		pr_debug("\t\t mode      is : %u\n", reg_sq_cmd.bitfields.mode);
-		pr_debug("\t\t vm_id     is : %u\n", reg_sq_cmd.bitfields.vm_id);
-		pr_debug("\t\t wave_id   is : %u\n", reg_sq_cmd.bitfields.wave_id);
-
-		pr_debug("\t\t ibw       is : %u\n", reg_gfx_index.bitfields.instance_broadcast_writes);
-		pr_debug("\t\t ii        is : %u\n", reg_gfx_index.bitfields.instance_index);
-		pr_debug("\t\t sebw      is : %u\n", reg_gfx_index.bitfields.se_broadcast_writes);
-		pr_debug("\t\t se_ind    is : %u\n", reg_gfx_index.bitfields.se_index);
-		pr_debug("\t\t sh_ind    is : %u\n", reg_gfx_index.bitfields.sh_index);
-		pr_debug("\t\t sbw       is : %u\n", reg_gfx_index.bitfields.sh_broadcast_writes);
-
-		pr_debug("\t\t %30s\n", "* * * * * * * * * * * * * * * * * *");
-
-		status = dbgdev->kq->ops.acquire_inline_ib(dbgdev->kq,
-				ib_size / sizeof(uint32_t),
-				&packet_buff_uint, &packet_buff_gpu_addr);
-
-		if (status != 0)
-			break;
+	/* we do not control the VMID in DIQ,so reset it to a known value */
+	reg_sq_cmd.bits.vm_id = 0;
+
+	pr_debug("\t\t %30s\n", "* * * * * * * * * * * * * * * * * *");
+
+	pr_debug("\t\t mode      is: %u\n", wac_info->mode);
+	pr_debug("\t\t operand   is: %u\n", wac_info->operand);
+	pr_debug("\t\t trap id   is: %u\n", wac_info->trapId);
+	pr_debug("\t\t msg value is: %u\n",
+			wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value);
+	pr_debug("\t\t vmid      is: N/A\n");
+
+	pr_debug("\t\t chk_vmid  is : %u\n", reg_sq_cmd.bitfields.check_vmid);
+	pr_debug("\t\t command   is : %u\n", reg_sq_cmd.bitfields.cmd);
+	pr_debug("\t\t queue id  is : %u\n", reg_sq_cmd.bitfields.queue_id);
+	pr_debug("\t\t simd id   is : %u\n", reg_sq_cmd.bitfields.simd_id);
+	pr_debug("\t\t mode      is : %u\n", reg_sq_cmd.bitfields.mode);
+	pr_debug("\t\t vm_id     is : %u\n", reg_sq_cmd.bitfields.vm_id);
+	pr_debug("\t\t wave_id   is : %u\n", reg_sq_cmd.bitfields.wave_id);
+
+	pr_debug("\t\t ibw       is : %u\n",
+			reg_gfx_index.bitfields.instance_broadcast_writes);
+	pr_debug("\t\t ii        is : %u\n",
+			reg_gfx_index.bitfields.instance_index);
+	pr_debug("\t\t sebw      is : %u\n",
+			reg_gfx_index.bitfields.se_broadcast_writes);
+	pr_debug("\t\t se_ind    is : %u\n", reg_gfx_index.bitfields.se_index);
+	pr_debug("\t\t sh_ind    is : %u\n", reg_gfx_index.bitfields.sh_index);
+	pr_debug("\t\t sbw       is : %u\n",
+			reg_gfx_index.bitfields.sh_broadcast_writes);
+
+	pr_debug("\t\t %30s\n", "* * * * * * * * * * * * * * * * * *");
+
+	status = kfd_gtt_sa_allocate(dbgdev->dev, ib_size, &mem_obj);
+
+	if (status != 0) {
+		pr_err("amdkfd: Failed to allocate GART memory\n");
+		return status;
+	}
 
-		memset(packet_buff_uint, 0, ib_size);
+	packet_buff_uint = mem_obj->cpu_ptr;
 
-		packets_vec =  (struct pm4__set_config_reg *) packet_buff_uint;
-		packets_vec[0].header.count = 1;
-		packets_vec[0].header.opcode = IT_SET_UCONFIG_REG;
-		packets_vec[0].header.type = PM4_TYPE_3;
-		packets_vec[0].bitfields2.reg_offset = GRBM_GFX_INDEX / (sizeof(uint32_t)) - USERCONFIG_REG_BASE;
-		packets_vec[0].bitfields2.insert_vmid = 0;
-		packets_vec[0].reg_data[0] = reg_gfx_index.u32All;
+	memset(packet_buff_uint, 0, ib_size);
 
-		packets_vec[1].header.count = 1;
-		packets_vec[1].header.opcode = IT_SET_CONFIG_REG;
-		packets_vec[1].header.type = PM4_TYPE_3;
-		packets_vec[1].bitfields2.reg_offset = SQ_CMD / (sizeof(uint32_t)) - CONFIG_REG_BASE;
-		packets_vec[1].bitfields2.vmid_shift = SQ_CMD_VMID_OFFSET;
-		packets_vec[1].bitfields2.insert_vmid = 1;
-		packets_vec[1].reg_data[0] = reg_sq_cmd.u32All;
+	packets_vec =  (struct pm4__set_config_reg *) packet_buff_uint;
+	packets_vec[0].header.count = 1;
+	packets_vec[0].header.opcode = IT_SET_UCONFIG_REG;
+	packets_vec[0].header.type = PM4_TYPE_3;
+	packets_vec[0].bitfields2.reg_offset =
+			GRBM_GFX_INDEX / (sizeof(uint32_t)) -
+				USERCONFIG_REG_BASE;
 
-		/* Restore the GRBM_GFX_INDEX register */
+	packets_vec[0].bitfields2.insert_vmid = 0;
+	packets_vec[0].reg_data[0] = reg_gfx_index.u32All;
 
-		reg_gfx_index.u32All = 0;
-		reg_gfx_index.bits.sh_broadcast_writes = 1;
-		reg_gfx_index.bits.instance_broadcast_writes = 1;
-		reg_gfx_index.bits.se_broadcast_writes = 1;
+	packets_vec[1].header.count = 1;
+	packets_vec[1].header.opcode = IT_SET_CONFIG_REG;
+	packets_vec[1].header.type = PM4_TYPE_3;
+	packets_vec[1].bitfields2.reg_offset = SQ_CMD / (sizeof(uint32_t)) -
+						AMD_CONFIG_REG_BASE;
 
+	packets_vec[1].bitfields2.vmid_shift = SQ_CMD_VMID_OFFSET;
+	packets_vec[1].bitfields2.insert_vmid = 1;
+	packets_vec[1].reg_data[0] = reg_sq_cmd.u32All;
 
-		packets_vec[2].ordinal1 = packets_vec[0].ordinal1;
-		packets_vec[2].bitfields2.reg_offset = GRBM_GFX_INDEX / (sizeof(uint32_t)) - USERCONFIG_REG_BASE;
-		packets_vec[2].bitfields2.insert_vmid = 0;
-		packets_vec[2].reg_data[0] = reg_gfx_index.u32All;
+	/* Restore the GRBM_GFX_INDEX register */
 
-		status = dbgdev_diq_submit_ib(
-				dbgdev,
-				wac_info->process->pasid,
-				packet_buff_gpu_addr,
-				packet_buff_uint,
-				ib_size, false);
+	reg_gfx_index.u32All = 0;
+	reg_gfx_index.bits.sh_broadcast_writes = 1;
+	reg_gfx_index.bits.instance_broadcast_writes = 1;
+	reg_gfx_index.bits.se_broadcast_writes = 1;
+
+
+	packets_vec[2].ordinal1 = packets_vec[0].ordinal1;
+	packets_vec[2].bitfields2.reg_offset =
+				GRBM_GFX_INDEX / (sizeof(uint32_t)) -
+					USERCONFIG_REG_BASE;
 
-		if (status != 0)
-			pr_debug("%s\n", " Critical Error ! Submit diq packet failed ");
+	packets_vec[2].bitfields2.insert_vmid = 0;
+	packets_vec[2].reg_data[0] = reg_gfx_index.u32All;
 
-	} while (false);
+	status = dbgdev_diq_submit_ib(
+			dbgdev,
+			wac_info->process->pasid,
+			mem_obj->gpu_addr,
+			packet_buff_uint,
+			ib_size);
+
+	if (status != 0)
+		pr_err("amdkfd: Failed to submit IB to DIQ\n");
+
+	kfd_gtt_sa_free(dbgdev->dev, mem_obj);
 
 	return status;
 }
@@ -728,69 +730,66 @@ static int dbgdev_wave_control_diq(struct kfd_dbgdev *dbgdev,
 static int dbgdev_wave_control_nodiq(struct kfd_dbgdev *dbgdev,
 					struct dbg_wave_control_info *wac_info)
 {
-	int status = 0;
-	unsigned int vmid = 0xffff;
+	int status;
 	union SQ_CMD_BITS reg_sq_cmd;
 	union GRBM_GFX_INDEX_BITS reg_gfx_index;
+	struct kfd_process_device *pdd;
 
-	struct kfd_process_device *pdd = NULL;
+	BUG_ON(!dbgdev || !dbgdev->dev || !wac_info);
 
 	reg_sq_cmd.u32All = 0;
-	status = 0;
 
 	/* taking the VMID for that process on the safe way using PDD */
 	pdd = kfd_get_process_device_data(dbgdev->dev, wac_info->process);
 
-	if (pdd) {
-		status = dbgdev_wave_control_set_registers(wac_info,
-				&reg_sq_cmd,
-				&reg_gfx_index,
-				dbgdev->dev->device_info->asic_family);
-		if (status == 0) {
-
-			/* for non DIQ we need to patch the VMID: */
-
-			vmid = pdd->qpd.vmid;
-			reg_sq_cmd.bits.vm_id = vmid;
-
-			pr_debug("\t\t %30s\n", "* * * * * * * * * * * * * * * * * *");
-
-			pr_debug("\t\t mode      is: %u\n", wac_info->mode);
-			pr_debug("\t\t operand   is: %u\n", wac_info->operand);
-			pr_debug("\t\t trap id   is: %u\n", wac_info->trapId);
-			pr_debug("\t\t msg value is: %u\n", wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value);
-			pr_debug("\t\t vmid      is: %u\n", vmid);
-
-			pr_debug("\t\t chk_vmid  is : %u\n", reg_sq_cmd.bitfields.check_vmid);
-			pr_debug("\t\t command   is : %u\n", reg_sq_cmd.bitfields.cmd);
-			pr_debug("\t\t queue id  is : %u\n", reg_sq_cmd.bitfields.queue_id);
-			pr_debug("\t\t simd id   is : %u\n", reg_sq_cmd.bitfields.simd_id);
-			pr_debug("\t\t mode      is : %u\n", reg_sq_cmd.bitfields.mode);
-			pr_debug("\t\t vm_id     is : %u\n", reg_sq_cmd.bitfields.vm_id);
-			pr_debug("\t\t wave_id   is : %u\n", reg_sq_cmd.bitfields.wave_id);
-
-			pr_debug("\t\t ibw       is : %u\n", reg_gfx_index.bitfields.instance_broadcast_writes);
-			pr_debug("\t\t ii        is : %u\n", reg_gfx_index.bitfields.instance_index);
-			pr_debug("\t\t sebw      is : %u\n", reg_gfx_index.bitfields.se_broadcast_writes);
-			pr_debug("\t\t se_ind    is : %u\n", reg_gfx_index.bitfields.se_index);
-			pr_debug("\t\t sh_ind    is : %u\n", reg_gfx_index.bitfields.sh_index);
-			pr_debug("\t\t sbw       is : %u\n", reg_gfx_index.bitfields.sh_broadcast_writes);
-
-			pr_debug("\t\t %30s\n", "* * * * * * * * * * * * * * * * * *");
-
-			dbgdev->dev->kfd2kgd
-				->wave_control_execute(dbgdev->dev->kgd,
-							reg_gfx_index.u32All,
-							reg_sq_cmd.u32All);
-		} else {
-			status = -EINVAL;
-		}
-	} else {
-		status = -EFAULT;
+	if (!pdd) {
+		pr_err("amdkfd: Failed to get pdd for wave control no DIQ\n");
+		return -EFAULT;
+	}
+	status = dbgdev_wave_control_set_registers(wac_info, &reg_sq_cmd,
+							&reg_gfx_index);
+	if (status) {
+		pr_err("amdkfd: Failed to set wave control registers\n");
+		return status;
 	}
 
-	return status;
+	/* for non DIQ we need to patch the VMID: */
 
+	reg_sq_cmd.bits.vm_id = pdd->qpd.vmid;
+
+	pr_debug("\t\t %30s\n", "* * * * * * * * * * * * * * * * * *");
+
+	pr_debug("\t\t mode      is: %u\n", wac_info->mode);
+	pr_debug("\t\t operand   is: %u\n", wac_info->operand);
+	pr_debug("\t\t trap id   is: %u\n", wac_info->trapId);
+	pr_debug("\t\t msg value is: %u\n",
+			wac_info->dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value);
+	pr_debug("\t\t vmid      is: %u\n", pdd->qpd.vmid);
+
+	pr_debug("\t\t chk_vmid  is : %u\n", reg_sq_cmd.bitfields.check_vmid);
+	pr_debug("\t\t command   is : %u\n", reg_sq_cmd.bitfields.cmd);
+	pr_debug("\t\t queue id  is : %u\n", reg_sq_cmd.bitfields.queue_id);
+	pr_debug("\t\t simd id   is : %u\n", reg_sq_cmd.bitfields.simd_id);
+	pr_debug("\t\t mode      is : %u\n", reg_sq_cmd.bitfields.mode);
+	pr_debug("\t\t vm_id     is : %u\n", reg_sq_cmd.bitfields.vm_id);
+	pr_debug("\t\t wave_id   is : %u\n", reg_sq_cmd.bitfields.wave_id);
+
+	pr_debug("\t\t ibw       is : %u\n",
+			reg_gfx_index.bitfields.instance_broadcast_writes);
+	pr_debug("\t\t ii        is : %u\n",
+			reg_gfx_index.bitfields.instance_index);
+	pr_debug("\t\t sebw      is : %u\n",
+			reg_gfx_index.bitfields.se_broadcast_writes);
+	pr_debug("\t\t se_ind    is : %u\n", reg_gfx_index.bitfields.se_index);
+	pr_debug("\t\t sh_ind    is : %u\n", reg_gfx_index.bitfields.sh_index);
+	pr_debug("\t\t sbw       is : %u\n",
+			reg_gfx_index.bitfields.sh_broadcast_writes);
+
+	pr_debug("\t\t %30s\n", "* * * * * * * * * * * * * * * * * *");
+
+	return dbgdev->dev->kfd2kgd->wave_control_execute(dbgdev->dev->kgd,
+							reg_gfx_index.u32All,
+							reg_sq_cmd.u32All);
 }
 
 int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p)
@@ -801,8 +800,13 @@ int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p)
 	union GRBM_GFX_INDEX_BITS reg_gfx_index;
 	struct kfd_process_device *pdd;
 	struct dbg_wave_control_info wac_info;
-	int first_vmid_to_scan = dev->vm_info.first_vmid_kfd;
-	int last_vmid_to_scan = dev->vm_info.last_vmid_kfd;
+	int temp;
+	int first_vmid_to_scan = 8;
+	int last_vmid_to_scan = 15;
+
+	first_vmid_to_scan = ffs(dev->shared_resources.compute_vmid_bitmap) - 1;
+	temp = dev->shared_resources.compute_vmid_bitmap >> first_vmid_to_scan;
+	last_vmid_to_scan = first_vmid_to_scan + ffz(temp);
 
 	reg_sq_cmd.u32All = 0;
 	status = 0;
@@ -819,7 +823,7 @@ int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p)
 	for (vmid = first_vmid_to_scan; vmid <= last_vmid_to_scan; vmid++) {
 		if (dev->kfd2kgd->get_atc_vmid_pasid_mapping_valid
 				(dev->kgd, vmid)) {
-			if (dev->kfd2kgd->get_atc_vmid_pasid_mapping_pasid
+			if (dev->kfd2kgd->get_atc_vmid_pasid_mapping_valid
 					(dev->kgd, vmid) == p->pasid) {
 				pr_debug("Killing wave fronts of vmid %d and pasid %d\n",
 						vmid, p->pasid);
@@ -829,7 +833,7 @@ int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p)
 	}
 
 	if (vmid > last_vmid_to_scan) {
-		pr_err("amdkfd: didn't find vmid for pasid (%d)\n", p->pasid);
+		pr_err("amdkfd: didn't found vmid for pasid (%d)\n", p->pasid);
 		return -EFAULT;
 	}
 
@@ -839,7 +843,7 @@ int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p)
 		return -EFAULT;
 
 	status = dbgdev_wave_control_set_registers(&wac_info, &reg_sq_cmd,
-			&reg_gfx_index, dev->device_info->asic_family);
+			&reg_gfx_index);
 	if (status != 0)
 		return -EINVAL;
 
@@ -854,12 +858,15 @@ int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p)
 }
 
 void kfd_dbgdev_init(struct kfd_dbgdev *pdbgdev, struct kfd_dev *pdev,
-			DBGDEV_TYPE type)
+			enum DBGDEV_TYPE type)
 {
+	BUG_ON(!pdbgdev || !pdev);
+
 	pdbgdev->dev = pdev;
 	pdbgdev->kq = NULL;
 	pdbgdev->type = type;
 	pdbgdev->pqm = NULL;
+
 	switch (type) {
 	case DBGDEV_TYPE_NODIQ:
 		pdbgdev->dbgdev_register = dbgdev_register_nodiq;
@@ -869,12 +876,10 @@ void kfd_dbgdev_init(struct kfd_dbgdev *pdbgdev, struct kfd_dev *pdev,
 		break;
 	case DBGDEV_TYPE_DIQ:
 	default:
-
 		pdbgdev->dbgdev_register = dbgdev_register_diq;
 		pdbgdev->dbgdev_unregister = dbgdev_unregister_diq;
 		pdbgdev->dbgdev_wave_control =  dbgdev_wave_control_diq;
 		pdbgdev->dbgdev_address_watch = dbgdev_address_watch_diq;
-
 		break;
 	}
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h
index 82f48ff..03424c2 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h
@@ -23,10 +23,6 @@
 #ifndef KFD_DBGDEV_H_
 #define KFD_DBGDEV_H_
 
-/*
- * SQ_IND_CMD_CMD enum
- */
-
 enum {
 	SQ_CMD_VMID_OFFSET = 28,
 	ADDRESS_WATCH_CNTL_OFFSET = 24
@@ -52,9 +48,9 @@ enum {
 
 /* CONFIG reg space definition */
 enum {
-	CONFIG_REG_BASE = 0x2000,	/* in dwords */
-	CONFIG_REG_END = 0x2B00,
-	CONFIG_REG_SIZE = CONFIG_REG_END - CONFIG_REG_BASE
+	AMD_CONFIG_REG_BASE = 0x2000,	/* in dwords */
+	AMD_CONFIG_REG_END = 0x2B00,
+	AMD_CONFIG_REG_SIZE = AMD_CONFIG_REG_END - AMD_CONFIG_REG_BASE
 };
 
 /* SH reg space definition */
@@ -64,43 +60,22 @@ enum {
 	SH_REG_SIZE = SH_REG_END - SH_REG_BASE
 };
 
-/* SQ_CMD definitions */
-
-enum {
-	SQ_IND_CMD_DATA_RESUME = 0,
-	SQ_IND_CMD_DATA_HALT = 1
-};
-
-enum SQ_IND_CMD_NEW {
-	SQ_IND_CMD_NEW_NULL = 0x00000000,
-	SQ_IND_CMD_NEW_SETHALT = 0x00000001,
-	SQ_IND_CMD_NEW_SAVECTX = 0x00000002,
-	SQ_IND_CMD_NEW_KILL = 0x00000003,
-	SQ_IND_CMD_NEW_DEBUG = 0x00000004,
-	SQ_IND_CMD_NEW_TRAP = 0x00000005,
-	SQ_IND_CMD_NEW_SET_PRIO = 0x00000006
-
-};
-
 enum SQ_IND_CMD_CMD {
 	SQ_IND_CMD_CMD_NULL = 0x00000000,
 	SQ_IND_CMD_CMD_HALT = 0x00000001,
 	SQ_IND_CMD_CMD_RESUME = 0x00000002,
 	SQ_IND_CMD_CMD_KILL = 0x00000003,
 	SQ_IND_CMD_CMD_DEBUG = 0x00000004,
-	SQ_IND_CMD_CMD_TRAP = 0x00000005
+	SQ_IND_CMD_CMD_TRAP = 0x00000005,
 };
-/*
- * SQ_IND_CMD_MODE enum
- */
 
-typedef enum SQ_IND_CMD_MODE {
+enum SQ_IND_CMD_MODE {
 	SQ_IND_CMD_MODE_SINGLE = 0x00000000,
 	SQ_IND_CMD_MODE_BROADCAST = 0x00000001,
 	SQ_IND_CMD_MODE_BROADCAST_QUEUE = 0x00000002,
 	SQ_IND_CMD_MODE_BROADCAST_PIPE = 0x00000003,
 	SQ_IND_CMD_MODE_BROADCAST_ME = 0x00000004,
-} SQ_IND_CMD_MODE;
+};
 
 union SQ_IND_INDEX_BITS {
 	struct {
@@ -131,32 +106,18 @@ union SQ_IND_CMD_BITS {
 union SQ_CMD_BITS {
 	struct {
 		uint32_t cmd:3;
-		uint32_t:1;
+		 uint32_t:1;
 		uint32_t mode:3;
 		uint32_t check_vmid:1;
 		uint32_t trap_id:3;
-		uint32_t:5;
+		 uint32_t:5;
 		uint32_t wave_id:4;
 		uint32_t simd_id:2;
-		uint32_t:2;
+		 uint32_t:2;
 		uint32_t queue_id:3;
-		uint32_t:1;
+		 uint32_t:1;
 		uint32_t vm_id:4;
 	} bitfields, bits;
-	struct {
-		uint32_t cmd:3;
-		uint32_t:1;
-		uint32_t mode:3;
-		uint32_t check_vmid:1;
-		uint32_t data:3;
-		uint32_t:5;
-		uint32_t wave_id:4;
-		uint32_t simd_id:2;
-		uint32_t:2;
-		uint32_t queue_id:3;
-		uint32_t:1;
-		uint32_t vm_id:4;
-	} bitfields_sethalt, bits_sethalt;
 	uint32_t u32All;
 	signed int i32All;
 	float f32All;
@@ -208,7 +169,7 @@ union TCP_WATCH_ADDR_L_BITS {
 };
 
 enum {
-	QUEUESTATE__INVALID = 0,	/* so by default we'll get invalid state */
+	QUEUESTATE__INVALID = 0, /* so by default we'll get invalid state */
 	QUEUESTATE__ACTIVE_COMPLETION_PENDING,
 	QUEUESTATE__ACTIVE
 };
@@ -226,6 +187,7 @@ union ULARGE_INTEGER {
 #define KFD_CIK_VMID_END_OFFSET (KFD_CIK_VMID_START_OFFSET + (8))
 
 
-void kfd_dbgdev_init(struct kfd_dbgdev *pdbgdev, struct kfd_dev *pdev, DBGDEV_TYPE type);
+void kfd_dbgdev_init(struct kfd_dbgdev *pdbgdev, struct kfd_dev *pdev,
+			enum DBGDEV_TYPE type);
 
-#endif				/* KFD_DBGDEV_H_ */
+#endif	/* KFD_DBGDEV_H_ */
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c b/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c
index 426f776..56d6763 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c
@@ -33,54 +33,45 @@
 #include "kfd_pm4_headers_diq.h"
 #include "kfd_dbgmgr.h"
 #include "kfd_dbgdev.h"
-#include "kfd_device_queue_manager.h"
 
 static DEFINE_MUTEX(kfd_dbgmgr_mutex);
 
-struct mutex *
-get_dbgmgr_mutex(void)
+struct mutex *kfd_get_dbgmgr_mutex(void)
 {
 	return &kfd_dbgmgr_mutex;
 }
 
-/*===========================================================================*/
 
-static void
-kfd_dbgmgr_uninitialize(struct kfd_dbgmgr *pmgr)
+static void kfd_dbgmgr_uninitialize(struct kfd_dbgmgr *pmgr)
 {
+	BUG_ON(!pmgr);
+
 	kfree(pmgr->dbgdev);
+
 	pmgr->dbgdev = NULL;
 	pmgr->pasid = 0;
 	pmgr->dev = NULL;
 }
 
-/*===========================================================================*/
-
-void
-kfd_dbgmgr_destroy(struct kfd_dbgmgr *pmgr)
+void kfd_dbgmgr_destroy(struct kfd_dbgmgr *pmgr)
 {
 	if (pmgr != NULL) {
 		kfd_dbgmgr_uninitialize(pmgr);
 		kfree(pmgr);
-		pmgr = NULL;
 	}
 }
 
-/*===========================================================================*/
-
-bool
-kfd_dbgmgr_create(struct kfd_dbgmgr **ppmgr, struct kfd_dev *pdev)
+bool kfd_dbgmgr_create(struct kfd_dbgmgr **ppmgr, struct kfd_dev *pdev)
 {
-	DBGDEV_TYPE  type = DBGDEV_TYPE_DIQ;
+	enum DBGDEV_TYPE type = DBGDEV_TYPE_DIQ;
 	struct kfd_dbgmgr *new_buff;
 
 	BUG_ON(pdev == NULL);
 	BUG_ON(!pdev->init_complete);
 
 	new_buff = kfd_alloc_struct(new_buff);
-	if (!new_buff)
-	{
-		dev_err(NULL, "Error! kfd: In func %s >> failed to allocate dbgmgr instance\n", __func__);
+	if (!new_buff) {
+		pr_err("amdkfd: Failed to allocate dbgmgr instance\n");
 		return false;
 	}
 
@@ -88,13 +79,13 @@ kfd_dbgmgr_create(struct kfd_dbgmgr **ppmgr, struct kfd_dev *pdev)
 	new_buff->dev = pdev;
 	new_buff->dbgdev = kfd_alloc_struct(new_buff->dbgdev);
 	if (!new_buff->dbgdev) {
-		dev_err(NULL, "Error! kfd: In func %s >> failed to allocate dbgdev\n", __func__);
+		pr_err("amdkfd: Failed to allocate dbgdev instance\n");
 		kfree(new_buff);
 		return false;
 	}
 
 	/* get actual type of DBGDevice cpsch or not */
-	if (pdev->dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS)
+	if (sched_policy == KFD_SCHED_POLICY_NO_HWS)
 		type = DBGDEV_TYPE_NODIQ;
 
 	kfd_dbgdev_init(new_buff->dbgdev, pdev, type);
@@ -103,200 +94,75 @@ kfd_dbgmgr_create(struct kfd_dbgmgr **ppmgr, struct kfd_dev *pdev)
 	return true;
 }
 
-/*===========================================================================*/
-
-long
-kfd_dbgmgr_register(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
+long kfd_dbgmgr_register(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
 {
-	long status = 0;
+	BUG_ON(!p || !pmgr || !pmgr->dbgdev);
 
-	do {
-
-		if ((pmgr == NULL) || (pmgr->dev == NULL) || (pmgr->dbgdev == NULL)) {
-			dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n", __func__);
-			/*  Invalid Pointer.  */
-			status = -EINVAL;
-			break;
-		}
-		if (pmgr->pasid != 0) {
-			/*  HW debugger is already active.  */
-			status = -EBUSY;
-			break;
-		}
-
-		/* remember pasid */
-
-		pmgr->pasid = p->pasid;
-
-		/* provide the pqm for diq generation */
+	if (pmgr->pasid != 0) {
+		pr_debug("H/W debugger is already active using pasid %d\n",
+				pmgr->pasid);
+		return -EBUSY;
+	}
 
-		pmgr->dbgdev->pqm = &p->pqm;
+	/* remember pasid */
+	pmgr->pasid = p->pasid;
 
-		/* activate the actual registering */
-		/* todo: you should lock with the process mutex here */
-		pmgr->dbgdev->dbgdev_register(pmgr->dbgdev);
-		/* todo: you should unlock with the process mutex here  */
+	/* provide the pqm for diq generation */
+	pmgr->dbgdev->pqm = &p->pqm;
 
-	} while (false);
+	/* activate the actual registering */
+	pmgr->dbgdev->dbgdev_register(pmgr->dbgdev);
 
-	return status;
+	return 0;
 }
 
-/* ========================================================================== */
-
-long
-kfd_dbgmgr_unregister(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
+long kfd_dbgmgr_unregister(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
 {
+	BUG_ON(!p || !pmgr || !pmgr->dbgdev);
 
-	long status = 0;
-
-	do {
-
-		if ((pmgr == NULL) || (pmgr->dev == NULL)
-				|| (pmgr->dbgdev == NULL) || (p == NULL)) {
-			dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n", __func__);
-			/* Invalid Pointer */
-			status = -EINVAL;
-			break;
-		}
-		if (pmgr->pasid != p->pasid) {
-			/* Is the requests coming from the already registered process? */
-			status = -EINVAL;
-			break;
-		}
-
-		/* todo: you should lock with the process mutex here */
-
-		pmgr->dbgdev->dbgdev_unregister(pmgr->dbgdev);
-
-		/* todo: you should unlock with the process mutex here  */
+	/* Is the requests coming from the already registered process? */
+	if (pmgr->pasid != p->pasid) {
+		pr_debug("H/W debugger is not registered by calling pasid %d\n",
+				p->pasid);
+		return -EINVAL;
+	}
 
-		pmgr->pasid = 0;
+	pmgr->dbgdev->dbgdev_unregister(pmgr->dbgdev);
 
-	} while (false);
+	pmgr->pasid = 0;
 
-	return status;
+	return 0;
 }
 
-/* =========================================================================== */
-
-long
-kfd_dbgmgr_wave_control(struct kfd_dbgmgr *pmgr, struct dbg_wave_control_info *wac_info)
+long kfd_dbgmgr_wave_control(struct kfd_dbgmgr *pmgr,
+				struct dbg_wave_control_info *wac_info)
 {
-	long status = 0;
-
-	dev_info(NULL, "kfd: In func %s\n", __func__);
+	BUG_ON(!pmgr || !pmgr->dbgdev || !wac_info);
 
-	do {
-
-		if ((pmgr == NULL) || (pmgr->dev == NULL) || (pmgr->dbgdev == NULL) || (wac_info == NULL)
-		    || (wac_info->process == NULL)) {
-			/* Invalid Pointer */
-			dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n", __func__);
-			status = -EINVAL;
-			break;
-		}
-		/* Is the requests coming from the already registered process? */
-		if (pmgr->pasid != wac_info->process->pasid) {
-			/* HW debugger support was not registered for requester process */
-			status = -EINVAL;
-			break;
-		}
-
-		status = (long) pmgr->dbgdev->dbgdev_wave_control(pmgr->dbgdev, wac_info);
-
-	} while (false);
-
-	return status;
+	/* Is the requests coming from the already registered process? */
+	if (pmgr->pasid != wac_info->process->pasid) {
+		pr_debug("H/W debugger support was not registered for requester pasid %d\n",
+				wac_info->process->pasid);
+		return -EINVAL;
+	}
 
+	return (long) pmgr->dbgdev->dbgdev_wave_control(pmgr->dbgdev, wac_info);
 }
 
-/* =========================================================================== */
-
-long
-kfd_dbgmgr_address_watch(struct kfd_dbgmgr *pmgr, struct dbg_address_watch_info *adw_info)
+long kfd_dbgmgr_address_watch(struct kfd_dbgmgr *pmgr,
+				struct dbg_address_watch_info *adw_info)
 {
-	long status = 0;
-
-	dev_info(NULL, "kfd: In func %s\n", __func__);
-
-	do {
-
-		if ((pmgr == NULL) || (pmgr->dev == NULL) || (pmgr->dbgdev == NULL) || (adw_info == NULL)
-		    || (adw_info->process == NULL)) {
-			/* Invalid Pointer */
-			dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n", __func__);
-			status = -EINVAL;
-			break;
-		}
-		/* Is the requests coming from the already registered process? */
-		if (pmgr->pasid != adw_info->process->pasid) {
-			/* HW debugger support was not registered for requester process */
-			status = -EINVAL;
-			break;
-		}
-
-		status = (long) pmgr->dbgdev->dbgdev_address_watch(pmgr->dbgdev, adw_info);
+	BUG_ON(!pmgr || !pmgr->dbgdev || !adw_info);
 
-	} while (false);
 
-	return status;
-
-}
-
-
-/* =========================================================================== */
-/*
- * Handle abnormal process termination
- * if we are in the midst of a debug session, we should kill all pending waves
- * of the debugged process and unregister the process from the Debugger.
- */
-long
-kfd_dbgmgr_abnormal_termination(struct kfd_dbgmgr *pmgr, struct kfd_process *process)
-{
-	long status = 0;
-	struct dbg_wave_control_info wac_info;
-
-	dev_info(NULL, "kfd: In func %s\n", __func__);
-
-	do {
-
-		if ((pmgr == NULL) || (pmgr->dev == NULL) || (pmgr->dbgdev == NULL)) {
-			/* Invalid Pointer */
-			dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n", __func__);
-			status = -EINVAL;
-			break;
-		}
-		/* first, we kill all the wavefronts of this process */
-
-		wac_info.process = process;
-		wac_info.mode = HSA_DBG_WAVEMODE_BROADCAST_PROCESS;
-		wac_info.operand = HSA_DBG_WAVEOP_KILL;
-		wac_info.trapId  = 0x0; /* not used for the KILL */
-		wac_info.dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value = 0; /* not used for kill */
-		wac_info.dbgWave_msg.MemoryVA = NULL; /* not used for kill  */
-
-		status = (long) pmgr->dbgdev->dbgdev_wave_control(pmgr->dbgdev, &wac_info);
-
-		if (status != 0) {
-			dev_info(NULL, "Error! kfd: In func %s: wave control failed, status is: %ld\n", __func__, status);
-			break;
-		}
-		if (pmgr->pasid == wac_info.process->pasid) {
-				/* if terminated process was registered for debug, then unregister it  */
-				status = kfd_dbgmgr_unregister(pmgr, process);
-				pmgr->pasid = 0;
-		}
-		if (status != 0)
-			dev_info(NULL,
-					"Error! kfd: In func %s: unregister failed, status is: %ld debugger can not be reused\n",
-					__func__, status);
-
-	} while (false);
-
-	return status;
+	/* Is the requests coming from the already registered process? */
+	if (pmgr->pasid != adw_info->process->pasid) {
+		pr_debug("H/W debugger support was not registered for requester pasid %d\n",
+				adw_info->process->pasid);
+		return -EINVAL;
+	}
 
+	return (long) pmgr->dbgdev->dbgdev_address_watch(pmgr->dbgdev,
+							adw_info);
 }
 
-
-/*///////////////////////////////////////////////////////////////////////////////////////// */
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.h b/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.h
index 2b6484e..257a745 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.h
@@ -26,242 +26,252 @@
 
 #include "kfd_priv.h"
 
-/*
- * SQ_IND_CMD_CMD enum
- */
-
-
-/* must align with hsakmttypes definition. */
+/* must align with hsakmttypes definition */
 #pragma pack(push, 4)
 
-typedef enum _HSA_DBG_WAVEOP {
-	HSA_DBG_WAVEOP_HALT = 1,	/* Halts a wavefront  */
-	HSA_DBG_WAVEOP_RESUME = 2,	/* Resumes a wavefront  */
-	HSA_DBG_WAVEOP_KILL = 3,	/* Kills a wavefront  */
-	HSA_DBG_WAVEOP_DEBUG = 4,	/* Causes wavefront to enter debug mode  */
-	HSA_DBG_WAVEOP_TRAP = 5,	/* Causes wavefront to take a trap  */
+enum HSA_DBG_WAVEOP {
+	HSA_DBG_WAVEOP_HALT = 1,	/* Halts a wavefront		*/
+	HSA_DBG_WAVEOP_RESUME = 2,	/* Resumes a wavefront		*/
+	HSA_DBG_WAVEOP_KILL = 3,	/* Kills a wavefront		*/
+	HSA_DBG_WAVEOP_DEBUG = 4,	/* Causes wavefront to enter
+						debug mode		*/
+	HSA_DBG_WAVEOP_TRAP = 5,	/* Causes wavefront to take
+						a trap			*/
 	HSA_DBG_NUM_WAVEOP = 5,
 	HSA_DBG_MAX_WAVEOP = 0xFFFFFFFF
-} HSA_DBG_WAVEOP;
+};
 
-typedef enum _HSA_DBG_WAVEMODE {
-	HSA_DBG_WAVEMODE_SINGLE = 0,	/* send command to a single wave  */
-	/* Broadcast to all wavefronts of all processes is not supported for HSA user mode */
-	HSA_DBG_WAVEMODE_BROADCAST_PROCESS = 2,	/* send to waves within current process  */
-	HSA_DBG_WAVEMODE_BROADCAST_PROCESS_CU = 3,	/* send to waves within current process on CU  */
+enum HSA_DBG_WAVEMODE {
+	/* send command to a single wave */
+	HSA_DBG_WAVEMODE_SINGLE = 0,
+	/*
+	 * Broadcast to all wavefronts of all processes is not
+	 * supported for HSA user mode
+	 */
+
+	/* send to waves within current process */
+	HSA_DBG_WAVEMODE_BROADCAST_PROCESS = 2,
+	/* send to waves within current process on CU  */
+	HSA_DBG_WAVEMODE_BROADCAST_PROCESS_CU = 3,
 	HSA_DBG_NUM_WAVEMODE = 3,
 	HSA_DBG_MAX_WAVEMODE = 0xFFFFFFFF
-} HSA_DBG_WAVEMODE;
+};
 
-typedef enum _HSA_DBG_WAVEMSG_TYPE {
+enum HSA_DBG_WAVEMSG_TYPE {
 	HSA_DBG_WAVEMSG_AUTO = 0,
 	HSA_DBG_WAVEMSG_USER = 1,
 	HSA_DBG_WAVEMSG_ERROR = 2,
 	HSA_DBG_NUM_WAVEMSG,
 	HSA_DBG_MAX_WAVEMSG = 0xFFFFFFFF
-} HSA_DBG_WAVEMSG_TYPE;
+};
 
-typedef enum _HSA_DBG_WATCH_MODE {
-	HSA_DBG_WATCH_READ = 0,	/* Read operations only  */
-	HSA_DBG_WATCH_NONREAD = 1,	/* Write or Atomic operations only  */
-	HSA_DBG_WATCH_ATOMIC = 2,	/* Atomic Operations only  */
-	HSA_DBG_WATCH_ALL = 3,	/* Read, Write or Atomic operations  */
+enum HSA_DBG_WATCH_MODE {
+	HSA_DBG_WATCH_READ = 0,		/* Read operations only */
+	HSA_DBG_WATCH_NONREAD = 1,	/* Write or Atomic operations only */
+	HSA_DBG_WATCH_ATOMIC = 2,	/* Atomic Operations only */
+	HSA_DBG_WATCH_ALL = 3,		/* Read, Write or Atomic operations */
 	HSA_DBG_WATCH_NUM,
 	HSA_DBG_WATCH_SIZE = 0xFFFFFFFF
-} HSA_DBG_WATCH_MODE;
+};
 
 /* This structure is hardware specific and may change in the future */
-typedef struct _HsaDbgWaveMsgAMDGen2 {
+struct HsaDbgWaveMsgAMDGen2 {
 	union {
-		struct {
-			uint32_t UserData:8;	/*  user data  */
-			uint32_t ShaderArray:1;	/*  Shader array  */
-			uint32_t Priv:1;	/*  Privileged  */
-			uint32_t Reserved0:4;	/*  This field is reserved, should be 0  */
-			uint32_t WaveId:4;	/*  wave id  */
-			uint32_t SIMD:2;	/*  SIMD id  */
-			uint32_t HSACU:4;	/*  Compute unit  */
-			uint32_t ShaderEngine:2;	/*  Shader engine  */
-			uint32_t MessageType:2;	/*  see HSA_DBG_WAVEMSG_TYPE  */
-			uint32_t Reserved1:4;	/*  This field is reserved, should be 0  */
+		struct ui32 {
+			uint32_t UserData:8;	/* user data */
+			uint32_t ShaderArray:1;	/* Shader array */
+			uint32_t Priv:1;	/* Privileged */
+			uint32_t Reserved0:4;	/* This field is reserved,
+						   should be 0 */
+			uint32_t WaveId:4;	/* wave id */
+			uint32_t SIMD:2;	/* SIMD id */
+			uint32_t HSACU:4;	/* Compute unit */
+			uint32_t ShaderEngine:2;/* Shader engine */
+			uint32_t MessageType:2;	/* see HSA_DBG_WAVEMSG_TYPE */
+			uint32_t Reserved1:4;	/* This field is reserved,
+						   should be 0 */
 		} ui32;
 		uint32_t Value;
 	};
-
 	uint32_t Reserved2;
+};
 
-} HsaDbgWaveMsgAMDGen2;
-
-typedef union _HsaDbgWaveMessageAMD {
-	HsaDbgWaveMsgAMDGen2 WaveMsgInfoGen2;
-	/* for future HsaDbgWaveMsgAMDGen3;  */
-} HsaDbgWaveMessageAMD;
-
-typedef struct _HsaDbgWaveMessage {
-	void *MemoryVA;		/*  ptr to associated host-accessible data  */
-	HsaDbgWaveMessageAMD DbgWaveMsg;
-} HsaDbgWaveMessage;
-
-/* TODO: This definitions to be MOVED to kfd_event, once it is implemented.
-
- HSA sync primitive, Event and HW Exception notification API definitions
- The API functions allow the runtime to define a so-called sync-primitive, a SW object
- combining a user-mode provided "syncvar" and a scheduler event that can be signaled
- through a defined GPU interrupt. A syncvar is a process virtual memory location of
- a certain size that can be accessed by CPU and GPU shader code within the process to set
- and query the content within that memory. The definition of the content is determined by
- the HSA runtime and potentially GPU shader code interfacing with the HSA runtime.
- The syncvar values may be commonly written through an PM4 WRITE_DATA packet in the
- user mode instruction stream. The OS scheduler event is typically associated and
- signaled by an interrupt issued by the GPU, but other HSA system interrupt conditions
- from other HW (e.g. IOMMUv2) may besurfaced by the KFD by this mechanism, too.  */
-
-/*  these are the new definitions for events  */
-
-typedef enum _HSA_EVENTTYPE {
-	HSA_EVENTTYPE_SIGNAL = 0,	/* /user-mode generated GPU signal  */
-	HSA_EVENTTYPE_NODECHANGE = 1,	/* HSA node change (attach/detach)  */
-	HSA_EVENTTYPE_DEVICESTATECHANGE = 2,	/* HSA device state change( start/stop )  */
-	HSA_EVENTTYPE_HW_EXCEPTION = 3,	/* GPU shader exception event  */
-	HSA_EVENTTYPE_SYSTEM_EVENT = 4,	/* GPU SYSCALL with parameter info  */
-	HSA_EVENTTYPE_DEBUG_EVENT = 5,	/* GPU signal for debugging  */
-	HSA_EVENTTYPE_PROFILE_EVENT = 6,	/* GPU signal for profiling  */
-	HSA_EVENTTYPE_QUEUE_EVENT = 7,	/* GPU signal queue idle state (EOP pm4)  */
+union HsaDbgWaveMessageAMD {
+	struct HsaDbgWaveMsgAMDGen2 WaveMsgInfoGen2;
+	/* for future HsaDbgWaveMsgAMDGen3; */
+};
+
+struct HsaDbgWaveMessage {
+	void *MemoryVA;		/* ptr to associated host-accessible data */
+	union HsaDbgWaveMessageAMD DbgWaveMsg;
+};
+
+/*
+ * TODO: This definitions to be MOVED to kfd_event, once it is implemented.
+ *
+ * HSA sync primitive, Event and HW Exception notification API definitions.
+ * The API functions allow the runtime to define a so-called sync-primitive,
+ * a SW object combining a user-mode provided "syncvar" and a scheduler event
+ * that can be signaled through a defined GPU interrupt. A syncvar is
+ * a process virtual memory location of a certain size that can be accessed
+ * by CPU and GPU shader code within the process to set and query the content
+ * within that memory. The definition of the content is determined by the HSA
+ * runtime and potentially GPU shader code interfacing with the HSA runtime.
+ * The syncvar values may be commonly written through an PM4 WRITE_DATA packet
+ * in the user mode instruction stream. The OS scheduler event is typically
+ * associated and signaled by an interrupt issued by the GPU, but other HSA
+ * system interrupt conditions from other HW (e.g. IOMMUv2) may be surfaced
+ * by the KFD by this mechanism, too. */
+
+/* these are the new definitions for events */
+enum HSA_EVENTTYPE {
+	HSA_EVENTTYPE_SIGNAL = 0,	/* user-mode generated GPU signal */
+	HSA_EVENTTYPE_NODECHANGE = 1,	/* HSA node change (attach/detach) */
+	HSA_EVENTTYPE_DEVICESTATECHANGE = 2,	/* HSA device state change
+						   (start/stop) */
+	HSA_EVENTTYPE_HW_EXCEPTION = 3,	/* GPU shader exception event */
+	HSA_EVENTTYPE_SYSTEM_EVENT = 4,	/* GPU SYSCALL with parameter info */
+	HSA_EVENTTYPE_DEBUG_EVENT = 5,	/* GPU signal for debugging */
+	HSA_EVENTTYPE_PROFILE_EVENT = 6,/* GPU signal for profiling */
+	HSA_EVENTTYPE_QUEUE_EVENT = 7,	/* GPU signal queue idle state
+					   (EOP pm4) */
 	/* ...  */
 	HSA_EVENTTYPE_MAXID,
 	HSA_EVENTTYPE_TYPE_SIZE = 0xFFFFFFFF
-} HSA_EVENTTYPE;
-
-typedef uint32_t HSA_EVENTID;
-
-/*  Subdefinitions for various event types: Syncvar  */
+};
 
-typedef struct _HsaSyncVar {
-	union {
-		void *UserData;	/* pointer to user mode data  */
-		uint64_t UserDataPtrValue;	/* 64bit compatibility of value  */
+/* Sub-definitions for various event types: Syncvar */
+struct HsaSyncVar {
+	union SyncVar {
+		void *UserData;	/* pointer to user mode data */
+		uint64_t UserDataPtrValue; /* 64bit compatibility of value */
 	} SyncVar;
 	uint64_t SyncVarSize;
-} HsaSyncVar;
+};
 
-/*
- Subdefinitions for various event types: NodeChange
-*/
+/* Sub-definitions for various event types: NodeChange */
 
-typedef enum _HSA_EVENTTYPE_NODECHANGE_FLAGS {
+enum HSA_EVENTTYPE_NODECHANGE_FLAGS {
 	HSA_EVENTTYPE_NODECHANGE_ADD = 0,
 	HSA_EVENTTYPE_NODECHANGE_REMOVE = 1,
 	HSA_EVENTTYPE_NODECHANGE_SIZE = 0xFFFFFFFF
-} HSA_EVENTTYPE_NODECHANGE_FLAGS;
-
-typedef struct _HsaNodeChange {
-	HSA_EVENTTYPE_NODECHANGE_FLAGS Flags;	/*  HSA node added/removed on the platform  */
-} HsaNodeChange;
+};
 
-/*
- Sub-definitions for various event types: DeviceStateChange
-*/
+struct HsaNodeChange {
+	/* HSA node added/removed on the platform */
+	enum HSA_EVENTTYPE_NODECHANGE_FLAGS Flags;
+};
 
-typedef enum _HSA_EVENTTYPE_DEVICESTATECHANGE_FLAGS {
-	HSA_EVENTTYPE_DEVICESTATUSCHANGE_START = 0,	/* device started (and available)  */
-	HSA_EVENTTYPE_DEVICESTATUSCHANGE_STOP = 1,	/* device stopped (i.e. unavailable)  */
+/* Sub-definitions for various event types: DeviceStateChange */
+enum HSA_EVENTTYPE_DEVICESTATECHANGE_FLAGS {
+	/* device started (and available) */
+	HSA_EVENTTYPE_DEVICESTATUSCHANGE_START = 0,
+	/* device stopped (i.e. unavailable) */
+	HSA_EVENTTYPE_DEVICESTATUSCHANGE_STOP = 1,
 	HSA_EVENTTYPE_DEVICESTATUSCHANGE_SIZE = 0xFFFFFFFF
-} HSA_EVENTTYPE_DEVICESTATECHANGE_FLAGS;
+};
 
-typedef enum _HSA_DEVICE {
+enum HSA_DEVICE {
 	HSA_DEVICE_CPU = 0,
 	HSA_DEVICE_GPU = 1,
 	MAX_HSA_DEVICE = 2
-} HSA_DEVICE;
+};
 
-typedef struct _HsaDeviceStateChange {
+struct HsaDeviceStateChange {
 	uint32_t NodeId;	/* F-NUMA node that contains the device */
-	HSA_DEVICE Device;	/* device type: GPU or CPU */
-	HSA_EVENTTYPE_DEVICESTATECHANGE_FLAGS Flags;	/* event flags   */
-} HsaDeviceStateChange;
+	enum HSA_DEVICE Device;	/* device type: GPU or CPU */
+	enum HSA_EVENTTYPE_DEVICESTATECHANGE_FLAGS Flags; /* event flags */
+};
 
-typedef struct _HsaEventData {
-	HSA_EVENTTYPE EventType;	/* event type */
-	union {
-		/* return data associated with HSA_EVENTTYPE_SIGNAL and other events */
-		HsaSyncVar SyncVar;
+struct HsaEventData {
+	enum HSA_EVENTTYPE EventType; /* event type */
+	union EventData {
+		/*
+		 * return data associated with HSA_EVENTTYPE_SIGNAL
+		 * and other events
+		 */
+		struct HsaSyncVar SyncVar;
 
 		/* data associated with HSA_EVENTTYPE_NODE_CHANGE */
-		HsaNodeChange NodeChangeState;
+		struct HsaNodeChange NodeChangeState;
 
 		/* data associated with HSA_EVENTTYPE_DEVICE_STATE_CHANGE */
-		HsaDeviceStateChange DeviceState;
+		struct HsaDeviceStateChange DeviceState;
 	} EventData;
 
-	/* the following data entries are internal to the KFD & thunk itself. */
+	/* the following data entries are internal to the KFD & thunk itself */
 
-	uint64_t HWData1;	/* internal thunk store for Event data  (OsEventHandle) */
-	uint64_t HWData2;	/* internal thunk store for Event data  (HWAddress) */
-	uint32_t HWData3;	/* internal thunk store for Event data  (HWData) */
-} HsaEventData;
-
-typedef struct _HsaEventDescriptor {
-	HSA_EVENTTYPE EventType;	/* event type to allocate */
-	uint32_t NodeId;	/* H-NUMA node containing GPU device that is event source */
-	HsaSyncVar SyncVar;	/* pointer to user mode syncvar data, syncvar->UserDataPtrValue may be NULL */
-} HsaEventDescriptor;
+	/* internal thunk store for Event data (OsEventHandle) */
+	uint64_t HWData1;
+	/* internal thunk store for Event data (HWAddress) */
+	uint64_t HWData2;
+	/* internal thunk store for Event data (HWData) */
+	uint32_t HWData3;
+};
 
-typedef struct _HsaEvent {
-	HSA_EVENTID EventId;
-	HsaEventData EventData;
-} HsaEvent;
+struct HsaEventDescriptor {
+	/* event type to allocate */
+	enum HSA_EVENTTYPE EventType;
+	/* H-NUMA node containing GPU device that is event source */
+	uint32_t NodeId;
+	/* pointer to user mode syncvar data, syncvar->UserDataPtrValue
+	 * may be NULL
+	 */
+	struct HsaSyncVar SyncVar;
+};
 
+struct HsaEvent {
+	uint32_t EventId;
+	struct HsaEventData EventData;
+};
 
 #pragma pack(pop)
 
-typedef enum _DBGDEV_TYPE {
+enum DBGDEV_TYPE {
 	DBGDEV_TYPE_ILLEGAL = 0,
 	DBGDEV_TYPE_NODIQ = 1,
 	DBGDEV_TYPE_DIQ = 2,
 	DBGDEV_TYPE_TEST = 3
-} DBGDEV_TYPE;
+};
 
 struct dbg_address_watch_info {
 	struct kfd_process *process;
-	HSA_DBG_WATCH_MODE *watch_mode;
+	enum HSA_DBG_WATCH_MODE *watch_mode;
 	uint64_t *watch_address;
 	uint64_t *watch_mask;
-	HsaEvent *watch_event;
+	struct HsaEvent *watch_event;
 	uint32_t num_watch_points;
 };
 
 struct dbg_wave_control_info {
 	struct kfd_process *process;
 	uint32_t trapId;
-	HSA_DBG_WAVEOP operand;
-	HSA_DBG_WAVEMODE mode;
-	HsaDbgWaveMessage dbgWave_msg;
+	enum HSA_DBG_WAVEOP operand;
+	enum HSA_DBG_WAVEMODE mode;
+	struct HsaDbgWaveMessage dbgWave_msg;
 };
 
 struct kfd_dbgdev {
 
 	/* The device that owns this data. */
-
 	struct kfd_dev *dev;
 
 	/* kernel queue for DIQ */
-
 	struct kernel_queue *kq;
 
 	/* a pointer to the pqm of the calling process */
-
 	struct process_queue_manager *pqm;
 
 	/* type of debug device ( DIQ, non DIQ, etc. ) */
-
-	DBGDEV_TYPE type;
+	enum DBGDEV_TYPE type;
 
 	/* virtualized function pointers to device dbg */
-
 	int (*dbgdev_register)(struct kfd_dbgdev *dbgdev);
 	int (*dbgdev_unregister)(struct kfd_dbgdev *dbgdev);
-	int (*dbgdev_address_watch)(struct kfd_dbgdev *dbgdev, struct dbg_address_watch_info *adw_info);
-	int (*dbgdev_wave_control)(struct kfd_dbgdev *dbgdev, struct dbg_wave_control_info *wac_info);
+	int (*dbgdev_address_watch)(struct kfd_dbgdev *dbgdev,
+				struct dbg_address_watch_info *adw_info);
+	int (*dbgdev_wave_control)(struct kfd_dbgdev *dbgdev,
+				struct dbg_wave_control_info *wac_info);
 
 };
 
@@ -272,12 +282,13 @@ struct kfd_dbgmgr {
 };
 
 /* prototypes for debug manager functions */
-struct mutex *get_dbgmgr_mutex(void);
+struct mutex *kfd_get_dbgmgr_mutex(void);
 void kfd_dbgmgr_destroy(struct kfd_dbgmgr *pmgr);
 bool kfd_dbgmgr_create(struct kfd_dbgmgr **ppmgr, struct kfd_dev *pdev);
 long kfd_dbgmgr_register(struct kfd_dbgmgr *pmgr, struct kfd_process *p);
 long kfd_dbgmgr_unregister(struct kfd_dbgmgr *pmgr, struct kfd_process *p);
-long kfd_dbgmgr_wave_control(struct kfd_dbgmgr *pmgr, struct dbg_wave_control_info *wac_info);
-long kfd_dbgmgr_address_watch(struct kfd_dbgmgr *pmgr, struct dbg_address_watch_info *adw_info);
-long kfd_dbgmgr_abnormal_termination(struct kfd_dbgmgr *pmgr, struct kfd_process *process);
-#endif				/* KFD_DBGMGR_H_ */
+long kfd_dbgmgr_wave_control(struct kfd_dbgmgr *pmgr,
+				struct dbg_wave_control_info *wac_info);
+long kfd_dbgmgr_address_watch(struct kfd_dbgmgr *pmgr,
+			struct dbg_address_watch_info *adw_info);
+#endif /* KFD_DBGMGR_H_ */
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_debugfs.c b/drivers/gpu/drm/amd/amdkfd/kfd_debugfs.c
deleted file mode 100644
index ccf982d..0000000
--- a/drivers/gpu/drm/amd/amdkfd/kfd_debugfs.c
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Copyright 2014 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- */
-
-#include <linux/debugfs.h>
-#include "kfd_priv.h"
-
-static struct dentry *debugfs_root;
-
-static int kfd_debugfs_open(struct inode *inode, struct file *file)
-{
-	int (*show)(struct seq_file *, void *) = inode->i_private;
-
-	return single_open(file, show, NULL);
-}
-
-static const struct file_operations kfd_debugfs_fops = {
-	.owner = THIS_MODULE,
-	.open = kfd_debugfs_open,
-	.read = seq_read,
-	.llseek = seq_lseek,
-	.release = single_release,
-};
-
-void kfd_debugfs_init(void)
-{
-	struct dentry *ent;
-
-	debugfs_root = debugfs_create_dir("kfd", NULL);
-	if (debugfs_root == NULL ||
-	    debugfs_root == ERR_PTR(-ENODEV)) {
-		pr_warn("Failed to create kfd debugfs dir\n");
-		return;
-	}
-
-	ent = debugfs_create_file("mqds", S_IFREG | S_IRUGO, debugfs_root,
-				  kfd_debugfs_mqds_by_process,
-				  &kfd_debugfs_fops);
-	if (ent == NULL)
-		pr_warn("Failed to create mqds in kfd debugfs\n");
-
-	ent = debugfs_create_file("hqds", S_IFREG | S_IRUGO, debugfs_root,
-				  kfd_debugfs_hqds_by_device,
-				  &kfd_debugfs_fops);
-	if (ent == NULL)
-		pr_warn("Failed to create hqds in kfd debugfs\n");
-
-	ent = debugfs_create_file("rls", S_IFREG | S_IRUGO, debugfs_root,
-				  kfd_debugfs_rls_by_device,
-				  &kfd_debugfs_fops);
-	if (ent == NULL)
-		pr_warn("Failed to create rls in kfd debugfs\n");
-}
-
-void kfd_debugfs_fini(void)
-{
-	debugfs_remove_recursive(debugfs_root);
-}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device.c b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
index 75daa32..3f95f7c 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
@@ -24,12 +24,9 @@
 #include <linux/bsearch.h>
 #include <linux/pci.h>
 #include <linux/slab.h>
-#include <linux/highmem.h>
-#include <linux/fence.h>
 #include "kfd_priv.h"
 #include "kfd_device_queue_manager.h"
 #include "kfd_pm4_headers.h"
-#include "cwsr_trap_handler_carrizo.h"
 
 #define MQD_SIZE_ALIGNED 768
 
@@ -41,22 +38,7 @@ static const struct kfd_device_info kaveri_device_info = {
 	.ih_ring_entry_size = 4 * sizeof(uint32_t),
 	.event_interrupt_class = &event_interrupt_class_cik,
 	.num_of_watch_points = 4,
-	.mqd_size_aligned = MQD_SIZE_ALIGNED,
-	.is_need_iommu_device = true,
-	.supports_cwsr = false,
-};
-
-static const struct kfd_device_info hawaii_device_info = {
-	.asic_family = CHIP_HAWAII,
-	.max_pasid_bits = 16,
-	/* max num of queues for KV.TODO should be a dynamic value */
-	.max_no_of_hqd	= 24,
-	.ih_ring_entry_size = 4 * sizeof(uint32_t),
-	.event_interrupt_class = &event_interrupt_class_cik,
-	.num_of_watch_points = 4,
-	.mqd_size_aligned = MQD_SIZE_ALIGNED,
-	.is_need_iommu_device = false,
-	.supports_cwsr = false,
+	.mqd_size_aligned = MQD_SIZE_ALIGNED
 };
 
 static const struct kfd_device_info carrizo_device_info = {
@@ -67,57 +49,7 @@ static const struct kfd_device_info carrizo_device_info = {
 	.ih_ring_entry_size = 4 * sizeof(uint32_t),
 	.event_interrupt_class = &event_interrupt_class_cik,
 	.num_of_watch_points = 4,
-	.mqd_size_aligned = MQD_SIZE_ALIGNED,
-	.is_need_iommu_device = true,
-	.supports_cwsr = true,
-};
-
-static const struct kfd_device_info tonga_device_info = {
-	.asic_family = CHIP_TONGA,
-	.max_pasid_bits = 16,
-	.max_no_of_hqd  = 24,
-	.ih_ring_entry_size = 4 * sizeof(uint32_t),
-	.event_interrupt_class = &event_interrupt_class_cik,
-	.num_of_watch_points = 4,
-	.mqd_size_aligned = MQD_SIZE_ALIGNED,
-	.is_need_iommu_device = false,
-	.supports_cwsr = false,
-};
-
-static const struct kfd_device_info fiji_device_info = {
-	.asic_family = CHIP_FIJI,
-	.max_pasid_bits = 16,
-	.max_no_of_hqd  = 24,
-	.ih_ring_entry_size = 4 * sizeof(uint32_t),
-	.event_interrupt_class = &event_interrupt_class_cik,
-	.num_of_watch_points = 4,
-	.mqd_size_aligned = MQD_SIZE_ALIGNED,
-	.is_need_iommu_device = false,
-	.supports_cwsr = true,
-};
-
-static const struct kfd_device_info polaris10_device_info = {
-	.asic_family = CHIP_POLARIS10,
-	.max_pasid_bits = 16,
-	.max_no_of_hqd  = 24,
-	.ih_ring_entry_size = 4 * sizeof(uint32_t),
-	.event_interrupt_class = &event_interrupt_class_cik,
-	.num_of_watch_points = 4,
-	.mqd_size_aligned = MQD_SIZE_ALIGNED,
-	.is_need_iommu_device = false,
-	.supports_cwsr = true,
-};
-
-static const struct kfd_device_info polaris11_device_info = {
-	.asic_family = CHIP_POLARIS11,
-	.max_pasid_bits = 16,
-	.max_no_of_hqd  = 24,
-	.ih_ring_entry_size = 4 * sizeof(uint32_t),
-	.event_interrupt_class = &event_interrupt_class_cik,
-	.num_of_watch_points = 4,
-	.mqd_size_aligned = MQD_SIZE_ALIGNED,
-	.is_need_iommu_device = false,
-	.supports_cwsr = true,
+	.mqd_size_aligned = MQD_SIZE_ALIGNED
 };
 
 struct kfd_deviceid {
@@ -125,19 +57,6 @@ struct kfd_deviceid {
 	const struct kfd_device_info *device_info;
 };
 
-/*
- * //
-// TONGA/AMETHYST device IDs (performance segment)
-//
-#define DEVICE_ID_VI_TONGA_P_6920               0x6920  // unfused
-#define DEVICE_ID_VI_TONGA_P_6921               0x6921  // Amethyst XT
-#define DEVICE_ID_VI_TONGA_P_6928               0x6928  // Tonga GL XT
-#define DEVICE_ID_VI_TONGA_P_692B               0x692B  // Tonga GL PRO
-#define DEVICE_ID_VI_TONGA_P_692F               0x692F  // Tonga GL PRO VF
-#define DEVICE_ID_VI_TONGA_P_6938               0x6938  // Tonga XT
-#define DEVICE_ID_VI_TONGA_P_6939               0x6939  // Tonga PRO
- *
- */
 /* Please keep this sorted by increasing device id. */
 static const struct kfd_deviceid supported_devices[] = {
 	{ 0x1304, &kaveri_device_info },	/* Kaveri */
@@ -162,47 +81,17 @@ static const struct kfd_deviceid supported_devices[] = {
 	{ 0x131B, &kaveri_device_info },	/* Kaveri */
 	{ 0x131C, &kaveri_device_info },	/* Kaveri */
 	{ 0x131D, &kaveri_device_info },	/* Kaveri */
-	{ 0x67A0, &hawaii_device_info },	/* Hawaii */
-	{ 0x67A1, &hawaii_device_info },	/* Hawaii */
-	{ 0x67A2, &hawaii_device_info },	/* Hawaii */
-	{ 0x67A8, &hawaii_device_info },	/* Hawaii */
-	{ 0x67A9, &hawaii_device_info },	/* Hawaii */
-	{ 0x67AA, &hawaii_device_info },	/* Hawaii */
-	{ 0x67B0, &hawaii_device_info },	/* Hawaii */
-	{ 0x67B1, &hawaii_device_info },	/* Hawaii */
-	{ 0x67B8, &hawaii_device_info },	/* Hawaii */
-	{ 0x67B9, &hawaii_device_info },	/* Hawaii */
-	{ 0x67BA, &hawaii_device_info },	/* Hawaii */
-	{ 0x67BE, &hawaii_device_info },	/* Hawaii */
 	{ 0x9870, &carrizo_device_info },	/* Carrizo */
 	{ 0x9874, &carrizo_device_info },	/* Carrizo */
 	{ 0x9875, &carrizo_device_info },	/* Carrizo */
 	{ 0x9876, &carrizo_device_info },	/* Carrizo */
-	{ 0x9877, &carrizo_device_info },	/* Carrizo */
-	{ 0x6920, &tonga_device_info   },	/* Tonga */
-	{ 0x6921, &tonga_device_info   },	/* Tonga */
-	{ 0x6928, &tonga_device_info   },	/* Tonga */
-	{ 0x692B, &tonga_device_info   },	/* Tonga */
-	{ 0x692F, &tonga_device_info   },	/* Tonga */
-	{ 0x6938, &tonga_device_info   },	/* Tonga */
-	{ 0x6939, &tonga_device_info   },	/* Tonga */
-	{ 0x7300, &fiji_device_info    },	/* Fiji */
-	{ 0x67C0, &polaris10_device_info },	/* Polaris10 */
-	{ 0x67C4, &polaris10_device_info },	/* Polaris10 */
-	{ 0x67C7, &polaris10_device_info },	/* Polaris10 */
-	{ 0x67DF, &polaris10_device_info },	/* Polaris10 */
-	{ 0x67E3, &polaris11_device_info },	/* Polaris11 */
-	{ 0x67E8, &polaris11_device_info },	/* Polaris11 */
-	{ 0x67EF, &polaris11_device_info },	/* Polaris11 */
-	{ 0x67FF, &polaris11_device_info }	/* Polaris11 */
+	{ 0x9877, &carrizo_device_info }	/* Carrizo */
 };
 
 static int kfd_gtt_sa_init(struct kfd_dev *kfd, unsigned int buf_size,
 				unsigned int chunk_size);
 static void kfd_gtt_sa_fini(struct kfd_dev *kfd);
 
-static int kfd_resume(struct kfd_dev *kfd);
-
 static const struct kfd_device_info *lookup_device_info(unsigned short did)
 {
 	size_t i;
@@ -228,8 +117,6 @@ struct kfd_dev *kgd2kfd_probe(struct kgd_dev *kgd,
 	if (!device_info)
 		return NULL;
 
-	BUG_ON(!f2g);
-
 	kfd = kzalloc(sizeof(*kfd), GFP_KERNEL);
 	if (!kfd)
 		return NULL;
@@ -283,8 +170,15 @@ static bool device_iommu_pasid_init(struct kfd_dev *kfd)
 				pasid_limit,
 				kfd->doorbell_process_limit - 1);
 
+	err = amd_iommu_init_device(kfd->pdev, pasid_limit);
+	if (err < 0) {
+		dev_err(kfd_device, "error initializing iommu device\n");
+		return false;
+	}
+
 	if (!kfd_set_pasid_limit(pasid_limit)) {
 		dev_err(kfd_device, "error setting pasid limit\n");
+		amd_iommu_free_device(kfd->pdev);
 		return false;
 	}
 
@@ -296,7 +190,7 @@ static void iommu_pasid_shutdown_callback(struct pci_dev *pdev, int pasid)
 	struct kfd_dev *dev = kfd_device_by_pci_dev(pdev);
 
 	if (dev)
-		kfd_process_iommu_unbind_callback(dev, pasid);
+		kfd_unbind_process_from_device(dev, pasid);
 }
 
 /*
@@ -325,94 +219,13 @@ static int iommu_invalid_ppr_cb(struct pci_dev *pdev, int pasid,
 	return AMD_IOMMU_INV_PRI_RSP_INVALID;
 }
 
-static int kfd_cwsr_init(struct kfd_dev *kfd)
-{
-	/*
-	 * Initialize the CWSR required memory for TBA and TMA
-	 */
-	if (cwsr_enable && kfd->device_info->supports_cwsr) {
-		void *cwsr_addr = NULL;
-		unsigned int size = sizeof(cwsr_trap_carrizo_hex);
-
-		if (size > PAGE_SIZE) {
-			pr_err("amdkfd: wrong CWSR ISA size.\n");
-			return -EINVAL;
-		}
-		kfd->cwsr_size =
-			ALIGN(size, PAGE_SIZE) + PAGE_SIZE;
-		kfd->cwsr_pages = alloc_pages(GFP_KERNEL | __GFP_HIGHMEM,
-					get_order(kfd->cwsr_size));
-		if (!kfd->cwsr_pages) {
-			pr_err("amdkfd: error alloc CWSR isa memory.\n");
-			return -ENOMEM;
-		}
-		/*Only first page used for cwsr ISA code */
-		cwsr_addr = kmap(kfd->cwsr_pages);
-		memset(cwsr_addr, 0, PAGE_SIZE);
-		memcpy(cwsr_addr, cwsr_trap_carrizo_hex, size);
-		kunmap(kfd->cwsr_pages);
-		kfd->tma_offset = ALIGN(size, PAGE_SIZE);
-		kfd->cwsr_enabled = true;
-		dev_info(kfd_device,
-			"Reserved %d pages for cwsr.\n",
-			(kfd->cwsr_size >> PAGE_SHIFT));
-	}
-
-	return 0;
-}
-
-static void kfd_cwsr_fini(struct kfd_dev *kfd)
-{
-	if (kfd->cwsr_pages)
-		__free_pages(kfd->cwsr_pages, get_order(kfd->cwsr_size));
-}
-
-static void kfd_ib_mem_init(struct kfd_dev *kdev)
-{
-	/* In certain cases we need to send IB from kernel using the GPU address
-	 * space created by user applications.
-	 * For example, on GFX v7, we need to flush TC associated to the VMID
-	 * before tearing down the VMID. In order to do so, we need an address
-	 * valid to the VMID to place the IB while this space was created on
-	 * the user's side, not the kernel.
-	 * Since kfd_set_process_dgpu_aperture reserves "cwsr_base + cwsr_size"
-	 * but CWSR only uses pages above cwsr_base, we'll use one page memory
-	 * under cwsr_base for IB submissions
-	 */
-	kdev->ib_size = PAGE_SIZE;
-}
-
 bool kgd2kfd_device_init(struct kfd_dev *kfd,
 			 const struct kgd2kfd_shared_resources *gpu_resources)
 {
 	unsigned int size;
-	unsigned int vmid_bitmap_kfd, vmid_num_kfd;
-
-	kfd->mec_fw_version = kfd->kfd2kgd->get_fw_version(kfd->kgd,
-			KGD_ENGINE_MEC1);
 
 	kfd->shared_resources = *gpu_resources;
 
-	vmid_bitmap_kfd = kfd->shared_resources.compute_vmid_bitmap;
-	kfd->vm_info.first_vmid_kfd = ffs(vmid_bitmap_kfd) - 1;
-	kfd->vm_info.last_vmid_kfd = fls(vmid_bitmap_kfd) - 1;
-	vmid_num_kfd = kfd->vm_info.last_vmid_kfd
-			- kfd->vm_info.first_vmid_kfd + 1;
-	kfd->vm_info.vmid_num_kfd = vmid_num_kfd;
-
-	/* If MEC firmware is too old, turn off hws multiple process mapping */
-	if (kfd->mec_fw_version	< KFD_MULTI_PROC_MAPPING_HWS_SUPPORT)
-		kfd->max_proc_per_quantum = 0;
-	/* Verify module parameters regarding mapped process number*/
-	else if ((hws_max_conc_proc < 0)
-			|| (hws_max_conc_proc > vmid_num_kfd)) {
-		dev_err(kfd_device,
-			"hws_max_conc_proc (%d) must be between 0 and %d, use %d instead\n",
-			hws_max_conc_proc, vmid_num_kfd, vmid_num_kfd);
-		kfd->max_proc_per_quantum = vmid_num_kfd;
-	} else
-		kfd->max_proc_per_quantum = hws_max_conc_proc;
-
 	/* calculate max size of mqds needed for queues */
 	size = max_num_of_queues_per_device *
 			kfd->device_info->mqd_size_aligned;
@@ -467,6 +280,16 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
 		goto kfd_interrupt_error;
 	}
 
+	if (!device_iommu_pasid_init(kfd)) {
+		dev_err(kfd_device,
+			"Error initializing iommuv2 for device (%x:%x)\n",
+			kfd->pdev->vendor, kfd->pdev->device);
+		goto device_iommu_pasid_error;
+	}
+	amd_iommu_set_invalidate_ctx_cb(kfd->pdev,
+						iommu_pasid_shutdown_callback);
+	amd_iommu_set_invalid_ppr_cb(kfd->pdev, iommu_invalid_ppr_cb);
+
 	kfd->dqm = device_queue_manager_init(kfd);
 	if (!kfd->dqm) {
 		dev_err(kfd_device,
@@ -475,23 +298,13 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
 		goto device_queue_manager_error;
 	}
 
-	if (kfd->device_info->is_need_iommu_device) {
-		if (!device_iommu_pasid_init(kfd)) {
-			dev_err(kfd_device,
-				"Error initializing iommuv2 for device (%x:%x)\n",
-				kfd->pdev->vendor, kfd->pdev->device);
-			goto device_iommu_pasid_error;
-		}
+	if (kfd->dqm->ops.start(kfd->dqm) != 0) {
+		dev_err(kfd_device,
+			"Error starting queuen manager for device (%x:%x)\n",
+			kfd->pdev->vendor, kfd->pdev->device);
+		goto dqm_start_error;
 	}
 
-	if (kfd_cwsr_init(kfd))
-		goto device_iommu_pasid_error;
-
-	kfd_ib_mem_init(kfd);
-
-	if (kfd_resume(kfd))
-		goto kfd_resume_error;
-
 	kfd->dbgmgr = NULL;
 
 	kfd->init_complete = true;
@@ -499,15 +312,15 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
 		 kfd->pdev->device);
 
 	pr_debug("kfd: Starting kfd with the following scheduling policy %d\n",
-		kfd->dqm->sched_policy);
+		sched_policy);
 
 	goto out;
 
-kfd_resume_error:
-	kfd_cwsr_fini(kfd);
-device_iommu_pasid_error:
+dqm_start_error:
 	device_queue_manager_uninit(kfd->dqm);
 device_queue_manager_error:
+	amd_iommu_free_device(kfd->pdev);
+device_iommu_pasid_error:
 	kfd_interrupt_exit(kfd);
 kfd_interrupt_error:
 	kfd_topology_remove_device(kfd);
@@ -525,9 +338,8 @@ out:
 void kgd2kfd_device_exit(struct kfd_dev *kfd)
 {
 	if (kfd->init_complete) {
-		kgd2kfd_suspend(kfd);
-		kfd_cwsr_fini(kfd);
 		device_queue_manager_uninit(kfd->dqm);
+		amd_iommu_free_device(kfd->pdev);
 		kfd_interrupt_exit(kfd);
 		kfd_topology_remove_device(kfd);
 		kfd_gtt_sa_fini(kfd);
@@ -541,342 +353,52 @@ void kgd2kfd_suspend(struct kfd_dev *kfd)
 {
 	BUG_ON(kfd == NULL);
 
-	if (!kfd->init_complete)
-		return;
-
-	kfd->dqm->ops.stop(kfd->dqm);
-	if (!kfd->device_info->is_need_iommu_device)
-		return;
-
-	kfd_unbind_processes_from_device(kfd);
-
-	amd_iommu_set_invalidate_ctx_cb(kfd->pdev, NULL);
-	amd_iommu_set_invalid_ppr_cb(kfd->pdev, NULL);
-	amd_iommu_free_device(kfd->pdev);
+	if (kfd->init_complete) {
+		kfd->dqm->ops.stop(kfd->dqm);
+		amd_iommu_set_invalidate_ctx_cb(kfd->pdev, NULL);
+		amd_iommu_set_invalid_ppr_cb(kfd->pdev, NULL);
+		amd_iommu_free_device(kfd->pdev);
+	}
 }
 
 int kgd2kfd_resume(struct kfd_dev *kfd)
 {
-	BUG_ON(kfd == NULL);
-
-	if (!kfd->init_complete)
-		return 0;
-
-	return kfd_resume(kfd);
-
-}
+	unsigned int pasid_limit;
+	int err;
 
-static int kfd_resume(struct kfd_dev *kfd)
-{
-	int err = 0;
+	BUG_ON(kfd == NULL);
 
-	if (kfd->device_info->is_need_iommu_device) {
-		unsigned int pasid_limit = kfd_get_pasid_limit();
+	pasid_limit = kfd_get_pasid_limit();
 
+	if (kfd->init_complete) {
 		err = amd_iommu_init_device(kfd->pdev, pasid_limit);
-		if (err)
+		if (err < 0)
 			return -ENXIO;
 		amd_iommu_set_invalidate_ctx_cb(kfd->pdev,
-				iommu_pasid_shutdown_callback);
-		amd_iommu_set_invalid_ppr_cb(kfd->pdev,
-				iommu_invalid_ppr_cb);
-
-		err = kfd_bind_processes_to_device(kfd);
-		if (err)
-			return -ENXIO;
+						iommu_pasid_shutdown_callback);
+		amd_iommu_set_invalid_ppr_cb(kfd->pdev, iommu_invalid_ppr_cb);
+		kfd->dqm->ops.start(kfd->dqm);
 	}
 
-	err = kfd->dqm->ops.start(kfd->dqm);
-	if (err) {
-		dev_err(kfd_device,
-			"Error starting queue manager for device (%x:%x)\n",
-			kfd->pdev->vendor, kfd->pdev->device);
-		goto dqm_start_error;
-	}
-
-	kfd->kfd2kgd->write_config_static_mem(kfd->kgd, true, 1, 3, 0);
-
-	return err;
-
-dqm_start_error:
-	if (kfd->device_info->is_need_iommu_device)
-		amd_iommu_free_device(kfd->pdev);
-
-	return err;
+	return 0;
 }
 
 /* This is called directly from KGD at ISR. */
 void kgd2kfd_interrupt(struct kfd_dev *kfd, const void *ih_ring_entry)
 {
-	uint32_t patched_ihre[DIV_ROUND_UP(
-				kfd->device_info->ih_ring_entry_size,
-				sizeof(uint32_t))];
-	bool is_patched = false;
-
 	if (!kfd->init_complete)
 		return;
 
 	spin_lock(&kfd->interrupt_lock);
 
 	if (kfd->interrupts_active
-	    && interrupt_is_wanted(kfd, ih_ring_entry, patched_ihre, &is_patched)
-	    && enqueue_ih_ring_entry(kfd, is_patched ? patched_ihre : ih_ring_entry))
-		queue_work(kfd->ih_wq, &kfd->interrupt_work);
+	    && interrupt_is_wanted(kfd, ih_ring_entry)
+	    && enqueue_ih_ring_entry(kfd, ih_ring_entry))
+		schedule_work(&kfd->interrupt_work);
 
 	spin_unlock(&kfd->interrupt_lock);
 }
 
-int kgd2kfd_quiesce_mm(struct kfd_dev *kfd, struct mm_struct *mm)
-{
-	struct kfd_process *p;
-	struct kfd_process_device *pdd;
-	int r;
-
-	BUG_ON(kfd == NULL);
-	if (!kfd->init_complete)
-		return 0;
-
-	/* Because we are called from arbitrary context (workqueue) as opposed
-	 * to process context, kfd_process could attempt to exit while we are
-	 * running so the lookup function increments the process ref count.
-	 */
-	p = kfd_lookup_process_by_mm(mm);
-	if (!p)
-		return -ENODEV;
-
-	r = -ENODEV;
-	pdd = kfd_get_process_device_data(kfd, p);
-	if (pdd)
-		r = process_evict_queues(kfd->dqm, &pdd->qpd);
-
-	kfd_unref_process(p);
-	return r;
-}
-
-int kgd2kfd_resume_mm(struct kfd_dev *kfd, struct mm_struct *mm)
-{
-	struct kfd_process *p;
-	struct kfd_process_device *pdd;
-	int r;
-
-	BUG_ON(kfd == NULL);
-	if (!kfd->init_complete)
-		return 0;
-
-	/* Because we are called from arbitrary context (workqueue) as opposed
-	 * to process context, kfd_process could attempt to exit while we are
-	 * running so the lookup function increments the process ref count.
-	 */
-	p = kfd_lookup_process_by_mm(mm);
-	if (!p)
-		return -ENODEV;
-
-	r = -ENODEV;
-	pdd = kfd_get_process_device_data(kfd, p);
-	if (pdd)
-		r = process_restore_queues(kfd->dqm, &pdd->qpd);
-
-	kfd_unref_process(p);
-	return r;
-}
-
-/* quiesce_process_mm -
- *  Quiesce all user queues that belongs to given process p
- */
-static int quiesce_process_mm(struct kfd_process *p)
-{
-	struct kfd_process_device *pdd;
-	int r = 0;
-	unsigned int n_evicted = 0;
-
-	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
-		r = process_evict_queues(pdd->dev->dqm, &pdd->qpd);
-		if (r != 0) {
-			pr_err("Failed to evict process queues\n");
-			goto fail;
-		}
-		n_evicted++;
-	}
-
-	return r;
-
-fail:
-	/* To keep state consistent, roll back partial eviction by
-	 * restoring queues
-	 */
-	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
-		if (n_evicted == 0)
-			break;
-		if (process_restore_queues(pdd->dev->dqm, &pdd->qpd))
-			pr_err("Failed to restore queues\n");
-
-		n_evicted--;
-	}
-
-	return r;
-}
-
-/* resume_process_mm -
- *  Resume all user queues that belongs to given process p. The caller must
- *  ensure that process p context is valid.
- */
-static int resume_process_mm(struct kfd_process *p)
-{
-	struct kfd_process_device *pdd;
-	struct mm_struct *mm = (struct mm_struct *)p->mm;
-	int r, ret = 0;
-
-	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
-		if (pdd->dev->dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS)
-			down_read(&mm->mmap_sem);
-
-		r = process_restore_queues(pdd->dev->dqm, &pdd->qpd);
-		if (r != 0) {
-			pr_err("Failed to restore process queues\n");
-			if (ret == 0)
-				ret = r;
-		}
-
-		if (pdd->dev->dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS)
-			up_read(&mm->mmap_sem);
-	}
-
-	return ret;
-}
-
-/** kfd_schedule_restore_bos_and_queues - Schedules work queue that will
- *   restore all BOs that belong to given process and then restore its queues
- *
- * @mm: mm_struct that identifies the KFD process
- *
- */
-static int kfd_schedule_restore_bos_and_queues(struct kfd_process *p)
-{
-	if (delayed_work_pending(&p->restore_work)) {
-		WARN(1, "Trying to evict an unrestored process\n");
-		cancel_delayed_work_sync(&p->restore_work);
-	}
-
-	/* During process initialization restore_work is initialized
-	 * to kfd_restore_bo_worker
-	 */
-	schedule_delayed_work(&p->restore_work, PROCESS_RESTORE_TIME_MS);
-	return 0;
-}
-
-void kfd_restore_bo_worker(struct work_struct *work)
-{
-	struct delayed_work *dwork;
-	struct kfd_process *p;
-	struct kfd_process_device *pdd;
-	int ret = 0;
-
-	dwork = to_delayed_work(work);
-
-	/* Process termination destroys this worker thread. So during the
-	 * lifetime of this thread, kfd_process p will be valid
-	 */
-	p = container_of(dwork, struct kfd_process, restore_work);
-
-	/* Call restore_process_bos on the first KGD device. This function
-	 * takes care of restoring the whole process including other devices.
-	 * Restore can fail if enough memory is not available. If so,
-	 * reschedule again.
-	 */
-	pdd = list_first_entry(&p->per_device_data,
-			       struct kfd_process_device,
-			       per_device_list);
-
-	ret = pdd->dev->kfd2kgd->restore_process_bos(p->master_vm);
-	if (ret) {
-		kfd_schedule_restore_bos_and_queues(p);
-		return;
-	}
-
-	ret = resume_process_mm(p);
-	if (ret)
-		pr_err("Failed to resume user queues\n");
-}
-
-/** kgd2kfd_schedule_evict_and_restore_process - Schedules work queue that will
- *   prepare for safe eviction of KFD BOs that belong to the specified
- *   process.
- *
- * @mm: mm_struct that identifies the specified KFD process
- * @fence: eviction fence attached to KFD process BOs
- *
- */
-int kgd2kfd_schedule_evict_and_restore_process(struct mm_struct *mm,
-					       struct fence *fence)
-{
-	struct kfd_process *p;
-
-	if (!fence)
-		return -EINVAL;
-
-	if (fence_is_signaled(fence))
-		return 0;
-
-	p = kfd_lookup_process_by_mm(mm);
-	if (!p)
-		return -ENODEV;
-
-	if (work_pending(&p->eviction_work.work)) {
-		/* It is possible has TTM has lined up couple of BOs of the same
-		 * process to be evicted. Check if the fence is same which
-		 * indicates that previous work item scheduled is not complted
-		 */
-		if (p->eviction_work.eviction_fence == fence)
-			goto out;
-		else {
-			WARN(1, "Starting new evict with previous evict is not completed\n");
-			cancel_work_sync(&p->eviction_work.work);
-		}
-	}
-
-	/* During process initialization eviction_work.work is initialized
-	 * to kfd_evict_bo_worker
-	 */
-	p->eviction_work.eviction_fence = fence_get(fence);
-	schedule_work(&p->eviction_work.work);
-out:
-	kfd_unref_process(p);
-	return 0;
-}
-
-void kfd_evict_bo_worker(struct work_struct *work)
-{
-	int ret;
-	struct kfd_process *p;
-	struct kfd_eviction_work *eviction_work;
-
-	eviction_work = container_of(work, struct kfd_eviction_work,
-				     work);
-
-	/* Process termination destroys this worker thread. So during the
-	 * lifetime of this thread, kfd_process p will be valid
-	 */
-	p = container_of(eviction_work, struct kfd_process, eviction_work);
-
-	/* Narrow window of overlap between restore and evict work item is
-	 * possible. Once amdgpu_amdkfd_gpuvm_restore_process_bos unreserves
-	 * KFD BOs, it is possible to evicted again. But restore has few more
-	 * steps of finish. So lets wait for the restore work to complete
-	 */
-	if (delayed_work_pending(&p->restore_work))
-		flush_delayed_work(&p->restore_work);
-
-	ret = quiesce_process_mm(p);
-	if (!ret) {
-		fence_signal(eviction_work->eviction_fence);
-		fence_put(eviction_work->eviction_fence);
-		kfd_schedule_restore_bos_and_queues(p);
-	} else {
-		pr_err("Failed to quiesce user queues. Cannot evict BOs\n");
-	}
-
-}
-
 static int kfd_gtt_sa_init(struct kfd_dev *kfd, unsigned int buf_size,
 				unsigned int chunk_size)
 {
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
index a5bf0c0..f49c551 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
@@ -44,11 +44,9 @@ static int create_compute_queue_nocpsch(struct device_queue_manager *dqm,
 					struct queue *q,
 					struct qcm_process_device *qpd);
 
-static int execute_queues_cpsch(struct device_queue_manager *dqm,
-				bool static_queues_included);
-static int unmap_queues_cpsch(struct device_queue_manager *dqm,
-		enum kfd_unmap_queues_filter filter,
-		uint32_t filter_param, bool reset);
+static int execute_queues_cpsch(struct device_queue_manager *dqm, bool lock);
+static int destroy_queues_cpsch(struct device_queue_manager *dqm,
+				bool preempt_static_queues, bool lock);
 
 static int create_sdma_queue_nocpsch(struct device_queue_manager *dqm,
 					struct queue *q,
@@ -102,11 +100,11 @@ static int allocate_vmid(struct device_queue_manager *dqm,
 	if (dqm->vmid_bitmap == 0)
 		return -ENOMEM;
 
-	bit = find_first_bit((unsigned long *)&dqm->vmid_bitmap,
-				dqm->dev->vm_info.vmid_num_kfd);
+	bit = find_first_bit((unsigned long *)&dqm->vmid_bitmap, CIK_VMID_NUM);
 	clear_bit(bit, (unsigned long *)&dqm->vmid_bitmap);
 
-	allocated_vmid = bit + dqm->dev->vm_info.first_vmid_kfd;
+	/* Kaveri kfd vmid's starts from vmid 8 */
+	allocated_vmid = bit + KFD_VMID_START_OFFSET;
 	pr_debug("kfd: vmid allocation %d\n", allocated_vmid);
 	qpd->vmid = allocated_vmid;
 	q->properties.vmid = allocated_vmid;
@@ -114,42 +112,14 @@ static int allocate_vmid(struct device_queue_manager *dqm,
 	set_pasid_vmid_mapping(dqm, q->process->pasid, q->properties.vmid);
 	program_sh_mem_settings(dqm, qpd);
 
-	/* qpd->page_table_base is set earlier when register_process()
-	 * is called, i.e. when the first queue is created.
-	 */
-	dqm->dev->kfd2kgd->set_vm_context_page_table_base(dqm->dev->kgd,
-			qpd->vmid,
-			qpd->page_table_base);
-	/*invalidate the VM context after pasid and vmid mapping is set up*/
-	kfd_flush_tlb(dqm->dev, qpd->pqm->process->pasid);
-
 	return 0;
 }
 
-static int flush_texture_cache_nocpsch(struct kfd_dev *kdev,
-				struct qcm_process_device *qpd)
-{
-	uint32_t len;
-
-	if (!qpd->ib_kaddr)
-		return -ENOMEM;
-
-	len = pm_create_release_mem(qpd->ib_base, (uint32_t *)qpd->ib_kaddr);
-
-	return kdev->kfd2kgd->submit_ib(kdev->kgd, KGD_ENGINE_MEC1, qpd->vmid,
-				qpd->ib_base, (uint32_t *)qpd->ib_kaddr, len);
-}
-
 static void deallocate_vmid(struct device_queue_manager *dqm,
 				struct qcm_process_device *qpd,
 				struct queue *q)
 {
-	int bit = qpd->vmid - dqm->dev->vm_info.first_vmid_kfd;
-
-	/* On GFX v7, CP doesn't flush TC at dequeue */
-	if (q->device->device_info->asic_family == CHIP_HAWAII)
-		if (flush_texture_cache_nocpsch(q->device, qpd))
-			pr_err("kfd: Failed to flush TC\n");
+	int bit = qpd->vmid - KFD_VMID_START_OFFSET;
 
 	/* Release the vmid mapping */
 	set_pasid_vmid_mapping(dqm, 0, qpd->vmid);
@@ -189,17 +159,6 @@ static int create_queue_nocpsch(struct device_queue_manager *dqm,
 	}
 	*allocated_vmid = qpd->vmid;
 	q->properties.vmid = qpd->vmid;
-	/*
-	 * Eviction state logic: we only mark active queues as evicted
-	 * to avoid the overhead of restoring inactive queues later
-	 */
-	if (qpd->evicted)
-		q->properties.is_evicted = (q->properties.queue_size > 0 &&
-					    q->properties.queue_percent > 0 &&
-					    q->properties.queue_address != 0);
-
-	q->properties.tba_addr = qpd->tba_addr;
-	q->properties.tma_addr = qpd->tma_addr;
 
 	if (q->properties.type == KFD_QUEUE_TYPE_COMPUTE)
 		retval = create_compute_queue_nocpsch(dqm, q, qpd);
@@ -302,14 +261,8 @@ static int create_compute_queue_nocpsch(struct device_queue_manager *dqm,
 			q->pipe,
 			q->queue);
 
-	dqm->dev->kfd2kgd->alloc_memory_of_scratch(
-			dqm->dev->kgd, qpd->sh_hidden_private_base, qpd->vmid);
-
-	if (!q->properties.is_active)
-		return 0;
-
-	retval = mqd->load_mqd(mqd, q->mqd, q->pipe, q->queue, &q->properties,
-			       q->process->mm);
+	retval = mqd->load_mqd(mqd, q->mqd, q->pipe,
+			q->queue, (uint32_t __user *) q->properties.write_ptr);
 	if (retval != 0) {
 		deallocate_hqd(dqm, q);
 		mqd->uninit_mqd(mqd, q->mqd, q->mqd_mem_obj);
@@ -319,41 +272,48 @@ static int create_compute_queue_nocpsch(struct device_queue_manager *dqm,
 	return 0;
 }
 
-/* Access to DQM has to be locked before calling destroy_queue_nocpsch_locked
- * to avoid asynchronized access
- */
-static int destroy_queue_nocpsch_locked(struct device_queue_manager *dqm,
+static int destroy_queue_nocpsch(struct device_queue_manager *dqm,
 				struct qcm_process_device *qpd,
 				struct queue *q)
 {
-	int retval = 0;
+	int retval;
 	struct mqd_manager *mqd;
 
-	WARN_ON(!dqm || !q || !q->mqd || !qpd);
+	BUG_ON(!dqm || !q || !q->mqd || !qpd);
 
-	mqd = dqm->ops.get_mqd_manager(dqm,
-		get_mqd_type_from_queue_type(q->properties.type));
-	if (!mqd) {
-		retval = -ENOMEM;
-		goto out;
-	}
+	retval = 0;
 
-	if (q->properties.type == KFD_QUEUE_TYPE_COMPUTE)
+	pr_debug("kfd: In Func %s\n", __func__);
+
+	mutex_lock(&dqm->lock);
+
+	if (q->properties.type == KFD_QUEUE_TYPE_COMPUTE) {
+		mqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_COMPUTE);
+		if (mqd == NULL) {
+			retval = -ENOMEM;
+			goto out;
+		}
 		deallocate_hqd(dqm, q);
-	else if (q->properties.type == KFD_QUEUE_TYPE_SDMA) {
+	} else if (q->properties.type == KFD_QUEUE_TYPE_SDMA) {
+		mqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_SDMA);
+		if (mqd == NULL) {
+			retval = -ENOMEM;
+			goto out;
+		}
 		dqm->sdma_queue_count--;
 		deallocate_sdma_queue(dqm, q->sdma_id);
 	} else {
 		pr_debug("q->properties.type is invalid (%d)\n",
-			q->properties.type);
+				q->properties.type);
 		retval = -EINVAL;
+		goto out;
 	}
-	dqm->total_queue_count--;
 
 	retval = mqd->destroy_mqd(mqd, q->mqd,
 				KFD_PREEMPT_TYPE_WAVEFRONT_RESET,
-				KFD_HIQ_TIMEOUT,
+				QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS,
 				q->pipe, q->queue);
+
 	if (retval != 0)
 		goto out;
 
@@ -365,22 +325,16 @@ static int destroy_queue_nocpsch_locked(struct device_queue_manager *dqm,
 	if (q->properties.is_active)
 		dqm->queue_count--;
 
-out:
-	return retval;
-}
-
-static int destroy_queue_nocpsch(struct device_queue_manager *dqm,
-				struct qcm_process_device *qpd,
-				struct queue *q)
-{
-	int retval;
-
-	BUG_ON(!dqm || !q || !q->mqd || !qpd);
+	/*
+	 * Unconditionally decrement this counter, regardless of the queue's
+	 * type
+	 */
+	dqm->total_queue_count--;
+	pr_debug("Total of %d queues are accountable so far\n",
+			dqm->total_queue_count);
 
-	mutex_lock(&dqm->lock);
-	retval = destroy_queue_nocpsch_locked(dqm, qpd, q);
+out:
 	mutex_unlock(&dqm->lock);
-
 	return retval;
 }
 
@@ -388,66 +342,36 @@ static int update_queue(struct device_queue_manager *dqm, struct queue *q)
 {
 	int retval;
 	struct mqd_manager *mqd;
-	struct kfd_process_device *pdd;
-
 	bool prev_active = false;
 
 	BUG_ON(!dqm || !q || !q->mqd);
 
 	mutex_lock(&dqm->lock);
-
-	pdd = kfd_get_process_device_data(q->device, q->process);
-	if (!pdd) {
-		retval = -ENODEV;
-		goto out_unlock;
-	}
 	mqd = dqm->ops.get_mqd_manager(dqm,
 			get_mqd_type_from_queue_type(q->properties.type));
 	if (mqd == NULL) {
-		retval = -ENOMEM;
-		goto out_unlock;
+		mutex_unlock(&dqm->lock);
+		return -ENOMEM;
 	}
-	/*
-	 * Eviction state logic: we only mark active queues as evicted
-	 * to avoid the overhead of restoring inactive queues later
-	 */
-	if (pdd->qpd.evicted > 0)
-		q->properties.is_evicted = (q->properties.queue_size > 0 &&
-					    q->properties.queue_percent > 0 &&
-					    q->properties.queue_address != 0);
 
-	/* save previous activity state for counters */
 	if (q->properties.is_active)
 		prev_active = true;
 
-
-	retval = mqd->update_mqd(mqd, q->mqd, &q->properties);
-	if (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS &&
-	    (q->properties.type == KFD_QUEUE_TYPE_COMPUTE ||
-	     q->properties.type == KFD_QUEUE_TYPE_SDMA)) {
-		if (q->properties.is_active)
-			retval = mqd->load_mqd(mqd, q->mqd, q->pipe, q->queue,
-					       &q->properties, q->process->mm);
-		else if (prev_active)
-			retval = mqd->destroy_mqd(mqd, q->mqd,
-				KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN,
-				KFD_UNMAP_LATENCY_MS, q->pipe, q->queue);
-	}
 	/*
-		 * check active state vs. the previous state
-		 * and modify counter accordingly
-	*/
+	 *
+	 * check active state vs. the previous state
+	 * and modify counter accordingly
+	 */
+	retval = mqd->update_mqd(mqd, q->mqd, &q->properties);
 	if ((q->properties.is_active) && (!prev_active))
 		dqm->queue_count++;
 	else if ((!q->properties.is_active) && (prev_active))
 		dqm->queue_count--;
 
-	if (dqm->sched_policy != KFD_SCHED_POLICY_NO_HWS)
+	if (sched_policy != KFD_SCHED_POLICY_NO_HWS)
 		retval = execute_queues_cpsch(dqm, false);
 
-out_unlock:
 	mutex_unlock(&dqm->lock);
-
 	return retval;
 }
 
@@ -471,141 +395,15 @@ static struct mqd_manager *get_mqd_manager_nocpsch(
 	return mqd;
 }
 
-int process_evict_queues(struct device_queue_manager *dqm,
-		struct qcm_process_device *qpd)
-{
-	struct queue *q, *next;
-	struct mqd_manager *mqd;
-	int retval = 0;
-
-	BUG_ON(!dqm || !qpd);
-
-	mutex_lock(&dqm->lock);
-	/* Eviction of queues can be ignored if no queues are available.
-	   We should not decrement qpd->evicted if no queues are available
-	   amdgpu chnages creating fence eviction signal. This chnages is needed
-	   even we fix the actual reason from amdgpu for eviction
-	*/
-	if(list_empty(&qpd->queues_list))  
-	{
-		mutex_unlock(&dqm->lock);
-		return 0;
-	}
-	if (qpd->evicted++ > 0) { /* already evicted, do nothing */
-		mutex_unlock(&dqm->lock);
-		return 0;
-	}
-	/* unactivate all active queues on the qpd */
-	list_for_each_entry_safe(q, next, &qpd->queues_list, list) {
-		mqd = dqm->ops.get_mqd_manager(dqm,
-			get_mqd_type_from_queue_type(q->properties.type));
-		if (!mqd) { /* should not be here */
-			BUG();
-			continue;
-		}
-		/* if the queue is not active anyway, it is not evicted */
-		if (q->properties.is_active == true)
-			q->properties.is_evicted = true;
-
-		retval = mqd->update_mqd(mqd, q->mqd, &q->properties);
-		if (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS &&
-		    (q->properties.type == KFD_QUEUE_TYPE_COMPUTE ||
-		     q->properties.type == KFD_QUEUE_TYPE_SDMA) &&
-		    q->properties.is_evicted)
-			retval = mqd->destroy_mqd(mqd, q->mqd,
-				KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN,
-				KFD_UNMAP_LATENCY_MS, q->pipe, q->queue);
-		if (q->properties.is_evicted)
-			dqm->queue_count--;
-	}
-	if (dqm->sched_policy != KFD_SCHED_POLICY_NO_HWS)
-		retval = execute_queues_cpsch(dqm, false);
-
-	mutex_unlock(&dqm->lock);
-	return retval;
-
-}
-
-int process_restore_queues(struct device_queue_manager *dqm,
-		struct qcm_process_device *qpd)
-{
-	struct queue *q, *next;
-	struct mqd_manager *mqd;
-	int retval = 0;
-	struct kfd_process_device *pdd;
-	uint32_t pd_base;
-
-	BUG_ON(!dqm || !qpd);
-
-	pdd = qpd_to_pdd(qpd);
-	/* Retrieve PD base */
-	pd_base = dqm->dev->kfd2kgd->get_process_page_dir(pdd->vm);
-
-	mutex_lock(&dqm->lock);
-	if (qpd->evicted == 0) /* already restored, do nothing */
-		goto out_unlock;
-
-	if (qpd->evicted > 1) { /* ref count still > 0, decrement & quit */
-		qpd->evicted--;
-		goto out_unlock;
-	}
-
-	/* Update PD Base in QPD */
-	qpd->page_table_base = pd_base;
-	pr_debug("Updated PD address to 0x%08x in %s\n", pd_base, __func__);
-
-	if (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {
-		dqm->dev->kfd2kgd->set_vm_context_page_table_base(
-				dqm->dev->kgd,
-				qpd->vmid,
-				qpd->page_table_base);
-
-		kfd_flush_tlb(dqm->dev, pdd->process->pasid);
-	}
-
-	/* activate all active queues on the qpd */
-	list_for_each_entry_safe(q, next, &qpd->queues_list, list) {
-		mqd = dqm->ops.get_mqd_manager(dqm,
-			get_mqd_type_from_queue_type(q->properties.type));
-		if (!mqd) { /* should not be here */
-			BUG();
-			continue;
-		}
-		if (q->properties.is_evicted) {
-			q->properties.is_evicted = false;
-			retval = mqd->update_mqd(mqd, q->mqd, &q->properties);
-			if (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS &&
-			    (q->properties.type == KFD_QUEUE_TYPE_COMPUTE ||
-			     q->properties.type == KFD_QUEUE_TYPE_SDMA))
-				retval = mqd->load_mqd(mqd, q->mqd, q->pipe,
-						       q->queue, &q->properties,
-						       q->process->mm);
-			dqm->queue_count++;
-		}
-	}
-	if (dqm->sched_policy != KFD_SCHED_POLICY_NO_HWS)
-		retval = execute_queues_cpsch(dqm, false);
-
-	if (retval == 0)
-		qpd->evicted = 0;
-
-out_unlock:
-	mutex_unlock(&dqm->lock);
-
-	return retval;
-}
-
-static int register_process(struct device_queue_manager *dqm,
+static int register_process_nocpsch(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd)
 {
 	struct device_process_node *n;
 	int retval;
-	struct kfd_process_device *pdd;
-	uint32_t pd_base;
 
 	BUG_ON(!dqm || !qpd);
 
-	pr_debug("In func %s\n", __func__);
+	pr_debug("kfd: In func %s\n", __func__);
 
 	n = kzalloc(sizeof(struct device_process_node), GFP_KERNEL);
 	if (!n)
@@ -613,18 +411,10 @@ static int register_process(struct device_queue_manager *dqm,
 
 	n->qpd = qpd;
 
-	pdd = qpd_to_pdd(qpd);
-	/* Retrieve PD base */
-	pd_base = dqm->dev->kfd2kgd->get_process_page_dir(pdd->vm);
-
 	mutex_lock(&dqm->lock);
 	list_add(&n->list, &dqm->queues);
 
-	/* Update PD Base in QPD */
-	qpd->page_table_base = pd_base;
-	pr_debug("Updated PD address to 0x%08x in %s\n", pd_base, __func__);
-
-	retval = dqm->asic_ops.update_qpd(dqm, qpd);
+	retval = dqm->ops_asic_specific.register_process(dqm, qpd);
 
 	dqm->processes_count++;
 
@@ -633,7 +423,7 @@ static int register_process(struct device_queue_manager *dqm,
 	return retval;
 }
 
-static int unregister_process(struct device_queue_manager *dqm,
+static int unregister_process_nocpsch(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd)
 {
 	int retval;
@@ -741,8 +531,10 @@ static void init_interrupts(struct device_queue_manager *dqm)
 	BUG_ON(dqm == NULL);
 
 	for (i = 0 ; i < get_pipes_num(dqm) ; i++)
-		dqm->dev->kfd2kgd->init_interrupts(dqm->dev->kgd, i);
+		dqm->dev->kfd2kgd->init_interrupts(dqm->dev->kgd,
+				i + get_first_pipe(dqm));
 }
+
 static int init_scheduler(struct device_queue_manager *dqm)
 {
 	int retval;
@@ -778,7 +570,7 @@ static int initialize_nocpsch(struct device_queue_manager *dqm)
 	for (i = 0; i < get_pipes_num(dqm); i++)
 		dqm->allocated_queues[i] = (1 << QUEUES_PER_PIPE) - 1;
 
-	dqm->vmid_bitmap = (1 << dqm->dev->vm_info.vmid_num_kfd) - 1;
+	dqm->vmid_bitmap = (1 << VMID_PER_DEVICE) - 1;
 	dqm->sdma_bitmap = (1 << CIK_SDMA_QUEUES) - 1;
 
 	init_scheduler(dqm);
@@ -851,14 +643,14 @@ static int create_sdma_queue_nocpsch(struct device_queue_manager *dqm,
 	if (retval != 0)
 		return retval;
 
-	q->properties.sdma_queue_id = q->sdma_id / CIK_SDMA_QUEUES_PER_ENGINE;
-	q->properties.sdma_engine_id = q->sdma_id % CIK_SDMA_QUEUES_PER_ENGINE;
+	q->properties.sdma_queue_id = q->sdma_id % CIK_SDMA_QUEUES_PER_ENGINE;
+	q->properties.sdma_engine_id = q->sdma_id / CIK_SDMA_ENGINE_NUM;
 
 	pr_debug("kfd: sdma id is:    %d\n", q->sdma_id);
 	pr_debug("     sdma queue id: %d\n", q->properties.sdma_queue_id);
 	pr_debug("     sdma engine id: %d\n", q->properties.sdma_engine_id);
 
-	dqm->asic_ops.init_sdma_vm(dqm, q, qpd);
+	dqm->ops_asic_specific.init_sdma_vm(dqm, q, qpd);
 	retval = mqd->init_mqd(mqd, &q->mqd, &q->mqd_mem_obj,
 				&q->gart_mqd_addr, &q->properties);
 	if (retval != 0) {
@@ -866,7 +658,8 @@ static int create_sdma_queue_nocpsch(struct device_queue_manager *dqm,
 		return retval;
 	}
 
-	retval = mqd->load_mqd(mqd, q->mqd, 0, 0, &q->properties, NULL);
+	retval = mqd->load_mqd(mqd, q->mqd, 0,
+				0, NULL);
 	if (retval != 0) {
 		deallocate_sdma_queue(dqm, q->sdma_id);
 		mqd->uninit_mqd(mqd, q->mqd, q->mqd_mem_obj);
@@ -891,7 +684,8 @@ static int set_sched_resources(struct device_queue_manager *dqm)
 
 	queue_num = get_pipes_num_cpsch() * QUEUES_PER_PIPE;
 	queue_mask = (1 << queue_num) - 1;
-	res.vmid_mask = dqm->dev->shared_resources.compute_vmid_bitmap;
+	res.vmid_mask = (1 << VMID_PER_DEVICE) - 1;
+	res.vmid_mask <<= KFD_VMID_START_OFFSET;
 	res.queue_mask = queue_mask << (get_first_pipe(dqm) * QUEUES_PER_PIPE);
 	res.gws_mask = res.oac_mask = res.gds_heap_base =
 						res.gds_heap_size = 0;
@@ -918,8 +712,7 @@ static int initialize_cpsch(struct device_queue_manager *dqm)
 	dqm->queue_count = dqm->processes_count = 0;
 	dqm->sdma_queue_count = 0;
 	dqm->active_runlist = false;
-	dqm->sdma_bitmap = (1 << CIK_SDMA_QUEUES) - 1;
-	retval = dqm->asic_ops.init_cpsch(dqm);
+	retval = dqm->ops_asic_specific.initialize(dqm);
 	if (retval != 0)
 		goto fail_init_pipelines;
 
@@ -932,13 +725,14 @@ fail_init_pipelines:
 
 static int start_cpsch(struct device_queue_manager *dqm)
 {
+	struct device_process_node *node;
 	int retval;
 
 	BUG_ON(!dqm);
 
 	retval = 0;
 
-	retval = pm_init(&dqm->packets, dqm, dqm->dev->mec_fw_version);
+	retval = pm_init(&dqm->packets, dqm);
 	if (retval != 0)
 		goto fail_packet_manager_init;
 
@@ -960,9 +754,12 @@ static int start_cpsch(struct device_queue_manager *dqm)
 
 	init_interrupts(dqm);
 
-	mutex_lock(&dqm->lock);
-	execute_queues_cpsch(dqm, false);
-	mutex_unlock(&dqm->lock);
+	list_for_each_entry(node, &dqm->queues, list)
+		if (node->qpd->pqm->process && dqm->dev)
+			kfd_bind_process_to_device(dqm->dev,
+						node->qpd->pqm->process);
+
+	execute_queues_cpsch(dqm, true);
 
 	return 0;
 fail_allocate_vidmem:
@@ -974,14 +771,17 @@ fail_packet_manager_init:
 
 static int stop_cpsch(struct device_queue_manager *dqm)
 {
-	BUG_ON(!dqm);
-
-	mutex_lock(&dqm->lock);
+	struct device_process_node *node;
+	struct kfd_process_device *pdd;
 
-	unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0, false);
+	BUG_ON(!dqm);
 
-	mutex_unlock(&dqm->lock);
+	destroy_queues_cpsch(dqm, true, true);
 
+	list_for_each_entry(node, &dqm->queues, list) {
+		pdd = qpd_to_pdd(node->qpd);
+		pdd->bound = false;
+	}
 	kfd_gtt_sa_free(dqm->dev, dqm->fence_mem);
 	pm_uninit(&dqm->packets);
 
@@ -1031,10 +831,11 @@ static void destroy_kernel_queue_cpsch(struct device_queue_manager *dqm,
 
 	mutex_lock(&dqm->lock);
 	/* here we actually preempt the DIQ */
+	destroy_queues_cpsch(dqm, true, false);
 	list_del(&kq->list);
 	dqm->queue_count--;
 	qpd->is_debug = false;
-	execute_queues_cpsch(dqm, true);
+	execute_queues_cpsch(dqm, false);
 	/*
 	 * Unconditionally decrement this counter, regardless of the queue's
 	 * type.
@@ -1045,6 +846,14 @@ static void destroy_kernel_queue_cpsch(struct device_queue_manager *dqm,
 	mutex_unlock(&dqm->lock);
 }
 
+static void select_sdma_engine_id(struct queue *q)
+{
+	static int sdma_id;
+
+	q->sdma_id = sdma_id;
+	sdma_id = (sdma_id + 1) % 2;
+}
+
 static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 			struct qcm_process_device *qpd, int *allocate_vmid)
 {
@@ -1067,15 +876,9 @@ static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 		goto out;
 	}
 
-	if (q->properties.type == KFD_QUEUE_TYPE_SDMA) {
-		retval = allocate_sdma_queue(dqm, &q->sdma_id);
-		if (retval != 0)
-			goto out;
-		q->properties.sdma_queue_id =
-			q->sdma_id / CIK_SDMA_QUEUES_PER_ENGINE;
-		q->properties.sdma_engine_id =
-			q->sdma_id % CIK_SDMA_QUEUES_PER_ENGINE;
-	}
+	if (q->properties.type == KFD_QUEUE_TYPE_SDMA)
+		select_sdma_engine_id(q);
+
 	mqd = dqm->ops.get_mqd_manager(dqm,
 			get_mqd_type_from_queue_type(q->properties.type));
 
@@ -1083,19 +886,8 @@ static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 		mutex_unlock(&dqm->lock);
 		return -ENOMEM;
 	}
-	/*
-	 * Eviction state logic: we only mark active queues as evicted
-	 * to avoid the overhead of restoring inactive queues later
-	 */
-	if (qpd->evicted)
-		q->properties.is_evicted = (q->properties.queue_size > 0 &&
-					    q->properties.queue_percent > 0 &&
-					    q->properties.queue_address != 0);
-
-	dqm->asic_ops.init_sdma_vm(dqm, q, qpd);
 
-	q->properties.tba_addr = qpd->tba_addr;
-	q->properties.tma_addr = qpd->tma_addr;
+	dqm->ops_asic_specific.init_sdma_vm(dqm, q, qpd);
 	retval = mqd->init_mqd(mqd, &q->mqd, &q->mqd_mem_obj,
 				&q->gart_mqd_addr, &q->properties);
 	if (retval != 0)
@@ -1125,16 +917,13 @@ out:
 
 int amdkfd_fence_wait_timeout(unsigned int *fence_addr,
 				unsigned int fence_value,
-				unsigned long timeout_ms)
+				unsigned long timeout)
 {
-	unsigned long end_jiffies;
-
 	BUG_ON(!fence_addr);
-
-	end_jiffies = (timeout_ms * HZ / 1000) + jiffies;
+	timeout += jiffies;
 
 	while (*fence_addr != fence_value) {
-		if (time_after(jiffies, end_jiffies)) {
+		if (time_after(jiffies, timeout)) {
 			pr_err("kfd: qcm fence wait loop timeout expired\n");
 			return -ETIME;
 		}
@@ -1144,40 +933,46 @@ int amdkfd_fence_wait_timeout(unsigned int *fence_addr,
 	return 0;
 }
 
-static int unmap_sdma_queues(struct device_queue_manager *dqm,
+static int destroy_sdma_queues(struct device_queue_manager *dqm,
 				unsigned int sdma_engine)
 {
 	return pm_send_unmap_queue(&dqm->packets, KFD_QUEUE_TYPE_SDMA,
-			KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0, false,
+			KFD_PREEMPT_TYPE_FILTER_DYNAMIC_QUEUES, 0, false,
 			sdma_engine);
 }
 
-/* dqm->lock mutex has to be locked before calling this function */
-static int unmap_queues_cpsch(struct device_queue_manager *dqm,
-		enum kfd_unmap_queues_filter filter,
-		uint32_t filter_param, bool reset)
+static int destroy_queues_cpsch(struct device_queue_manager *dqm,
+				bool preempt_static_queues, bool lock)
 {
 	int retval;
+	enum kfd_preempt_type_filter preempt_type;
+	struct kfd_process_device *pdd;
 
 	BUG_ON(!dqm);
 
 	retval = 0;
 
+	if (lock)
+		mutex_lock(&dqm->lock);
 	if (!dqm->active_runlist)
-		return retval;
+		goto out;
 
 	pr_debug("kfd: Before destroying queues, sdma queue count is : %u\n",
 		dqm->sdma_queue_count);
 
 	if (dqm->sdma_queue_count > 0) {
-		unmap_sdma_queues(dqm, 0);
-		unmap_sdma_queues(dqm, 1);
+		destroy_sdma_queues(dqm, 0);
+		destroy_sdma_queues(dqm, 1);
 	}
 
+	preempt_type = preempt_static_queues ?
+			KFD_PREEMPT_TYPE_FILTER_ALL_QUEUES :
+			KFD_PREEMPT_TYPE_FILTER_DYNAMIC_QUEUES;
+
 	retval = pm_send_unmap_queue(&dqm->packets, KFD_QUEUE_TYPE_COMPUTE,
-			filter, filter_param, reset, 0);
+			preempt_type, 0, false, 0);
 	if (retval != 0)
-		return retval;
+		goto out;
 
 	*dqm->fence_addr = KFD_FENCE_INIT;
 	pm_send_query_status(&dqm->packets, dqm->fence_gpu_addr,
@@ -1186,52 +981,55 @@ static int unmap_queues_cpsch(struct device_queue_manager *dqm,
 	retval = amdkfd_fence_wait_timeout(dqm->fence_addr, KFD_FENCE_COMPLETED,
 				QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS);
 	if (retval != 0) {
-		pr_err("kfd: unmapping queues failed.");
-		return retval;
+		pdd = kfd_get_process_device_data(dqm->dev,
+				kfd_get_process(current));
+		pdd->reset_wavefronts = true;
+		goto out;
 	}
-
 	pm_release_ib(&dqm->packets);
 	dqm->active_runlist = false;
 
+out:
+	if (lock)
+		mutex_unlock(&dqm->lock);
 	return retval;
 }
 
-/* dqm->lock mutex has to be locked before calling this function */
-static int execute_queues_cpsch(struct device_queue_manager *dqm,
-				bool static_queues_included)
+static int execute_queues_cpsch(struct device_queue_manager *dqm, bool lock)
 {
 	int retval;
-	enum kfd_unmap_queues_filter filter;
 
 	BUG_ON(!dqm);
 
-	filter = static_queues_included ?
-			KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES :
-			KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES;
+	if (lock)
+		mutex_lock(&dqm->lock);
 
-	retval = unmap_queues_cpsch(dqm, filter, 0, false);
+	retval = destroy_queues_cpsch(dqm, false, false);
 	if (retval != 0) {
 		pr_err("kfd: the cp might be in an unrecoverable state due to an unsuccessful queues preemption");
-		return retval;
+		goto out;
 	}
 
 	if (dqm->queue_count <= 0 || dqm->processes_count <= 0) {
 		retval = 0;
-		return retval;
+		goto out;
 	}
 
 	if (dqm->active_runlist) {
 		retval = 0;
-		return retval;
+		goto out;
 	}
 
 	retval = pm_send_runlist(&dqm->packets, &dqm->queues);
 	if (retval != 0) {
 		pr_err("kfd: failed to execute runlist");
-		return retval;
+		goto out;
 	}
 	dqm->active_runlist = true;
 
+out:
+	if (lock)
+		mutex_unlock(&dqm->lock);
 	return retval;
 }
 
@@ -1269,16 +1067,14 @@ static int destroy_queue_cpsch(struct device_queue_manager *dqm,
 		goto failed;
 	}
 
-	if (q->properties.type == KFD_QUEUE_TYPE_SDMA) {
+	if (q->properties.type == KFD_QUEUE_TYPE_SDMA)
 		dqm->sdma_queue_count--;
-		deallocate_sdma_queue(dqm, q->sdma_id);
-	}
 
 	list_del(&q->list);
 	if (q->properties.is_active)
 		dqm->queue_count--;
 
-	retval = execute_queues_cpsch(dqm, false);
+	execute_queues_cpsch(dqm, false);
 
 	mqd->uninit_mqd(mqd, q->mqd, q->mqd_mem_obj);
 
@@ -1292,7 +1088,7 @@ static int destroy_queue_cpsch(struct device_queue_manager *dqm,
 
 	mutex_unlock(&dqm->lock);
 
-	return retval;
+	return 0;
 
 failed:
 failed_try_destroy_debugged_queue:
@@ -1353,7 +1149,7 @@ static bool set_cache_memory_policy(struct device_queue_manager *dqm,
 		qpd->sh_mem_ape1_limit = limit >> 16;
 	}
 
-	retval = dqm->asic_ops.set_cache_memory_policy(
+	retval = dqm->ops_asic_specific.set_cache_memory_policy(
 			dqm,
 			qpd,
 			default_policy,
@@ -1361,7 +1157,7 @@ static bool set_cache_memory_policy(struct device_queue_manager *dqm,
 			alternate_aperture_base,
 			alternate_aperture_size);
 
-	if ((dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) && (qpd->vmid != 0))
+	if ((sched_policy == KFD_SCHED_POLICY_NO_HWS) && (qpd->vmid != 0))
 		program_sh_mem_settings(dqm, qpd);
 
 	pr_debug("kfd: sh_mem_config: 0x%x, ape1_base: 0x%x, ape1_limit: 0x%x\n",
@@ -1376,120 +1172,6 @@ out:
 	return false;
 }
 
-static int set_trap_handler(struct device_queue_manager *dqm,
-				struct qcm_process_device *qpd,
-				uint64_t tba_addr,
-				uint64_t tma_addr)
-{
-	uint64_t *tma;
-
-	if (dqm->dev->cwsr_enabled) {
-		/* Jump from CWSR trap handler to user trap */
-		tma = (uint64_t *)(qpd->cwsr_kaddr + dqm->dev->tma_offset);
-		tma[0] = tba_addr;
-		tma[1] = tma_addr;
-	} else {
-		qpd->tba_addr = tba_addr;
-		qpd->tma_addr = tma_addr;
-	}
-
-	return 0;
-}
-
-static int process_termination_nocpsch(struct device_queue_manager *dqm,
-		struct qcm_process_device *qpd)
-{
-	struct queue *q, *next;
-	struct device_process_node *cur, *next_dpn;
-	int retval = 0;
-
-	mutex_lock(&dqm->lock);
-
-	/* Clear all user mode queues */
-	list_for_each_entry_safe(q, next, &qpd->queues_list, list) {
-		retval = destroy_queue_nocpsch_locked(dqm, qpd, q);
-		if (retval)
-			goto out;
-	}
-
-	/* Unregister process */
-	list_for_each_entry_safe(cur, next_dpn, &dqm->queues, list) {
-		if (qpd == cur->qpd) {
-			list_del(&cur->list);
-			kfree(cur);
-			dqm->processes_count--;
-			break;
-		}
-	}
-
-out:
-	mutex_unlock(&dqm->lock);
-	return retval;
-}
-
-
-static int process_termination_cpsch(struct device_queue_manager *dqm,
-		struct qcm_process_device *qpd)
-{
-	int retval;
-	struct queue *q, *next;
-	struct kernel_queue *kq, *kq_next;
-	struct mqd_manager *mqd;
-	struct device_process_node *cur, *next_dpn;
-
-	retval = 0;
-
-	mutex_lock(&dqm->lock);
-
-	/* Clean all kernel queues */
-	list_for_each_entry_safe(kq, kq_next, &qpd->priv_queue_list, list) {
-		list_del(&kq->list);
-		dqm->queue_count--;
-		qpd->is_debug = false;
-		dqm->total_queue_count--;
-	}
-
-	/* Clear all user mode queues */
-	list_for_each_entry(q, &qpd->queues_list, list) {
-		if (q->properties.type == KFD_QUEUE_TYPE_SDMA) {
-			dqm->sdma_queue_count--;
-			deallocate_sdma_queue(dqm, q->sdma_id);
-		}
-
-		if (q->properties.is_active)
-			dqm->queue_count--;
-
-		dqm->total_queue_count--;
-	}
-
-	/* Unregister process */
-	list_for_each_entry_safe(cur, next_dpn, &dqm->queues, list) {
-		if (qpd == cur->qpd) {
-			list_del(&cur->list);
-			kfree(cur);
-			dqm->processes_count--;
-			break;
-		}
-	}
-
-	retval = execute_queues_cpsch(dqm, true);
-
-	/* lastly, free mqd resources */
-	list_for_each_entry_safe(q, next, &qpd->queues_list, list) {
-		mqd = dqm->ops.get_mqd_manager(dqm,
-			get_mqd_type_from_queue_type(q->properties.type));
-		if (!mqd) {
-			mutex_unlock(&dqm->lock);
-			return -ENOMEM;
-		}
-		list_del(&q->list);
-		mqd->uninit_mqd(mqd, q->mqd, q->mqd_mem_obj);
-	}
-
-	mutex_unlock(&dqm->lock);
-	return retval;
-}
-
 struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 {
 	struct device_queue_manager *dqm;
@@ -1502,18 +1184,8 @@ struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 	if (!dqm)
 		return NULL;
 
-	switch (dev->device_info->asic_family) {
-	case CHIP_HAWAII:
-	case CHIP_TONGA:
-		dqm->sched_policy = KFD_SCHED_POLICY_NO_HWS;
-		break;
-	default:
-		dqm->sched_policy = sched_policy;
-		break;
-	}
-
 	dqm->dev = dev;
-	switch (dqm->sched_policy) {
+	switch (sched_policy) {
 	case KFD_SCHED_POLICY_HWS:
 	case KFD_SCHED_POLICY_HWS_NO_OVERSUBSCRIPTION:
 		/* initialize dqm for cp scheduling */
@@ -1524,14 +1196,12 @@ struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 		dqm->ops.destroy_queue = destroy_queue_cpsch;
 		dqm->ops.update_queue = update_queue;
 		dqm->ops.get_mqd_manager = get_mqd_manager_nocpsch;
-		dqm->ops.register_process = register_process;
-		dqm->ops.unregister_process = unregister_process;
+		dqm->ops.register_process = register_process_nocpsch;
+		dqm->ops.unregister_process = unregister_process_nocpsch;
 		dqm->ops.uninitialize = uninitialize_nocpsch;
 		dqm->ops.create_kernel_queue = create_kernel_queue_cpsch;
 		dqm->ops.destroy_kernel_queue = destroy_kernel_queue_cpsch;
 		dqm->ops.set_cache_memory_policy = set_cache_memory_policy;
-		dqm->ops.set_trap_handler = set_trap_handler;
-		dqm->ops.process_termination = process_termination_cpsch;
 		break;
 	case KFD_SCHED_POLICY_NO_HWS:
 		/* initialize dqm for no cp scheduling */
@@ -1541,13 +1211,11 @@ struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 		dqm->ops.destroy_queue = destroy_queue_nocpsch;
 		dqm->ops.update_queue = update_queue;
 		dqm->ops.get_mqd_manager = get_mqd_manager_nocpsch;
-		dqm->ops.register_process = register_process;
-		dqm->ops.unregister_process = unregister_process;
+		dqm->ops.register_process = register_process_nocpsch;
+		dqm->ops.unregister_process = unregister_process_nocpsch;
 		dqm->ops.initialize = initialize_nocpsch;
 		dqm->ops.uninitialize = uninitialize_nocpsch;
 		dqm->ops.set_cache_memory_policy = set_cache_memory_policy;
-		dqm->ops.set_trap_handler = set_trap_handler;
-		dqm->ops.process_termination = process_termination_nocpsch;
 		break;
 	default:
 		BUG();
@@ -1556,22 +1224,11 @@ struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 
 	switch (dev->device_info->asic_family) {
 	case CHIP_CARRIZO:
-		device_queue_manager_init_vi(&dqm->asic_ops);
+		device_queue_manager_init_vi(&dqm->ops_asic_specific);
 		break;
 
 	case CHIP_KAVERI:
-		device_queue_manager_init_cik(&dqm->asic_ops);
-		break;
-
-	case CHIP_HAWAII:
-		device_queue_manager_init_cik_hawaii(&dqm->asic_ops);
-		break;
-
-	case CHIP_TONGA:
-	case CHIP_FIJI:
-	case CHIP_POLARIS10:
-	case CHIP_POLARIS11:
-		device_queue_manager_init_vi_tonga(&dqm->asic_ops);
+		device_queue_manager_init_cik(&dqm->ops_asic_specific);
 		break;
 	}
 
@@ -1590,81 +1247,3 @@ void device_queue_manager_uninit(struct device_queue_manager *dqm)
 	dqm->ops.uninitialize(dqm);
 	kfree(dqm);
 }
-
-int kfd_process_vm_fault(struct device_queue_manager *dqm,
-				unsigned int pasid)
-{
-	struct kfd_process_device *pdd;
-	struct kfd_process *p = kfd_lookup_process_by_pasid(pasid);
-	int ret = 0;
-
-	if (!p)
-		return -EINVAL;
-	pdd = kfd_get_process_device_data(dqm->dev, p);
-	if (pdd)
-		ret = process_evict_queues(dqm, &pdd->qpd);
-	kfd_unref_process(p);
-
-	return ret;
-}
-
-static void seq_reg_dump(struct seq_file *m,
-			 uint32_t (*dump)[2], uint32_t n_regs)
-{
-	uint32_t i, count;
-
-	for (i = 0, count = 0; i < n_regs; i++) {
-		if (count == 0 ||
-		    dump[i-1][0] + sizeof(uint32_t) != dump[i][0]) {
-			seq_printf(m, "%s    %08x: %08x",
-				   i ? "\n" : "",
-				   dump[i][0], dump[i][1]);
-			count = 7;
-		} else {
-			seq_printf(m, " %08x", dump[i][1]);
-			count--;
-		}
-	}
-
-	seq_puts(m, "\n");
-}
-
-int device_queue_manager_debugfs_hqds(struct seq_file *m, void *data)
-{
-	struct device_queue_manager *dqm = data;
-	uint32_t (*dump)[2], n_regs;
-	int pipe, queue;
-	int r = 0;
-
-	for (pipe = 0; pipe < get_pipes_num(dqm); pipe++) {
-		for (queue = 0; queue < QUEUES_PER_PIPE; queue++) {
-			r = dqm->dev->kfd2kgd->hqd_dump(
-				dqm->dev->kgd, pipe, queue, &dump, &n_regs);
-			if (r != 0)
-				break;
-
-			seq_printf(m, "  CP Pipe %d, Queue %d\n",
-				  pipe, queue);
-			seq_reg_dump(m, dump, n_regs);
-
-			kfree(dump);
-		}
-	}
-
-	for (pipe = 0; pipe < CIK_SDMA_ENGINE_NUM; pipe++) {
-		for (queue = 0; queue < CIK_SDMA_QUEUES_PER_ENGINE; queue++) {
-			r = dqm->dev->kfd2kgd->hqd_sdma_dump(
-				dqm->dev->kgd, pipe, queue, &dump, &n_regs);
-			if (r != 0)
-				break;
-
-			seq_printf(m, "  SDMA Engine %d, RLC %d\n",
-				  pipe, queue);
-			seq_reg_dump(m, dump, n_regs);
-
-			kfree(dump);
-		}
-	}
-
-	return r;
-}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index a0ac658..a625b91 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -29,12 +29,12 @@
 #include "kfd_priv.h"
 #include "kfd_mqd_manager.h"
 
-#define KFD_HIQ_TIMEOUT				(500)
-#define KFD_UNMAP_LATENCY_MS			(4000)
-#define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS (2 * KFD_UNMAP_LATENCY_MS + 1000)
-
+#define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS	(500)
 #define QUEUES_PER_PIPE				(8)
 #define PIPE_PER_ME_CP_SCHEDULING		(3)
+#define CIK_VMID_NUM				(8)
+#define KFD_VMID_START_OFFSET			(8)
+#define VMID_PER_DEVICE				CIK_VMID_NUM
 #define KFD_DQM_FIRST_PIPE			(0)
 #define CIK_SDMA_QUEUES				(4)
 #define CIK_SDMA_QUEUES_PER_ENGINE		(2)
@@ -81,12 +81,6 @@ struct device_process_node {
  * @set_cache_memory_policy: Sets memory policy (cached/ non cached) for the
  * memory apertures.
  *
- * @set_page_directory_base: Sets the PD base address (GPU local memory)
- * in all the queues of the relevant process running on the specified device.
- * It preempts the queues, updates the value and execute the runlist again.
- *
- * @process_termination: Clears all process queues belongs to that device.
- *
  */
 
 struct device_queue_manager_ops {
@@ -130,20 +124,12 @@ struct device_queue_manager_ops {
 					   enum cache_policy alternate_policy,
 					   void __user *alternate_aperture_base,
 					   uint64_t alternate_aperture_size);
-
-	int	(*set_trap_handler)(struct device_queue_manager *dqm,
-				    struct qcm_process_device *qpd,
-				    uint64_t tba_addr,
-				    uint64_t tma_addr);
-
-	int (*process_termination)(struct device_queue_manager *dqm,
-			struct qcm_process_device *qpd);
 };
 
 struct device_queue_manager_asic_ops {
-	int	(*update_qpd)(struct device_queue_manager *dqm,
+	int	(*register_process)(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
-	int	(*init_cpsch)(struct device_queue_manager *dqm);
+	int	(*initialize)(struct device_queue_manager *dqm);
 	bool	(*set_cache_memory_policy)(struct device_queue_manager *dqm,
 					   struct qcm_process_device *qpd,
 					   enum cache_policy default_policy,
@@ -169,7 +155,7 @@ struct device_queue_manager_asic_ops {
 
 struct device_queue_manager {
 	struct device_queue_manager_ops ops;
-	struct device_queue_manager_asic_ops asic_ops;
+	struct device_queue_manager_asic_ops ops_asic_specific;
 
 	struct mqd_manager	*mqds[KFD_MQD_TYPE_MAX];
 	struct packet_manager	packets;
@@ -190,17 +176,10 @@ struct device_queue_manager {
 	unsigned int		*fence_addr;
 	struct kfd_mem_obj	*fence_mem;
 	bool			active_runlist;
-	int			sched_policy;
 };
 
-void device_queue_manager_init_cik(
-		struct device_queue_manager_asic_ops *asic_ops);
-void device_queue_manager_init_cik_hawaii(
-		struct device_queue_manager_asic_ops *asic_ops);
-void device_queue_manager_init_vi(
-		struct device_queue_manager_asic_ops *asic_ops);
-void device_queue_manager_init_vi_tonga(
-		struct device_queue_manager_asic_ops *asic_ops);
+void device_queue_manager_init_cik(struct device_queue_manager_asic_ops *ops);
+void device_queue_manager_init_vi(struct device_queue_manager_asic_ops *ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 int init_pipelines(struct device_queue_manager *dqm,
@@ -208,12 +187,6 @@ int init_pipelines(struct device_queue_manager *dqm,
 unsigned int get_first_pipe(struct device_queue_manager *dqm);
 unsigned int get_pipes_num(struct device_queue_manager *dqm);
 
-int process_evict_queues(struct device_queue_manager *dqm,
-		struct qcm_process_device *qpd);
-int process_restore_queues(struct device_queue_manager *dqm,
-		struct qcm_process_device *qpd);
-
-
 static inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
 {
 	return (pdd->lds_base >> 16) & 0xFF;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_cik.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_cik.c
index 485f143..c6f435a 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_cik.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_cik.c
@@ -24,7 +24,6 @@
 #include "kfd_device_queue_manager.h"
 #include "cik_regs.h"
 #include "oss/oss_2_4_sh_mask.h"
-#include "gca/gfx_7_2_sh_mask.h"
 
 static bool set_cache_memory_policy_cik(struct device_queue_manager *dqm,
 				   struct qcm_process_device *qpd,
@@ -32,33 +31,18 @@ static bool set_cache_memory_policy_cik(struct device_queue_manager *dqm,
 				   enum cache_policy alternate_policy,
 				   void __user *alternate_aperture_base,
 				   uint64_t alternate_aperture_size);
-static int update_qpd_cik(struct device_queue_manager *dqm,
-					struct qcm_process_device *qpd);
-static int update_qpd_cik_hawaii(struct device_queue_manager *dqm,
+static int register_process_cik(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 static int initialize_cpsch_cik(struct device_queue_manager *dqm);
 static void init_sdma_vm(struct device_queue_manager *dqm, struct queue *q,
 				struct qcm_process_device *qpd);
-static void init_sdma_vm_hawaii(struct device_queue_manager *dqm,
-				struct queue *q,
-				struct qcm_process_device *qpd);
-
-void device_queue_manager_init_cik(
-		struct device_queue_manager_asic_ops *asic_ops)
-{
-	asic_ops->set_cache_memory_policy = set_cache_memory_policy_cik;
-	asic_ops->update_qpd = update_qpd_cik;
-	asic_ops->init_cpsch = initialize_cpsch_cik;
-	asic_ops->init_sdma_vm = init_sdma_vm;
-}
 
-void device_queue_manager_init_cik_hawaii(
-		struct device_queue_manager_asic_ops *asic_ops)
+void device_queue_manager_init_cik(struct device_queue_manager_asic_ops *ops)
 {
-	asic_ops->set_cache_memory_policy = set_cache_memory_policy_cik;
-	asic_ops->update_qpd = update_qpd_cik_hawaii;
-	asic_ops->init_cpsch = initialize_cpsch_cik;
-	asic_ops->init_sdma_vm = init_sdma_vm_hawaii;
+	ops->set_cache_memory_policy = set_cache_memory_policy_cik;
+	ops->register_process = register_process_cik;
+	ops->initialize = initialize_cpsch_cik;
+	ops->init_sdma_vm = init_sdma_vm;
 }
 
 static uint32_t compute_sh_mem_bases_64bit(unsigned int top_address_nybble)
@@ -114,7 +98,7 @@ static bool set_cache_memory_policy_cik(struct device_queue_manager *dqm,
 	return true;
 }
 
-static int update_qpd_cik(struct device_queue_manager *dqm,
+static int register_process_cik(struct device_queue_manager *dqm,
 		struct qcm_process_device *qpd)
 {
 	struct kfd_process_device *pdd;
@@ -141,7 +125,6 @@ static int update_qpd_cik(struct device_queue_manager *dqm,
 	} else {
 		temp = get_sh_mem_bases_nybble_64(pdd);
 		qpd->sh_mem_bases = compute_sh_mem_bases_64bit(temp);
-		qpd->sh_mem_config |= 1  << SH_MEM_CONFIG__PRIVATE_ATC__SHIFT;
 	}
 
 	pr_debug("kfd: is32bit process: %d sh_mem_bases nybble: 0x%X and register 0x%X\n",
@@ -150,37 +133,6 @@ static int update_qpd_cik(struct device_queue_manager *dqm,
 	return 0;
 }
 
-static int update_qpd_cik_hawaii(struct device_queue_manager *dqm,
-		struct qcm_process_device *qpd)
-{
-	struct kfd_process_device *pdd;
-	unsigned int temp;
-
-	BUG_ON(!dqm || !qpd);
-
-	pdd = qpd_to_pdd(qpd);
-
-	/* check if sh_mem_config register already configured */
-	if (qpd->sh_mem_config == 0) {
-		qpd->sh_mem_config =
-			ALIGNMENT_MODE(SH_MEM_ALIGNMENT_MODE_UNALIGNED) |
-			DEFAULT_MTYPE(MTYPE_NONCACHED) |
-			APE1_MTYPE(MTYPE_NONCACHED);
-		qpd->sh_mem_ape1_limit = 0;
-		qpd->sh_mem_ape1_base = 0;
-	}
-
-	/* On dGPU we're always in GPUVM64 addressing mode with 64-bit
-	 * aperture addresses. */
-	temp = get_sh_mem_bases_nybble_64(pdd);
-	qpd->sh_mem_bases = compute_sh_mem_bases_64bit(temp);
-
-	pr_debug("kfd: is32bit process: %d sh_mem_bases nybble: 0x%X and register 0x%X\n",
-		qpd->pqm->process->is_32bit_user_mode, temp, qpd->sh_mem_bases);
-
-	return 0;
-}
-
 static void init_sdma_vm(struct device_queue_manager *dqm, struct queue *q,
 				struct qcm_process_device *qpd)
 {
@@ -197,18 +149,6 @@ static void init_sdma_vm(struct device_queue_manager *dqm, struct queue *q,
 	q->properties.sdma_vm_addr = value;
 }
 
-static void init_sdma_vm_hawaii(struct device_queue_manager *dqm,
-				struct queue *q,
-				struct qcm_process_device *qpd)
-{
-	/* On dGPU we're always in GPUVM64 addressing mode with 64-bit
-	 * aperture addresses. */
-	q->properties.sdma_vm_addr =
-		((get_sh_mem_bases_nybble_64(qpd_to_pdd(qpd))) <<
-		 SDMA0_RLC0_VIRTUAL_ADDR__SHARED_BASE__SHIFT) &
-		SDMA0_RLC0_VIRTUAL_ADDR__SHARED_BASE_MASK;
-}
-
 static int initialize_cpsch_cik(struct device_queue_manager *dqm)
 {
 	return init_pipelines(dqm, get_pipes_num(dqm), get_first_pipe(dqm));
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_vi.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_vi.c
index abd71c6..7e9cae9 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_vi.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_vi.c
@@ -33,44 +33,18 @@ static bool set_cache_memory_policy_vi(struct device_queue_manager *dqm,
 				   enum cache_policy alternate_policy,
 				   void __user *alternate_aperture_base,
 				   uint64_t alternate_aperture_size);
-static int update_qpd_vi(struct device_queue_manager *dqm,
+static int register_process_vi(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 static int initialize_cpsch_vi(struct device_queue_manager *dqm);
 static void init_sdma_vm(struct device_queue_manager *dqm, struct queue *q,
 				struct qcm_process_device *qpd);
 
-/*
- * Tonga device queue manager functions
- */
-static bool set_cache_memory_policy_vi_tonga(struct device_queue_manager *dqm,
-			struct qcm_process_device *qpd,
-			enum cache_policy default_policy,
-			enum cache_policy alternate_policy,
-			void __user *alternate_aperture_base,
-			uint64_t alternate_aperture_size);
-static int update_qpd_vi_tonga(struct device_queue_manager *dqm,
-			struct qcm_process_device *qpd);
-static void init_sdma_vm_tonga(struct device_queue_manager *dqm,
-			struct queue *q,
-			struct qcm_process_device *qpd);
-
-void device_queue_manager_init_vi_tonga(
-		struct device_queue_manager_asic_ops *asic_ops)
-{
-	asic_ops->set_cache_memory_policy = set_cache_memory_policy_vi_tonga;
-	asic_ops->update_qpd = update_qpd_vi_tonga;
-	asic_ops->init_cpsch = initialize_cpsch_vi;
-	asic_ops->init_sdma_vm = init_sdma_vm_tonga;
-}
-
-
-void device_queue_manager_init_vi(
-		struct device_queue_manager_asic_ops *asic_ops)
+void device_queue_manager_init_vi(struct device_queue_manager_asic_ops *ops)
 {
-	asic_ops->set_cache_memory_policy = set_cache_memory_policy_vi;
-	asic_ops->update_qpd = update_qpd_vi;
-	asic_ops->init_cpsch = initialize_cpsch_vi;
-	asic_ops->init_sdma_vm = init_sdma_vm;
+	ops->set_cache_memory_policy = set_cache_memory_policy_vi;
+	ops->register_process = register_process_vi;
+	ops->initialize = initialize_cpsch_vi;
+	ops->init_sdma_vm = init_sdma_vm;
 }
 
 static uint32_t compute_sh_mem_bases_64bit(unsigned int top_address_nybble)
@@ -130,34 +104,7 @@ static bool set_cache_memory_policy_vi(struct device_queue_manager *dqm,
 	return true;
 }
 
-static bool set_cache_memory_policy_vi_tonga(struct device_queue_manager *dqm,
-		struct qcm_process_device *qpd,
-		enum cache_policy default_policy,
-		enum cache_policy alternate_policy,
-		void __user *alternate_aperture_base,
-		uint64_t alternate_aperture_size)
-{
-	uint32_t default_mtype;
-	uint32_t ape1_mtype;
-
-	default_mtype = (default_policy == cache_policy_coherent) ?
-			MTYPE_UC :
-			MTYPE_NC_NV;
-
-	ape1_mtype = (alternate_policy == cache_policy_coherent) ?
-			MTYPE_UC :
-			MTYPE_NC_NV;
-
-	qpd->sh_mem_config =
-			SH_MEM_ALIGNMENT_MODE_UNALIGNED <<
-				   SH_MEM_CONFIG__ALIGNMENT_MODE__SHIFT |
-			default_mtype << SH_MEM_CONFIG__DEFAULT_MTYPE__SHIFT |
-			ape1_mtype << SH_MEM_CONFIG__APE1_MTYPE__SHIFT;
-
-	return true;
-}
-
-static int update_qpd_vi(struct device_queue_manager *dqm,
+static int register_process_vi(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd)
 {
 	struct kfd_process_device *pdd;
@@ -190,8 +137,6 @@ static int update_qpd_vi(struct device_queue_manager *dqm,
 		qpd->sh_mem_bases = compute_sh_mem_bases_64bit(temp);
 		qpd->sh_mem_config |= SH_MEM_ADDRESS_MODE_HSA64 <<
 			SH_MEM_CONFIG__ADDRESS_MODE__SHIFT;
-		qpd->sh_mem_config |= 1  <<
-			SH_MEM_CONFIG__PRIVATE_ATC__SHIFT;
 	}
 
 	pr_debug("kfd: is32bit process: %d sh_mem_bases nybble: 0x%X and register 0x%X\n",
@@ -200,41 +145,6 @@ static int update_qpd_vi(struct device_queue_manager *dqm,
 	return 0;
 }
 
-static int update_qpd_vi_tonga(struct device_queue_manager *dqm,
-			struct qcm_process_device *qpd)
-{
-	struct kfd_process_device *pdd;
-	unsigned int temp;
-
-	BUG_ON(!dqm || !qpd);
-
-	pdd = qpd_to_pdd(qpd);
-
-	/* check if sh_mem_config register already configured */
-	if (qpd->sh_mem_config == 0) {
-		qpd->sh_mem_config =
-				SH_MEM_ALIGNMENT_MODE_UNALIGNED <<
-					SH_MEM_CONFIG__ALIGNMENT_MODE__SHIFT |
-				MTYPE_UC <<
-					SH_MEM_CONFIG__DEFAULT_MTYPE__SHIFT |
-				MTYPE_UC <<
-					SH_MEM_CONFIG__APE1_MTYPE__SHIFT;
-
-		qpd->sh_mem_ape1_limit = 0;
-		qpd->sh_mem_ape1_base = 0;
-	}
-
-	/* On dGPU we're always in GPUVM64 addressing mode with 64-bit
-	 * aperture addresses. */
-	temp = get_sh_mem_bases_nybble_64(pdd);
-	qpd->sh_mem_bases = compute_sh_mem_bases_64bit(temp);
-
-	pr_debug("kfd: sh_mem_bases nybble: 0x%X and register 0x%X\n",
-		temp, qpd->sh_mem_bases);
-
-	return 0;
-}
-
 static void init_sdma_vm(struct device_queue_manager *dqm, struct queue *q,
 				struct qcm_process_device *qpd)
 {
@@ -251,19 +161,6 @@ static void init_sdma_vm(struct device_queue_manager *dqm, struct queue *q,
 	q->properties.sdma_vm_addr = value;
 }
 
-static void init_sdma_vm_tonga(struct device_queue_manager *dqm,
-			struct queue *q,
-			struct qcm_process_device *qpd)
-{
-	/* On dGPU we're always in GPUVM64 addressing mode with 64-bit
-	 * aperture addresses. */
-	q->properties.sdma_vm_addr =
-		((get_sh_mem_bases_nybble_64(qpd_to_pdd(qpd))) <<
-		 SDMA0_RLC0_VIRTUAL_ADDR__SHARED_BASE__SHIFT) &
-		SDMA0_RLC0_VIRTUAL_ADDR__SHARED_BASE_MASK;
-}
-
-
 static int initialize_cpsch_vi(struct device_queue_manager *dqm)
 {
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c b/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
index ecc4228..453c5d6 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
@@ -51,7 +51,7 @@
  */
 
 /* # of doorbell bytes allocated for each process. */
-size_t kfd_doorbell_process_slice(void)
+static inline size_t doorbell_process_allocation(void)
 {
 	return roundup(KFD_SIZE_OF_DOORBELL_IN_BYTES *
 			KFD_MAX_NUM_OF_QUEUES_PER_PROCESS,
@@ -73,16 +73,16 @@ void kfd_doorbell_init(struct kfd_dev *kfd)
 
 	doorbell_start_offset =
 			roundup(kfd->shared_resources.doorbell_start_offset,
-					kfd_doorbell_process_slice());
+					doorbell_process_allocation());
 
 	doorbell_aperture_size =
 			rounddown(kfd->shared_resources.doorbell_aperture_size,
-					kfd_doorbell_process_slice());
+					doorbell_process_allocation());
 
 	if (doorbell_aperture_size > doorbell_start_offset)
 		doorbell_process_limit =
 			(doorbell_aperture_size - doorbell_start_offset) /
-						kfd_doorbell_process_slice();
+						doorbell_process_allocation();
 	else
 		doorbell_process_limit = 0;
 
@@ -93,7 +93,7 @@ void kfd_doorbell_init(struct kfd_dev *kfd)
 	kfd->doorbell_process_limit = doorbell_process_limit - 1;
 
 	kfd->doorbell_kernel_ptr = ioremap(kfd->doorbell_base,
-						kfd_doorbell_process_slice());
+						doorbell_process_allocation());
 
 	BUG_ON(!kfd->doorbell_kernel_ptr);
 
@@ -126,7 +126,7 @@ int kfd_doorbell_mmap(struct kfd_process *process, struct vm_area_struct *vma)
 	 * For simplicitly we only allow mapping of the entire doorbell
 	 * allocation of a single device & process.
 	 */
-	if (vma->vm_end - vma->vm_start != kfd_doorbell_process_slice())
+	if (vma->vm_end - vma->vm_start != doorbell_process_allocation())
 		return -EINVAL;
 
 	/* Find kfd device according to gpu id */
@@ -142,19 +142,20 @@ int kfd_doorbell_mmap(struct kfd_process *process, struct vm_area_struct *vma)
 
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
-	pr_debug("kfd: mapping doorbell page in kfd_doorbell_mmap\n"
+	pr_debug("kfd: mapping doorbell page in %s\n"
 		 "     target user address == 0x%08llX\n"
 		 "     physical address    == 0x%08llX\n"
 		 "     vm_flags            == 0x%04lX\n"
 		 "     size                == 0x%04lX\n",
+		 __func__,
 		 (unsigned long long) vma->vm_start, address, vma->vm_flags,
-		 kfd_doorbell_process_slice());
+		 doorbell_process_allocation());
 
 
 	return io_remap_pfn_range(vma,
 				vma->vm_start,
 				address >> PAGE_SHIFT,
-				kfd_doorbell_process_slice(),
+				doorbell_process_allocation(),
 				vma->vm_page_prot);
 }
 
@@ -181,11 +182,11 @@ u32 __iomem *kfd_get_kernel_doorbell(struct kfd_dev *kfd,
 	 * Calculating the kernel doorbell offset using "faked" kernel
 	 * pasid that allocated for kernel queues only
 	 */
-	*doorbell_off = KERNEL_DOORBELL_PASID * (kfd_doorbell_process_slice() /
+	*doorbell_off = KERNEL_DOORBELL_PASID * (doorbell_process_allocation() /
 							sizeof(u32)) + inx;
 
 	pr_debug("kfd: get kernel queue doorbell\n"
-			 "     doorbell offset   == 0x%08d\n"
+			 "     doorbell offset   == 0x%08X\n"
 			 "     kernel address    == 0x%08lX\n",
 		*doorbell_off, (uintptr_t)(kfd->doorbell_kernel_ptr + inx));
 
@@ -223,11 +224,11 @@ unsigned int kfd_queue_id_to_doorbell(struct kfd_dev *kfd,
 {
 	/*
 	 * doorbell_id_offset accounts for doorbells taken by KGD.
-	 * pasid * kfd_doorbell_process_slice/sizeof(u32) adjusts
+	 * pasid * doorbell_process_allocation/sizeof(u32) adjusts
 	 * to the process's doorbells
 	 */
 	return kfd->doorbell_id_offset +
-		process->pasid * (kfd_doorbell_process_slice()/sizeof(u32)) +
+		process->pasid * (doorbell_process_allocation()/sizeof(u32)) +
 		queue_id;
 }
 
@@ -235,7 +236,7 @@ uint64_t kfd_get_number_elems(struct kfd_dev *kfd)
 {
 	uint64_t num_of_elems = (kfd->shared_resources.doorbell_aperture_size -
 				kfd->shared_resources.doorbell_start_offset) /
-					kfd_doorbell_process_slice() + 1;
+					doorbell_process_allocation() + 1;
 
 	return num_of_elems;
 
@@ -245,5 +246,5 @@ phys_addr_t kfd_get_process_doorbells(struct kfd_dev *dev,
 					struct kfd_process *process)
 {
 	return dev->doorbell_base +
-		process->pasid * kfd_doorbell_process_slice();
+		process->pasid * doorbell_process_allocation();
 }
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_events.c b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
index a587925..a6a4b2b 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_events.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
@@ -32,10 +32,11 @@
 #include "kfd_events.h"
 #include <linux/device.h>
 
-/* A task can only be on a single wait_queue at a time, but we need to support
+/*
+ * A task can only be on a single wait_queue at a time, but we need to support
  * waiting on multiple events (any/all).
- * Instead of each event simply having a wait_queue with sleeping tasks, it has a
- * singly-linked list of tasks.
+ * Instead of each event simply having a wait_queue with sleeping tasks, it
+ * has a singly-linked list of tasks.
  * A thread that wants to sleep creates an array of these, one for each event
  * and adds one to each event's waiter chain.
  */
@@ -51,14 +52,12 @@ struct kfd_event_waiter {
 	uint32_t input_index;
 };
 
-#define SLOTS_PER_PAGE KFD_SIGNAL_EVENT_LIMIT
-#define SLOT_BITMAP_LONGS BITS_TO_LONGS(SLOTS_PER_PAGE)
-
-/* Over-complicated pooled allocator for event notification slots.
+/*
+ * Over-complicated pooled allocator for event notification slots.
  *
- * Each signal event needs a 64-bit signal slot where the signaler will write a 1
- * before sending an interrupt.l (This is needed because some interrupts do not
- * contain enough spare data bits to identify an event.)
+ * Each signal event needs a 64-bit signal slot where the signaler will write
+ * a 1 before sending an interrupt.l (This is needed because some interrupts
+ * do not contain enough spare data bits to identify an event.)
  * We get whole pages from vmalloc and map them to the process VA.
  * Individual signal events are then allocated a slot in a page.
  */
@@ -66,13 +65,18 @@ struct kfd_event_waiter {
 struct signal_page {
 	struct list_head event_pages;	/* kfd_process.signal_event_pages */
 	uint64_t *kernel_address;
-	uint64_t handle;
 	uint64_t __user *user_address;
 	uint32_t page_index;		/* Index into the mmap aperture. */
 	unsigned int free_slots;
-	unsigned long used_slot_bitmap[SLOT_BITMAP_LONGS];
+	unsigned long used_slot_bitmap[0];
 };
 
+#define SLOTS_PER_PAGE KFD_SIGNAL_EVENT_LIMIT
+#define SLOT_BITMAP_SIZE BITS_TO_LONGS(SLOTS_PER_PAGE)
+#define BITS_PER_PAGE (ilog2(SLOTS_PER_PAGE)+1)
+#define SIGNAL_PAGE_SIZE (sizeof(struct signal_page) + \
+				SLOT_BITMAP_SIZE * sizeof(long))
+
 /*
  * For signal events, the event ID is used as the interrupt user data.
  * For SQ s_sendmsg interrupts, this is limited to 8 bits.
@@ -81,27 +85,23 @@ struct signal_page {
 #define INTERRUPT_DATA_BITS 8
 #define SIGNAL_EVENT_ID_SLOT_SHIFT 0
 
-/* We can only create 8 debug events */
-
-#define KFD_DEBUG_EVENT_LIMIT 8
-#define KFD_DEBUG_EVENT_MASK 0x1F
-#define KFD_DEBUG_EVENT_SHIFT 5
-
 static uint64_t *page_slots(struct signal_page *page)
 {
 	return page->kernel_address;
 }
 
-static bool
-allocate_free_slot(struct kfd_process *process,
-		   struct signal_page **out_page,
-		   unsigned int *out_slot_index)
+static bool allocate_free_slot(struct kfd_process *process,
+				struct signal_page **out_page,
+				unsigned int *out_slot_index)
 {
 	struct signal_page *page;
 
 	list_for_each_entry(page, &process->signal_event_pages, event_pages) {
 		if (page->free_slots > 0) {
-			unsigned int slot = find_first_zero_bit(page->used_slot_bitmap, SLOTS_PER_PAGE);
+			unsigned int slot =
+				find_first_zero_bit(page->used_slot_bitmap,
+							SLOTS_PER_PAGE);
+
 			__set_bit(slot, page->used_slot_bitmap);
 			page->free_slots--;
 
@@ -130,31 +130,23 @@ static bool allocate_signal_page(struct file *devkfd, struct kfd_process *p)
 {
 	void *backing_store;
 	struct signal_page *page;
-	unsigned int slot;
-	int i;
 
-	page = kzalloc(sizeof(struct signal_page), GFP_KERNEL);
+	page = kzalloc(SIGNAL_PAGE_SIZE, GFP_KERNEL);
 	if (!page)
 		goto fail_alloc_signal_page;
 
 	page->free_slots = SLOTS_PER_PAGE;
 
-	backing_store = (void *) __get_free_pages(GFP_KERNEL | __GFP_ZERO, \
+	backing_store = (void *) __get_free_pages(GFP_KERNEL | __GFP_ZERO,
 					get_order(KFD_SIGNAL_EVENT_LIMIT * 8));
 	if (!backing_store)
 		goto fail_alloc_signal_store;
 
 	/* prevent user-mode info leaks */
-	memset(backing_store, (uint8_t) UNSIGNALED_EVENT_SLOT, KFD_SIGNAL_EVENT_LIMIT * 8);
-	page->kernel_address = backing_store;
+	memset(backing_store, (uint8_t) UNSIGNALED_EVENT_SLOT,
+		KFD_SIGNAL_EVENT_LIMIT * 8);
 
-	/* Set bits of debug events to prevent allocation */
-	for (i = 0 ; i < KFD_DEBUG_EVENT_LIMIT ; i++) {
-		slot = (i << KFD_DEBUG_EVENT_SHIFT) |
-				KFD_DEBUG_EVENT_MASK;
-		__set_bit(slot, page->used_slot_bitmap);
-		page->free_slots--;
-	}
+	page->kernel_address = backing_store;
 
 	if (list_empty(&p->signal_event_pages))
 		page->page_index = 0;
@@ -177,10 +169,10 @@ fail_alloc_signal_page:
 	return false;
 }
 
-static bool
-allocate_event_notification_slot(struct file *devkfd, struct kfd_process *p,
-				 struct signal_page **page,
-				 unsigned int *signal_slot_index)
+static bool allocate_event_notification_slot(struct file *devkfd,
+					struct kfd_process *p,
+					struct signal_page **page,
+					unsigned int *signal_slot_index)
 {
 	bool ret;
 
@@ -194,88 +186,6 @@ allocate_event_notification_slot(struct file *devkfd, struct kfd_process *p,
 	return ret;
 }
 
-static bool
-allocate_signal_page_dgpu(struct kfd_process *p,
-		uint64_t *kernel_address, uint64_t handle)
-{
-	struct signal_page *my_page;
-
-	my_page = kzalloc(sizeof(struct signal_page), GFP_KERNEL);
-	if (!my_page)
-		return false;
-
-	/* prevent user-mode info leaks */
-	memset(kernel_address, (uint8_t) UNSIGNALED_EVENT_SLOT,
-			KFD_SIGNAL_EVENT_LIMIT * 8);
-
-	my_page->kernel_address = kernel_address;
-	my_page->handle = handle;
-	my_page->user_address = NULL;
-	my_page->free_slots = SLOTS_PER_PAGE;
-	if (list_empty(&p->signal_event_pages))
-			my_page->page_index = 0;
-	else
-		my_page->page_index = list_tail_entry(&p->signal_event_pages,
-		   struct signal_page,
-		   event_pages)->page_index + 1;
-
-	pr_debug("allocated new event signal page at %p, for process %p\n",
-			my_page, p);
-	pr_debug("page index is %d\n", my_page->page_index);
-
-	list_add(&my_page->event_pages, &p->signal_event_pages);
-
-	return true;
-}
-
-void kfd_free_signal_page_dgpu(struct kfd_process *p, uint64_t handle)
-{
-	struct signal_page *page, *tmp;
-
-	list_for_each_entry_safe(page, tmp, &p->signal_event_pages,
-				event_pages) {
-		if (page->handle == handle) {
-			list_del(&page->event_pages);
-			kfree(page);
-			break;
-		}
-	}
-}
-
-static bool
-allocate_debug_event_notification_slot(struct file *devkfd,
-				struct kfd_process *p,
-				struct signal_page **out_page,
-				unsigned int *out_slot_index)
-{
-	struct signal_page *page;
-	unsigned int slot;
-	bool ret;
-
-	if (list_empty(&p->signal_event_pages)) {
-		ret = allocate_signal_page(devkfd, p);
-		if (ret == false)
-			return ret;
-	}
-
-	page = list_entry((&p->signal_event_pages)->next, struct signal_page,
-				event_pages);
-	slot = (p->debug_event_count << KFD_DEBUG_EVENT_SHIFT) |
-			KFD_DEBUG_EVENT_MASK;
-
-	pr_debug("page == %p\n", page);
-	pr_debug("slot == %d\n", slot);
-
-	page_slots(page)[slot] = UNSIGNALED_EVENT_SLOT;
-	*out_page = page;
-	*out_slot_index = slot;
-
-	pr_debug("allocated debug event signal slot in page %p, slot %d\n",
-			page, slot);
-
-	return true;
-}
-
 /* Assumes that the process's event_mutex is locked. */
 static void release_event_notification_slot(struct signal_page *page,
 						size_t slot_index)
@@ -292,7 +202,10 @@ static struct signal_page *lookup_signal_page_by_index(struct kfd_process *p,
 {
 	struct signal_page *page;
 
-	/* This is safe because we don't delete signal pages until the process exits. */
+	/*
+	 * This is safe because we don't delete signal pages until the
+	 * process exits.
+	 */
 	list_for_each_entry(page, &p->signal_event_pages, event_pages)
 		if (page->page_index == page_index)
 			return page;
@@ -300,7 +213,10 @@ static struct signal_page *lookup_signal_page_by_index(struct kfd_process *p,
 	return NULL;
 }
 
-/* Assumes that p->event_mutex is held and of course that p is not going away (current or locked). */
+/*
+ * Assumes that p->event_mutex is held and of course that p is not going
+ * away (current or locked).
+ */
 static struct kfd_event *lookup_event_by_id(struct kfd_process *p, uint32_t id)
 {
 	struct kfd_event *ev;
@@ -315,27 +231,32 @@ static struct kfd_event *lookup_event_by_id(struct kfd_process *p, uint32_t id)
 static u32 make_signal_event_id(struct signal_page *page,
 					 unsigned int signal_slot_index)
 {
-	return page->page_index | (signal_slot_index << SIGNAL_EVENT_ID_SLOT_SHIFT);
+	return page->page_index |
+			(signal_slot_index << SIGNAL_EVENT_ID_SLOT_SHIFT);
 }
 
-/* Produce a kfd event id for a nonsignal event.
- * These are arbitrary numbers, so we do a sequential search through the hash table
- * for an unused number.
+/*
+ * Produce a kfd event id for a nonsignal event.
+ * These are arbitrary numbers, so we do a sequential search through
+ * the hash table for an unused number.
  */
 static u32 make_nonsignal_event_id(struct kfd_process *p)
 {
 	u32 id;
 
 	for (id = p->next_nonsignal_event_id;
-	     id < KFD_LAST_NONSIGNAL_EVENT_ID && lookup_event_by_id(p, id) != NULL;
-	     id++)
+		id < KFD_LAST_NONSIGNAL_EVENT_ID &&
+		lookup_event_by_id(p, id) != NULL;
+		id++)
 		;
 
 	if (id < KFD_LAST_NONSIGNAL_EVENT_ID) {
 
-		/* What if id == LAST_NONSIGNAL_EVENT_ID - 1?
-		 * Then next_nonsignal_event_id = LAST_NONSIGNAL_EVENT_ID so the first loop
-		 * fails immediately and we proceed with the wraparound loop below.
+		/*
+		 * What if id == LAST_NONSIGNAL_EVENT_ID - 1?
+		 * Then next_nonsignal_event_id = LAST_NONSIGNAL_EVENT_ID so
+		 * the first loop fails immediately and we proceed with the
+		 * wraparound loop below.
 		 */
 		p->next_nonsignal_event_id = id + 1;
 
@@ -343,68 +264,54 @@ static u32 make_nonsignal_event_id(struct kfd_process *p)
 	}
 
 	for (id = KFD_FIRST_NONSIGNAL_EVENT_ID;
-	     id < KFD_LAST_NONSIGNAL_EVENT_ID && lookup_event_by_id(p, id) != NULL;
-	     id++)
+		id < KFD_LAST_NONSIGNAL_EVENT_ID &&
+		lookup_event_by_id(p, id) != NULL;
+		id++)
 		;
 
 
 	if (id < KFD_LAST_NONSIGNAL_EVENT_ID) {
 		p->next_nonsignal_event_id = id + 1;
 		return id;
-	} else {
-		p->next_nonsignal_event_id = KFD_FIRST_NONSIGNAL_EVENT_ID;
-		return 0;
 	}
+
+	p->next_nonsignal_event_id = KFD_FIRST_NONSIGNAL_EVENT_ID;
+	return 0;
 }
 
-static struct kfd_event *
-lookup_event_by_page_slot(struct kfd_process *p,
-			  struct signal_page *page, unsigned int signal_slot)
+static struct kfd_event *lookup_event_by_page_slot(struct kfd_process *p,
+						struct signal_page *page,
+						unsigned int signal_slot)
 {
 	return lookup_event_by_id(p, make_signal_event_id(page, signal_slot));
 }
 
-static int
-create_signal_event(struct file *devkfd, struct kfd_process *p, struct kfd_event *ev)
+static int create_signal_event(struct file *devkfd,
+				struct kfd_process *p,
+				struct kfd_event *ev)
 {
-	if ((ev->type == KFD_EVENT_TYPE_SIGNAL) &&
-			(p->signal_event_count == KFD_SIGNAL_EVENT_LIMIT)) {
+	if (p->signal_event_count == KFD_SIGNAL_EVENT_LIMIT) {
 		pr_warn("amdkfd: Signal event wasn't created because limit was reached\n");
 		return -ENOMEM;
-	} else if ((ev->type == KFD_EVENT_TYPE_DEBUG) &&
-			(p->debug_event_count == KFD_DEBUG_EVENT_LIMIT)) {
-		pr_warn("amdkfd: Debug event wasn't created because limit was reached\n");
-		return -ENOMEM;
 	}
 
-	if (ev->type == KFD_EVENT_TYPE_SIGNAL) {
-		if (!allocate_event_notification_slot(devkfd, p,
-						&ev->signal_page,
+	if (!allocate_event_notification_slot(devkfd, p, &ev->signal_page,
 						&ev->signal_slot_index)) {
-			pr_warn("amdkfd: Signal event wasn't created because out of kernel memory\n");
-			return -ENOMEM;
-		}
-
-		p->signal_event_count++;
+		pr_warn("amdkfd: Signal event wasn't created because out of kernel memory\n");
+		return -ENOMEM;
+	}
 
-		if ((p->signal_event_count & KFD_DEBUG_EVENT_MASK) ==
-				KFD_DEBUG_EVENT_MASK)
-			p->signal_event_count++;
+	p->signal_event_count++;
 
-	} else if (ev->type == KFD_EVENT_TYPE_DEBUG) {
-		if (!allocate_debug_event_notification_slot(devkfd, p,
-						&ev->signal_page,
-						&ev->signal_slot_index)) {
-			pr_warn("amdkfd: Debug event wasn't created because out of kernel memory\n");
-			return -ENOMEM;
-		}
+	ev->user_signal_address =
+			&ev->signal_page->user_address[ev->signal_slot_index];
 
-		p->debug_event_count++;
-	}
+	ev->event_id = make_signal_event_id(ev->signal_page,
+						ev->signal_slot_index);
 
-	ev->user_signal_address = &ev->signal_page->user_address[ev->signal_slot_index];
-
-	ev->event_id = make_signal_event_id(ev->signal_page, ev->signal_slot_index);
+	pr_debug("signal event number %zu created with id %d, address %p\n",
+			p->signal_event_count, ev->event_id,
+			ev->user_signal_address);
 
 	pr_debug("signal event number %zu created with id %d, address %p\n",
 			p->signal_event_count, ev->event_id,
@@ -413,10 +320,12 @@ create_signal_event(struct file *devkfd, struct kfd_process *p, struct kfd_event
 	return 0;
 }
 
-/* No non-signal events are supported yet.
- * We create them as events that never signal. Set event calls from user-mode are failed. */
-static int
-create_other_event(struct kfd_process *p, struct kfd_event *ev)
+/*
+ * No non-signal events are supported yet.
+ * We create them as events that never signal.
+ * Set event calls from user-mode are failed.
+ */
+static int create_other_event(struct kfd_process *p, struct kfd_event *ev)
 {
 	ev->event_id = make_nonsignal_event_id(p);
 	if (ev->event_id == 0)
@@ -432,25 +341,20 @@ void kfd_event_init_process(struct kfd_process *p)
 	INIT_LIST_HEAD(&p->signal_event_pages);
 	p->next_nonsignal_event_id = KFD_FIRST_NONSIGNAL_EVENT_ID;
 	p->signal_event_count = 0;
-	p->debug_event_count = 0;
 }
 
 static void destroy_event(struct kfd_process *p, struct kfd_event *ev)
 {
 	if (ev->signal_page != NULL) {
-		if (ev->type == KFD_EVENT_TYPE_SIGNAL) {
-			release_event_notification_slot(ev->signal_page,
-							ev->signal_slot_index);
-			p->signal_event_count--;
-			if ((p->signal_event_count & KFD_DEBUG_EVENT_MASK) ==
-					KFD_DEBUG_EVENT_MASK)
-				p->signal_event_count--;
-		} else if (ev->type == KFD_EVENT_TYPE_DEBUG) {
-			p->debug_event_count--;
-		}
+		release_event_notification_slot(ev->signal_page,
+						ev->signal_slot_index);
+		p->signal_event_count--;
 	}
 
-	/* Abandon the list of waiters. Individual waiting threads will clean up their own data.*/
+	/*
+	 * Abandon the list of waiters. Individual waiting threads will
+	 * clean up their own data.
+	 */
 	list_del(&ev->waiters);
 
 	hash_del(&ev->events);
@@ -467,17 +371,18 @@ static void destroy_events(struct kfd_process *p)
 		destroy_event(p, ev);
 }
 
-/* We assume that the process is being destroyed and there is no need to unmap the pages
- * or keep bookkeeping data in order. */
+/*
+ * We assume that the process is being destroyed and there is no need to
+ * unmap the pages or keep bookkeeping data in order.
+ */
 static void shutdown_signal_pages(struct kfd_process *p)
 {
 	struct signal_page *page, *tmp;
 
-	list_for_each_entry_safe(page, tmp, &p->signal_event_pages, event_pages) {
-		if (page->user_address) {
-			free_pages((unsigned long)page->kernel_address,
-					get_order(KFD_SIGNAL_EVENT_LIMIT * 8));
-		}
+	list_for_each_entry_safe(page, tmp, &p->signal_event_pages,
+					event_pages) {
+		free_pages((unsigned long)page->kernel_address,
+				get_order(KFD_SIGNAL_EVENT_LIMIT * 8));
 		kfree(page);
 	}
 }
@@ -490,7 +395,8 @@ void kfd_event_free_process(struct kfd_process *p)
 
 static bool event_can_be_gpu_signaled(const struct kfd_event *ev)
 {
-	return ev->type == KFD_EVENT_TYPE_SIGNAL || ev->type == KFD_EVENT_TYPE_DEBUG;
+	return ev->type == KFD_EVENT_TYPE_SIGNAL ||
+					ev->type == KFD_EVENT_TYPE_DEBUG;
 }
 
 static bool event_can_be_cpu_signaled(const struct kfd_event *ev)
@@ -501,12 +407,11 @@ static bool event_can_be_cpu_signaled(const struct kfd_event *ev)
 int kfd_event_create(struct file *devkfd, struct kfd_process *p,
 		     uint32_t event_type, bool auto_reset, uint32_t node_id,
 		     uint32_t *event_id, uint32_t *event_trigger_data,
-		     uint64_t *event_page_offset, uint32_t *event_slot_index,
-		     void *kern_addr)
+		     uint64_t *event_page_offset, uint32_t *event_slot_index)
 {
 	int ret = 0;
-
 	struct kfd_event *ev = kzalloc(sizeof(*ev), GFP_KERNEL);
+
 	if (!ev)
 		return -ENOMEM;
 
@@ -516,20 +421,17 @@ int kfd_event_create(struct file *devkfd, struct kfd_process *p,
 
 	INIT_LIST_HEAD(&ev->waiters);
 
-	mutex_lock(&p->event_mutex);
-
-	if (kern_addr && list_empty(&p->signal_event_pages))
-		allocate_signal_page_dgpu(p, kern_addr, *event_page_offset);
-
 	*event_page_offset = 0;
 
+	mutex_lock(&p->event_mutex);
+
 	switch (event_type) {
 	case KFD_EVENT_TYPE_SIGNAL:
 	case KFD_EVENT_TYPE_DEBUG:
 		ret = create_signal_event(devkfd, p, ev);
 		if (!ret) {
 			*event_page_offset = (ev->signal_page->page_index |
-					KFD_MMAP_TYPE_EVENTS);
+					KFD_MMAP_EVENTS_MASK);
 			*event_page_offset <<= PAGE_SHIFT;
 			*event_slot_index = ev->signal_slot_index;
 		}
@@ -636,7 +538,8 @@ int kfd_reset_event(struct kfd_process *p, uint32_t event_id)
 
 static void acknowledge_signal(struct kfd_process *p, struct kfd_event *ev)
 {
-	page_slots(ev->signal_page)[ev->signal_slot_index] = UNSIGNALED_EVENT_SLOT;
+	page_slots(ev->signal_page)[ev->signal_slot_index] =
+						UNSIGNALED_EVENT_SLOT;
 }
 
 static bool is_slot_signaled(struct signal_page *page, unsigned int index)
@@ -644,7 +547,8 @@ static bool is_slot_signaled(struct signal_page *page, unsigned int index)
 	return page_slots(page)[index] != UNSIGNALED_EVENT_SLOT;
 }
 
-static void set_event_from_interrupt(struct kfd_process *p, struct kfd_event *ev)
+static void set_event_from_interrupt(struct kfd_process *p,
+					struct kfd_event *ev)
 {
 	if (ev && event_can_be_gpu_signaled(ev)) {
 		acknowledge_signal(p, ev);
@@ -657,40 +561,42 @@ void kfd_signal_event_interrupt(unsigned int pasid, uint32_t partial_id,
 {
 	struct kfd_event *ev;
 
-	/* Because we are called from arbitrary context (workqueue) as opposed
+	/*
+	 * Because we are called from arbitrary context (workqueue) as opposed
 	 * to process context, kfd_process could attempt to exit while we are
-	 * running so the lookup function increments the process ref count.
+	 * running so the lookup function returns a locked process.
 	 */
 	struct kfd_process *p = kfd_lookup_process_by_pasid(pasid);
+
 	if (!p)
 		return; /* Presumably process exited. */
 
 	mutex_lock(&p->event_mutex);
 
-	if ((valid_id_bits >= INTERRUPT_DATA_BITS) &&
-			((partial_id & KFD_DEBUG_EVENT_MASK) ==
-					KFD_DEBUG_EVENT_MASK)) {
+	if (valid_id_bits >= INTERRUPT_DATA_BITS) {
 		/* Partial ID is a full ID. */
 		ev = lookup_event_by_id(p, partial_id);
 		set_event_from_interrupt(p, ev);
 	} else {
-		/* Partial ID is in fact partial. For now we completely ignore it,
-		 * but we could use any bits we did receive to search faster. */
+		/*
+		 * Partial ID is in fact partial. For now we completely
+		 * ignore it, but we could use any bits we did receive to
+		 * search faster.
+		 */
 		struct signal_page *page;
 		unsigned i;
 
-		list_for_each_entry(page, &p->signal_event_pages, event_pages) {
-			for (i = 0; i < SLOTS_PER_PAGE; i++) {
+		list_for_each_entry(page, &p->signal_event_pages, event_pages)
+			for (i = 0; i < SLOTS_PER_PAGE; i++)
 				if (is_slot_signaled(page, i)) {
-					ev = lookup_event_by_page_slot(p, page, i);
+					ev = lookup_event_by_page_slot(p,
+								page, i);
 					set_event_from_interrupt(p, ev);
 				}
-			}
-		}
 	}
 
 	mutex_unlock(&p->event_mutex);
-	kfd_unref_process(p);
+	mutex_unlock(&p->mutex);
 }
 
 static struct kfd_event_waiter *alloc_event_waiters(uint32_t num_events)
@@ -698,20 +604,20 @@ static struct kfd_event_waiter *alloc_event_waiters(uint32_t num_events)
 	struct kfd_event_waiter *event_waiters;
 	uint32_t i;
 
-	event_waiters = kmalloc(num_events * sizeof(struct kfd_event_waiter), GFP_KERNEL);
+	event_waiters = kmalloc_array(num_events,
+					sizeof(struct kfd_event_waiter),
+					GFP_KERNEL);
 
-	if (event_waiters) {
-		for (i = 0; i < num_events; i++) {
-			INIT_LIST_HEAD(&event_waiters[i].waiters);
-			event_waiters[i].sleeping_task = current;
-			event_waiters[i].activated = false;
-		}
+	for (i = 0; (event_waiters) && (i < num_events) ; i++) {
+		INIT_LIST_HEAD(&event_waiters[i].waiters);
+		event_waiters[i].sleeping_task = current;
+		event_waiters[i].activated = false;
 	}
 
 	return event_waiters;
 }
 
-static int init_event_waiter_get_status(struct kfd_process *p,
+static int init_event_waiter(struct kfd_process *p,
 		struct kfd_event_waiter *waiter,
 		uint32_t event_id,
 		uint32_t input_index)
@@ -726,21 +632,13 @@ static int init_event_waiter_get_status(struct kfd_process *p,
 	waiter->activated = ev->signaled;
 	ev->signaled = ev->signaled && !ev->auto_reset;
 
-	return 0;
-}
-
-static void init_event_waiter_add_to_waitlist(struct kfd_event_waiter *waiter)
-{
-	struct kfd_event *ev = waiter->event;
+	list_add(&waiter->waiters, &ev->waiters);
 
-	/* Only add to the wait list if we actually need to
-	 * wait on this event. */
-	if (!waiter->activated)
-		list_add(&waiter->waiters, &ev->waiters);
+	return 0;
 }
 
 static bool test_event_condition(bool all, uint32_t num_events,
-		struct kfd_event_waiter *event_waiters)
+				struct kfd_event_waiter *event_waiters)
 {
 	uint32_t i;
 	uint32_t activated_count = 0;
@@ -765,15 +663,23 @@ static bool copy_signaled_event_data(uint32_t num_events,
 		struct kfd_event_waiter *event_waiters,
 		struct kfd_event_data __user *data)
 {
+	struct kfd_hsa_memory_exception_data *src;
+	struct kfd_hsa_memory_exception_data __user *dst;
+	struct kfd_event_waiter *waiter;
+	struct kfd_event *event;
 	uint32_t i;
 
-	for (i = 0; i < num_events; i++)
-		if (event_waiters[i].activated &&
-			event_waiters[i].event->type == KFD_EVENT_TYPE_MEMORY)
-			if (copy_to_user(&data[event_waiters[i].input_index].memory_exception_data,
-					&event_waiters[i].event->memory_exception_data,
-					sizeof(struct kfd_hsa_memory_exception_data)))
+	for (i = 0; i < num_events; i++) {
+		waiter = &event_waiters[i];
+		event = waiter->event;
+		if (waiter->activated && event->type == KFD_EVENT_TYPE_MEMORY) {
+			dst = &data[waiter->input_index].memory_exception_data;
+			src = &event->memory_exception_data;
+			if (copy_to_user(dst, src,
+				sizeof(struct kfd_hsa_memory_exception_data)))
 				return false;
+		}
+	}
 
 	return true;
 
@@ -789,9 +695,11 @@ static long user_timeout_to_jiffies(uint32_t user_timeout_ms)
 	if (user_timeout_ms == KFD_EVENT_TIMEOUT_INFINITE)
 		return MAX_SCHEDULE_TIMEOUT;
 
-	/* msecs_to_jiffies interprets all values above 2^31-1 as infinite,
+	/*
+	 * msecs_to_jiffies interprets all values above 2^31-1 as infinite,
 	 * but we consider them finite.
-	 * This hack is wrong, but nobody is likely to notice. */
+	 * This hack is wrong, but nobody is likely to notice.
+	 */
 	user_timeout_ms = min_t(uint32_t, user_timeout_ms, 0x7FFFFFFF);
 
 	return msecs_to_jiffies(user_timeout_ms) + 1;
@@ -816,16 +724,11 @@ int kfd_wait_on_events(struct kfd_process *p,
 			(struct kfd_event_data __user *) data;
 	uint32_t i;
 	int ret = 0;
-
 	struct kfd_event_waiter *event_waiters = NULL;
 	long timeout = user_timeout_to_jiffies(user_timeout_ms);
 
 	mutex_lock(&p->event_mutex);
 
-	/* Set to something unreasonable - this is really
-	 * just a bool for now. */
-	*wait_result = KFD_WAIT_TIMEOUT;
-
 	event_waiters = alloc_event_waiters(num_events);
 	if (!event_waiters) {
 		ret = -ENOMEM;
@@ -839,34 +742,14 @@ int kfd_wait_on_events(struct kfd_process *p,
 				sizeof(struct kfd_event_data)))
 			goto fail;
 
-		ret = init_event_waiter_get_status(p, &event_waiters[i],
+		ret = init_event_waiter(p, &event_waiters[i],
 				event_data.event_id, i);
 		if (ret)
 			goto fail;
 	}
 
-	/* Check condition once. */
-	if (test_event_condition(all, num_events, event_waiters)) {
-		if (copy_signaled_event_data(num_events,
-				event_waiters, events))
-			*wait_result = KFD_WAIT_COMPLETE;
-		else
-			*wait_result = KFD_WAIT_ERROR;
-		free_waiters(num_events, event_waiters);
-	} else {
-		/* Add to wait lists if we need to wait. */
-		for (i = 0; i < num_events; i++)
-			init_event_waiter_add_to_waitlist(&event_waiters[i]);
-	}
-
 	mutex_unlock(&p->event_mutex);
 
-	/* Return if all waits were already satisfied. */
-	if (*wait_result != KFD_WAIT_TIMEOUT) {
-		__set_current_state(TASK_RUNNING);
-		return ret;
-	}
-
 	while (true) {
 		if (fatal_signal_pending(current)) {
 			ret = -EINTR;
@@ -875,17 +758,17 @@ int kfd_wait_on_events(struct kfd_process *p,
 
 		if (signal_pending(current)) {
 			/*
-			 * This is wrong when a nonzero, non-infinite timeout is specified.
-			 * We need to use ERESTARTSYS_RESTARTBLOCK, but struct restart_block
-			 * contains a union with data for each user and it's in generic
-			 * kernel code that I don't want to touch yet.
+			 * This is wrong when a nonzero, non-infinite timeout
+			 * is specified. We need to use
+			 * ERESTARTSYS_RESTARTBLOCK, but struct restart_block
+			 * contains a union with data for each user and it's
+			 * in generic kernel code that I don't want to
+			 * touch yet.
 			 */
 			ret = -ERESTARTSYS;
 			break;
 		}
 
-		set_current_state(TASK_INTERRUPTIBLE);
-
 		if (test_event_condition(all, num_events, event_waiters)) {
 			if (copy_signaled_event_data(num_events,
 					event_waiters, events))
@@ -900,7 +783,7 @@ int kfd_wait_on_events(struct kfd_process *p,
 			break;
 		}
 
-		timeout = schedule_timeout(timeout);
+		timeout = schedule_timeout_interruptible(timeout);
 	}
 	__set_current_state(TASK_RUNNING);
 
@@ -940,7 +823,8 @@ int kfd_event_mmap(struct kfd_process *p, struct vm_area_struct *vma)
 	page = lookup_signal_page_by_index(p, page_index);
 	if (!page) {
 		/* Probably KFD bug, but mmap is user-accessible. */
-		pr_debug("signal page could not be found for page_index %u\n", page_index);
+		pr_debug("signal page could not be found for page_index %u\n",
+				page_index);
 		return -EINVAL;
 	}
 
@@ -972,29 +856,23 @@ int kfd_event_mmap(struct kfd_process *p, struct vm_area_struct *vma)
 static void lookup_events_by_type_and_signal(struct kfd_process *p,
 		int type, void *event_data)
 {
+	struct kfd_hsa_memory_exception_data *ev_data;
 	struct kfd_event *ev;
 	int bkt;
 	bool send_signal = true;
 
-	hash_for_each(p->events, bkt, ev, events) {
+	ev_data = (struct kfd_hsa_memory_exception_data *) event_data;
+
+	hash_for_each(p->events, bkt, ev, events)
 		if (ev->type == type) {
 			send_signal = false;
 			dev_dbg(kfd_device,
 					"Event found: id %X type %d",
 					ev->event_id, ev->type);
 			set_event(ev);
-			if (ev->type == KFD_EVENT_TYPE_MEMORY && event_data)
-				ev->memory_exception_data =
-						*(struct kfd_hsa_memory_exception_data *)event_data;
+			if (ev->type == KFD_EVENT_TYPE_MEMORY && ev_data)
+				ev->memory_exception_data = *ev_data;
 		}
-	}
-
-	if (type == KFD_EVENT_TYPE_MEMORY) {
-		dev_warn(kfd_device,
-			"Sending SIGSEGV to HSA Process with PID %d ",
-				p->lead_thread->pid);
-		send_sig(SIGSEGV, p->lead_thread, 0);
-	}
 
 	/* Send SIGTERM no event of type "type" has been found*/
 	if (send_signal) {
@@ -1021,54 +899,43 @@ void kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,
 	/*
 	 * Because we are called from arbitrary context (workqueue) as opposed
 	 * to process context, kfd_process could attempt to exit while we are
-	 * running so the lookup function increments the process ref count.
+	 * running so the lookup function returns a locked process.
 	 */
 	struct kfd_process *p = kfd_lookup_process_by_pasid(pasid);
-	struct mm_struct *mm;
 
 	if (!p)
 		return; /* Presumably process exited. */
 
-	/* Take a safe reference to the mm_struct, which may otherwise
-	 * disappear even while the kfd_process is still referenced.
-	 */
-	mm = get_task_mm(p->lead_thread);
-	if (!mm) {
-		kfd_unref_process(p);
-		return; /* Process is exiting */
-	}
-
 	memset(&memory_exception_data, 0, sizeof(memory_exception_data));
 
-	down_read(&mm->mmap_sem);
-	vma = find_vma(mm, address);
+	down_read(&p->mm->mmap_sem);
+	vma = find_vma(p->mm, address);
 
 	memory_exception_data.gpu_id = dev->id;
 	memory_exception_data.va = address;
 	/* Set failure reason */
-	memory_exception_data.failure.NotPresent = true;
-	memory_exception_data.failure.NoExecute = false;
-	memory_exception_data.failure.ReadOnly = false;
+	memory_exception_data.failure.NotPresent = 1;
+	memory_exception_data.failure.NoExecute = 0;
+	memory_exception_data.failure.ReadOnly = 0;
 	if (vma) {
 		if (vma->vm_start > address) {
-			memory_exception_data.failure.NotPresent = true;
-			memory_exception_data.failure.NoExecute = false;
-			memory_exception_data.failure.ReadOnly = false;
+			memory_exception_data.failure.NotPresent = 1;
+			memory_exception_data.failure.NoExecute = 0;
+			memory_exception_data.failure.ReadOnly = 0;
 		} else {
-			memory_exception_data.failure.NotPresent = false;
+			memory_exception_data.failure.NotPresent = 0;
 			if (is_write_requested && !(vma->vm_flags & VM_WRITE))
-				memory_exception_data.failure.ReadOnly = true;
+				memory_exception_data.failure.ReadOnly = 1;
 			else
-				memory_exception_data.failure.ReadOnly = false;
+				memory_exception_data.failure.ReadOnly = 0;
 			if (is_execute_requested && !(vma->vm_flags & VM_EXEC))
-				memory_exception_data.failure.NoExecute = true;
+				memory_exception_data.failure.NoExecute = 1;
 			else
-				memory_exception_data.failure.NoExecute = false;
+				memory_exception_data.failure.NoExecute = 0;
 		}
 	}
 
-	up_read(&mm->mmap_sem);
-	mmput(mm);
+	up_read(&p->mm->mmap_sem);
 
 	mutex_lock(&p->event_mutex);
 
@@ -1077,8 +944,7 @@ void kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,
 			&memory_exception_data);
 
 	mutex_unlock(&p->event_mutex);
-
-	kfd_unref_process(p);
+	mutex_unlock(&p->mutex);
 }
 
 void kfd_signal_hw_exception_event(unsigned int pasid)
@@ -1086,7 +952,7 @@ void kfd_signal_hw_exception_event(unsigned int pasid)
 	/*
 	 * Because we are called from arbitrary context (workqueue) as opposed
 	 * to process context, kfd_process could attempt to exit while we are
-	 * running so the lookup function increments the process ref count.
+	 * running so the lookup function returns a locked process.
 	 */
 	struct kfd_process *p = kfd_lookup_process_by_pasid(pasid);
 
@@ -1099,43 +965,5 @@ void kfd_signal_hw_exception_event(unsigned int pasid)
 	lookup_events_by_type_and_signal(p, KFD_EVENT_TYPE_HW_EXCEPTION, NULL);
 
 	mutex_unlock(&p->event_mutex);
-	kfd_unref_process(p);
-}
-
-void kfd_signal_vm_fault_event(struct kfd_dev *dev, unsigned int pasid,
-				struct kfd_vm_fault_info *info)
-{
-	struct kfd_event *ev;
-	int bkt;
-	struct kfd_process *p = kfd_lookup_process_by_pasid(pasid);
-	struct kfd_hsa_memory_exception_data memory_exception_data;
-
-	if (!p)
-		return; /* Presumably process exited. */
-	memset(&memory_exception_data, 0, sizeof(memory_exception_data));
-	memory_exception_data.gpu_id = dev->id;
-	memory_exception_data.failure.imprecise = true;
-	/* Set failure reason */
-	if (info) {
-		memory_exception_data.va = (info->page_addr) << PAGE_SHIFT;
-		memory_exception_data.failure.NotPresent =
-			info->prot_valid ? true : false;
-		memory_exception_data.failure.NoExecute =
-			info->prot_exec ? true : false;
-		memory_exception_data.failure.ReadOnly =
-			info->prot_write ? true : false;
-		memory_exception_data.failure.imprecise = false;
-	}
-	mutex_lock(&p->event_mutex);
-
-	hash_for_each(p->events, bkt, ev, events) {
-		if (ev->type == KFD_EVENT_TYPE_MEMORY) {
-			ev->memory_exception_data = memory_exception_data;
-			set_event(ev);
-		}
-	}
-
-	mutex_unlock(&p->event_mutex);
-	kfd_unref_process(p);
+	mutex_unlock(&p->mutex);
 }
-
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_events.h b/drivers/gpu/drm/amd/amdkfd/kfd_events.h
index d7987eb..28f6838 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_events.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_events.h
@@ -34,7 +34,8 @@
 #define KFD_FIRST_NONSIGNAL_EVENT_ID KFD_EVENT_ID_NONSIGNAL_MASK
 #define KFD_LAST_NONSIGNAL_EVENT_ID UINT_MAX
 
-/* Written into kfd_signal_slot_t to indicate that the event is not signaled.
+/*
+ * Written into kfd_signal_slot_t to indicate that the event is not signaled.
  * Since the event protocol may need to write the event ID into memory, this
  * must not be a valid event ID.
  * For the sake of easy memset-ing, this must be a byte pattern.
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c b/drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c
index e14bf8e..2b65510 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c
@@ -33,7 +33,7 @@
 #include <linux/time.h>
 #include "kfd_priv.h"
 #include <linux/mm.h>
-#include <uapi/asm-generic/mman-common.h>
+#include <linux/mman.h>
 #include <asm/processor.h>
 
 /*
@@ -278,37 +278,21 @@
 #define MAKE_GPUVM_APP_BASE(gpu_num) \
 	(((uint64_t)(gpu_num) << 61) + 0x1000000000000L)
 
-#define MAKE_GPUVM_APP_LIMIT(base, size) \
-	(((uint64_t)(base) & 0xFFFFFF0000000000UL) + (size) - 1)
+#define MAKE_GPUVM_APP_LIMIT(base) \
+	(((uint64_t)(base) & \
+		0xFFFFFF0000000000UL) | 0xFFFFFFFFFFL)
 
-#define MAKE_SCRATCH_APP_BASE() \
-	(((uint64_t)(0x1UL) << 61) + 0x100000000L)
+#define MAKE_SCRATCH_APP_BASE(gpu_num) \
+	(((uint64_t)(gpu_num) << 61) + 0x100000000L)
 
 #define MAKE_SCRATCH_APP_LIMIT(base) \
 	(((uint64_t)base & 0xFFFFFFFF00000000UL) | 0xFFFFFFFF)
 
-#define MAKE_LDS_APP_BASE() \
-	(((uint64_t)(0x1UL) << 61) + 0x0)
-
+#define MAKE_LDS_APP_BASE(gpu_num) \
+	(((uint64_t)(gpu_num) << 61) + 0x0)
 #define MAKE_LDS_APP_LIMIT(base) \
 	(((uint64_t)(base) & 0xFFFFFFFF00000000UL) | 0xFFFFFFFF)
 
-
-#define DGPU_VM_BASE_DEFAULT 0x100000
-#define DGPU_IB_BASE_DEFAULT (DGPU_VM_BASE_DEFAULT - PAGE_SIZE)
-
-int kfd_set_process_dgpu_aperture(struct kfd_process_device *pdd,
-					uint64_t base, uint64_t limit)
-{
-	if (base < (pdd->qpd.cwsr_base + pdd->dev->cwsr_size)) {
-		pr_err("Set dgpu vm base 0x%llx failed.\n", base);
-		return -EINVAL;
-	}
-	pdd->dgpu_base = base;
-	pdd->dgpu_limit = limit;
-	return 0;
-}
-
 int kfd_init_apertures(struct kfd_process *process)
 {
 	uint8_t id  = 0;
@@ -316,16 +300,13 @@ int kfd_init_apertures(struct kfd_process *process)
 	struct kfd_process_device *pdd;
 
 	/*Iterating over all devices*/
-	while (kfd_topology_enum_kfd_devices(id, &dev) == 0) {
-		if (!dev) {
-			id++; /* Skip non GPU devices */
-			continue;
-		}
+	while ((dev = kfd_topology_enum_kfd_devices(id)) != NULL &&
+		id < NUM_OF_SUPPORTED_GPUS) {
 
 		pdd = kfd_create_process_device_data(dev, process);
 		if (pdd == NULL) {
 			pr_err("Failed to create process device data\n");
-			goto err;
+			return -1;
 		}
 		/*
 		 * For 64 bit process aperture will be statically reserved in
@@ -341,25 +322,19 @@ int kfd_init_apertures(struct kfd_process *process)
 			 * node id couldn't be 0 - the three MSB bits of
 			 * aperture shoudn't be 0
 			 */
-			pdd->lds_base = MAKE_LDS_APP_BASE();
+			pdd->lds_base = MAKE_LDS_APP_BASE(id + 1);
 
 			pdd->lds_limit = MAKE_LDS_APP_LIMIT(pdd->lds_base);
 
 			pdd->gpuvm_base = MAKE_GPUVM_APP_BASE(id + 1);
 
-			pdd->gpuvm_limit = MAKE_GPUVM_APP_LIMIT(
-				pdd->gpuvm_base,
-				dev->shared_resources.gpuvm_size);
+			pdd->gpuvm_limit =
+					MAKE_GPUVM_APP_LIMIT(pdd->gpuvm_base);
 
-			pdd->scratch_base = MAKE_SCRATCH_APP_BASE();
+			pdd->scratch_base = MAKE_SCRATCH_APP_BASE(id + 1);
 
 			pdd->scratch_limit =
 				MAKE_SCRATCH_APP_LIMIT(pdd->scratch_base);
-
-			if (KFD_IS_DGPU(dev->device_info->asic_family)) {
-				pdd->qpd.cwsr_base = DGPU_VM_BASE_DEFAULT;
-				pdd->qpd.ib_base = DGPU_IB_BASE_DEFAULT;
-			}
 		}
 
 		dev_dbg(kfd_device, "node id %u\n", id);
@@ -375,32 +350,6 @@ int kfd_init_apertures(struct kfd_process *process)
 	}
 
 	return 0;
-
-err:
-	return -1;
 }
 
-void kfd_flush_tlb(struct kfd_dev *dev, uint32_t pasid)
-{
-	uint8_t vmid;
-	int first_vmid_to_scan = 8;
-	int last_vmid_to_scan = 15;
 
-	const struct kfd2kgd_calls *f2g = dev->kfd2kgd;
-	/* Scan all registers in the range ATC_VMID8_PASID_MAPPING .. ATC_VMID15_PASID_MAPPING
-	 * to check which VMID the current process is mapped to
-	 * and flush TLB for this VMID if found*/
-	for (vmid = first_vmid_to_scan; vmid <= last_vmid_to_scan; vmid++) {
-		if (f2g->get_atc_vmid_pasid_mapping_valid(
-			dev->kgd, vmid)) {
-			if (f2g->get_atc_vmid_pasid_mapping_pasid(
-				dev->kgd, vmid) == pasid) {
-				dev_dbg(kfd_device,
-					"flushing TLB of vmid %u", vmid);
-				f2g->write_vmid_invalidate_request(
-					dev->kgd, vmid);
-				break;
-			}
-		}
-	}
-}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_interrupt.c b/drivers/gpu/drm/amd/amdkfd/kfd_interrupt.c
index d737df0..7f134aa 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_interrupt.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_interrupt.c
@@ -42,26 +42,26 @@
 
 #include <linux/slab.h>
 #include <linux/device.h>
-#include <linux/kfifo.h>
 #include "kfd_priv.h"
 
-#define KFD_IH_NUM_ENTRIES 8192
+#define KFD_INTERRUPT_RING_SIZE 1024
 
 static void interrupt_wq(struct work_struct *);
 
 int kfd_interrupt_init(struct kfd_dev *kfd)
 {
-	int r;
-
-	r = kfifo_alloc(&kfd->ih_fifo,
-			KFD_IH_NUM_ENTRIES * kfd->device_info->ih_ring_entry_size,
-			GFP_KERNEL);
-	if (r) {
-		dev_err(kfd_chardev(), "Failed to allocate IH fifo\n");
-		return r;
-	}
+	void *interrupt_ring = kmalloc_array(KFD_INTERRUPT_RING_SIZE,
+					kfd->device_info->ih_ring_entry_size,
+					GFP_KERNEL);
+	if (!interrupt_ring)
+		return -ENOMEM;
+
+	kfd->interrupt_ring = interrupt_ring;
+	kfd->interrupt_ring_size =
+		KFD_INTERRUPT_RING_SIZE * kfd->device_info->ih_ring_entry_size;
+	atomic_set(&kfd->interrupt_ring_wptr, 0);
+	atomic_set(&kfd->interrupt_ring_rptr, 0);
 
-	kfd->ih_wq = alloc_workqueue("KFD IH", WQ_HIGHPRI, 1);
 	spin_lock_init(&kfd->interrupt_lock);
 
 	INIT_WORK(&kfd->interrupt_work, interrupt_wq);
@@ -92,47 +92,74 @@ void kfd_interrupt_exit(struct kfd_dev *kfd)
 	spin_unlock_irqrestore(&kfd->interrupt_lock, flags);
 
 	/*
-	 * flush_work ensures that there are no outstanding
+	 * Flush_scheduled_work ensures that there are no outstanding
 	 * work-queue items that will access interrupt_ring. New work items
 	 * can't be created because we stopped interrupt handling above.
 	 */
-	flush_workqueue(kfd->ih_wq);
+	flush_scheduled_work();
 
-	kfifo_free(&kfd->ih_fifo);
+	kfree(kfd->interrupt_ring);
 }
 
 /*
- * Assumption: single reader/writer. This function is not re-entrant
+ * This assumes that it can't be called concurrently with itself
+ * but only with dequeue_ih_ring_entry.
  */
 bool enqueue_ih_ring_entry(struct kfd_dev *kfd,	const void *ih_ring_entry)
 {
-	int count;
+	unsigned int rptr = atomic_read(&kfd->interrupt_ring_rptr);
+	unsigned int wptr = atomic_read(&kfd->interrupt_ring_wptr);
 
-	count = kfifo_in(&kfd->ih_fifo, ih_ring_entry,
-				kfd->device_info->ih_ring_entry_size);
-	if (count != kfd->device_info->ih_ring_entry_size) {
+	if ((rptr - wptr) % kfd->interrupt_ring_size ==
+					kfd->device_info->ih_ring_entry_size) {
+		/* This is very bad, the system is likely to hang. */
 		dev_err_ratelimited(kfd_chardev(),
-			"Interrupt ring overflow, dropping interrupt %d\n",
-			count);
+			"Interrupt ring overflow, dropping interrupt.\n");
 		return false;
 	}
 
+	memcpy(kfd->interrupt_ring + wptr, ih_ring_entry,
+			kfd->device_info->ih_ring_entry_size);
+
+	wptr = (wptr + kfd->device_info->ih_ring_entry_size) %
+			kfd->interrupt_ring_size;
+	smp_wmb(); /* Ensure memcpy'd data is visible before wptr update. */
+	atomic_set(&kfd->interrupt_ring_wptr, wptr);
+
 	return true;
 }
 
 /*
- * Assumption: single reader/writer. This function is not re-entrant
+ * This assumes that it can't be called concurrently with itself
+ * but only with enqueue_ih_ring_entry.
  */
 static bool dequeue_ih_ring_entry(struct kfd_dev *kfd, void *ih_ring_entry)
 {
-	int count;
+	/*
+	 * Assume that wait queues have an implicit barrier, i.e. anything that
+	 * happened in the ISR before it queued work is visible.
+	 */
+
+	unsigned int wptr = atomic_read(&kfd->interrupt_ring_wptr);
+	unsigned int rptr = atomic_read(&kfd->interrupt_ring_rptr);
 
-	count = kfifo_out(&kfd->ih_fifo, ih_ring_entry,
-				kfd->device_info->ih_ring_entry_size);
+	if (rptr == wptr)
+		return false;
+
+	memcpy(ih_ring_entry, kfd->interrupt_ring + rptr,
+			kfd->device_info->ih_ring_entry_size);
+
+	rptr = (rptr + kfd->device_info->ih_ring_entry_size) %
+			kfd->interrupt_ring_size;
 
-	WARN_ON(count && count != kfd->device_info->ih_ring_entry_size);
+	/*
+	 * Ensure the rptr write update is not visible until
+	 * memcpy has finished reading.
+	 */
+	smp_mb();
+	atomic_set(&kfd->interrupt_ring_rptr, rptr);
 
-	return count == kfd->device_info->ih_ring_entry_size;
+	return true;
 }
 
 static void interrupt_wq(struct work_struct *work)
@@ -145,18 +172,17 @@ static void interrupt_wq(struct work_struct *work)
 				sizeof(uint32_t))];
 
 	while (dequeue_ih_ring_entry(dev, ih_ring_entry))
-		dev->device_info->event_interrupt_class->interrupt_wq(dev, ih_ring_entry);
+		dev->device_info->event_interrupt_class->interrupt_wq(dev,
+								ih_ring_entry);
 }
 
-bool interrupt_is_wanted(struct kfd_dev *dev,
-			const uint32_t *ih_ring_entry,
-			uint32_t *patched_ihre, bool *flag)
+bool interrupt_is_wanted(struct kfd_dev *dev, const uint32_t *ih_ring_entry)
 {
 	/* integer and bitwise OR so there is no boolean short-circuiting */
 	unsigned wanted = 0;
 
 	wanted |= dev->device_info->event_interrupt_class->interrupt_isr(dev,
-					 ih_ring_entry, patched_ihre, flag);
+								ih_ring_entry);
 
 	return wanted != 0;
 }
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c b/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c
deleted file mode 100644
index 5391f8a..0000000
--- a/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c
+++ /dev/null
@@ -1,337 +0,0 @@
-/*
- * Copyright 2014 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- */
-
-#include <linux/dma-buf.h>
-#include <linux/assoc_array.h>
-#include <linux/slab.h>
-#include <linux/random.h>
-
-#include "kfd_ipc.h"
-#include "kfd_priv.h"
-
-/**
- * The assoc_array data structure provides a non-intrusive
- * key-value data store mechanism where the key can be
- * arbitratily long (and value is just a void*).
- *
- * This provides us good control for managing IPC handle length
- * in case we require extra entropy to discourage handle-guessing.
- */
-static struct assoc_array ipc_handles;
-
-static unsigned long ipc_get_key_chunk(const void *index_key, int level)
-{
-	unsigned long chunk;
-	int index = level/ASSOC_ARRAY_KEY_CHUNK_SIZE;
-
-	/* no more data, but we can't fail */
-	if (level > IPC_KEY_SIZE_BYTES*8)
-		return 0;
-
-	chunk = ((unsigned long *)index_key)[index];
-	return chunk;
-}
-
-static unsigned long ipc_get_object_key_chunk(const void *object, int level)
-{
-	const struct kfd_ipc_obj *obj = (const struct kfd_ipc_obj *)object;
-
-	return ipc_get_key_chunk(&obj->key, level);
-}
-
-static bool ipc_compare_object(const void *object, const void *index_key)
-{
-	const struct kfd_ipc_obj *obj = (const struct kfd_ipc_obj *)object;
-
-	return memcmp(obj->key, index_key, IPC_KEY_SIZE_BYTES) == 0;
-}
-
-/*
- * Compare the index keys of a pair of objects and determine the bit position
- * at which they differ - if they differ.
- */
-static int ipc_diff_objects(const void *object, const void *index_key)
-{
-	const struct kfd_ipc_obj *obj = (const struct kfd_ipc_obj *)object;
-	int i;
-
-	/* naive linear byte search */
-	for (i = 0; i < IPC_KEY_SIZE_BYTES; ++i) {
-		if (obj->key[i] != ((char *)index_key)[i])
-			return i * 8;
-	}
-
-	return -1;
-}
-
-/*
- * Free an object after stripping the ipc flag off of the pointer.
- */
-static void ipc_free_object(void *object)
-{
-}
-
-/*
- * Operations for ipc management by the index-tree routines.
- */
-static const struct assoc_array_ops ipc_assoc_array_ops = {
-	.get_key_chunk		= ipc_get_key_chunk,
-	.get_object_key_chunk	= ipc_get_object_key_chunk,
-	.compare_object		= ipc_compare_object,
-	.diff_objects		= ipc_diff_objects,
-	.free_object		= ipc_free_object,
-};
-
-static void ipc_gen_key(void *buf)
-{
-	uint32_t *key = (uint32_t *) buf;
-
-	get_random_bytes(buf, IPC_KEY_SIZE_BYTES);
-
-	/* last byte of the key is reserved */
-	key[3] = (key[3] & ~0xFF) | 0x1;
-}
-
-static int ipc_store_insert(void *val, void *key, struct kfd_ipc_obj **ipc_obj)
-{
-	struct kfd_ipc_obj *obj;
-	struct assoc_array_edit *edit;
-
-	obj = kmalloc(sizeof(struct kfd_ipc_obj), GFP_KERNEL);
-	if (!obj)
-		return -ENOMEM;
-
-	/* The initial ref belongs to the allocator process.
-	 * The IPC object store itself does not hold a ref since
-	 * there is no specific moment in time where that ref should
-	 * be dropped, except "when there are no more userspace processes
-	 * holding a ref to the object". Therefore the removal from IPC
-	 * storage happens at ipc_obj release time.
-	 */
-	kref_init(&obj->ref);
-	obj->data = val;
-	ipc_gen_key(obj->key);
-
-	memcpy(key, obj->key, IPC_KEY_SIZE_BYTES);
-
-	pr_debug("ipc: val::%p ref:%p\n", val, &obj->ref);
-
-	edit = assoc_array_insert(&ipc_handles,
-				  &ipc_assoc_array_ops,
-				  obj->key, obj);
-	assoc_array_apply_edit(edit);
-
-	if (ipc_obj)
-		*ipc_obj = obj;
-
-	return 0;
-}
-
-static int ipc_store_remove(void *key)
-{
-	struct assoc_array_edit *edit;
-
-	edit = assoc_array_delete(&ipc_handles, &ipc_assoc_array_ops, key);
-	assoc_array_apply_edit(edit);
-	return 0;
-}
-
-static void ipc_obj_release(struct kref *r)
-{
-	struct kfd_ipc_obj *obj;
-
-	obj = container_of(r, struct kfd_ipc_obj, ref);
-
-	ipc_store_remove(obj->key);
-	dma_buf_put(obj->data);
-	kfree(obj);
-}
-
-void ipc_obj_get(struct kfd_ipc_obj *obj)
-{
-	kref_get(&obj->ref);
-}
-
-void ipc_obj_put(struct kfd_ipc_obj **obj)
-{
-	kref_put(&(*obj)->ref, ipc_obj_release);
-	*obj = NULL;
-}
-
-int kfd_ipc_init(void)
-{
-	assoc_array_init(&ipc_handles);
-	return 0;
-}
-
-static int kfd_import_dmabuf_create_kfd_bo(struct kfd_dev *dev,
-			  struct kfd_process *p,
-			  uint32_t gpu_id, struct dma_buf *dmabuf,
-			  uint64_t va_addr, uint64_t *handle,
-			  uint64_t *mmap_offset,
-			  struct kfd_ipc_obj *ipc_obj)
-{
-	int r;
-	void *mem;
-	uint64_t size;
-	int idr_handle;
-	struct kfd_process_device *pdd = NULL;
-	uint64_t kfd_mmap_flags = KFD_MMAP_TYPE_MAP_BO |
-				  KFD_MMAP_GPU_ID(gpu_id);
-
-	if (!handle)
-		return -EINVAL;
-
-	if (!dev || !dev->kfd2kgd->import_dmabuf)
-		return -EINVAL;
-
-	down_write(&p->lock);
-	pdd = kfd_bind_process_to_device(dev, p);
-	up_write(&p->lock);
-	if (IS_ERR(pdd) < 0)
-		return PTR_ERR(pdd);
-
-	r = dev->kfd2kgd->import_dmabuf(dev->kgd, dmabuf,
-					va_addr, pdd->vm,
-					(struct kgd_mem **)&mem, &size,
-					mmap_offset);
-	if (r)
-		return r;
-
-	down_write(&p->lock);
-	idr_handle = kfd_process_device_create_obj_handle(pdd, mem,
-							  va_addr, size,
-							  ipc_obj);
-	up_write(&p->lock);
-	if (idr_handle < 0) {
-		dev->kfd2kgd->free_memory_of_gpu(dev->kgd,
-						 (struct kgd_mem *)mem,
-						 pdd->vm);
-		return -EFAULT;
-	}
-
-	*handle = MAKE_HANDLE(gpu_id, idr_handle);
-
-	if (mmap_offset)
-		*mmap_offset = (kfd_mmap_flags << PAGE_SHIFT) | *mmap_offset;
-
-	return r;
-}
-
-int kfd_ipc_import_dmabuf(struct kfd_dev *dev,
-					   struct kfd_process *p,
-					   uint32_t gpu_id, int dmabuf_fd,
-					   uint64_t va_addr, uint64_t *handle,
-					   uint64_t *mmap_offset)
-{
-	int r;
-	struct dma_buf *dmabuf = dma_buf_get(dmabuf_fd);
-
-	if (dmabuf == NULL)
-		return -EINVAL;
-
-	r = kfd_import_dmabuf_create_kfd_bo(dev, p, gpu_id, dmabuf,
-					    va_addr, handle, mmap_offset,
-					    NULL);
-	dma_buf_put(dmabuf);
-	return r;
-}
-
-int kfd_ipc_import_handle(struct kfd_dev *dev, struct kfd_process *p,
-			  uint32_t gpu_id, uint32_t *share_handle,
-			  uint64_t va_addr, uint64_t *handle,
-			  uint64_t *mmap_offset)
-{
-	int r;
-	struct kfd_ipc_obj *found;
-
-	found = assoc_array_find(&ipc_handles,
-				 &ipc_assoc_array_ops,
-				 share_handle);
-	if (!found)
-		return -EINVAL;
-	ipc_obj_get(found);
-
-	pr_debug("ipc: found ipc_dma_buf: %p\n", found->data);
-
-	r = kfd_import_dmabuf_create_kfd_bo(dev, p, gpu_id, found->data,
-					    va_addr, handle, mmap_offset,
-					    found);
-	if (r)
-		goto error_unref;
-
-	return r;
-
-error_unref:
-	ipc_obj_put(&found);
-	return r;
-}
-
-int kfd_ipc_export_as_handle(struct kfd_dev *dev, struct kfd_process *p,
-			     uint64_t handle, uint32_t *ipc_handle)
-{
-	struct kfd_process_device *pdd = NULL;
-	struct kfd_ipc_obj *obj;
-	struct kfd_bo *kfd_bo = NULL;
-	struct dma_buf *dmabuf;
-	int r;
-
-	if (!dev || !ipc_handle)
-		return -EINVAL;
-
-	down_write(&p->lock);
-	pdd = kfd_bind_process_to_device(dev, p);
-	up_write(&p->lock);
-
-	if (IS_ERR(pdd) < 0) {
-		pr_err("failed to get pdd\n");
-		return PTR_ERR(pdd);
-	}
-
-	down_write(&p->lock);
-	kfd_bo = kfd_process_device_find_bo(pdd, GET_IDR_HANDLE(handle));
-	up_write(&p->lock);
-
-	if (!kfd_bo) {
-		pr_err("failed to get bo");
-		return -EINVAL;
-	}
-	if (kfd_bo->kfd_ipc_obj) {
-		memcpy(ipc_handle, kfd_bo->kfd_ipc_obj->key,
-		       sizeof(kfd_bo->kfd_ipc_obj->key));
-		return 0;
-	}
-
-	r = dev->kfd2kgd->export_dmabuf(dev->kgd, pdd->vm,
-					(struct kgd_mem *)kfd_bo->mem,
-					&dmabuf);
-	if (r)
-		goto err;
-
-	r = ipc_store_insert(dmabuf, ipc_handle, &obj);
-	if (r)
-		goto err;
-
-	kfd_bo->kfd_ipc_obj = obj;
-err:
-	return r;
-}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_ipc.h b/drivers/gpu/drm/amd/amdkfd/kfd_ipc.h
deleted file mode 100644
index 52049d2..0000000
--- a/drivers/gpu/drm/amd/amdkfd/kfd_ipc.h
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Copyright 2014 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef KFD_IPC_H_
-#define KFD_IPC_H_
-
-#include <linux/types.h>
-#include "kfd_priv.h"
-
-#define IPC_KEY_SIZE_BYTES 16
-
-struct kfd_ipc_obj {
-	struct kref ref;
-	void *data;
-	char key[IPC_KEY_SIZE_BYTES];
-};
-
-int kfd_ipc_import_handle(struct kfd_dev *dev, struct kfd_process *p,
-			  uint32_t gpu_id, uint32_t *share_handle,
-			  uint64_t va_addr, uint64_t *handle,
-			  uint64_t *mmap_offset);
-int kfd_ipc_import_dmabuf(struct kfd_dev *kfd, struct kfd_process *p,
-			  uint32_t gpu_id, int dmabuf_fd,
-			  uint64_t va_addr, uint64_t *handle,
-			  uint64_t *mmap_offset);
-int kfd_ipc_export_as_handle(struct kfd_dev *dev, struct kfd_process *p,
-			     uint64_t handle, uint32_t *ipc_handle);
-
-void ipc_obj_get(struct kfd_ipc_obj *obj);
-void ipc_obj_put(struct kfd_ipc_obj **obj);
-
-#endif /* KFD_IPC_H_ */
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
index 126d848..d135cd0 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
@@ -123,7 +123,6 @@ static bool initialize(struct kernel_queue *kq, struct kfd_dev *dev,
 	prop.write_ptr = (uint32_t *) kq->wptr_gpu_addr;
 	prop.eop_ring_buffer_address = kq->eop_gpu_addr;
 	prop.eop_ring_buffer_size = PAGE_SIZE;
-	prop.cu_mask = NULL;
 
 	if (init_queue(&kq->queue, &prop) != 0)
 		goto err_init_queue;
@@ -144,8 +143,7 @@ static bool initialize(struct kernel_queue *kq, struct kfd_dev *dev,
 		kq->queue->pipe = KFD_CIK_HIQ_PIPE;
 		kq->queue->queue = KFD_CIK_HIQ_QUEUE;
 		kq->mqd->load_mqd(kq->mqd, kq->queue->mqd, kq->queue->pipe,
-				  kq->queue->queue, &kq->queue->properties,
-				  NULL);
+					kq->queue->queue, NULL);
 	} else {
 		/* allocate fence for DIQ */
 
@@ -187,7 +185,7 @@ static void uninitialize(struct kernel_queue *kq)
 	if (kq->queue->properties.type == KFD_QUEUE_TYPE_HIQ)
 		kq->mqd->destroy_mqd(kq->mqd,
 					NULL,
-					KFD_PREEMPT_TYPE_WAVEFRONT_RESET,
+					false,
 					QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS,
 					kq->queue->pipe,
 					kq->queue->queue);
@@ -215,23 +213,20 @@ static int acquire_packet_buffer(struct kernel_queue *kq,
 
 	BUG_ON(!kq || !buffer_ptr);
 
-	/* When rptr == wptr, the buffer is empty.
-	 * When rptr == wptr + 1, the buffer is full.
-	 * It is always rptr that advances to the position of wptr, rather than
-	 * the opposite. So we can only use up to queue_size_dwords - 1 dwords.
-	 */
 	rptr = *kq->rptr_kernel;
-	wptr = kq->pending_wptr;
+	wptr = *kq->wptr_kernel;
 	queue_address = (unsigned int *)kq->pq_kernel_addr;
 	queue_size_dwords = kq->queue->properties.queue_size / sizeof(uint32_t);
 
-	pr_debug("amdkfd: In func %s\n rptr: %d\n wptr: %d\n queue_address 0x%p\n",
-			__func__, rptr, wptr, queue_address);
+	pr_debug("rptr: %d\n", rptr);
+	pr_debug("wptr: %d\n", wptr);
+	pr_debug("queue_address 0x%p\n", queue_address);
 
-	available_size = (rptr + queue_size_dwords - 1 - wptr) %
+	available_size = (rptr - 1 - wptr + queue_size_dwords) %
 							queue_size_dwords;
 
-	if (packet_size_in_dwords > available_size) {
+	if (packet_size_in_dwords >= queue_size_dwords ||
+			packet_size_in_dwords >= available_size) {
 		/*
 		 * make sure calling functions know
 		 * acquire_packet_buffer() failed
@@ -241,13 +236,6 @@ static int acquire_packet_buffer(struct kernel_queue *kq,
 	}
 
 	if (wptr + packet_size_in_dwords >= queue_size_dwords) {
-		/* make sure after rolling back to position 0, there is
-		 * still enough space. */
-		if (packet_size_in_dwords >= rptr) {
-			*buffer_ptr = NULL;
-			return -ENOMEM;
-		}
-		/* fill nops, roll back and start at position 0 */
 		while (wptr > 0) {
 			queue_address[wptr] = kq->nop_packet;
 			wptr = (wptr + 1) % queue_size_dwords;
@@ -260,39 +248,6 @@ static int acquire_packet_buffer(struct kernel_queue *kq,
 	return 0;
 }
 
-static int acquire_inline_ib(struct kernel_queue *kq,
-			     size_t size_in_dwords,
-			     unsigned int **buffer_ptr,
-			     uint64_t *gpu_addr)
-{
-	int ret;
-	unsigned int *buf;
-	union PM4_MES_TYPE_3_HEADER nop;
-
-	if (size_in_dwords >= (1 << 14))
-		return -EINVAL;
-
-	/* Allocate size_in_dwords on the ring, plus an extra dword
-	 * for a NOP packet header
-	 */
-	ret = acquire_packet_buffer(kq, size_in_dwords + 1,  &buf);
-	if (ret)
-		return ret;
-
-	/* Build a NOP packet that contains the IB as "payload". */
-	nop.u32all = 0;
-	nop.opcode = IT_NOP;
-	nop.count = size_in_dwords - 1;
-	nop.type = PM4_TYPE_3;
-
-	*buf = nop.u32all;
-	*buffer_ptr = buf + 1;
-	*gpu_addr = kq->pq_gpu_addr + ((unsigned long)*buffer_ptr -
-				       (unsigned long)kq->pq_kernel_addr);
-
-	return 0;
-}
-
 static void submit_packet(struct kernel_queue *kq)
 {
 #ifdef DEBUG
@@ -335,21 +290,15 @@ struct kernel_queue *kernel_queue_init(struct kfd_dev *dev,
 	kq->ops.initialize = initialize;
 	kq->ops.uninitialize = uninitialize;
 	kq->ops.acquire_packet_buffer = acquire_packet_buffer;
-	kq->ops.acquire_inline_ib = acquire_inline_ib;
 	kq->ops.submit_packet = submit_packet;
 	kq->ops.rollback_packet = rollback_packet;
 
 	switch (dev->device_info->asic_family) {
 	case CHIP_CARRIZO:
-	case CHIP_TONGA:
-	case CHIP_FIJI:
-	case CHIP_POLARIS10:
-	case CHIP_POLARIS11:
 		kernel_queue_init_vi(&kq->ops_asic_specific);
 		break;
 
 	case CHIP_KAVERI:
-	case CHIP_HAWAII:
 		kernel_queue_init_cik(&kq->ops_asic_specific);
 		break;
 	}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h
index a217f42..5940531 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h
@@ -42,12 +42,6 @@
  * pending write pointer to that location so subsequent calls to
  * acquire_packet_buffer will get a correct write pointer
  *
- * @acquire_inline_ib: Returns a pointer to the location in the kernel
- * queue ring buffer where the calling function can write an inline IB. It is
- * Guaranteed that there is enough space for that IB. It also updates the
- * pending write pointer to that location so subsequent calls to
- * acquire_packet_buffer will get a correct write pointer
- *
  * @submit_packet: Update the write pointer and doorbell of a kernel queue.
  *
  * @sync_with_hw: Wait until the write pointer and the read pointer of a kernel
@@ -65,10 +59,6 @@ struct kernel_queue_ops {
 	int	(*acquire_packet_buffer)(struct kernel_queue *kq,
 					size_t packet_size_in_dwords,
 					unsigned int **buffer_ptr);
-	int	(*acquire_inline_ib)(struct kernel_queue *kq,
-				     size_t packet_size_in_dwords,
-				     unsigned int **buffer_ptr,
-				     uint64_t *gpu_addr);
 
 	void	(*submit_packet)(struct kernel_queue *kq);
 	void	(*rollback_packet)(struct kernel_queue *kq);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_module.c b/drivers/gpu/drm/amd/amdkfd/kfd_module.c
index 274312f..850a562 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_module.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_module.c
@@ -29,11 +29,10 @@
 #define KFD_DRIVER_AUTHOR	"AMD Inc. and others"
 
 #define KFD_DRIVER_DESC		"Standalone HSA driver for AMD's GPUs"
-#define KFD_DRIVER_DATE		"20160408"
-#define KFD_DRIVER_MAJOR	2
-#define KFD_DRIVER_MINOR	0
-#define KFD_DRIVER_PATCHLEVEL	0
-#define KFD_DRIVER_RC_LEVEL	""
+#define KFD_DRIVER_DATE		"20150421"
+#define KFD_DRIVER_MAJOR	0
+#define KFD_DRIVER_MINOR	7
+#define KFD_DRIVER_PATCHLEVEL	2
 
 static const struct kgd2kfd_calls kgd2kfd = {
 	.exit		= kgd2kfd_exit,
@@ -43,10 +42,6 @@ static const struct kgd2kfd_calls kgd2kfd = {
 	.interrupt	= kgd2kfd_interrupt,
 	.suspend	= kgd2kfd_suspend,
 	.resume		= kgd2kfd_resume,
-	.quiesce_mm	= kgd2kfd_quiesce_mm,
-	.resume_mm	= kgd2kfd_resume_mm,
-	.schedule_evict_and_restore_process =
-			  kgd2kfd_schedule_evict_and_restore_process,
 };
 
 int sched_policy = KFD_SCHED_POLICY_HWS;
@@ -54,15 +49,6 @@ module_param(sched_policy, int, 0444);
 MODULE_PARM_DESC(sched_policy,
 	"Scheduling policy (0 = HWS (Default), 1 = HWS without over-subscription, 2 = Non-HWS (Used for debugging only)");
 
-int hws_max_conc_proc = 8;
-module_param(hws_max_conc_proc, int, 0444);
-MODULE_PARM_DESC(hws_max_conc_proc,
-	"Max # processes HWS can execute concurrently when sched_policy=0 (0 = no concurrency, #VMIDs for KFD = Maximum(default))");
-
-int cwsr_enable = 1;
-module_param(cwsr_enable, int, 0444);
-MODULE_PARM_DESC(cwsr_enable, "CWSR enable (0 = Off, 1 = On (Default))");
-
 int max_num_of_queues_per_device = KFD_MAX_NUM_OF_QUEUES_PER_DEVICE_DEFAULT;
 module_param(max_num_of_queues_per_device, int, 0444);
 MODULE_PARM_DESC(max_num_of_queues_per_device,
@@ -75,11 +61,6 @@ MODULE_PARM_DESC(send_sigterm,
 
 static int amdkfd_init_completed;
 
-int debug_largebar = 0;
-module_param(debug_largebar, int, 0444);
-MODULE_PARM_DESC(debug_largebar,
-	"Debug large-bar flag used to simulate large-bar capability on non-large bar machine (0 = disable, 1 = enable)");
-
 int kgd2kfd_init(unsigned interface_version, const struct kgd2kfd_calls **g2f)
 {
 	if (!amdkfd_init_completed)
@@ -133,16 +114,8 @@ static int __init kfd_module_init(void)
 	if (err < 0)
 		goto err_topology;
 
-	err = kfd_ipc_init();
-	if (err < 0)
-		goto err_topology;
-
 	kfd_process_create_wq();
 
-	kfd_init_peer_direct();
-
-	kfd_debugfs_init();
-
 	amdkfd_init_completed = 1;
 
 	dev_info(kfd_device, "Initialized module\n");
@@ -161,8 +134,6 @@ static void __exit kfd_module_exit(void)
 {
 	amdkfd_init_completed = 0;
 
-	kfd_debugfs_fini();
-	kfd_close_peer_direct();
 	kfd_process_destroy_wq();
 	kfd_topology_shutdown();
 	kfd_chardev_exit();
@@ -178,5 +149,4 @@ MODULE_DESCRIPTION(KFD_DRIVER_DESC);
 MODULE_LICENSE("GPL and additional rights");
 MODULE_VERSION(__stringify(KFD_DRIVER_MAJOR) "."
 	       __stringify(KFD_DRIVER_MINOR) "."
-	       __stringify(KFD_DRIVER_PATCHLEVEL)
-	       KFD_DRIVER_RC_LEVEL);
+	       __stringify(KFD_DRIVER_PATCHLEVEL));
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c
index 99c2535..b1ef136 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c
@@ -29,15 +29,8 @@ struct mqd_manager *mqd_manager_init(enum KFD_MQD_TYPE type,
 	switch (dev->device_info->asic_family) {
 	case CHIP_KAVERI:
 		return mqd_manager_init_cik(type, dev);
-	case CHIP_HAWAII:
-		return mqd_manager_init_cik_hawaii(type, dev);
 	case CHIP_CARRIZO:
 		return mqd_manager_init_vi(type, dev);
-	case CHIP_TONGA:
-	case CHIP_FIJI:
-	case CHIP_POLARIS10:
-	case CHIP_POLARIS11:
-		return mqd_manager_init_vi_tonga(type, dev);
 	}
 
 	return NULL;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h
index 8972bcf..213a71e 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h
@@ -67,8 +67,7 @@ struct mqd_manager {
 
 	int	(*load_mqd)(struct mqd_manager *mm, void *mqd,
 				uint32_t pipe_id, uint32_t queue_id,
-				struct queue_properties *p,
-				struct mm_struct *mms);
+				uint32_t __user *wptr);
 
 	int	(*update_mqd)(struct mqd_manager *mm, void *mqd,
 				struct queue_properties *q);
@@ -85,10 +84,6 @@ struct mqd_manager {
 				uint64_t queue_address,	uint32_t pipe_id,
 				uint32_t queue_id);
 
-#if defined(CONFIG_DEBUG_FS)
-	int	(*debugfs_show_mqd)(struct seq_file *m, void *data);
-#endif
-
 	struct mutex	mqd_mutex;
 	struct kfd_dev	*dev;
 };
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
index 43a0aa3..d83de98 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
@@ -29,70 +29,11 @@
 #include "cik_structs.h"
 #include "oss/oss_2_4_sh_mask.h"
 
-#define AQL_ENABLE 1
-
 static inline struct cik_mqd *get_mqd(void *mqd)
 {
 	return (struct cik_mqd *)mqd;
 }
 
-static inline struct cik_sdma_rlc_registers *get_sdma_mqd(void *mqd)
-{
-	return (struct cik_sdma_rlc_registers *)mqd;
-}
-
-static void update_cu_mask(struct mqd_manager *mm, void *mqd,
-			struct queue_properties *q)
-{
-	struct cik_mqd *m;
-	struct kfd_cu_info cu_info;
-	uint32_t se_mask[4] = {0}; /* 4 is the max # of SEs */
-	uint32_t cu_mask_count = q->cu_mask_count;
-	const uint32_t *cu_mask = q->cu_mask;
-	int se, cu_per_sh, cu_index, i;
-
-	if (cu_mask_count == 0)
-		return;
-
-	m = get_mqd(mqd);
-	m->compute_static_thread_mgmt_se0 = 0;
-	m->compute_static_thread_mgmt_se1 = 0;
-	m->compute_static_thread_mgmt_se2 = 0;
-	m->compute_static_thread_mgmt_se3 = 0;
-
-	mm->dev->kfd2kgd->get_cu_info(mm->dev->kgd, &cu_info);
-
-	/* If # CU mask bits > # CUs, set it to the # of CUs */
-	if (cu_mask_count > cu_info.cu_active_number)
-		cu_mask_count = cu_info.cu_active_number;
-
-	cu_index = 0;
-	for (se = 0; se < cu_info.num_shader_engines; se++) {
-		cu_per_sh = 0;
-
-		/* Get the number of CUs on this Shader Engine */
-		for (i = 0; i < 4; i++)
-			cu_per_sh += hweight32(cu_info.cu_bitmap[se][i]);
-
-		se_mask[se] = cu_mask[cu_index / 32] >> (cu_index % 32);
-		if ((cu_per_sh + (cu_index % 32)) > 32)
-			se_mask[se] |= cu_mask[(cu_index / 32) + 1]
-					<< (32 - (cu_index % 32));
-		se_mask[se] &= (1 << cu_per_sh) - 1;
-		cu_index += cu_per_sh;
-	}
-	m->compute_static_thread_mgmt_se0 = se_mask[0];
-	m->compute_static_thread_mgmt_se1 = se_mask[1];
-	m->compute_static_thread_mgmt_se2 = se_mask[2];
-	m->compute_static_thread_mgmt_se3 = se_mask[3];
-
-	pr_debug("kfd: update cu mask to %#x %#x %#x %#x\n",
-		m->compute_static_thread_mgmt_se0,
-		m->compute_static_thread_mgmt_se1,
-		m->compute_static_thread_mgmt_se2,
-		m->compute_static_thread_mgmt_se3);
-}
-
 static int init_mqd(struct mqd_manager *mm, void **mqd,
 		struct kfd_mem_obj **mqd_mem_obj, uint64_t *gart_addr,
 		struct queue_properties *q)
@@ -135,6 +76,10 @@ static int init_mqd(struct mqd_manager *mm, void **mqd,
 	m->cp_mqd_base_addr_lo        = lower_32_bits(addr);
 	m->cp_mqd_base_addr_hi        = upper_32_bits(addr);
 
+	m->cp_hqd_ib_control = DEFAULT_MIN_IB_AVAIL_SIZE | IB_ATC_EN;
+	/* Although WinKFD writes this, I suspect it should not be necessary */
+	m->cp_hqd_ib_control = IB_ATC_EN | DEFAULT_MIN_IB_AVAIL_SIZE;
+
 	m->cp_hqd_quantum = QUANTUM_EN | QUANTUM_SCALE_1MS |
 				QUANTUM_DURATION(10);
 
@@ -205,29 +150,21 @@ static void uninit_mqd_sdma(struct mqd_manager *mm, void *mqd,
 }
 
 static int load_mqd(struct mqd_manager *mm, void *mqd, uint32_t pipe_id,
-		    uint32_t queue_id, struct queue_properties *p,
-		    struct mm_struct *mms)
+			uint32_t queue_id, uint32_t __user *wptr)
 {
-	/* AQL write pointer counts in 64B packets, PM4/CP counts in dwords. */
-	uint32_t wptr_shift = (p->format == KFD_QUEUE_FORMAT_AQL ? 4 : 0);
-	uint32_t wptr_mask = (uint32_t)((p->queue_size / sizeof(uint32_t)) - 1);
-
-	return mm->dev->kfd2kgd->hqd_load(mm->dev->kgd, mqd, pipe_id, queue_id,
-					  (uint32_t __user *)p->write_ptr,
-					  wptr_shift, wptr_mask, mms);
+	return mm->dev->kfd2kgd->hqd_load
+		(mm->dev->kgd, mqd, pipe_id, queue_id, wptr);
 }
 
 static int load_mqd_sdma(struct mqd_manager *mm, void *mqd,
-			 uint32_t pipe_id, uint32_t queue_id,
-			 struct queue_properties *p, struct mm_struct *mms)
+			uint32_t pipe_id, uint32_t queue_id,
+			uint32_t __user *wptr)
 {
-	return mm->dev->kfd2kgd->hqd_sdma_load(mm->dev->kgd, mqd,
-					       (uint32_t __user *)p->write_ptr,
-					       mms);
+	return mm->dev->kfd2kgd->hqd_sdma_load(mm->dev->kgd, mqd);
 }
 
-static int __update_mqd(struct mqd_manager *mm, void *mqd,
-			struct queue_properties *q, unsigned int atc_bit)
+static int update_mqd(struct mqd_manager *mm, void *mqd,
+			struct queue_properties *q)
 {
 	struct cik_mqd *m;
 
@@ -237,12 +174,7 @@ static int __update_mqd(struct mqd_manager *mm, void *mqd,
 
 	m = get_mqd(mqd);
 	m->cp_hqd_pq_control = DEFAULT_RPTR_BLOCK_SIZE |
-				DEFAULT_MIN_AVAIL_SIZE;
-	m->cp_hqd_ib_control = DEFAULT_MIN_IB_AVAIL_SIZE;
-        if (atc_bit) {
-		m->cp_hqd_pq_control |= PQ_ATC_EN;
-		m->cp_hqd_ib_control |= IB_ATC_EN;
-	}
+				DEFAULT_MIN_AVAIL_SIZE | PQ_ATC_EN;
 
 	/*
 	 * Calculating queue size which is log base 2 of actual queue size -1
@@ -263,14 +195,11 @@ static int __update_mqd(struct mqd_manager *mm, void *mqd,
 		m->cp_hqd_pq_control |= NO_UPDATE_RPTR;
 	}
 
-	update_cu_mask(mm, mqd, q);
-
 	m->cp_hqd_active = 0;
 	q->is_active = false;
 	if (q->queue_size > 0 &&
 			q->queue_address != 0 &&
-			q->queue_percent > 0 &&
-			!q->is_evicted) {
+			q->queue_percent > 0) {
 		m->cp_hqd_active = 1;
 		q->is_active = true;
 	}
@@ -278,18 +207,6 @@ static int __update_mqd(struct mqd_manager *mm, void *mqd,
 	return 0;
 }
 
-static int update_mqd(struct mqd_manager *mm, void *mqd,
-			struct queue_properties *q)
-{
-	return __update_mqd(mm, mqd, q, 1);
-}
-
-static int update_mqd_hawaii(struct mqd_manager *mm, void *mqd,
-			struct queue_properties *q)
-{
-	return __update_mqd(mm, mqd, q, 0);
-}
-
 static int update_mqd_sdma(struct mqd_manager *mm, void *mqd,
 				struct queue_properties *q)
 {
@@ -298,8 +215,8 @@ static int update_mqd_sdma(struct mqd_manager *mm, void *mqd,
 	BUG_ON(!mm || !mqd || !q);
 
 	m = get_sdma_mqd(mqd);
-	m->sdma_rlc_rb_cntl = (ffs(q->queue_size / sizeof(unsigned int)) - 1)
-			<< SDMA0_RLC0_RB_CNTL__RB_SIZE__SHIFT |
+	m->sdma_rlc_rb_cntl = ffs(q->queue_size / sizeof(unsigned int)) <<
+			SDMA0_RLC0_RB_CNTL__RB_SIZE__SHIFT |
 			q->vmid << SDMA0_RLC0_RB_CNTL__RB_VMID__SHIFT |
 			1 << SDMA0_RLC0_RB_CNTL__RPTR_WRITEBACK_ENABLE__SHIFT |
 			6 << SDMA0_RLC0_RB_CNTL__RPTR_WRITEBACK_TIMER__SHIFT;
@@ -320,8 +237,7 @@ static int update_mqd_sdma(struct mqd_manager *mm, void *mqd,
 	q->is_active = false;
 	if (q->queue_size > 0 &&
 			q->queue_address != 0 &&
-			q->queue_percent > 0 &&
-			!q->is_evicted) {
+			q->queue_percent > 0) {
 		m->sdma_rlc_rb_cntl |=
 				1 << SDMA0_RLC0_RB_CNTL__RB_ENABLE__SHIFT;
 
@@ -470,8 +386,7 @@ static int update_mqd_hiq(struct mqd_manager *mm, void *mqd,
 	q->is_active = false;
 	if (q->queue_size > 0 &&
 			q->queue_address != 0 &&
-			q->queue_percent > 0 &&
-			!q->is_evicted) {
+			q->queue_percent > 0) {
 		m->cp_hqd_active = 1;
 		q->is_active = true;
 	}
@@ -479,24 +394,16 @@ static int update_mqd_hiq(struct mqd_manager *mm, void *mqd,
 	return 0;
 }
 
-#if defined(CONFIG_DEBUG_FS)
-
-static int debugfs_show_mqd(struct seq_file *m, void *data)
+struct cik_sdma_rlc_registers *get_sdma_mqd(void *mqd)
 {
-	seq_hex_dump(m, "    ", DUMP_PREFIX_OFFSET, 32, 4,
-		     data, sizeof(struct cik_mqd), false);
-	return 0;
-}
+	struct cik_sdma_rlc_registers *m;
 
-static int debugfs_show_mqd_sdma(struct seq_file *m, void *data)
-{
-	seq_hex_dump(m, "    ", DUMP_PREFIX_OFFSET, 32, 4,
-		     data, sizeof(struct cik_sdma_rlc_registers), false);
-	return 0;
-}
+	BUG_ON(!mqd);
 
-#endif
+	m = (struct cik_sdma_rlc_registers *)mqd;
 
+	return m;
+}
 
 struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 		struct kfd_dev *dev)
@@ -523,9 +430,6 @@ struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
-#if defined(CONFIG_DEBUG_FS)
-		mqd->debugfs_show_mqd = debugfs_show_mqd;
-#endif
 		break;
 	case KFD_MQD_TYPE_HIQ:
 		mqd->init_mqd = init_mqd_hiq;
@@ -534,9 +438,6 @@ struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_hiq;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
-#if defined(CONFIG_DEBUG_FS)
-		mqd->debugfs_show_mqd = debugfs_show_mqd;
-#endif
 		break;
 	case KFD_MQD_TYPE_SDMA:
 		mqd->init_mqd = init_mqd_sdma;
@@ -545,9 +446,6 @@ struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_sdma;
 		mqd->destroy_mqd = destroy_mqd_sdma;
 		mqd->is_occupied = is_occupied_sdma;
-#if defined(CONFIG_DEBUG_FS)
-		mqd->debugfs_show_mqd = debugfs_show_mqd_sdma;
-#endif
 		break;
 	default:
 		kfree(mqd);
@@ -557,15 +455,3 @@ struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 	return mqd;
 }
 
-struct mqd_manager *mqd_manager_init_cik_hawaii(enum KFD_MQD_TYPE type,
-			struct kfd_dev *dev)
-{
-	struct mqd_manager *mqd;
-
-	mqd = mqd_manager_init_cik(type, dev);
-	if (!mqd)
-		return NULL;
-	if ((type == KFD_MQD_TYPE_CP) || (type == KFD_MQD_TYPE_COMPUTE))
-		mqd->update_mqd = update_mqd_hawaii;
-	return mqd;
-}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
index 2d06221..fa32c32 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
@@ -26,9 +26,9 @@
 #include "kfd_priv.h"
 #include "kfd_mqd_manager.h"
 #include "vi_structs.h"
-#include "asic_reg/gca/gfx_8_0_sh_mask.h"
-#include "asic_reg/gca/gfx_8_0_enum.h"
-#include "oss/oss_3_0_sh_mask.h"
+#include "gca/gfx_8_0_sh_mask.h"
+#include "gca/gfx_8_0_enum.h"
+
 #define CP_MQD_CONTROL__PRIV_STATE__SHIFT 0x8
 
 static inline struct vi_mqd *get_mqd(void *mqd)
@@ -36,63 +36,6 @@ static inline struct vi_mqd *get_mqd(void *mqd)
 	return (struct vi_mqd *)mqd;
 }
 
-static inline struct vi_sdma_mqd *get_sdma_mqd(void *mqd)
-{
-	return (struct vi_sdma_mqd *)mqd;
-}
-
-static void update_cu_mask(struct mqd_manager *mm, void *mqd,
-			struct queue_properties *q)
-{
-	struct vi_mqd *m;
-	struct kfd_cu_info cu_info;
-	uint32_t se_mask[4] = {0}; /* 4 is the max # of SEs */
-	uint32_t cu_mask_count = q->cu_mask_count;
-	const uint32_t *cu_mask = q->cu_mask;
-	int se, cu_per_sh, cu_index, i;
-
-	if (cu_mask_count == 0)
-		return;
-
-	m = get_mqd(mqd);
-	m->compute_static_thread_mgmt_se0 = 0;
-	m->compute_static_thread_mgmt_se1 = 0;
-	m->compute_static_thread_mgmt_se2 = 0;
-	m->compute_static_thread_mgmt_se3 = 0;
-
-	mm->dev->kfd2kgd->get_cu_info(mm->dev->kgd, &cu_info);
-
-	/* If # CU mask bits > # CUs, set it to the # of CUs */
-	if (cu_mask_count > cu_info.cu_active_number)
-		cu_mask_count = cu_info.cu_active_number;
-
-	cu_index = 0;
-	for (se = 0; se < cu_info.num_shader_engines; se++) {
-		cu_per_sh = 0;
-
-		/* Get the number of CUs on this Shader Engine */
-		for (i = 0; i < 4; i++)
-			cu_per_sh += hweight32(cu_info.cu_bitmap[se][i]);
-
-		se_mask[se] = cu_mask[cu_index / 32] >> (cu_index % 32);
-		if ((cu_per_sh + (cu_index % 32)) > 32)
-			se_mask[se] |= cu_mask[(cu_index / 32) + 1]
-					<< (32 - (cu_index % 32));
-		se_mask[se] &= (1 << cu_per_sh) - 1;
-		cu_index += cu_per_sh;
-	}
-	m->compute_static_thread_mgmt_se0 = se_mask[0];
-	m->compute_static_thread_mgmt_se1 = se_mask[1];
-	m->compute_static_thread_mgmt_se2 = se_mask[2];
-	m->compute_static_thread_mgmt_se3 = se_mask[3];
-
-	pr_debug("kfd: update cu mask to %#x %#x %#x %#x\n",
-		m->compute_static_thread_mgmt_se0,
-		m->compute_static_thread_mgmt_se1,
-		m->compute_static_thread_mgmt_se2,
-		m->compute_static_thread_mgmt_se3);
-}
-
 static int init_mqd(struct mqd_manager *mm, void **mqd,
 			struct kfd_mem_obj **mqd_mem_obj, uint64_t *gart_addr,
 			struct queue_properties *q)
@@ -139,28 +82,6 @@ static int init_mqd(struct mqd_manager *mm, void **mqd,
 	if (q->format == KFD_QUEUE_FORMAT_AQL)
 		m->cp_hqd_iq_rptr = 1;
 
-	if (q->tba_addr) {
-		m->compute_tba_lo = lower_32_bits(q->tba_addr >> 8);
-		m->compute_tba_hi = upper_32_bits(q->tba_addr >> 8);
-		m->compute_tma_lo = lower_32_bits(q->tma_addr >> 8);
-		m->compute_tma_hi = upper_32_bits(q->tma_addr >> 8);
-		m->compute_pgm_rsrc2 |=
-			(1 << COMPUTE_PGM_RSRC2__TRAP_PRESENT__SHIFT);
-	}
-
-	if (mm->dev->cwsr_enabled) {
-		m->cp_hqd_persistent_state |=
-			(1 << CP_HQD_PERSISTENT_STATE__QSWITCH_MODE__SHIFT);
-		m->cp_hqd_ctx_save_base_addr_lo =
-			lower_32_bits(q->ctx_save_restore_area_address);
-		m->cp_hqd_ctx_save_base_addr_hi =
-			upper_32_bits(q->ctx_save_restore_area_address);
-		m->cp_hqd_ctx_save_size = q->ctx_save_restore_area_size;
-		m->cp_hqd_cntl_stack_size = q->ctl_stack_size;
-		m->cp_hqd_cntl_stack_offset = q->ctl_stack_size;
-		m->cp_hqd_wg_state_offset = q->ctl_stack_size;
-	}
-
 	*mqd = m;
 	if (gart_addr != NULL)
 		*gart_addr = addr;
@@ -171,15 +92,10 @@ static int init_mqd(struct mqd_manager *mm, void **mqd,
 
 static int load_mqd(struct mqd_manager *mm, void *mqd,
 			uint32_t pipe_id, uint32_t queue_id,
-			struct queue_properties *p, struct mm_struct *mms)
+			uint32_t __user *wptr)
 {
-	/* AQL write pointer counts in 64B packets, PM4/CP counts in dwords. */
-	uint32_t wptr_shift = (p->format == KFD_QUEUE_FORMAT_AQL ? 4 : 0);
-	uint32_t wptr_mask = (uint32_t)((p->queue_size / sizeof(uint32_t)) - 1);
-
-	return mm->dev->kfd2kgd->hqd_load(mm->dev->kgd, mqd, pipe_id, queue_id,
-					  (uint32_t __user *)p->write_ptr,
-					  wptr_shift, wptr_mask, mms);
+	return mm->dev->kfd2kgd->hqd_load
+		(mm->dev->kgd, mqd, pipe_id, queue_id, wptr);
 }
 
 static int __update_mqd(struct mqd_manager *mm, void *mqd,
@@ -221,15 +137,8 @@ static int __update_mqd(struct mqd_manager *mm, void *mqd,
 			3 << CP_HQD_IB_CONTROL__MIN_IB_AVAIL_SIZE__SHIFT |
 			mtype << CP_HQD_IB_CONTROL__MTYPE__SHIFT;
 
-	/*
-	 * HW does not clamp this field correctly. Maximum EOP queue size
-	 * is constrained by per-SE EOP done signal count, which is 8-bit.
-	 * Limit is 0xFF EOP entries (= 0x7F8 dwords). CP will not submit
-	 * more than (EOP entry count - 1) so a queue size of 0x800 dwords
-	 * is safe, giving a maximum field value of 0xA.
-	 */
-	m->cp_hqd_eop_control |= min(0xA,
-		ffs(q->eop_ring_buffer_size / sizeof(unsigned int)) - 1 - 1);
+	m->cp_hqd_eop_control |=
+		ffs(q->eop_ring_buffer_size / sizeof(unsigned int)) - 1 - 1;
 	m->cp_hqd_eop_base_addr_lo =
 			lower_32_bits(q->eop_ring_buffer_address >> 8);
 	m->cp_hqd_eop_base_addr_hi =
@@ -244,19 +153,12 @@ static int __update_mqd(struct mqd_manager *mm, void *mqd,
 		m->cp_hqd_pq_control |= CP_HQD_PQ_CONTROL__NO_UPDATE_RPTR_MASK |
 				2 << CP_HQD_PQ_CONTROL__SLOT_BASED_WPTR__SHIFT;
 	}
-	if (mm->dev->cwsr_enabled)
-		m->cp_hqd_ctx_save_control =
-			atc_bit << CP_HQD_CTX_SAVE_CONTROL__ATC__SHIFT |
-			mtype << CP_HQD_CTX_SAVE_CONTROL__MTYPE__SHIFT;
-
-	update_cu_mask(mm, mqd, q);
 
 	m->cp_hqd_active = 0;
 	q->is_active = false;
 	if (q->queue_size > 0 &&
 			q->queue_address != 0 &&
-			q->queue_percent > 0 &&
-			!q->is_evicted) {
+			q->queue_percent > 0) {
 		m->cp_hqd_active = 1;
 		q->is_active = true;
 	}
@@ -271,12 +173,6 @@ static int update_mqd(struct mqd_manager *mm, void *mqd,
 	return __update_mqd(mm, mqd, q, MTYPE_CC, 1);
 }
 
-static int update_mqd_tonga(struct mqd_manager *mm, void *mqd,
-			struct queue_properties *q)
-{
-	return __update_mqd(mm, mqd, q, MTYPE_UC, 0);
-}
-
 static int destroy_mqd(struct mqd_manager *mm, void *mqd,
 			enum kfd_preempt_type type,
 			unsigned int timeout, uint32_t pipe_id,
@@ -335,129 +231,6 @@ static int update_mqd_hiq(struct mqd_manager *mm, void *mqd,
 	return retval;
 }
 
-static int init_mqd_sdma(struct mqd_manager *mm, void **mqd,
-		struct kfd_mem_obj **mqd_mem_obj, uint64_t *gart_addr,
-		struct queue_properties *q)
-{
-	int retval;
-	struct vi_sdma_mqd *m;
-
-
-	BUG_ON(!mm || !mqd || !mqd_mem_obj);
-
-	retval = kfd_gtt_sa_allocate(mm->dev,
-			sizeof(struct vi_sdma_mqd),
-			mqd_mem_obj);
-
-	if (retval != 0)
-		return -ENOMEM;
-
-	m = (struct vi_sdma_mqd *) (*mqd_mem_obj)->cpu_ptr;
-
-	memset(m, 0, sizeof(struct vi_sdma_mqd));
-
-	*mqd = m;
-	if (gart_addr != NULL)
-		*gart_addr = (*mqd_mem_obj)->gpu_addr;
-
-	retval = mm->update_mqd(mm, m, q);
-
-	return retval;
-}
-
-static void uninit_mqd_sdma(struct mqd_manager *mm, void *mqd,
-		struct kfd_mem_obj *mqd_mem_obj)
-{
-	BUG_ON(!mm || !mqd);
-	kfd_gtt_sa_free(mm->dev, mqd_mem_obj);
-}
-
-static int load_mqd_sdma(struct mqd_manager *mm, void *mqd,
-		uint32_t pipe_id, uint32_t queue_id,
-		struct queue_properties *p, struct mm_struct *mms)
-{
-	return mm->dev->kfd2kgd->hqd_sdma_load(mm->dev->kgd, mqd,
-					       (uint32_t __user *)p->write_ptr,
-					       mms);
-}
-
-static int update_mqd_sdma(struct mqd_manager *mm, void *mqd,
-		struct queue_properties *q)
-{
-	struct vi_sdma_mqd *m;
-	BUG_ON(!mm || !mqd || !q);
-
-	m = get_sdma_mqd(mqd);
-	m->sdmax_rlcx_rb_cntl = (ffs(q->queue_size / sizeof(unsigned int)) - 1)
-		<< SDMA0_RLC0_RB_CNTL__RB_SIZE__SHIFT |
-		q->vmid << SDMA0_RLC0_RB_CNTL__RB_VMID__SHIFT |
-		1 << SDMA0_RLC0_RB_CNTL__RPTR_WRITEBACK_ENABLE__SHIFT |
-		6 << SDMA0_RLC0_RB_CNTL__RPTR_WRITEBACK_TIMER__SHIFT;
-
-	m->sdmax_rlcx_rb_base = lower_32_bits(q->queue_address >> 8);
-	m->sdmax_rlcx_rb_base_hi = upper_32_bits(q->queue_address >> 8);
-	m->sdmax_rlcx_rb_rptr_addr_lo = lower_32_bits((uint64_t)q->read_ptr);
-	m->sdmax_rlcx_rb_rptr_addr_hi = upper_32_bits((uint64_t)q->read_ptr);
-	m->sdmax_rlcx_doorbell = q->doorbell_off <<
-		SDMA0_RLC0_DOORBELL__OFFSET__SHIFT |
-		1 << SDMA0_RLC0_DOORBELL__ENABLE__SHIFT;
-
-	m->sdmax_rlcx_virtual_addr = q->sdma_vm_addr;
-
-	m->sdma_engine_id = q->sdma_engine_id;
-	m->sdma_queue_id = q->sdma_queue_id;
-
-	q->is_active = false;
-	if (q->queue_size > 0 &&
-			q->queue_address != 0 &&
-			q->queue_percent > 0 &&
-			!q->is_evicted) {
-		m->sdmax_rlcx_rb_cntl |=
-			1 << SDMA0_RLC0_RB_CNTL__RB_ENABLE__SHIFT;
-
-		q->is_active = true;
-	}
-
-	return 0;
-}
-
-/*
- *  * preempt type here is ignored because there is only one way
- *  * to preempt sdma queue
- */
-static int destroy_mqd_sdma(struct mqd_manager *mm, void *mqd,
-		enum kfd_preempt_type type,
-		unsigned int timeout, uint32_t pipe_id,
-		uint32_t queue_id)
-{
-	return mm->dev->kfd2kgd->hqd_sdma_destroy(mm->dev->kgd, mqd, timeout);
-}
-
-static bool is_occupied_sdma(struct mqd_manager *mm, void *mqd,
-		uint64_t queue_address, uint32_t pipe_id,
-		uint32_t queue_id)
-{
-	return mm->dev->kfd2kgd->hqd_sdma_is_occupied(mm->dev->kgd, mqd);
-}
-
-#if defined(CONFIG_DEBUG_FS)
-
-static int debugfs_show_mqd(struct seq_file *m, void *data)
-{
-	seq_hex_dump(m, "    ", DUMP_PREFIX_OFFSET, 32, 4,
-		     data, sizeof(struct vi_mqd), false);
-	return 0;
-}
-
-static int debugfs_show_mqd_sdma(struct seq_file *m, void *data)
-{
-	seq_hex_dump(m, "    ", DUMP_PREFIX_OFFSET, 32, 4,
-		     data, sizeof(struct vi_sdma_mqd), false);
-	return 0;
-}
-
-#endif
-
 struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 		struct kfd_dev *dev)
 {
@@ -483,9 +256,6 @@ struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
-#if defined(CONFIG_DEBUG_FS)
-		mqd->debugfs_show_mqd = debugfs_show_mqd;
-#endif
 		break;
 	case KFD_MQD_TYPE_HIQ:
 		mqd->init_mqd = init_mqd_hiq;
@@ -494,20 +264,8 @@ struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_hiq;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
-#if defined(CONFIG_DEBUG_FS)
-		mqd->debugfs_show_mqd = debugfs_show_mqd;
-#endif
 		break;
 	case KFD_MQD_TYPE_SDMA:
-		mqd->init_mqd = init_mqd_sdma;
-		mqd->uninit_mqd = uninit_mqd_sdma;
-		mqd->load_mqd = load_mqd_sdma;
-		mqd->update_mqd = update_mqd_sdma;
-		mqd->destroy_mqd = destroy_mqd_sdma;
-		mqd->is_occupied = is_occupied_sdma;
-#if defined(CONFIG_DEBUG_FS)
-		mqd->debugfs_show_mqd = debugfs_show_mqd_sdma;
-#endif
 		break;
 	default:
 		kfree(mqd);
@@ -516,17 +274,3 @@ struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 
 	return mqd;
 }
-
-struct mqd_manager *mqd_manager_init_vi_tonga(enum KFD_MQD_TYPE type,
-			struct kfd_dev *dev)
-{
-	struct mqd_manager *mqd;
-
-	mqd = mqd_manager_init_vi(type, dev);
-	if (!mqd)
-		return NULL;
-	if ((type == KFD_MQD_TYPE_CP) || (type == KFD_MQD_TYPE_COMPUTE))
-		mqd->update_mqd = update_mqd_tonga;
-	return mqd;
-}
-
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
index 7ad7e63..ca8c093 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
@@ -55,41 +55,28 @@ static void pm_calc_rlib_size(struct packet_manager *pm,
 				unsigned int *rlib_size,
 				bool *over_subscription)
 {
-	unsigned int process_count, queue_count, compute_queue_count;
+	unsigned int process_count, queue_count;
 	unsigned int map_queue_size;
-	unsigned int max_proc_per_quantum = 1;
 
-	struct kfd_dev	*dev = pm->dqm->dev;
-
-	BUG_ON(!pm || !rlib_size || !over_subscription || !dev);
+	BUG_ON(!pm || !rlib_size || !over_subscription);
 
 	process_count = pm->dqm->processes_count;
 	queue_count = pm->dqm->queue_count;
-	compute_queue_count = queue_count - pm->dqm->sdma_queue_count;
-
-	/* check if there is over subscription
-	 * Note: the arbitration between the number of VMIDs and
-	 * hws_max_conc_proc has been done in
-	 * kgd2kfd_device_init().
-	 */
 
+	/* check if there is over subscription*/
 	*over_subscription = false;
-
-	if (dev->max_proc_per_quantum > 1)
-		max_proc_per_quantum = dev->max_proc_per_quantum;
-
-	if ((process_count > max_proc_per_quantum) ||
-		compute_queue_count >
-			PIPE_PER_ME_CP_SCHEDULING * QUEUES_PER_PIPE) {
+	if ((process_count > 1) ||
+		queue_count > PIPE_PER_ME_CP_SCHEDULING * QUEUES_PER_PIPE) {
 		*over_subscription = true;
 		pr_debug("kfd: over subscribed runlist\n");
 	}
 
-	map_queue_size = KFD_IS_VI(pm->dqm->dev->device_info->asic_family) ?
+	map_queue_size =
+		(pm->dqm->dev->device_info->asic_family == CHIP_CARRIZO) ?
 		sizeof(struct pm4_mes_map_queues) :
 		sizeof(struct pm4_map_queues);
 	/* calculate run list ib allocation size */
-	*rlib_size = process_count * pm->pmf->get_map_process_packet_size() +
+	*rlib_size = process_count * sizeof(struct pm4_map_process) +
 		     queue_count * map_queue_size;
 
 	/*
@@ -116,14 +103,11 @@ static int pm_allocate_runlist_ib(struct packet_manager *pm,
 
 	pm_calc_rlib_size(pm, rl_buffer_size, is_over_subscription);
 
-	mutex_lock(&pm->lock);
-
 	retval = kfd_gtt_sa_allocate(pm->dqm->dev, *rl_buffer_size,
 					&pm->ib_buffer_obj);
 
 	if (retval != 0) {
 		pr_err("kfd: failed to allocate runlist IB\n");
-		mutex_unlock(&pm->lock);
 		return retval;
 	}
 
@@ -132,8 +116,6 @@ static int pm_allocate_runlist_ib(struct packet_manager *pm,
 
 	memset(*rl_buffer, 0, *rl_buffer_size);
 	pm->allocated = true;
-
-	mutex_unlock(&pm->lock);
 	return retval;
 }
 
@@ -141,24 +123,9 @@ static int pm_create_runlist(struct packet_manager *pm, uint32_t *buffer,
 			uint64_t ib, size_t ib_size_in_dwords, bool chain)
 {
 	struct pm4_runlist *packet;
-	int concurrent_proc_cnt = 0;
-	struct kfd_dev *kfd = pm->dqm->dev;
 
 	BUG_ON(!pm || !buffer || !ib);
 
-	/* Determine the number of processes to map together to HW:
-	 * it can not exceed the number of VMIDs available to the
-	 * scheduler, and it is determined by the smaller of the number
-	 * of processes in the runlist and kfd module parameter
-	 * hws_max_conc_proc.
-	 * Note: the arbitration between the number of VMIDs and
-	 * hws_max_conc_proc has been done in
-	 * kgd2kfd_device_init().
-	 */
-	concurrent_proc_cnt = min(pm->dqm->processes_count,
-			kfd->max_proc_per_quantum);
-
-
 	packet = (struct pm4_runlist *)buffer;
 
 	memset(buffer, 0, sizeof(struct pm4_runlist));
@@ -169,7 +136,6 @@ static int pm_create_runlist(struct packet_manager *pm, uint32_t *buffer,
 	packet->bitfields4.chain = chain ? 1 : 0;
 	packet->bitfields4.offload_polling = 0;
 	packet->bitfields4.valid = 1;
-	packet->bitfields4.process_cnt = concurrent_proc_cnt;
 	packet->ordinal2 = lower_32_bits(ib);
 	packet->bitfields3.ib_base_hi = upper_32_bits(ib);
 
@@ -216,90 +182,6 @@ static int pm_create_map_process(struct packet_manager *pm, uint32_t *buffer,
 	return 0;
 }
 
-static int pm_create_map_process_scratch_kv(struct packet_manager *pm,
-		uint32_t *buffer, struct qcm_process_device *qpd)
-{
-	struct pm4_map_process_scratch_kv *packet;
-	struct queue *cur;
-	uint32_t num_queues;
-
-	BUG_ON(!pm || !buffer || !qpd);
-
-	packet = (struct pm4_map_process_scratch_kv *)buffer;
-
-	pr_debug("kfd: In func %s\n", __func__);
-
-	memset(buffer, 0, sizeof(struct pm4_map_process_scratch_kv));
-
-	packet->header.u32all = build_pm4_header(IT_MAP_PROCESS,
-				sizeof(struct pm4_map_process_scratch_kv));
-	packet->bitfields2.diq_enable = (qpd->is_debug) ? 1 : 0;
-	packet->bitfields2.process_quantum = 1;
-	packet->bitfields2.pasid = qpd->pqm->process->pasid;
-	packet->bitfields3.page_table_base = qpd->page_table_base;
-	packet->bitfields14.gds_size = qpd->gds_size;
-	packet->bitfields14.num_gws = qpd->num_gws;
-	packet->bitfields14.num_oac = qpd->num_oac;
-	num_queues = 0;
-	list_for_each_entry(cur, &qpd->queues_list, list)
-		num_queues++;
-	packet->bitfields14.num_queues = (qpd->is_debug) ? 0 : num_queues;
-
-	packet->sh_mem_config = qpd->sh_mem_config;
-	packet->sh_mem_bases = qpd->sh_mem_bases;
-	packet->sh_mem_ape1_base = qpd->sh_mem_ape1_base;
-	packet->sh_mem_ape1_limit = qpd->sh_mem_ape1_limit;
-
-	packet->sh_hidden_private_base_vmid = qpd->sh_hidden_private_base;
-
-	packet->gds_addr_lo = lower_32_bits(qpd->gds_context_area);
-	packet->gds_addr_hi = upper_32_bits(qpd->gds_context_area);
-
-	return 0;
-}
-
-static int pm_create_map_process_scratch(struct packet_manager *pm,
-		uint32_t *buffer, struct qcm_process_device *qpd)
-{
-	struct pm4_map_process_scratch *packet;
-	struct queue *cur;
-	uint32_t num_queues;
-
-	BUG_ON(!pm || !buffer || !qpd);
-
-	packet = (struct pm4_map_process_scratch *)buffer;
-
-	pr_debug("kfd: In func %s\n", __func__);
-
-	memset(buffer, 0, sizeof(struct pm4_map_process_scratch));
-
-	packet->header.u32all = build_pm4_header(IT_MAP_PROCESS,
-					sizeof(struct pm4_map_process_scratch));
-	packet->bitfields2.diq_enable = (qpd->is_debug) ? 1 : 0;
-	packet->bitfields2.process_quantum = 1;
-	packet->bitfields2.pasid = qpd->pqm->process->pasid;
-	packet->bitfields3.page_table_base = qpd->page_table_base;
-	packet->bitfields10.gds_size = qpd->gds_size;
-	packet->bitfields10.num_gws = qpd->num_gws;
-	packet->bitfields10.num_oac = qpd->num_oac;
-	num_queues = 0;
-	list_for_each_entry(cur, &qpd->queues_list, list)
-		num_queues++;
-	packet->bitfields10.num_queues = (qpd->is_debug) ? 0 : num_queues;
-
-	packet->sh_mem_config = qpd->sh_mem_config;
-	packet->sh_mem_bases = qpd->sh_mem_bases;
-	packet->sh_mem_ape1_base = qpd->sh_mem_ape1_base;
-	packet->sh_mem_ape1_limit = qpd->sh_mem_ape1_limit;
-
-	packet->sh_hidden_private_base_vmid = qpd->sh_hidden_private_base;
-
-	packet->gds_addr_lo = lower_32_bits(qpd->gds_context_area);
-	packet->gds_addr_hi = upper_32_bits(qpd->gds_context_area);
-
-	return 0;
-}
-
 static int pm_create_map_queue_vi(struct packet_manager *pm, uint32_t *buffer,
 		struct queue *q, bool is_static)
 {
@@ -337,7 +219,7 @@ static int pm_create_map_queue_vi(struct packet_manager *pm, uint32_t *buffer,
 			queue_type__mes_map_queues__debug_interface_queue_vi;
 		break;
 	case KFD_QUEUE_TYPE_SDMA:
-		packet->bitfields2.engine_sel = q->properties.sdma_engine_id +
+		packet->bitfields2.engine_sel =
 				engine_sel__mes_map_queues__sdma0_vi;
 		use_static = false; /* no static queues under SDMA */
 		break;
@@ -397,7 +279,7 @@ static int pm_create_map_queue(struct packet_manager *pm, uint32_t *buffer,
 				engine_sel__mes_map_queues__compute;
 		break;
 	case KFD_QUEUE_TYPE_SDMA:
-		packet->bitfields2.engine_sel = q->properties.sdma_engine_id +
+		packet->bitfields2.engine_sel =
 				engine_sel__mes_map_queues__sdma0;
 		use_static = false; /* no static queues under SDMA */
 		break;
@@ -451,7 +333,6 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 		return retval;
 
 	*rl_size_bytes = alloc_size_bytes;
-	pm->ib_size_bytes = alloc_size_bytes;
 
 	pr_debug("kfd: In func %s\n", __func__);
 	pr_debug("kfd: building runlist ib process count: %d queues count %d\n",
@@ -467,12 +348,12 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 			return -ENOMEM;
 		}
 
-		retval = pm->pmf->map_process(pm, &rl_buffer[rl_wptr], qpd);
+		retval = pm_create_map_process(pm, &rl_buffer[rl_wptr], qpd);
 		if (retval != 0)
 			return retval;
 
 		proccesses_mapped++;
-		inc_wptr(&rl_wptr, pm->pmf->get_map_process_packet_size(),
+		inc_wptr(&rl_wptr, sizeof(struct pm4_map_process),
 				alloc_size_bytes);
 
 		list_for_each_entry(kq, &qpd->priv_queue_list, list) {
@@ -482,7 +363,8 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 			pr_debug("kfd: static_queue, mapping kernel q %d, is debug status %d\n",
 				kq->queue->queue, qpd->is_debug);
 
-			if (KFD_IS_VI(pm->dqm->dev->device_info->asic_family))
+			if (pm->dqm->dev->device_info->asic_family ==
+					CHIP_CARRIZO)
 				retval = pm_create_map_queue_vi(pm,
 						&rl_buffer[rl_wptr],
 						kq->queue,
@@ -507,7 +389,8 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 			pr_debug("kfd: static_queue, mapping user queue %d, is debug status %d\n",
 				q->queue, qpd->is_debug);
 
-			if (KFD_IS_VI(pm->dqm->dev->device_info->asic_family))
+			if (pm->dqm->dev->device_info->asic_family ==
+					CHIP_CARRIZO)
 				retval = pm_create_map_queue_vi(pm,
 						&rl_buffer[rl_wptr],
 						q,
@@ -540,60 +423,7 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 	return 0;
 }
 
-static int get_map_process_packet_size(void)
-{
-	return sizeof(struct pm4_map_process);
-}
-
-static int get_map_process_packet_size_scratch_kv(void)
-{
-	return sizeof(struct pm4_map_process_scratch_kv);
-}
-
-static int get_map_process_packet_size_scratch(void)
-{
-	return sizeof(struct pm4_map_process_scratch);
-}
-
-/* pm_create_release_mem - Create a RELEASE_MEM packet and return the size
- *	of this packet
- *	@gpu_addr - GPU address of the packet. It's a virtual address.
- *	@buffer - buffer to fill up with the packet. It's a CPU kernel pointer
- *	Return - length of the packet
- */
-uint32_t pm_create_release_mem(uint64_t gpu_addr, uint32_t *buffer)
-{
-	struct pm4__release_mem *packet;
-
-	WARN_ON(!buffer);
-
-	packet = (struct pm4__release_mem *)buffer;
-	memset(buffer, 0, sizeof(struct pm4__release_mem));
-
-	packet->header.u32all = build_pm4_header(IT_RELEASE_MEM,
-					sizeof(struct pm4__release_mem));
-
-	packet->bitfields2.event_type = CACHE_FLUSH_AND_INV_TS_EVENT;
-	packet->bitfields2.event_index = event_index___release_mem__end_of_pipe;
-	packet->bitfields2.tcl1_action_ena = 1;
-	packet->bitfields2.tc_action_ena = 1;
-	packet->bitfields2.cache_policy = cache_policy___release_mem__lru;
-	packet->bitfields2.atc = 0;
-
-	packet->bitfields3.data_sel = data_sel___release_mem__send_32_bit_low;
-	packet->bitfields3.int_sel =
-		int_sel___release_mem__send_interrupt_after_write_confirm;
-
-	packet->bitfields4.address_lo_32b = (gpu_addr & 0xffffffff) >> 2;
-	packet->address_hi = upper_32_bits(gpu_addr);
-
-	packet->data_lo = 0;
-
-	return sizeof(struct pm4__release_mem) / sizeof(unsigned int);
-}
-
-int pm_init(struct packet_manager *pm, struct device_queue_manager *dqm,
-		uint16_t fw_ver)
+int pm_init(struct packet_manager *pm, struct device_queue_manager *dqm)
 {
 	BUG_ON(!dqm);
 
@@ -604,40 +434,8 @@ int pm_init(struct packet_manager *pm, struct device_queue_manager *dqm,
 		mutex_destroy(&pm->lock);
 		return -ENOMEM;
 	}
-	pm->pmf = kzalloc(sizeof(struct packet_manager_firmware), GFP_KERNEL);
 	pm->allocated = false;
 
-	switch (pm->dqm->dev->device_info->asic_family) {
-	case CHIP_KAVERI:
-	case CHIP_HAWAII:
-		if (fw_ver >= KFD_SCRATCH_KV_FW_VER) {
-			pm->pmf->map_process = pm_create_map_process_scratch_kv;
-			pm->pmf->get_map_process_packet_size =
-					get_map_process_packet_size_scratch_kv;
-		} else {
-			pm->pmf->map_process = pm_create_map_process;
-			pm->pmf->get_map_process_packet_size =
-						get_map_process_packet_size;
-		}
-		break;
-	case CHIP_CARRIZO:
-	case CHIP_TONGA:
-	case CHIP_FIJI:
-	case CHIP_POLARIS10:
-	case CHIP_POLARIS11:
-		if (fw_ver >= KFD_SCRATCH_CZ_FW_VER) {
-			pm->pmf->map_process = pm_create_map_process_scratch;
-			pm->pmf->get_map_process_packet_size =
-					get_map_process_packet_size_scratch;
-		} else {
-			pm->pmf->map_process = pm_create_map_process;
-			pm->pmf->get_map_process_packet_size =
-						get_map_process_packet_size;
-		}
-		break;
-
-	}
-
 	return 0;
 }
 
@@ -647,7 +445,6 @@ void pm_uninit(struct packet_manager *pm)
 
 	mutex_destroy(&pm->lock);
 	kernel_queue_uninit(pm->priv_queue);
-	kfree(pm->pmf);
 }
 
 int pm_send_set_resources(struct packet_manager *pm,
@@ -676,7 +473,7 @@ int pm_send_set_resources(struct packet_manager *pm,
 	packet->bitfields2.queue_type =
 			queue_type__mes_set_resources__hsa_interface_queue_hiq;
 	packet->bitfields2.vmid_mask = res->vmid_mask;
-	packet->bitfields2.unmap_latency = KFD_UNMAP_LATENCY_MS / 100;
+	packet->bitfields2.unmap_latency = KFD_UNMAP_LATENCY;
 	packet->bitfields7.oac_mask = res->oac_mask;
 	packet->bitfields8.gds_heap_base = res->gds_heap_base;
 	packet->bitfields8.gds_heap_size = res->gds_heap_size;
@@ -780,7 +577,7 @@ fail_acquire_packet_buffer:
 }
 
 int pm_send_unmap_queue(struct packet_manager *pm, enum kfd_queue_type type,
-			enum kfd_unmap_queues_filter filter,
+			enum kfd_preempt_type_filter mode,
 			uint32_t filter_param, bool reset,
 			unsigned int sdma_engine)
 {
@@ -800,8 +597,8 @@ int pm_send_unmap_queue(struct packet_manager *pm, enum kfd_queue_type type,
 
 	packet = (struct pm4_unmap_queues *)buffer;
 	memset(buffer, 0, sizeof(struct pm4_unmap_queues));
-	pr_debug("kfd: static_queue: unmapping queues: filter is %d , reset is %d , type is %d\n",
-		filter, reset, type);
+	pr_debug("kfd: static_queue: unmapping queues: mode is %d , reset is %d , type is %d\n",
+		mode, reset, type);
 	packet->header.u32all = build_pm4_header(IT_UNMAP_QUEUES,
 					sizeof(struct pm4_unmap_queues));
 	switch (type) {
@@ -826,26 +623,26 @@ int pm_send_unmap_queue(struct packet_manager *pm, enum kfd_queue_type type,
 		packet->bitfields2.action =
 				action__mes_unmap_queues__preempt_queues;
 
-	switch (filter) {
-	case KFD_UNMAP_QUEUES_FILTER_SINGLE_QUEUE:
+	switch (mode) {
+	case KFD_PREEMPT_TYPE_FILTER_SINGLE_QUEUE:
 		packet->bitfields2.queue_sel =
 				queue_sel__mes_unmap_queues__perform_request_on_specified_queues;
 		packet->bitfields2.num_queues = 1;
 		packet->bitfields3b.doorbell_offset0 = filter_param;
 		break;
-	case KFD_UNMAP_QUEUES_FILTER_BY_PASID:
+	case KFD_PREEMPT_TYPE_FILTER_BY_PASID:
 		packet->bitfields2.queue_sel =
 				queue_sel__mes_unmap_queues__perform_request_on_pasid_queues;
 		packet->bitfields3a.pasid = filter_param;
 		break;
-	case KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES:
+	case KFD_PREEMPT_TYPE_FILTER_ALL_QUEUES:
 		packet->bitfields2.queue_sel =
 				queue_sel__mes_unmap_queues__perform_request_on_all_active_queues;
 		break;
-	case KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES:
+	case KFD_PREEMPT_TYPE_FILTER_DYNAMIC_QUEUES:
 		/* in this case, we do not preempt static queues */
-		packet->bitfields2.queue_sel = 
-			queue_sel__mes_unmap_queues__perform_request_on_dynamic_queues_only;
+		packet->bitfields2.queue_sel =
+				queue_sel__mes_unmap_queues__perform_request_on_dynamic_queues_only;
 		break;
 	default:
 		BUG();
@@ -873,18 +670,3 @@ void pm_release_ib(struct packet_manager *pm)
 	}
 	mutex_unlock(&pm->lock);
 }
-
-int pm_debugfs_runlist(struct seq_file *m, void *data)
-{
-	struct packet_manager *pm = data;
-
-	if (!pm->allocated) {
-		seq_puts(m, "  No active runlist\n");
-		return 0;
-	}
-
-	seq_hex_dump(m, "  ", DUMP_PREFIX_OFFSET, 32, 4,
-		     pm->ib_buffer_obj->cpu_ptr, pm->ib_size_bytes, false);
-
-	return 0;
-}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c b/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c
deleted file mode 100644
index 3fb8896..0000000
--- a/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c
+++ /dev/null
@@ -1,513 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- */
-
-
-/* NOTE:
- *
- * This file contains logic to dynamically detect and enable PeerDirect
- * suppor. PeerDirect support is delivered e.g. as part of OFED
- * from Mellanox. Because we are not able to rely on the fact that the
- * corresponding OFED will be installed we should:
- *  - copy PeerDirect definitions locally to avoid dependency on
- *    corresponding header file
- *  - try dynamically detect address of PeerDirect function
- *    pointers.
- *
- * If dynamic detection failed then PeerDirect support should be
- * enabled using the standard PeerDirect bridge driver from:
- * https://github.com/RadeonOpenCompute/ROCnRDMA
- *
- *
- * Logic to support PeerDirect relies only on official public API to be
- * non-intrusive as much as possible.
- *
- **/
-
-#include <linux/device.h>
-#include <linux/export.h>
-#include <linux/pid.h>
-#include <linux/err.h>
-#include <linux/slab.h>
-#include <linux/scatterlist.h>
-#include <linux/module.h>
-
-#include "kfd_priv.h"
-#include "amd_rdma.h"
-
-
-
-/* ----------------------- PeerDirect interface ------------------------------*/
-
-/*
- * Copyright (c) 2013,  Mellanox Technologies. All rights reserved.
- *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
- *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
- */
-#define IB_PEER_MEMORY_NAME_MAX 64
-#define IB_PEER_MEMORY_VER_MAX 16
-
-struct peer_memory_client {
-	char	name[IB_PEER_MEMORY_NAME_MAX];
-	char	version[IB_PEER_MEMORY_VER_MAX];
-	/* acquire return code: 1-mine, 0-not mine */
-	int (*acquire)(unsigned long addr, size_t size,
-			void *peer_mem_private_data,
-					char *peer_mem_name,
-					void **client_context);
-	int (*get_pages)(unsigned long addr,
-			  size_t size, int write, int force,
-			  struct sg_table *sg_head,
-			  void *client_context, void *core_context);
-	int (*dma_map)(struct sg_table *sg_head, void *client_context,
-			struct device *dma_device, int dmasync, int *nmap);
-	int (*dma_unmap)(struct sg_table *sg_head, void *client_context,
-			   struct device  *dma_device);
-	void (*put_pages)(struct sg_table *sg_head, void *client_context);
-	unsigned long (*get_page_size)(void *client_context);
-	void (*release)(void *client_context);
-
-};
-
-typedef int (*invalidate_peer_memory)(void *reg_handle,
-					  void *core_context);
-
-void *ib_register_peer_memory_client(struct peer_memory_client *peer_client,
-				  invalidate_peer_memory *invalidate_callback);
-void ib_unregister_peer_memory_client(void *reg_handle);
-
-
-/*------------------- PeerDirect bridge driver ------------------------------*/
-
-#define AMD_PEER_BRIDGE_DRIVER_VERSION	"1.0"
-#define AMD_PEER_BRIDGE_DRIVER_NAME	"amdkfd"
-
-
-static void* (*pfn_ib_register_peer_memory_client)(struct peer_memory_client
-							*peer_client,
-					invalidate_peer_memory
-							*invalidate_callback);
-
-static void (*pfn_ib_unregister_peer_memory_client)(void *reg_handle);
-
-static const struct amd_rdma_interface *rdma_interface;
-
-static invalidate_peer_memory ib_invalidate_callback;
-static void *ib_reg_handle;
-
-struct amd_mem_context {
-	uint64_t	va;
-	uint64_t	size;
-	struct pid	*pid;
-
-	struct amd_p2p_info  *p2p_info;
-
-	/* Flag that free callback was called */
-	int free_callback_called;
-
-	/* Context received from PeerDirect call */
-	void *core_context;
-};
-
-
-static void free_callback(void *client_priv)
-{
-	struct amd_mem_context *mem_context =
-		(struct amd_mem_context *)client_priv;
-
-	pr_debug("free_callback: data 0x%p\n", mem_context);
-
-	if (!mem_context) {
-		pr_warn("free_callback: Invalid client context\n");
-		return;
-	}
-
-	pr_debug("mem_context->core_context 0x%p\n", mem_context->core_context);
-
-	/* Call back IB stack asking to invalidate memory */
-	(*ib_invalidate_callback) (ib_reg_handle, mem_context->core_context);
-
-	/* amdkfd will free resources when we return from this callback.
-	 * Set flag to inform that there is nothing to do on "put_pages", etc.
-	 */
-	ACCESS_ONCE(mem_context->free_callback_called) = 1;
-}
-
-
-static int amd_acquire(unsigned long addr, size_t size,
-			void *peer_mem_private_data,
-			char *peer_mem_name, void **client_context)
-{
-	int ret;
-	struct amd_mem_context *mem_context;
-	struct pid *pid;
-
-	/* Get pointer to structure describing current process */
-	pid = get_task_pid(current, PIDTYPE_PID);
-
-	pr_debug("acquire: addr:0x%lx,size:0x%x, pid 0x%p\n",
-					addr, (unsigned int)size, pid);
-
-	/* Check if address is handled by AMD GPU driver */
-	ret = rdma_interface->is_gpu_address(addr, pid);
-
-	if (!ret) {
-		pr_debug("acquire: Not GPU Address\n");
-		/* This is not GPU address */
-		return 0;
-	}
-
-	pr_debug("acquire: GPU address\n");
-
-	/* Initialize context used for operation with given address */
-	mem_context = kzalloc(sizeof(struct amd_mem_context), GFP_KERNEL);
-
-	if (!mem_context)
-		return 0;	/* Error case handled as not GPU address  */
-
-	mem_context->free_callback_called = 0;
-	mem_context->va   = addr;
-	mem_context->size = size;
-
-	/* Save PID. It is guaranteed that the function will be
-	 * called in the correct process context as opposite to others.
-	 */
-	mem_context->pid  = pid;
-
-	pr_debug("acquire: Client context %p\n", mem_context);
-
-	/* Return pointer to allocated context */
-	*client_context = mem_context;
-
-	/* Return 1 to inform that this address which will be handled
-	 * by AMD GPU driver
-	 */
-	return 1;
-}
-
-static int amd_get_pages(unsigned long addr, size_t size, int write, int force,
-			  struct sg_table *sg_head,
-			  void *client_context, void *core_context)
-{
-	int ret;
-	struct amd_mem_context *mem_context =
-		(struct amd_mem_context *)client_context;
-
-	pr_debug("get_pages: addr:0x%lx,size:0x%x, core_context:%p\n",
-		addr, (unsigned int)size, core_context);
-
-	if (!mem_context) {
-		pr_warn("get_pages: Invalid client context");
-		return -EINVAL;
-	}
-
-	pr_debug("get_pages: pid :0x%p\n", mem_context->pid);
-
-
-	if (addr != mem_context->va) {
-		pr_warn("get_pages: Context address (0x%llx) is not the same\n",
-			mem_context->va);
-		return -EINVAL;
-	}
-
-	if (size != mem_context->size) {
-		pr_warn("get_pages: Context size (0x%llx) is not the same\n",
-			mem_context->size);
-		return -EINVAL;
-	}
-
-	ret = rdma_interface->get_pages(addr,
-					size,
-					mem_context->pid,
-					&mem_context->p2p_info,
-					free_callback,
-					mem_context);
-
-	if (ret || !mem_context->p2p_info) {
-		pr_err("Could not rdma::get_pages failure: %d\n", ret);
-		return ret;
-	}
-
-	mem_context->core_context = core_context;
-
-	/* Note: At this stage it is OK not to fill sg_table */
-	return 0;
-}
-
-
-static int amd_dma_map(struct sg_table *sg_head, void *client_context,
-			struct device *dma_device, int dmasync, int *nmap)
-{
-	/*
-	 * NOTE/TODO:
-	 * We could have potentially three cases for real memory
-	 *	location:
-	 *		- all memory in the local
-	 *		- all memory in the system (RAM)
-	 *		- memory is spread (s/g) between local and system.
-	 *
-	 *	In the case of all memory in the system we could use
-	 *	iommu driver to build DMA addresses but not in the case
-	 *	of local memory because currently iommu driver doesn't
-	 *	deal with local/device memory addresses (it requires "struct
-	 *	page").
-	 *
-	 *	Accordingly returning assumes that iommu funcutionality
-	 *	should be disabled so we can assume that sg_table already
-	 *	contains DMA addresses.
-	 *
-	 */
-	struct amd_mem_context *mem_context =
-		(struct amd_mem_context *)client_context;
-
-	pr_debug("dma_map: Context 0x%p, sg_head 0x%p\n",
-			client_context, sg_head);
-
-	pr_debug("dma_map: pid 0x%p, address 0x%llx, size:0x%llx\n",
-			mem_context->pid,
-			mem_context->va,
-			mem_context->size);
-
-	if (!mem_context->p2p_info) {
-		pr_err("dma_map: No sg table were allocated\n");
-		return -EINVAL;
-	}
-
-	/* Copy information about previosly allocated sg_table */
-	*sg_head = *mem_context->p2p_info->pages;
-
-	/* Return number of pages */
-	*nmap = mem_context->p2p_info->pages->nents;
-
-	return 0;
-}
-
-static int amd_dma_unmap(struct sg_table *sg_head, void *client_context,
-			   struct device  *dma_device)
-{
-	struct amd_mem_context *mem_context =
-		(struct amd_mem_context *)client_context;
-
-	pr_debug("dma_unmap: Context 0x%p, sg_table 0x%p\n",
-			client_context, sg_head);
-
-	pr_debug("dma_unmap: pid 0x%p, address 0x%llx, size:0x%llx\n",
-			mem_context->pid,
-			mem_context->va,
-			mem_context->size);
-
-	/* Assume success */
-	return 0;
-}
-static void amd_put_pages(struct sg_table *sg_head, void *client_context)
-{
-	int ret = 0;
-	struct amd_mem_context *mem_context =
-		(struct amd_mem_context *)client_context;
-
-	pr_debug("put_pages: sg_head %p client_context: 0x%p\n",
-			sg_head, client_context);
-	pr_debug("put_pages: pid 0x%p, address 0x%llx, size:0x%llx\n",
-			mem_context->pid,
-			mem_context->va,
-			mem_context->size);
-
-	pr_debug("put_pages: mem_context->p2p_info %p\n",
-				mem_context->p2p_info);
-
-	if (ACCESS_ONCE(mem_context->free_callback_called)) {
-		pr_debug("put_pages: free callback was called\n");
-		return;
-	}
-
-	if (mem_context->p2p_info) {
-		ret = rdma_interface->put_pages(&mem_context->p2p_info);
-		mem_context->p2p_info = NULL;
-
-		if (ret)
-			pr_err("put_pages failure: %d (callback status %d)\n",
-					ret, mem_context->free_callback_called);
-	} else
-		pr_err("put_pages: Pointer to p2p info is null\n");
-}
-static unsigned long amd_get_page_size(void *client_context)
-{
-	unsigned long page_size;
-	int result;
-	struct amd_mem_context *mem_context =
-		(struct amd_mem_context *)client_context;
-
-	pr_debug("get_page_size: context: %p\n", client_context);
-	pr_debug("get_page_size: pid 0x%p, address 0x%llx, size:0x%llx\n",
-			mem_context->pid,
-			mem_context->va,
-			mem_context->size);
-
-
-	result = rdma_interface->get_page_size(
-				mem_context->va,
-				mem_context->size,
-				mem_context->pid,
-				&page_size);
-
-	if (result) {
-		pr_err("Could not get page size. %d\n", result);
-		/* If we failed to get page size then do not know what to do.
-		 * Let's return some default value
-		 */
-		return PAGE_SIZE;
-	}
-
-	return page_size;
-}
-
-static void amd_release(void *client_context)
-{
-	struct amd_mem_context *mem_context =
-		(struct amd_mem_context *)client_context;
-
-	pr_debug("release: context: 0x%p\n", client_context);
-	pr_debug("release: pid 0x%p, address 0x%llx, size:0x%llx\n",
-			mem_context->pid,
-			mem_context->va,
-			mem_context->size);
-
-	kfree(mem_context);
-}
-
-
-static struct peer_memory_client amd_mem_client = {
-	.acquire = amd_acquire,
-	.get_pages = amd_get_pages,
-	.dma_map = amd_dma_map,
-	.dma_unmap = amd_dma_unmap,
-	.put_pages = amd_put_pages,
-	.get_page_size = amd_get_page_size,
-	.release = amd_release,
-};
-
-/** Initialize PeerDirect interface with RDMA Network stack.
- *
- *  Because network stack could potentially be loaded later we check
- *  presence of PeerDirect when HSA process is created. If PeerDirect was
- *  already initialized we do nothing otherwise try to detect and register.
- */
-void kfd_init_peer_direct(void)
-{
-	int result;
-
-	if (pfn_ib_unregister_peer_memory_client) {
-		pr_debug("PeerDirect support was already initialized\n");
-		return;
-	}
-
-	pr_debug("Try to initialize PeerDirect support\n");
-
-	pfn_ib_register_peer_memory_client =
-		(void *(*)(struct peer_memory_client *,
-			  invalidate_peer_memory *))
-		symbol_request(ib_register_peer_memory_client);
-
-	pfn_ib_unregister_peer_memory_client = (void (*)(void *))
-		symbol_request(ib_unregister_peer_memory_client);
-
-	if (!pfn_ib_register_peer_memory_client ||
-		!pfn_ib_unregister_peer_memory_client) {
-		pr_debug("amdkfd: PeerDirect interface was not detected\n");
-		/* Do cleanup */
-		kfd_close_peer_direct();
-		return;
-	}
-
-	result = amdkfd_query_rdma_interface(&rdma_interface);
-
-	if (result < 0) {
-		pr_err("amdkfd: Cannot get RDMA Interface (result = %d)\n",
-				result);
-		return;
-	}
-
-	strcpy(amd_mem_client.name,    AMD_PEER_BRIDGE_DRIVER_NAME);
-	strcpy(amd_mem_client.version, AMD_PEER_BRIDGE_DRIVER_VERSION);
-
-	ib_reg_handle = pfn_ib_register_peer_memory_client(&amd_mem_client,
-						&ib_invalidate_callback);
-
-	if (!ib_reg_handle) {
-		pr_err("amdkfd: Cannot register peer memory client\n");
-		/* Do cleanup */
-		kfd_close_peer_direct();
-		return;
-	}
-
-	pr_info("amdkfd: PeerDirect support was initialized successfully\n");
-	return;
-}
-
-/**
- * Close connection with PeerDirect interface with RDMA Network stack.
- *
- */
-void kfd_close_peer_direct(void)
-{
-	if (pfn_ib_unregister_peer_memory_client) {
-		if (ib_reg_handle)
-			pfn_ib_unregister_peer_memory_client(ib_reg_handle);
-
-		symbol_put(ib_unregister_peer_memory_client);
-	}
-
-	if (pfn_ib_register_peer_memory_client)
-		symbol_put(ib_register_peer_memory_client);
-
-
-	/* Reset pointers to be safe */
-	pfn_ib_unregister_peer_memory_client = NULL;
-	pfn_ib_register_peer_memory_client   = NULL;
-	ib_reg_handle = NULL;
-}
-
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers.h b/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers.h
index 058ba1b..5b393f3 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers.h
@@ -127,8 +127,7 @@ struct pm4_runlist {
 			uint32_t offload_polling:1;
 			uint32_t reserved3:1;
 			uint32_t valid:1;
-			uint32_t process_cnt:4;
-			uint32_t reserved4:4;
+			uint32_t reserved4:8;
 		} bitfields4;
 		uint32_t ordinal4;
 	};
@@ -187,123 +186,6 @@ struct pm4_map_process {
 };
 #endif
 
-/*--------------------MES_MAP_PROCESS_SCRATCH-------------------- */
-
-#ifndef PM4_MES_MAP_PROCESS_SCRATCH_DEFINED
-#define PM4_MES_MAP_PROCESS_SCRATCH_DEFINED
-
-struct pm4_map_process_scratch {
-	union {
-		union PM4_MES_TYPE_3_HEADER header;	/* header */
-		uint32_t ordinal1;
-	};
-
-	union {
-		struct {
-			uint32_t pasid:16;
-			uint32_t reserved1:8;
-			uint32_t diq_enable:1;
-			uint32_t process_quantum:7;
-		} bitfields2;
-		uint32_t ordinal2;
-	};
-
-	union {
-		struct {
-			uint32_t page_table_base:28;
-			uint32_t reserved3:4;
-		} bitfields3;
-		uint32_t ordinal3;
-	};
-
-	uint32_t reserved;
-
-	uint32_t sh_mem_bases;
-	uint32_t sh_mem_config;
-	uint32_t sh_mem_ape1_base;
-	uint32_t sh_mem_ape1_limit;
-
-	uint32_t sh_hidden_private_base_vmid;
-
-	uint32_t reserved2;
-	uint32_t reserved3;
-
-	uint32_t gds_addr_lo;
-	uint32_t gds_addr_hi;
-
-	union {
-		struct {
-			uint32_t num_gws:6;
-			uint32_t reserved4:2;
-			uint32_t num_oac:4;
-			uint32_t reserved5:4;
-			uint32_t gds_size:6;
-			uint32_t num_queues:10;
-		} bitfields10;
-		uint32_t ordinal10;
-	};
-
-	uint32_t completion_signal_lo;
-	uint32_t completion_signal_hi;
-
-};
-#endif
-
-#ifndef PM4_MES_MAP_PROCESS_DEFINED_KV_SCRATCH
-#define PM4_MES_MAP_PROCESS_DEFINED_KV_SCRATCH
-
-struct pm4_map_process_scratch_kv {
-	union {
-		union PM4_MES_TYPE_3_HEADER   header; /* header */
-		uint32_t            ordinal1;
-	};
-
-	union {
-		struct {
-			uint32_t pasid:16;
-			uint32_t reserved1:8;
-			uint32_t diq_enable:1;
-			uint32_t process_quantum:7;
-		} bitfields2;
-		uint32_t ordinal2;
-	};
-
-	union {
-		struct {
-			uint32_t page_table_base:28;
-			uint32_t reserved2:4;
-		} bitfields3;
-		uint32_t ordinal3;
-	};
-
-	uint32_t reserved3;
-	uint32_t sh_mem_bases;
-	uint32_t sh_mem_config;
-	uint32_t sh_mem_ape1_base;
-	uint32_t sh_mem_ape1_limit;
-	uint32_t sh_hidden_private_base_vmid;
-	uint32_t reserved4;
-	uint32_t reserved5;
-	uint32_t gds_addr_lo;
-	uint32_t gds_addr_hi;
-
-	union {
-		struct {
-			uint32_t num_gws:6;
-			uint32_t reserved6:2;
-			uint32_t num_oac:4;
-			uint32_t reserved7:4;
-			uint32_t gds_size:6;
-			uint32_t num_queues:10;
-		} bitfields14;
-		uint32_t ordinal14;
-	};
-
-	uint32_t completion_signal_lo32;
-uint32_t completion_signal_hi32;
-};
-#endif
-
 /*--------------------MES_MAP_QUEUES--------------------*/
 
 #ifndef PM4_MES_MAP_QUEUES_DEFINED
@@ -518,101 +400,6 @@ struct pm4_unmap_queues {
 };
 #endif
 
-/*--------------------_RELEASE_MEM-------------------- */
-
-#ifndef PM4__RELEASE_MEM_DEFINED
-#define PM4__RELEASE_MEM_DEFINED
-enum RELEASE_MEM_event_index_enum {
-	event_index___release_mem__end_of_pipe = 5,
-	event_index___release_mem__shader_done = 6
-};
-
-enum RELEASE_MEM_cache_policy_enum {
-	cache_policy___release_mem__lru = 0,
-	cache_policy___release_mem__stream = 1,
-	cache_policy___release_mem__bypass = 2
-};
-
-enum RELEASE_MEM_dst_sel_enum {
-	dst_sel___release_mem__memory_controller = 0,
-	dst_sel___release_mem__tc_l2 = 1,
-	dst_sel___release_mem__queue_write_pointer_register = 2,
-	dst_sel___release_mem__queue_write_pointer_poll_mask_bit = 3
-};
-
-enum RELEASE_MEM_int_sel_enum {
-	int_sel___release_mem__none = 0,
-	int_sel___release_mem__send_interrupt_only = 1,
-	int_sel___release_mem__send_interrupt_after_write_confirm = 2,
-	int_sel___release_mem__send_data_after_write_confirm = 3
-};
-
-enum RELEASE_MEM_data_sel_enum {
-	data_sel___release_mem__none = 0,
-	data_sel___release_mem__send_32_bit_low = 1,
-	data_sel___release_mem__send_64_bit_data = 2,
-	data_sel___release_mem__send_gpu_clock_counter = 3,
-	data_sel___release_mem__send_cp_perfcounter_hi_lo = 4,
-	data_sel___release_mem__store_gds_data_to_memory = 5
-};
-
-struct pm4__release_mem {
-	union {
-		union PM4_MES_TYPE_3_HEADER header;     /*header */
-		unsigned int ordinal1;
-	};
-
-	union {
-		struct {
-			unsigned int event_type:6;
-			unsigned int reserved1:2;
-			enum RELEASE_MEM_event_index_enum event_index:4;
-			unsigned int tcl1_vol_action_ena:1;
-			unsigned int tc_vol_action_ena:1;
-			unsigned int reserved2:1;
-			unsigned int tc_wb_action_ena:1;
-			unsigned int tcl1_action_ena:1;
-			unsigned int tc_action_ena:1;
-			unsigned int reserved3:6;
-			unsigned int atc:1;
-			enum RELEASE_MEM_cache_policy_enum cache_policy:2;
-			unsigned int reserved4:5;
-		} bitfields2;
-		unsigned int ordinal2;
-	};
-
-	union {
-		struct {
-			unsigned int reserved5:16;
-			enum RELEASE_MEM_dst_sel_enum dst_sel:2;
-			unsigned int reserved6:6;
-			enum RELEASE_MEM_int_sel_enum int_sel:3;
-			unsigned int reserved7:2;
-			enum RELEASE_MEM_data_sel_enum data_sel:3;
-		} bitfields3;
-		unsigned int ordinal3;
-	};
-
-	union {
-		struct {
-			unsigned int reserved8:2;
-			unsigned int address_lo_32b:30;
-		} bitfields4;
-		struct {
-			unsigned int reserved9:3;
-			unsigned int address_lo_64b:29;
-		} bitfields5;
-		unsigned int ordinal4;
-	};
-
-	unsigned int address_hi;
-
-	unsigned int data_lo;
-
-	unsigned int data_hi;
-};
-#endif
-
 enum {
 	CACHE_FLUSH_AND_INV_TS_EVENT = 0x00000014
 };
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h b/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h
index 0b314a8..a0ff348 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h
@@ -77,6 +77,103 @@ struct pm4__indirect_buffer_pasid {
 
 #endif
 
+/*--------------------_RELEASE_MEM-------------------- */
+
+#ifndef _PM4__RELEASE_MEM_DEFINED
+#define _PM4__RELEASE_MEM_DEFINED
+enum _RELEASE_MEM_event_index_enum {
+	event_index___release_mem__end_of_pipe = 5,
+	event_index___release_mem__shader_done = 6
+};
+
+enum _RELEASE_MEM_cache_policy_enum {
+	cache_policy___release_mem__lru = 0,
+	cache_policy___release_mem__stream = 1,
+	cache_policy___release_mem__bypass = 2
+};
+
+enum _RELEASE_MEM_dst_sel_enum {
+	dst_sel___release_mem__memory_controller = 0,
+	dst_sel___release_mem__tc_l2 = 1,
+	dst_sel___release_mem__queue_write_pointer_register = 2,
+	dst_sel___release_mem__queue_write_pointer_poll_mask_bit = 3
+};
+
+enum _RELEASE_MEM_int_sel_enum {
+	int_sel___release_mem__none = 0,
+	int_sel___release_mem__send_interrupt_only = 1,
+	int_sel___release_mem__send_interrupt_after_write_confirm = 2,
+	int_sel___release_mem__send_data_after_write_confirm = 3
+};
+
+enum _RELEASE_MEM_data_sel_enum {
+	data_sel___release_mem__none = 0,
+	data_sel___release_mem__send_32_bit_low = 1,
+	data_sel___release_mem__send_64_bit_data = 2,
+	data_sel___release_mem__send_gpu_clock_counter = 3,
+	data_sel___release_mem__send_cp_perfcounter_hi_lo = 4,
+	data_sel___release_mem__store_gds_data_to_memory = 5
+};
+
+struct pm4__release_mem {
+	union {
+		union PM4_MES_TYPE_3_HEADER header;	/*header */
+		unsigned int ordinal1;
+	};
+
+	union {
+		struct {
+			unsigned int event_type:6;
+			unsigned int reserved1:2;
+			enum _RELEASE_MEM_event_index_enum event_index:4;
+			unsigned int tcl1_vol_action_ena:1;
+			unsigned int tc_vol_action_ena:1;
+			unsigned int reserved2:1;
+			unsigned int tc_wb_action_ena:1;
+			unsigned int tcl1_action_ena:1;
+			unsigned int tc_action_ena:1;
+			unsigned int reserved3:6;
+			unsigned int atc:1;
+			enum _RELEASE_MEM_cache_policy_enum cache_policy:2;
+			unsigned int reserved4:5;
+		} bitfields2;
+		unsigned int ordinal2;
+	};
+
+	union {
+		struct {
+			unsigned int reserved5:16;
+			enum _RELEASE_MEM_dst_sel_enum dst_sel:2;
+			unsigned int reserved6:6;
+			enum _RELEASE_MEM_int_sel_enum int_sel:3;
+			unsigned int reserved7:2;
+			enum _RELEASE_MEM_data_sel_enum data_sel:3;
+		} bitfields3;
+		unsigned int ordinal3;
+	};
+
+	union {
+		struct {
+			unsigned int reserved8:2;
+			unsigned int address_lo_32b:30;
+		} bitfields4;
+		struct {
+			unsigned int reserved9:3;
+			unsigned int address_lo_64b:29;
+		} bitfields5;
+		unsigned int ordinal4;
+	};
+
+	unsigned int address_hi;
+
+	unsigned int data_lo;
+
+	unsigned int data_hi;
+
+};
+#endif
+
+
 /*--------------------_SET_CONFIG_REG-------------------- */
 
 #ifndef _PM4__SET_CONFIG_REG_DEFINED
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_priv.h b/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
index 207b5bc..4750cab 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
@@ -30,48 +30,13 @@
 #include <linux/atomic.h>
 #include <linux/workqueue.h>
 #include <linux/spinlock.h>
-#include <linux/idr.h>
 #include <linux/kfd_ioctl.h>
-#include <linux/pid.h>
-#include <linux/interval_tree.h>
-#include <linux/seq_file.h>
-#include <linux/kref.h>
-#include <linux/kfifo.h>
 #include <kgd_kfd_interface.h>
 
-#include "amd_rdma.h"
-
 #define KFD_SYSFS_FILE_MODE 0444
 
-/* GPU ID hash width in bits */
-#define KFD_GPU_ID_HASH_WIDTH 16
-
-/* Use upper bits of mmap offset to store KFD driver specific information.
- * BITS[63:62] - Encode MMAP type
- * BITS[61:46] - Encode gpu_id. To identify to which GPU the offset belongs to
- * BITS[45:40] - Reserved. Not Used.
- * BITS[39:0]  - MMAP offset value. Used by TTM.
- *
- * NOTE: struct vm_area_struct.vm_pgoff uses offset in pages. Hence, these
- *  defines are w.r.t to PAGE_SIZE
- */
-#define KFD_MMAP_TYPE_SHIFT	(62 - PAGE_SHIFT)
-#define KFD_MMAP_TYPE_MASK	(0x3ULL << KFD_MMAP_TYPE_SHIFT)
-#define KFD_MMAP_TYPE_DOORBELL	(0x3ULL << KFD_MMAP_TYPE_SHIFT)
-#define KFD_MMAP_TYPE_EVENTS	(0x2ULL << KFD_MMAP_TYPE_SHIFT)
-#define KFD_MMAP_TYPE_MAP_BO	(0x1ULL << KFD_MMAP_TYPE_SHIFT)
-#define KFD_MMAP_TYPE_RESERVED_MEM	(0x0ULL << KFD_MMAP_TYPE_SHIFT)
-
-#define KFD_MMAP_GPU_ID_SHIFT (46 - PAGE_SHIFT)
-#define KFD_MMAP_GPU_ID_MASK (((1ULL << KFD_GPU_ID_HASH_WIDTH) - 1) \
-				<< KFD_MMAP_GPU_ID_SHIFT)
-#define KFD_MMAP_GPU_ID(gpu_id) ((((uint64_t)gpu_id) << KFD_MMAP_GPU_ID_SHIFT)\
-				& KFD_MMAP_GPU_ID_MASK)
-#define KFD_MMAP_GPU_ID_GET(offset)    ((offset & KFD_MMAP_GPU_ID_MASK) \
-				>> KFD_MMAP_GPU_ID_SHIFT)
-
-#define KFD_MMAP_OFFSET_VALUE_MASK	(0xFFFFFFFFFFULL >> PAGE_SHIFT)
-#define KFD_MMAP_OFFSET_VALUE_GET(offset) (offset & KFD_MMAP_OFFSET_VALUE_MASK)
+#define KFD_MMAP_DOORBELL_MASK 0x8000000000000
+#define KFD_MMAP_EVENTS_MASK 0x4000000000000
 
 /*
  * When working with cp scheduler we should assign the HIQ manually or via
@@ -83,6 +48,8 @@
 #define KFD_CIK_HIQ_PIPE 4
 #define KFD_CIK_HIQ_QUEUE 0
 
+/* GPU ID hash width in bits */
+#define KFD_GPU_ID_HASH_WIDTH 16
 
 /* Macro for allocating structures */
 #define kfd_alloc_struct(ptr_to_struct)	\
@@ -107,26 +74,12 @@ extern int max_num_of_queues_per_device;
 /* Kernel module parameter to specify the scheduling policy */
 extern int sched_policy;
 
-extern int cwsr_enable;
-
-/*
- * Kernel module parameter to specify the maximum process
- * number per HW scheduler
- */
-extern int hws_max_conc_proc;
-
 /*
  * Kernel module parameter to specify whether to send sigterm to HSA process on
  * unhandled exception
  */
 extern int send_sigterm;
 
-/*
- * This kernel module is used to simulate large bar machine on non-large bar
- * enabled machines.
- */
-extern int debug_largebar;
-
 /**
  * enum kfd_sched_policy
  *
@@ -161,23 +114,14 @@ enum cache_policy {
 
 enum asic_family_type {
 	CHIP_KAVERI = 0,
-	CHIP_HAWAII,
-	CHIP_CARRIZO,
-	CHIP_TONGA,
-	CHIP_FIJI,
-	CHIP_POLARIS10,
-	CHIP_POLARIS11
+	CHIP_CARRIZO
 };
 
-#define KFD_IS_VI(chip) ((chip) >= CHIP_CARRIZO && (chip) <= CHIP_POLARIS11)
-#define KFD_IS_DGPU(chip) (((chip) >= CHIP_TONGA && \
-			   (chip) <= CHIP_POLARIS11) || \
-			   (chip) == CHIP_HAWAII)
-
 struct kfd_event_interrupt_class {
-	bool (*interrupt_isr)(struct kfd_dev *dev, const uint32_t *ih_ring_entry,
-				uint32_t *patched_ihre, bool *patched_flag);
-	void (*interrupt_wq)(struct kfd_dev *dev, const uint32_t *ih_ring_entry);
+	bool (*interrupt_isr)(struct kfd_dev *dev,
+				const uint32_t *ih_ring_entry);
+	void (*interrupt_wq)(struct kfd_dev *dev,
+				const uint32_t *ih_ring_entry);
 };
 
 struct kfd_device_info {
@@ -188,8 +132,6 @@ struct kfd_device_info {
 	size_t ih_ring_entry_size;
 	uint8_t num_of_watch_points;
 	uint16_t mqd_size_aligned;
-	bool is_need_iommu_device;
-	bool supports_cwsr;
 };
 
 struct kfd_mem_obj {
@@ -199,12 +141,6 @@ struct kfd_mem_obj {
 	uint32_t *cpu_ptr;
 };
 
-struct kfd_vmid_info {
-	uint32_t first_vmid_kfd;
-	uint32_t last_vmid_kfd;
-	uint32_t vmid_num_kfd;
-};
-
 struct kfd_dev {
 	struct kgd_dev *kgd;
 
@@ -229,12 +165,11 @@ struct kfd_dev {
 					   */
 
 	struct kgd2kfd_shared_resources shared_resources;
-	struct kfd_vmid_info vm_info;
 
 	const struct kfd2kgd_calls *kfd2kgd;
 	struct mutex doorbell_mutex;
-	unsigned long doorbell_available_index[DIV_ROUND_UP(
-		KFD_MAX_NUM_OF_QUEUES_PER_PROCESS, BITS_PER_LONG)];
+	DECLARE_BITMAP(doorbell_available_index,
+			KFD_MAX_NUM_OF_QUEUES_PER_PROCESS);
 
 	void *gtt_mem;
 	uint64_t gtt_start_gpu_addr;
@@ -244,17 +179,18 @@ struct kfd_dev {
 	unsigned int gtt_sa_chunk_size;
 	unsigned int gtt_sa_num_of_chunks;
 
-	/* QCM Device instance */
-	struct device_queue_manager *dqm;
-
-	bool init_complete;
-
 	/* Interrupts */
-	struct kfifo ih_fifo;
-	struct workqueue_struct *ih_wq;
+	void *interrupt_ring;
+	size_t interrupt_ring_size;
+	atomic_t interrupt_ring_rptr;
+	atomic_t interrupt_ring_wptr;
 	struct work_struct interrupt_work;
 	spinlock_t interrupt_lock;
 
+	/* QCM Device instance */
+	struct device_queue_manager *dqm;
+
+	bool init_complete;
 	/*
 	 * Interrupts of interest to KFD are copied
 	 * from the HW ring into a SW ring.
@@ -262,32 +198,7 @@ struct kfd_dev {
 	bool interrupts_active;
 
 	/* Debug manager */
-	struct kfd_dbgmgr *dbgmgr;
-
-	/* MEC firmware version*/
-	uint16_t mec_fw_version;
-
-	/* Maximum process number mapped to HW scheduler */
-	unsigned int max_proc_per_quantum;
-
-	/* cwsr */
-	bool cwsr_enabled;
-	struct page *cwsr_pages;
-	uint32_t cwsr_size;
-	uint32_t tma_offset;  /*Offset for TMA from the  start of cwsr_mem*/
-
-	/* IB usage */
-	uint32_t ib_size;
-};
-
-struct kfd_ipc_obj;
-
-struct kfd_bo {
-	void *mem;
-	struct interval_tree_node it;
-	struct kfd_dev *dev;
-	struct list_head cb_data_head;
-	struct kfd_ipc_obj *kfd_ipc_obj;
+	struct kfd_dbgmgr           *dbgmgr;
 };
 
 /* KGD2KFD callbacks */
@@ -310,22 +221,27 @@ void kfd_chardev_exit(void);
 struct device *kfd_chardev(void);
 
 /**
- * enum kfd_unmap_queues_filter
+ * enum kfd_preempt_type_filter
  *
- * @KFD_UNMAP_QUEUES_FILTER_SINGLE_QUEUE: Preempts single queue.
+ * @KFD_PREEMPT_TYPE_FILTER_SINGLE_QUEUE: Preempts single queue.
  *
- * @KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES: Preempts all queues in the
+ * @KFD_PRERMPT_TYPE_FILTER_ALL_QUEUES: Preempts all queues in the
  *						running queues list.
  *
- * @KFD_UNMAP_QUEUES_FILTER_BY_PASID: Preempts queues that belongs to
+ * @KFD_PRERMPT_TYPE_FILTER_BY_PASID: Preempts queues that belongs to
  *						specific process.
  *
  */
-enum kfd_unmap_queues_filter {
-	KFD_UNMAP_QUEUES_FILTER_SINGLE_QUEUE,
-	KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES,
-	KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES,
-	KFD_UNMAP_QUEUES_FILTER_BY_PASID
+enum kfd_preempt_type_filter {
+	KFD_PREEMPT_TYPE_FILTER_SINGLE_QUEUE,
+	KFD_PREEMPT_TYPE_FILTER_ALL_QUEUES,
+	KFD_PREEMPT_TYPE_FILTER_DYNAMIC_QUEUES,
+	KFD_PREEMPT_TYPE_FILTER_BY_PASID
+};
+
+enum kfd_preempt_type {
+	KFD_PREEMPT_TYPE_WAVEFRONT,
+	KFD_PREEMPT_TYPE_WAVEFRONT_RESET
 };
 
 /**
@@ -408,7 +324,6 @@ struct queue_properties {
 	uint32_t __iomem *doorbell_ptr;
 	uint32_t doorbell_off;
 	bool is_interop;
-	bool is_evicted; /* true -> queue is evicted */
 	bool is_active;
 	/* Not relevant for user mode queues in cp scheduling */
 	unsigned int vmid;
@@ -421,12 +336,6 @@ struct queue_properties {
 	uint32_t eop_ring_buffer_size;
 	uint64_t ctx_save_restore_area_address;
 	uint32_t ctx_save_restore_area_size;
-	uint32_t ctl_stack_size;
-	uint64_t tba_addr;
-	uint64_t tma_addr;
-	/* Relevant for CU */
-	uint32_t cu_mask_count; /* Must be a multiple of 32 */
-	uint32_t *cu_mask;
 };
 
 /**
@@ -515,7 +424,6 @@ struct qcm_process_device {
 	unsigned int queue_count;
 	unsigned int vmid;
 	bool is_debug;
-	unsigned evicted; /* eviction counter, 0=active */
 	/*
 	 * All the memory management data should be here too
 	 */
@@ -528,46 +436,6 @@ struct qcm_process_device {
 	uint32_t gds_size;
 	uint32_t num_gws;
 	uint32_t num_oac;
-	uint32_t sh_hidden_private_base;
-
-	/*cwsr memory*/
-	uint64_t cwsr_base;
-	uint64_t tba_addr;
-	uint64_t tma_addr;
-	void *cwsr_kaddr;
-
-	/* IB memory */
-	uint64_t ib_base; /* ib_base+ib_size must be below cwsr_base */
-	void *ib_kaddr;
-};
-
-/* KFD Memory Eviction */
-struct kfd_eviction_work {
-	struct work_struct work;
-	struct fence *eviction_fence;
-};
-
-/* Appox. wait time before attempting to restore evicted BOs */
-#define PROCESS_RESTORE_TIME_MS 2000
-/* Approx. back off time if restore fails due to lack of memory */
-#define PROCESS_BACK_OFF_TIME_MS 1000
-
-void kfd_evict_bo_worker(struct work_struct *work);
-void kfd_restore_bo_worker(struct work_struct *work);
-int kgd2kfd_schedule_evict_and_restore_process(struct mm_struct *mm,
-					       struct fence *fence);
-
-
-/*8 byte handle containing GPU ID in the most significant 4 bytes and
- * idr_handle in the least significant 4 bytes*/
-#define MAKE_HANDLE(gpu_id, idr_handle) (((uint64_t)(gpu_id) << 32) + idr_handle)
-#define GET_GPU_ID(handle) (handle >> 32)
-#define GET_IDR_HANDLE(handle) (handle & 0xFFFFFFFF)
-
-enum kfd_pdd_bound {
-	PDD_UNBOUND = 0,
-	PDD_BOUND,
-	PDD_BOUND_SUSPENDED,
 };
 
 /* Data that is per-process-per device. */
@@ -581,8 +449,6 @@ struct kfd_process_device {
 	/* The device that owns this data. */
 	struct kfd_dev *dev;
 
-	/* The process that owns this kfd_process_device. */
-	struct kfd_process *process;
 
 	/* per-process-per device QCM data structure */
 	struct qcm_process_device qpd;
@@ -594,31 +460,14 @@ struct kfd_process_device {
 	uint64_t gpuvm_limit;
 	uint64_t scratch_base;
 	uint64_t scratch_limit;
-	uint64_t dgpu_base;
-	uint64_t dgpu_limit;
-
-	uint64_t sh_hidden_private_base_vmid;
 
 	/* Is this process/pasid bound to this device? (amd_iommu_bind_pasid) */
-	enum kfd_pdd_bound bound;
-
-	/* VM context for GPUVM allocations */
-	void *vm;
-
-	/* GPUVM allocations storage */
-	struct idr alloc_idr;
+	bool bound;
 
 	/* This flag tells if we should reset all
 	 * wavefronts on process termination
 	 */
 	bool reset_wavefronts;
-
-	/* Flag used to tell the pdd has dequeued from the dqm.
-	 * This is used to prevent dev->dqm->ops.process_termination() from
-	 * being called twice when it is already called in IOMMU callback
-	 * function.
-	 */
-	bool already_dequeued;
 };
 
 #define qpd_to_pdd(x) container_of(x, struct kfd_process_device, qpd)
@@ -631,24 +480,14 @@ struct kfd_process {
 	 */
 	struct hlist_node kfd_processes;
 
-	/*
-	 * Opaque pointer to mm_struct. We don't hold a reference to
-	 * it so it should never be dereferenced from here. This is
-	 * only used for looking up processes by their mm.
-	 */
-	void *mm;
+	struct mm_struct *mm;
 
-	struct kref ref;
-	struct work_struct release_work;
-
-	struct rw_semaphore lock;
+	struct mutex mutex;
 
 	/*
 	 * In any process, the thread that started main() is the lead
 	 * thread and outlives the rest.
 	 * It is here because amd_iommu_bind_pasid wants a task_struct.
-	 * It can also be used for safely getting a reference to the
-	 * mm_struct of the process.
 	 */
 	struct task_struct *lead_thread;
 
@@ -668,7 +507,11 @@ struct kfd_process {
 
 	struct process_queue_manager pqm;
 
-	unsigned long allocated_queue_bitmap[DIV_ROUND_UP(KFD_MAX_NUM_OF_QUEUES_PER_PROCESS, BITS_PER_LONG)];
+	/* The process's queues. */
+	size_t queue_array_size;
+
+	/* Size is queue_array_size, up to MAX_PROCESS_QUEUES. */
+	struct kfd_queue **queues;
 
 	/*Is the user space process 32 bit?*/
 	bool is_32bit_user_mode;
@@ -677,18 +520,10 @@ struct kfd_process {
 	struct mutex event_mutex;
 	/* All events in process hashed by ID, linked on kfd_event.events. */
 	DECLARE_HASHTABLE(events, 4);
-	struct list_head signal_event_pages;	/* struct slot_page_header.event_pages */
+	struct list_head signal_event_pages;	/* struct slot_page_header.
+								event_pages */
 	u32 next_nonsignal_event_id;
 	size_t signal_event_count;
-	size_t debug_event_count;
-
-	struct rb_root bo_interval_tree;
-
-	void *master_vm;
-
-	/* Work items for evicting and restoring BOs */
-	struct kfd_eviction_work eviction_work;
-	struct delayed_work restore_work;
 };
 
 /**
@@ -711,46 +546,18 @@ struct amdkfd_ioctl_desc {
 
 void kfd_process_create_wq(void);
 void kfd_process_destroy_wq(void);
-struct kfd_process *kfd_create_process(struct file *filep);
+struct kfd_process *kfd_create_process(const struct task_struct *);
 struct kfd_process *kfd_get_process(const struct task_struct *);
 struct kfd_process *kfd_lookup_process_by_pasid(unsigned int pasid);
-struct kfd_process *kfd_lookup_process_by_mm(const struct mm_struct *mm);
-void kfd_unref_process(struct kfd_process *p);
 
 struct kfd_process_device *kfd_bind_process_to_device(struct kfd_dev *dev,
-						struct kfd_process *p);
-int kfd_bind_processes_to_device(struct kfd_dev *dev);
-void kfd_unbind_processes_from_device(struct kfd_dev *dev);
-void kfd_process_iommu_unbind_callback(struct kfd_dev *dev, unsigned int pasid);
+							struct kfd_process *p);
+void kfd_unbind_process_from_device(struct kfd_dev *dev, unsigned int pasid);
 struct kfd_process_device *kfd_get_process_device_data(struct kfd_dev *dev,
 							struct kfd_process *p);
 struct kfd_process_device *kfd_create_process_device_data(struct kfd_dev *dev,
 							struct kfd_process *p);
 
-int kfd_reserved_mem_mmap(struct kfd_process *process, struct vm_area_struct *vma);
-
-/* KFD process API for creating and translating handles */
-int kfd_process_device_create_obj_handle(struct kfd_process_device *pdd,
-					void *mem, uint64_t start,
-					uint64_t length,
-					struct kfd_ipc_obj *ipc_obj);
-void *kfd_process_device_translate_handle(struct kfd_process_device *p,
-					int handle);
-struct kfd_bo *kfd_process_device_find_bo(struct kfd_process_device *pdd,
-					int handle);
-void *kfd_process_find_bo_from_interval(struct kfd_process *p,
-					uint64_t start_addr,
-					uint64_t last_addr);
-void kfd_process_device_remove_obj_handle(struct kfd_process_device *pdd,
-					int handle);
-
-void run_rdma_free_callback(struct kfd_bo *buf_obj);
-struct kfd_process *kfd_lookup_process_by_pid(struct pid *pid);
-
-/* kfd dgpu memory */
-int kfd_map_memory_to_gpu(void *mem, struct kfd_process_device *pdd);
-int kfd_unmap_memory_from_gpu(void *mem, struct kfd_process_device *pdd);
-
 /* Process device data iterator */
 struct kfd_process_device *kfd_get_first_process_device_data(struct kfd_process *p);
 struct kfd_process_device *kfd_get_next_process_device_data(struct kfd_process *p,
@@ -766,7 +573,6 @@ unsigned int kfd_pasid_alloc(void);
 void kfd_pasid_free(unsigned int pasid);
 
 /* Doorbells */
-size_t kfd_doorbell_process_slice(void);
 void kfd_doorbell_init(struct kfd_dev *kfd);
 int kfd_doorbell_mmap(struct kfd_process *process, struct vm_area_struct *vma);
 u32 __iomem *kfd_get_kernel_doorbell(struct kfd_dev *kfd,
@@ -794,20 +600,14 @@ int kfd_topology_add_device(struct kfd_dev *gpu);
 int kfd_topology_remove_device(struct kfd_dev *gpu);
 struct kfd_dev *kfd_device_by_id(uint32_t gpu_id);
 struct kfd_dev *kfd_device_by_pci_dev(const struct pci_dev *pdev);
-struct kfd_dev *kfd_device_by_kgd(const struct kgd_dev *kgd);
-uint32_t kfd_get_gpu_id(struct kfd_dev *dev);
-int kfd_topology_enum_kfd_devices(uint8_t idx, struct kfd_dev **kdev);
-int kfd_numa_node_to_apic_id(int numa_node_id);
-int kfd_get_proximity_domain(const struct pci_bus *bus);
+struct kfd_dev *kfd_topology_enum_kfd_devices(uint8_t idx);
 
 /* Interrupts */
 int kfd_interrupt_init(struct kfd_dev *dev);
 void kfd_interrupt_exit(struct kfd_dev *dev);
 void kgd2kfd_interrupt(struct kfd_dev *kfd, const void *ih_ring_entry);
 bool enqueue_ih_ring_entry(struct kfd_dev *kfd,	const void *ih_ring_entry);
-bool interrupt_is_wanted(struct kfd_dev *dev,
-				const uint32_t *ih_ring_entry,
-				uint32_t *patched_ihre, bool *flag);
+bool interrupt_is_wanted(struct kfd_dev *dev, const uint32_t *ih_ring_entry);
 
 /* Power Management */
 void kgd2kfd_suspend(struct kfd_dev *kfd);
@@ -815,12 +615,9 @@ int kgd2kfd_resume(struct kfd_dev *kfd);
 
 /* amdkfd Apertures */
 int kfd_init_apertures(struct kfd_process *process);
-int kfd_set_process_dgpu_aperture(struct kfd_process_device *pdd,
-				uint64_t base, uint64_t limit);
 
 /* Queue Context Management */
-inline uint32_t lower_32(uint64_t x);
-inline uint32_t upper_32(uint64_t x);
+struct cik_sdma_rlc_registers *get_sdma_mqd(void *mqd);
 
 int init_queue(struct queue **q, const struct queue_properties *properties);
 void uninit_queue(struct queue *q);
@@ -831,19 +628,13 @@ struct mqd_manager *mqd_manager_init(enum KFD_MQD_TYPE type,
 					struct kfd_dev *dev);
 struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 		struct kfd_dev *dev);
-struct mqd_manager *mqd_manager_init_cik_hawaii(enum KFD_MQD_TYPE type,
-		struct kfd_dev *dev);
 struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 		struct kfd_dev *dev);
-struct mqd_manager *mqd_manager_init_vi_tonga(enum KFD_MQD_TYPE type,
-		struct kfd_dev *dev);
 struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev);
 void device_queue_manager_uninit(struct device_queue_manager *dqm);
 struct kernel_queue *kernel_queue_init(struct kfd_dev *dev,
 					enum kfd_queue_type type);
 void kernel_queue_uninit(struct kernel_queue *kq);
-int kfd_process_vm_fault(struct device_queue_manager *dqm,
-				unsigned int pasid);
 
 /* Process Queue Manager */
 struct process_queue_node {
@@ -852,31 +643,32 @@ struct process_queue_node {
 	struct list_head process_queue_list;
 };
 
-void kfd_process_dequeue_from_device(struct kfd_process_device *pdd);
-void kfd_process_dequeue_from_all_devices(struct kfd_process *p);
 int pqm_init(struct process_queue_manager *pqm, struct kfd_process *p);
 void pqm_uninit(struct process_queue_manager *pqm);
 int pqm_create_queue(struct process_queue_manager *pqm,
 			    struct kfd_dev *dev,
 			    struct file *f,
 			    struct queue_properties *properties,
+			    unsigned int flags,
+			    enum kfd_queue_type type,
 			    unsigned int *qid);
 int pqm_destroy_queue(struct process_queue_manager *pqm, unsigned int qid);
 int pqm_update_queue(struct process_queue_manager *pqm, unsigned int qid,
 			struct queue_properties *p);
-int pqm_set_cu_mask(struct process_queue_manager *pqm, unsigned int qid,
-			struct queue_properties *p);
 struct kernel_queue *pqm_get_kernel_queue(struct process_queue_manager *pqm,
 						unsigned int qid);
-int kgd2kfd_quiesce_mm(struct kfd_dev *kfd, struct mm_struct *mm);
-int kgd2kfd_resume_mm(struct kfd_dev *kfd, struct mm_struct *mm);
+
+int amdkfd_fence_wait_timeout(unsigned int *fence_addr,
+				unsigned int fence_value,
+				unsigned long timeout);
 
 /* Packet Manager */
 
+#define KFD_HIQ_TIMEOUT (500)
+
 #define KFD_FENCE_COMPLETED (100)
 #define KFD_FENCE_INIT   (10)
-
-struct packet_manager_firmware;
+#define KFD_UNMAP_LATENCY (150)
 
 struct packet_manager {
 	struct device_queue_manager *dqm;
@@ -884,21 +676,9 @@ struct packet_manager {
 	struct mutex lock;
 	bool allocated;
 	struct kfd_mem_obj *ib_buffer_obj;
-	unsigned ib_size_bytes;
-
-	struct packet_manager_firmware *pmf;
 };
 
-struct packet_manager_firmware {
-	/* Support different firmware versions for map process packet */
-	int (*map_process)(struct packet_manager *pm, uint32_t *buffer,
-				struct qcm_process_device *qpd);
-	int (*get_map_process_packet_size)(void);
-};
-
-uint32_t pm_create_release_mem(uint64_t gpu_addr, uint32_t *buffer);
-int pm_init(struct packet_manager *pm, struct device_queue_manager *dqm,
-		uint16_t fw_ver);
+int pm_init(struct packet_manager *pm, struct device_queue_manager *dqm);
 void pm_uninit(struct packet_manager *pm);
 int pm_send_set_resources(struct packet_manager *pm,
 				struct scheduling_resources *res);
@@ -907,7 +687,7 @@ int pm_send_query_status(struct packet_manager *pm, uint64_t fence_address,
 				uint32_t fence_value);
 
 int pm_send_unmap_queue(struct packet_manager *pm, enum kfd_queue_type type,
-			enum kfd_unmap_queues_filter mode,
+			enum kfd_preempt_type_filter mode,
 			uint32_t filter_param, bool reset,
 			unsigned int sdma_engine);
 
@@ -916,9 +696,6 @@ void pm_release_ib(struct packet_manager *pm);
 uint64_t kfd_get_number_elems(struct kfd_dev *kfd);
 phys_addr_t kfd_get_process_doorbells(struct kfd_dev *dev,
 					struct kfd_process *process);
-int amdkfd_fence_wait_timeout(unsigned int *fence_addr,
-				unsigned int fence_value,
-				unsigned long timeout_ms);
 
 /* Events */
 extern const struct kfd_event_interrupt_class event_interrupt_class_cik;
@@ -937,7 +714,8 @@ int kfd_wait_on_events(struct kfd_process *p,
 		       uint32_t num_events, void __user *data,
 		       bool all, uint32_t user_timeout_ms,
 		       enum kfd_event_wait_result *wait_result);
-void kfd_signal_event_interrupt(unsigned int pasid, uint32_t partial_id, uint32_t valid_id_bits);
+void kfd_signal_event_interrupt(unsigned int pasid, uint32_t partial_id,
+				uint32_t valid_id_bits);
 void kfd_signal_iommu_event(struct kfd_dev *dev,
 		unsigned int pasid, unsigned long address,
 		bool is_write_requested, bool is_execute_requested);
@@ -945,49 +723,11 @@ void kfd_signal_hw_exception_event(unsigned int pasid);
 int kfd_set_event(struct kfd_process *p, uint32_t event_id);
 int kfd_reset_event(struct kfd_process *p, uint32_t event_id);
 int kfd_event_create(struct file *devkfd, struct kfd_process *p,
-	     uint32_t event_type, bool auto_reset, uint32_t node_id,
-	     uint32_t *event_id, uint32_t *event_trigger_data,
-	     uint64_t *event_page_offset, uint32_t *event_slot_index,
-	     void *kern_addr);
+		     uint32_t event_type, bool auto_reset, uint32_t node_id,
+		     uint32_t *event_id, uint32_t *event_trigger_data,
+		     uint64_t *event_page_offset, uint32_t *event_slot_index);
 int kfd_event_destroy(struct kfd_process *p, uint32_t event_id);
-void kfd_free_signal_page_dgpu(struct kfd_process *p, uint64_t handle);
-
-void kfd_signal_vm_fault_event(struct kfd_dev *dev, unsigned int pasid,
-				struct kfd_vm_fault_info *info);
-
-void kfd_flush_tlb(struct kfd_dev *dev, uint32_t pasid);
 
 int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p);
 
-#define KFD_SCRATCH_CZ_FW_VER 600
-#define KFD_SCRATCH_KV_FW_VER 413
-#define KFD_MULTI_PROC_MAPPING_HWS_SUPPORT 600
-#define KFD_CWSR_CZ_FW_VER 625
-
-/* PeerDirect support */
-void kfd_init_peer_direct(void);
-void kfd_close_peer_direct(void);
-
-/* IPC Support */
-int kfd_ipc_init(void);
-
-/* Debugfs */
-#if defined(CONFIG_DEBUG_FS)
-
-void kfd_debugfs_init(void);
-void kfd_debugfs_fini(void);
-int kfd_debugfs_mqds_by_process(struct seq_file *m, void *data);
-int pqm_debugfs_mqds(struct seq_file *m, void *data);
-int kfd_debugfs_hqds_by_device(struct seq_file *m, void *data);
-int device_queue_manager_debugfs_hqds(struct seq_file *m, void *data);
-int kfd_debugfs_rls_by_device(struct seq_file *m, void *data);
-int pm_debugfs_runlist(struct seq_file *m, void *data);
-
-#else
-
-static inline void kfd_debugfs_init(void) {}
-static inline void kfd_debugfs_fini(void) {}
-
-#endif
-
 #endif
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process.c b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
index e908bee..ef7c8de 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_process.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
@@ -27,11 +27,6 @@
 #include <linux/amd-iommu.h>
 #include <linux/notifier.h>
 #include <linux/compat.h>
-#include <linux/mm.h>
-#include <asm/tlb.h>
-#include <linux/highmem.h>
-#include <uapi/asm-generic/mman-common.h>
-#include "kfd_ipc.h"
 
 struct mm_struct;
 
@@ -39,6 +34,13 @@ struct mm_struct;
 #include "kfd_dbgmgr.h"
 
 /*
+ * Initial size for the array of queues.
+ * The allocated size is doubled each time
+ * it is exceeded up to MAX_PROCESS_QUEUES.
+ */
+#define INITIAL_QUEUE_ARRAY_SIZE 16
+
+/*
  * List of struct kfd_process (field kfd_process).
  * Unique/indexed by mm_struct*
  */
@@ -50,143 +52,32 @@ DEFINE_STATIC_SRCU(kfd_processes_srcu);
 
 static struct workqueue_struct *kfd_process_wq;
 
-#define MIN_IDR_ID 1
-#define MAX_IDR_ID 0 /*0 - for unlimited*/
-
-static struct kfd_process *find_process(const struct task_struct *thread,
-		bool lock);
-static void kfd_process_ref_release(struct kref *ref);
-static struct kfd_process *create_process(const struct task_struct *thread,
-					struct file *filep);
-static int kfd_process_init_cwsr(struct kfd_process *p, struct file *filep);
+struct kfd_process_release_work {
+	struct work_struct kfd_work;
+	struct kfd_process *p;
+};
 
+static struct kfd_process *find_process(const struct task_struct *thread);
+static struct kfd_process *create_process(const struct task_struct *thread);
 
 void kfd_process_create_wq(void)
 {
 	if (!kfd_process_wq)
-		kfd_process_wq = create_workqueue("kfd_process_wq");
+		kfd_process_wq = alloc_workqueue("kfd_process_wq", 0, 0);
 }
 
 void kfd_process_destroy_wq(void)
 {
 	if (kfd_process_wq) {
-		flush_workqueue(kfd_process_wq);
 		destroy_workqueue(kfd_process_wq);
 		kfd_process_wq = NULL;
 	}
 }
 
-static void kfd_process_free_gpuvm(struct kgd_mem *mem,
-			struct kfd_process_device *pdd)
-{
-	kfd_unmap_memory_from_gpu(mem, pdd);
-	pdd->dev->kfd2kgd->free_memory_of_gpu(pdd->dev->kgd, mem, pdd->vm);
-}
-
-/* kfd_process_alloc_gpuvm - Allocate GPU VM for the KFD process
- *	This function should be only called right after the process
- *	is created and when kfd_processes_mutex is still being held
- *	to avoid concurrency. Because of that exclusiveness, we do
- *	not need to take p->lock. Because kfd_processes_mutex instead
- *	of p->lock is held, we do not need to release the lock when
- *	calling into kgd through functions such as alloc_memory_of_gpu()
- *	etc.
- */
-static int kfd_process_alloc_gpuvm(struct kfd_process *p,
-		struct kfd_dev *kdev, uint64_t gpu_va, uint32_t size,
-		void **kptr, struct kfd_process_device *pdd)
-{
-	int err;
-	void *mem = NULL;
-
-	err = kdev->kfd2kgd->alloc_memory_of_gpu(kdev->kgd, gpu_va, size,
-				pdd->vm,
-				(struct kgd_mem **)&mem, NULL, kptr,
-				ALLOC_MEM_FLAGS_GTT |
-				ALLOC_MEM_FLAGS_NONPAGED |
-				ALLOC_MEM_FLAGS_EXECUTE_ACCESS |
-				ALLOC_MEM_FLAGS_NO_SUBSTITUTE);
-	if (err)
-		goto err_alloc_mem;
-
-	err = kfd_map_memory_to_gpu(mem, pdd);
-	if (err)
-		goto err_map_mem;
-
-	/* Create an obj handle so kfd_process_device_remove_obj_handle
-	 * will take care of the bo removal when the process finishes.
-	 * We do not need to take p->lock, because the process is just
-	 * created and the ioctls have not had the chance to run.
-	 */
-	if (kfd_process_device_create_obj_handle(
-			pdd, mem, gpu_va, size, NULL) < 0) {
-		err = -ENOMEM;
-		*kptr = NULL;
-		goto free_gpuvm;
-	}
-
-	return err;
-
-free_gpuvm:
-	kfd_process_free_gpuvm(mem, pdd);
-	return err;
-
-err_map_mem:
-	kdev->kfd2kgd->free_memory_of_gpu(kdev->kgd, mem, pdd->vm);
-err_alloc_mem:
-	*kptr = NULL;
-	return err;
-}
-
-/* kfd_process_reserve_ib_mem - Reserve memory inside the process for IB usage
- *	The memory reserved is for KFD to submit IB to AMDGPU from kernel.
- *	If the memory is reserved successfully, ib_kaddr_assigned will have
- *	the CPU/kernel address. Check ib_kaddr_assigned before accessing the
- *	memory.
- */
-static int kfd_process_reserve_ib_mem(struct kfd_process *p)
-{
-	int err = 0;
-	struct kfd_process_device *temp, *pdd = NULL;
-	struct kfd_dev *kdev = NULL;
-	struct qcm_process_device *qpd = NULL;
-	void *kaddr;
-
-	list_for_each_entry_safe(pdd, temp, &p->per_device_data,
-				per_device_list) {
-		kdev = pdd->dev;
-		qpd = &pdd->qpd;
-		if (!kdev->ib_size || qpd->ib_kaddr)
-			continue;
-
-		if (qpd->ib_base) { /* is dGPU */
-			err = kfd_process_alloc_gpuvm(p, kdev,
-				qpd->ib_base, kdev->ib_size,
-				&kaddr, pdd);
-			if (!err)
-				qpd->ib_kaddr = kaddr;
-			else
-				goto err_out;
-		} else {
-			/* FIXME: Support APU */
-			continue;
-		}
-	}
-
-err_out:
-	/* In case of error, the kfd_bos for some pdds which are already
-	 * allocated successfully will be freed in upper level function
-	 * i.e. create_process().
-	 */
-	return err;
-}
-
-struct kfd_process *kfd_create_process(struct file *filep)
+struct kfd_process *kfd_create_process(const struct task_struct *thread)
 {
 	struct kfd_process *process;
 
-	struct task_struct *thread = current;
-
 	BUG_ON(!kfd_process_wq);
 
 	if (thread->mm == NULL)
@@ -196,6 +87,9 @@ struct kfd_process *kfd_create_process(struct file *filep)
 	if (thread->group_leader->mm != thread->mm)
 		return ERR_PTR(-EINVAL);
 
+	/* Take mmap_sem because we call __mmu_notifier_register inside */
+	down_write(&thread->mm->mmap_sem);
+
 	/*
 	 * take kfd processes mutex before starting of process creation
 	 * so there won't be a case where two threads of the same process
@@ -204,14 +98,17 @@ struct kfd_process *kfd_create_process(struct file *filep)
 	mutex_lock(&kfd_processes_mutex);
 
 	/* A prior open of /dev/kfd could have already created the process. */
-	process = find_process(thread, false);
+	process = find_process(thread);
 	if (process)
 		pr_debug("kfd: process already found\n");
-	else
-		process = create_process(thread, filep);
+
+	if (!process)
+		process = create_process(thread);
 
 	mutex_unlock(&kfd_processes_mutex);
 
+	up_write(&thread->mm->mmap_sem);
+
 	return process;
 }
 
@@ -226,7 +123,7 @@ struct kfd_process *kfd_get_process(const struct task_struct *thread)
 	if (thread->group_leader->mm != thread->mm)
 		return ERR_PTR(-EINVAL);
 
-	process = find_process(thread, false);
+	process = find_process(thread);
 
 	return process;
 }
@@ -243,140 +140,81 @@ static struct kfd_process *find_process_by_mm(const struct mm_struct *mm)
 	return NULL;
 }
 
-static struct kfd_process *find_process(const struct task_struct *thread,
-		bool ref)
+static struct kfd_process *find_process(const struct task_struct *thread)
 {
 	struct kfd_process *p;
 	int idx;
 
 	idx = srcu_read_lock(&kfd_processes_srcu);
 	p = find_process_by_mm(thread->mm);
-	if (p && ref)
-		kref_get(&p->ref);
 	srcu_read_unlock(&kfd_processes_srcu, idx);
 
 	return p;
 }
 
-void kfd_unref_process(struct kfd_process *p)
-{
-	kref_put(&p->ref, kfd_process_ref_release);
-}
-
-/* This increments the process->ref counter. */
-struct kfd_process *kfd_lookup_process_by_pid(struct pid *pid)
-{
-	struct task_struct *task = NULL;
-	struct kfd_process *p    = NULL;
-
-	if (!pid)
-		task = current;
-	else
-		task = get_pid_task(pid, PIDTYPE_PID);
-
-	if (task)
-		p = find_process(task, true);
-
-	return p;
-}
-
-static void kfd_process_free_outstanding_kfd_bos(struct kfd_process *p)
-{
-	struct kfd_process_device *pdd, *peer_pdd;
-	struct kfd_bo *buf_obj;
-	int id;
-
-	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
-		/*
-		 * Remove all handles from idr and release appropriate
-		 * local memory object
-		 */
-		idr_for_each_entry(&pdd->alloc_idr, buf_obj, id) {
-			list_for_each_entry(peer_pdd, &p->per_device_data,
-					per_device_list) {
-				peer_pdd->dev->kfd2kgd->unmap_memory_to_gpu(
-						peer_pdd->dev->kgd,
-						buf_obj->mem, peer_pdd->vm);
-			}
-
-			run_rdma_free_callback(buf_obj);
-			pdd->dev->kfd2kgd->free_memory_of_gpu(
-					pdd->dev->kgd, buf_obj->mem, pdd->vm);
-			kfd_process_device_remove_obj_handle(pdd, id);
-		}
-	}
-}
-
-static void kfd_process_destroy_pdds(struct kfd_process *p)
+static void kfd_process_wq_release(struct work_struct *work)
 {
+	struct kfd_process_release_work *my_work;
 	struct kfd_process_device *pdd, *temp;
+	struct kfd_process *p;
 
-	list_for_each_entry_safe(pdd, temp, &p->per_device_data,
-				 per_device_list) {
-		kfd_flush_tlb(pdd->dev, p->pasid);
-		/* Destroy the GPUVM VM context */
-		if (pdd->vm)
-			pdd->dev->kfd2kgd->destroy_process_vm(
-				pdd->dev->kgd, pdd->vm);
-		list_del(&pdd->per_device_list);
-		kfree(pdd);
-	}
-}
+	my_work = (struct kfd_process_release_work *) work;
 
-/* No process locking is needed in this function, because the process
- * is not findable any more. We must assume that no other thread is
- * using it any more, otherwise we couldn't safely free the process
- * stucture in the end. */
-static void kfd_process_ref_release(struct kref *ref)
-{
-	struct kfd_process *p = container_of(ref, struct kfd_process, ref);
-	struct kfd_process_device *pdd;
+	p = my_work->p;
 
-	pr_debug("Releasing process (pasid %d)\n",
+	pr_debug("Releasing process (pasid %d) in workqueue\n",
 			p->pasid);
 
-	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
-		pr_debug("Releasing pdd (topology id %d) for process (pasid %d)\n",
+	mutex_lock(&p->mutex);
+
+	list_for_each_entry_safe(pdd, temp, &p->per_device_data,
+							per_device_list) {
+		pr_debug("Releasing pdd (topology id %d) for process (pasid %d) in workqueue\n",
 				pdd->dev->id, p->pasid);
 
-		if (pdd->dev->device_info->is_need_iommu_device) {
-			if (pdd->bound == PDD_BOUND) {
-				amd_iommu_unbind_pasid(pdd->dev->pdev,
-						p->pasid);
-				pdd->bound = PDD_UNBOUND;
-			}
-		}
-	}
+		if (pdd->reset_wavefronts)
+			dbgdev_wave_reset_wavefronts(pdd->dev, p);
 
-	kfd_process_free_outstanding_kfd_bos(p);
+		amd_iommu_unbind_pasid(pdd->dev->pdev, p->pasid);
+		list_del(&pdd->per_device_list);
 
-	kfd_process_destroy_pdds(p);
+		kfree(pdd);
+	}
 
 	kfd_event_free_process(p);
 
 	kfd_pasid_free(p->pasid);
 
-	put_task_struct(p->lead_thread);
+	mutex_unlock(&p->mutex);
 
-	kfree(p);
-}
+	mutex_destroy(&p->mutex);
 
-static void kfd_process_wq_release(struct work_struct *work)
-{
-	struct kfd_process *p = container_of(work, struct kfd_process,
-					     release_work);
+	kfree(p->queues);
 
-	kref_put(&p->ref, kfd_process_ref_release);
+	kfree(p);
+
+	kfree(work);
 }
 
 static void kfd_process_destroy_delayed(struct rcu_head *rcu)
 {
-	struct kfd_process *p = container_of(rcu, struct kfd_process, rcu);
+	struct kfd_process_release_work *work;
+	struct kfd_process *p;
 
 	BUG_ON(!kfd_process_wq);
 
-	INIT_WORK(&p->release_work, kfd_process_wq_release);
-	queue_work(kfd_process_wq, &p->release_work);
+	p = container_of(rcu, struct kfd_process, rcu);
+	BUG_ON(atomic_read(&p->mm->mm_count) <= 0);
+
+	mmdrop(p->mm);
+
+	work = kmalloc(sizeof(struct kfd_process_release_work), GFP_ATOMIC);
+
+	if (work) {
+		INIT_WORK((struct work_struct *) work, kfd_process_wq_release);
+		work->p = p;
+		queue_work(kfd_process_wq, (struct work_struct *) work);
+	}
 }
 
 static void kfd_process_notifier_release(struct mmu_notifier *mn,
@@ -384,8 +222,6 @@ static void kfd_process_notifier_release(struct mmu_notifier *mn,
 {
 	struct kfd_process *p;
 	struct kfd_process_device *pdd = NULL;
-	struct kfd_dev *dev = NULL;
-	long status = -EFAULT;
 
 	/*
 	 * The kfd_process structure can not be free because the
@@ -394,47 +230,40 @@ static void kfd_process_notifier_release(struct mmu_notifier *mn,
 	p = container_of(mn, struct kfd_process, mmu_notifier);
 	BUG_ON(p->mm != mm);
 
-	cancel_work_sync(&p->eviction_work.work);
-	cancel_delayed_work_sync(&p->restore_work);
-
 	mutex_lock(&kfd_processes_mutex);
 	hash_del_rcu(&p->kfd_processes);
 	mutex_unlock(&kfd_processes_mutex);
 	synchronize_srcu(&kfd_processes_srcu);
 
-	down_write(&p->lock);
+	mutex_lock(&p->mutex);
 
-	/* Iterate over all process device data structures and if the pdd is in
-	 * debug mode,we should first force unregistration, then we will be
-	 * able to destroy the queues   */
+	/* In case our notifier is called before IOMMU notifier */
+	pqm_uninit(&p->pqm);
+
+	/* Iterate over all process device data structure and check
+	 * if we should delete debug managers and reset all wavefronts
+	 */
 	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
-		dev = pdd->dev;
-		mutex_lock(get_dbgmgr_mutex());
-
-		if ((dev != NULL) &&
-			(dev->dbgmgr) &&
-			(dev->dbgmgr->pasid == p->pasid)) {
-
-			status = kfd_dbgmgr_unregister(dev->dbgmgr, p);
-			if (status == 0) {
-				kfd_dbgmgr_destroy(dev->dbgmgr);
-				dev->dbgmgr = NULL;
-			}
+		if ((pdd->dev->dbgmgr) &&
+				(pdd->dev->dbgmgr->pasid == p->pasid))
+			kfd_dbgmgr_destroy(pdd->dev->dbgmgr);
+
+		if (pdd->reset_wavefronts) {
+			pr_warn("amdkfd: Resetting all wave fronts\n");
+			dbgdev_wave_reset_wavefronts(pdd->dev, p);
+			pdd->reset_wavefronts = false;
 		}
-		mutex_unlock(get_dbgmgr_mutex());
 	}
 
-	kfd_process_dequeue_from_all_devices(p);
-
-	/* now we can uninit the pqm: */
-	pqm_uninit(&p->pqm);
-
-	/* Indicate to other users that MM is no longer valid */
-	p->mm = NULL;
+	mutex_unlock(&p->mutex);
 
-	up_write(&p->lock);
-
-	mmu_notifier_unregister_no_release(&p->mmu_notifier, mm);
+	/*
+	 * Because we drop mm_count inside kfd_process_destroy_delayed
+	 * and because the mmu_notifier_unregister function also drop
+	 * mm_count we need to take an extra count here.
+	 */
+	atomic_inc(&p->mm->mm_count);
+	mmu_notifier_unregister_no_release(&p->mmu_notifier, p->mm);
 	mmu_notifier_call_srcu(&p->rcu, &kfd_process_destroy_delayed);
 }
 
@@ -442,66 +271,7 @@ static const struct mmu_notifier_ops kfd_process_mmu_notifier_ops = {
 	.release = kfd_process_notifier_release,
 };
 
-static int kfd_process_init_cwsr(struct kfd_process *p, struct file *filep)
-{
-	int err = 0;
-	unsigned long  offset;
-	struct kfd_process_device *temp, *pdd = NULL;
-	struct kfd_dev *dev = NULL;
-	struct qcm_process_device *qpd = NULL;
-	void *kaddr;
-
-	list_for_each_entry_safe(pdd, temp, &p->per_device_data,
-				per_device_list) {
-		dev = pdd->dev;
-		qpd = &pdd->qpd;
-		if (!dev->cwsr_enabled || qpd->cwsr_kaddr)
-			continue;
-		if (qpd->cwsr_base) {
-			/* cwsr_base is only set for DGPU */
-			err = kfd_process_alloc_gpuvm(p, dev, qpd->cwsr_base,
-					dev->cwsr_size,	&kaddr, pdd);
-			if (!err) {
-				qpd->cwsr_kaddr = kaddr;
-				memcpy(qpd->cwsr_kaddr, kmap(dev->cwsr_pages),
-				       PAGE_SIZE);
-				kunmap(dev->cwsr_pages);
-				qpd->tba_addr = qpd->cwsr_base;
-			} else
-				goto out;
-		} else {
-			offset = (kfd_get_gpu_id(dev) |
-				KFD_MMAP_TYPE_RESERVED_MEM) << PAGE_SHIFT;
-			qpd->tba_addr = (uint64_t)vm_mmap(filep, 0,
-				dev->cwsr_size,	PROT_READ | PROT_EXEC,
-				MAP_SHARED, offset);
-
-			if (IS_ERR_VALUE(qpd->tba_addr)) {
-				pr_err("Failure to set tba address. error -%d.\n",
-					(int)qpd->tba_addr);
-				qpd->tba_addr = 0;
-				qpd->cwsr_kaddr = NULL;
-				err = -ENOMEM;
-				goto out;
-			} else
-				qpd->cwsr_kaddr = (void *)qpd->tba_addr;
-		}
-
-		qpd->tma_addr = qpd->tba_addr + dev->tma_offset;
-		pr_debug("set tba :0x%llx, tma:0x%llx for pqm.\n",
-			qpd->tba_addr, qpd->tma_addr);
-	}
-
-out:
-	/* In case of error, the kfd_bos for some pdds which are already
-	 * allocated successfully will be freed in upper level function
-	 * i.e. create_process().
-	 */
-	return err;
-}
-
-static struct kfd_process *create_process(const struct task_struct *thread,
-					struct file *filep)
+static struct kfd_process *create_process(const struct task_struct *thread)
 {
 	struct kfd_process *process;
 	int err = -ENOMEM;
@@ -511,20 +281,22 @@ static struct kfd_process *create_process(const struct task_struct *thread,
 	if (!process)
 		goto err_alloc_process;
 
-	process->bo_interval_tree = RB_ROOT;
+	process->queues = kmalloc_array(INITIAL_QUEUE_ARRAY_SIZE,
+					sizeof(process->queues[0]), GFP_KERNEL);
+	if (!process->queues)
+		goto err_alloc_queues;
 
 	process->pasid = kfd_pasid_alloc();
 	if (process->pasid == 0)
 		goto err_alloc_pasid;
 
-	kref_init(&process->ref);
-	init_rwsem(&process->lock);
+	mutex_init(&process->mutex);
 
 	process->mm = thread->mm;
 
 	/* register notifier */
 	process->mmu_notifier.ops = &kfd_process_mmu_notifier_ops;
-	err = mmu_notifier_register(&process->mmu_notifier, process->mm);
+	err = __mmu_notifier_register(&process->mmu_notifier, process->mm);
 	if (err)
 		goto err_mmu_notifier;
 
@@ -532,7 +304,8 @@ static struct kfd_process *create_process(const struct task_struct *thread,
 			(uintptr_t)process->mm);
 
 	process->lead_thread = thread->group_leader;
-	get_task_struct(process->lead_thread);
+
+	process->queue_array_size = INITIAL_QUEUE_ARRAY_SIZE;
 
 	INIT_LIST_HEAD(&process->per_device_data);
 
@@ -543,41 +316,24 @@ static struct kfd_process *create_process(const struct task_struct *thread,
 		goto err_process_pqm_init;
 
 	/* init process apertures*/
-	process->is_32bit_user_mode = is_compat_task();
+	process->is_32bit_user_mode = in_compat_syscall();
 	if (kfd_init_apertures(process) != 0)
 		goto err_init_apretures;
 
-	err = kfd_process_reserve_ib_mem(process);
-	if (err)
-		goto err_reserve_ib_mem;
-	err = kfd_process_init_cwsr(process, filep);
-	if (err)
-		goto err_init_cwsr;
-
-	INIT_WORK(&process->eviction_work.work, kfd_evict_bo_worker);
-	INIT_DELAYED_WORK(&process->restore_work, kfd_restore_bo_worker);
-
-	/* If PeerDirect interface was not detected try to detect it again
-	* in case if network driver was loaded later.
-	*/
-	kfd_init_peer_direct();
-
 	return process;
 
-err_init_cwsr:
-err_reserve_ib_mem:
-	kfd_process_free_outstanding_kfd_bos(process);
-	kfd_process_destroy_pdds(process);
 err_init_apretures:
 	pqm_uninit(&process->pqm);
 err_process_pqm_init:
 	hash_del_rcu(&process->kfd_processes);
 	synchronize_rcu();
-	mmu_notifier_unregister_no_release(&process->mmu_notifier,
-					process->mm);
+	mmu_notifier_unregister_no_release(&process->mmu_notifier, process->mm);
 err_mmu_notifier:
+	mutex_destroy(&process->mutex);
 	kfd_pasid_free(process->pasid);
 err_alloc_pasid:
+	kfree(process->queues);
+err_alloc_queues:
 	kfree(process);
 err_alloc_process:
 	return ERR_PTR(err);
@@ -590,9 +346,9 @@ struct kfd_process_device *kfd_get_process_device_data(struct kfd_dev *dev,
 
 	list_for_each_entry(pdd, &p->per_device_data, per_device_list)
 		if (pdd->dev == dev)
-			return pdd;
+			break;
 
-	return NULL;
+	return pdd;
 }
 
 struct kfd_process_device *kfd_create_process_device_data(struct kfd_dev *dev,
@@ -606,27 +362,8 @@ struct kfd_process_device *kfd_create_process_device_data(struct kfd_dev *dev,
 		INIT_LIST_HEAD(&pdd->qpd.queues_list);
 		INIT_LIST_HEAD(&pdd->qpd.priv_queue_list);
 		pdd->qpd.dqm = dev->dqm;
-		pdd->qpd.pqm = &p->pqm;
-		pdd->qpd.evicted = 0;
 		pdd->reset_wavefronts = false;
-		pdd->process = p;
-		pdd->bound = PDD_UNBOUND;
-		pdd->already_dequeued = false;
 		list_add(&pdd->per_device_list, &p->per_device_data);
-
-		/* Init idr used for memory handle translation */
-		idr_init(&pdd->alloc_idr);
-
-		/* Create the GPUVM context for this specific device */
-		if (dev->kfd2kgd->create_process_vm(dev->kgd, &pdd->vm,
-						    p->master_vm)) {
-			pr_err("Failed to create process VM object\n");
-			list_del(&pdd->per_device_list);
-			kfree(pdd);
-			pdd = NULL;
-		}
-		if (p->master_vm == NULL)
-			p->master_vm = pdd->vm;
 	}
 
 	return pdd;
@@ -651,117 +388,63 @@ struct kfd_process_device *kfd_bind_process_to_device(struct kfd_dev *dev,
 		return ERR_PTR(-ENOMEM);
 	}
 
-	if (pdd->bound == PDD_BOUND)
+	if (pdd->bound)
 		return pdd;
 
-	if (pdd->bound == PDD_BOUND_SUSPENDED) {
-		pr_err("kfd: binding PDD_BOUND_SUSPENDED pdd is unexpected!\n");
-		return ERR_PTR(-EINVAL);
-	}
-
-	if (dev->device_info->is_need_iommu_device) {
-		err = amd_iommu_bind_pasid(dev->pdev, p->pasid, p->lead_thread);
-		if (err < 0)
-			return ERR_PTR(err);
-	}
+	err = amd_iommu_bind_pasid(dev->pdev, p->pasid, p->lead_thread);
+	if (err < 0)
+		return ERR_PTR(err);
 
-	pdd->bound = PDD_BOUND;
+	pdd->bound = true;
 
 	return pdd;
 }
 
-int kfd_bind_processes_to_device(struct kfd_dev *dev)
-{
-	struct kfd_process_device *pdd;
-	struct kfd_process *p;
-	unsigned int temp;
-	int err = 0;
-
-	int idx = srcu_read_lock(&kfd_processes_srcu);
-
-	hash_for_each_rcu(kfd_processes_table, temp, p, kfd_processes) {
-		down_write(&p->lock);
-		pdd = kfd_get_process_device_data(dev, p);
-		if (pdd->bound != PDD_BOUND_SUSPENDED) {
-			up_write(&p->lock);
-			continue;
-		}
-
-		err = amd_iommu_bind_pasid(dev->pdev, p->pasid,
-				p->lead_thread);
-		if (err < 0) {
-			pr_err("unexpected pasid %d binding failure\n",
-					p->pasid);
-			up_write(&p->lock);
-			break;
-		}
-
-		pdd->bound = PDD_BOUND;
-		up_write(&p->lock);
-	}
-
-	srcu_read_unlock(&kfd_processes_srcu, idx);
-
-	return err;
-}
-
-void kfd_unbind_processes_from_device(struct kfd_dev *dev)
-{
-	struct kfd_process_device *pdd;
-	struct kfd_process *p;
-	unsigned int temp;
-
-	int idx = srcu_read_lock(&kfd_processes_srcu);
-
-	hash_for_each_rcu(kfd_processes_table, temp, p, kfd_processes) {
-		down_write(&p->lock);
-		pdd = kfd_get_process_device_data(dev, p);
-
-		if (pdd->bound == PDD_BOUND)
-			pdd->bound = PDD_BOUND_SUSPENDED;
-		up_write(&p->lock);
-	}
-
-	srcu_read_unlock(&kfd_processes_srcu, idx);
-}
-
-void kfd_process_iommu_unbind_callback(struct kfd_dev *dev, unsigned int pasid)
+void kfd_unbind_process_from_device(struct kfd_dev *dev, unsigned int pasid)
 {
 	struct kfd_process *p;
 	struct kfd_process_device *pdd;
-	long status = -EFAULT;
 
 	BUG_ON(dev == NULL);
 
+	/*
+	 * Look for the process that matches the pasid. If there is no such
+	 * process, we either released it in amdkfd's own notifier, or there
+	 * is a bug. Unfortunately, there is no way to tell...
+	 */
 	p = kfd_lookup_process_by_pasid(pasid);
+	if (!p)
+		return;
 
-	BUG_ON(!p);
-
-	mutex_lock(get_dbgmgr_mutex());
+	pr_debug("Unbinding process %d from IOMMU\n", pasid);
 
-	if ((dev->dbgmgr) && (dev->dbgmgr->pasid == p->pasid)) {
+	if ((dev->dbgmgr) && (dev->dbgmgr->pasid == p->pasid))
+		kfd_dbgmgr_destroy(dev->dbgmgr);
 
-		status = kfd_dbgmgr_unregister(dev->dbgmgr, p);
-		if (status == 0) {
-			kfd_dbgmgr_destroy(dev->dbgmgr);
-			dev->dbgmgr = NULL;
-		}
-	}
+	pqm_uninit(&p->pqm);
 
-	mutex_unlock(get_dbgmgr_mutex());
+	pdd = kfd_get_process_device_data(dev, p);
 
-	down_write(&p->lock);
+	if (!pdd) {
+		mutex_unlock(&p->mutex);
+		return;
+	}
 
-	pdd = kfd_get_process_device_data(dev, p);
-	if (pdd)
-		/* For GPU relying on IOMMU, we need to dequeue here
-		 * when PASID is still bound.
-		 */
-		kfd_process_dequeue_from_device(pdd);
+	if (pdd->reset_wavefronts) {
+		dbgdev_wave_reset_wavefronts(pdd->dev, p);
+		pdd->reset_wavefronts = false;
+	}
 
-	up_write(&p->lock);
+	/*
+	 * Just mark pdd as unbound, because we still need it
+	 * to call amd_iommu_unbind_pasid() in when the
+	 * process exits.
+	 * We don't call amd_iommu_unbind_pasid() here
+	 * because the IOMMU called us.
+	 */
+	pdd->bound = false;
 
-	kfd_unref_process(p);
+	mutex_unlock(&p->mutex);
 }
 
 struct kfd_process_device *kfd_get_first_process_device_data(struct kfd_process *p)
@@ -784,218 +467,22 @@ bool kfd_has_process_device_data(struct kfd_process *p)
 	return !(list_empty(&p->per_device_data));
 }
 
-/* Create specific handle mapped to mem from process local memory idr
- * Assumes that the process lock is held. */
-int kfd_process_device_create_obj_handle(struct kfd_process_device *pdd,
-					void *mem, uint64_t start,
-					uint64_t length,
-					struct kfd_ipc_obj *ipc_obj)
-{
-	int handle;
-	struct kfd_bo *buf_obj;
-	struct kfd_process *p;
-
-	BUG_ON(pdd == NULL);
-	BUG_ON(mem == NULL);
-
-	p = pdd->process;
-
-	buf_obj = kzalloc(sizeof(*buf_obj), GFP_KERNEL);
-
-	if (!buf_obj)
-		return -ENOMEM;
-
-	buf_obj->it.start = start;
-	buf_obj->it.last = start + length - 1;
-	interval_tree_insert(&buf_obj->it, &p->bo_interval_tree);
-
-	buf_obj->mem = mem;
-	buf_obj->dev = pdd->dev;
-	buf_obj->kfd_ipc_obj = ipc_obj;
-
-	INIT_LIST_HEAD(&buf_obj->cb_data_head);
-
-	idr_preload(GFP_KERNEL);
-
-	handle = idr_alloc(&pdd->alloc_idr, buf_obj, MIN_IDR_ID, MAX_IDR_ID,
-			GFP_NOWAIT);
-
-	idr_preload_end();
-
-	if (handle < 0)
-		kfree(buf_obj);
-
-	return handle;
-}
-
-struct kfd_bo *kfd_process_device_find_bo(struct kfd_process_device *pdd,
-					int handle)
-{
-	BUG_ON(pdd == NULL);
-
-	if (handle < 0)
-		return NULL;
-
-	return (struct kfd_bo *)idr_find(&pdd->alloc_idr, handle);
-}
-
-/* Translate specific handle from process local memory idr
- * Assumes that the process lock is held. */
-void *kfd_process_device_translate_handle(struct kfd_process_device *pdd,
-					int handle)
-{
-	struct kfd_bo *buf_obj;
-
-	buf_obj = kfd_process_device_find_bo(pdd, handle);
-
-	return buf_obj->mem;
-}
-
-void *kfd_process_find_bo_from_interval(struct kfd_process *p,
-					uint64_t start_addr,
-					uint64_t last_addr)
-{
-	struct interval_tree_node *it_node;
-	struct kfd_bo *buf_obj;
-
-	it_node = interval_tree_iter_first(&p->bo_interval_tree,
-			start_addr, last_addr);
-	if (!it_node) {
-		pr_err("%llu - %llu does not relate to an existing buffer\n",
-				start_addr, last_addr);
-		return NULL;
-	}
-
-	BUG_ON(NULL != interval_tree_iter_next(it_node,
-			start_addr, last_addr));
-
-	buf_obj = container_of(it_node, struct kfd_bo, it);
-
-	return buf_obj;
-}
-
-/* Remove specific handle from process local memory idr
- * Assumes that the process lock is held. */
-void kfd_process_device_remove_obj_handle(struct kfd_process_device *pdd,
-					int handle)
-{
-	struct kfd_bo *buf_obj;
-	struct kfd_process *p;
-
-	BUG_ON(pdd == NULL);
-
-	p = pdd->process;
-
-	if (handle < 0)
-		return;
-
-	buf_obj = kfd_process_device_find_bo(pdd, handle);
-
-	if (buf_obj->kfd_ipc_obj)
-		ipc_obj_put(&buf_obj->kfd_ipc_obj);
-
-	idr_remove(&pdd->alloc_idr, handle);
-
-	interval_tree_remove(&buf_obj->it, &p->bo_interval_tree);
-
-	kfree(buf_obj);
-}
-
-/* This increments the process->ref counter. */
+/* This returns with process->mutex locked. */
 struct kfd_process *kfd_lookup_process_by_pasid(unsigned int pasid)
 {
-	struct kfd_process *p, *ret_p = NULL;
+	struct kfd_process *p;
 	unsigned int temp;
 
 	int idx = srcu_read_lock(&kfd_processes_srcu);
 
 	hash_for_each_rcu(kfd_processes_table, temp, p, kfd_processes) {
 		if (p->pasid == pasid) {
-			kref_get(&p->ref);
-			ret_p = p;
+			mutex_lock(&p->mutex);
 			break;
 		}
 	}
 
 	srcu_read_unlock(&kfd_processes_srcu, idx);
 
-	return ret_p;
-}
-
-/* This increments the process->ref counter. */
-struct kfd_process *kfd_lookup_process_by_mm(const struct mm_struct *mm)
-{
-	struct kfd_process *p;
-
-	int idx = srcu_read_lock(&kfd_processes_srcu);
-
-	p = find_process_by_mm(mm);
-	if (p != NULL)
-		kref_get(&p->ref);
-
-	srcu_read_unlock(&kfd_processes_srcu, idx);
-
 	return p;
 }
-
-int kfd_reserved_mem_mmap(struct kfd_process *process, struct vm_area_struct *vma)
-{
-	unsigned long pfn, i;
-	int ret = 0;
-	struct kfd_dev *dev = kfd_device_by_id(vma->vm_pgoff);
-
-	if (dev == NULL)
-		return -EINVAL;
-	if ((vma->vm_start & (PAGE_SIZE - 1)) ||
-		(vma->vm_end & (PAGE_SIZE - 1))) {
-		pr_err("KFD only support page aligned memory map.\n");
-		return -EINVAL;
-	}
-
-	pr_debug("kfd reserved mem mmap been called.\n");
-	/* We supported  two reserved memory mmap in the future .
-	    1. Trap handler code and parameter (TBA and TMA , 2 pages total)
-	    2. Relaunch stack (control  block, 1 page for Carrizo)
-	 */
-
-	for (i = 0; i < ((vma->vm_end - vma->vm_start) >> PAGE_SHIFT); ++i) {
-		pfn = page_to_pfn(&dev->cwsr_pages[i]);
-		vma->vm_flags |= VM_IO | VM_DONTCOPY | VM_DONTEXPAND
-			| VM_NORESERVE | VM_DONTDUMP | VM_PFNMAP;
-		/* mapping the page to user process */
-		ret = remap_pfn_range(vma, vma->vm_start + (i << PAGE_SHIFT),
-				pfn, PAGE_SIZE, vma->vm_page_prot);
-		if (ret)
-			break;
-	}
-	return ret;
-}
-
-#if defined(CONFIG_DEBUG_FS)
-
-int kfd_debugfs_mqds_by_process(struct seq_file *m, void *data)
-{
-	struct kfd_process *p;
-	unsigned int temp;
-	int r = 0;
-
-	int idx = srcu_read_lock(&kfd_processes_srcu);
-
-	hash_for_each_rcu(kfd_processes_table, temp, p, kfd_processes) {
-		seq_printf(m, "Process %d PASID %d:\n",
-			   p->lead_thread->tgid, p->pasid);
-
-		down_read(&p->lock);
-		r = pqm_debugfs_mqds(m, &p->pqm);
-		up_read(&p->lock);
-
-		if (r != 0)
-			break;
-	}
-
-	srcu_read_unlock(&kfd_processes_srcu, idx);
-
-	return r;
-}
-
-#endif
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
index 564d1cd..e1fb40b 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
@@ -70,37 +70,6 @@ static int find_available_queue_slot(struct process_queue_manager *pqm,
 	return 0;
 }
 
-void kfd_process_dequeue_from_device(struct kfd_process_device *pdd)
-{
-	struct kfd_dev *dev = pdd->dev;
-	struct kfd_process *p = pdd->process;
-	int retval;
-
-	if (pdd->already_dequeued)
-		return;
-
-	retval = dev->dqm->ops.process_termination(dev->dqm, &pdd->qpd);
-	pdd->already_dequeued = true;
-	/* Checking pdd->reset_wavefronts may not be needed, because
-	 * if reset_wavefronts was set to true before, which means unmapping
-	 * failed, process_termination should fail too until we reset
-	 * wavefronts. Now we put the check there to be safe.
-	 */
-	if (retval || pdd->reset_wavefronts) {
-		pr_warn("amdkfd: Resetting wave fronts on dev %p\n", dev);
-		dbgdev_wave_reset_wavefronts(dev, p);
-		pdd->reset_wavefronts = false;
-	}
-}
-
-void kfd_process_dequeue_from_all_devices(struct kfd_process *p)
-{
-	struct kfd_process_device *pdd;
-
-	list_for_each_entry(pdd, &p->per_device_data, per_device_list)
-		kfd_process_dequeue_from_device(pdd);
-}
-
 int pqm_init(struct process_queue_manager *pqm, struct kfd_process *p)
 {
 	BUG_ON(!pqm);
@@ -118,6 +87,7 @@ int pqm_init(struct process_queue_manager *pqm, struct kfd_process *p)
 
 void pqm_uninit(struct process_queue_manager *pqm)
 {
+	int retval;
 	struct process_queue_node *pqn, *next;
 
 	BUG_ON(!pqm);
@@ -125,11 +95,17 @@ void pqm_uninit(struct process_queue_manager *pqm)
 	pr_debug("In func %s\n", __func__);
 
 	list_for_each_entry_safe(pqn, next, &pqm->queues, process_queue_list) {
-		uninit_queue(pqn->q);
-		list_del(&pqn->process_queue_list);
-		kfree(pqn);
-	}
+		retval = pqm_destroy_queue(
+				pqm,
+				(pqn->q != NULL) ?
+					pqn->q->properties.queue_id :
+					pqn->kq->queue->properties.queue_id);
 
+		if (retval != 0) {
+			pr_err("kfd: failed to destroy queue\n");
+			return;
+		}
+	}
 	kfree(pqm->queue_slot_bitmap);
 	pqm->queue_slot_bitmap = NULL;
 }
@@ -172,19 +148,23 @@ int pqm_create_queue(struct process_queue_manager *pqm,
 			    struct kfd_dev *dev,
 			    struct file *f,
 			    struct queue_properties *properties,
+			    unsigned int flags,
+			    enum kfd_queue_type type,
 			    unsigned int *qid)
 {
 	int retval;
 	struct kfd_process_device *pdd;
+	struct queue_properties q_properties;
 	struct queue *q;
 	struct process_queue_node *pqn;
 	struct kernel_queue *kq;
 	int num_queues = 0;
 	struct queue *cur;
-	enum kfd_queue_type type = properties->type;
 
 	BUG_ON(!pqm || !dev || !properties || !qid);
 
+	memset(&q_properties, 0, sizeof(struct queue_properties));
+	memcpy(&q_properties, properties, sizeof(struct queue_properties));
 	q = NULL;
 	kq = NULL;
 
@@ -212,9 +192,10 @@ int pqm_create_queue(struct process_queue_manager *pqm,
 	if (retval != 0)
 		return retval;
 
-	if (list_empty(&pdd->qpd.queues_list) &&
-			list_empty(&pdd->qpd.priv_queue_list))
+	if (list_empty(&pqm->queues)) {
+		pdd->qpd.pqm = pqm;
 		dev->dqm->ops.register_process(dev->dqm, &pdd->qpd);
+	}
 
 	pqn = kzalloc(sizeof(struct process_queue_node), GFP_KERNEL);
 	if (!pqn) {
@@ -224,34 +205,17 @@ int pqm_create_queue(struct process_queue_manager *pqm,
 
 	switch (type) {
 	case KFD_QUEUE_TYPE_SDMA:
-		if (dev->dqm->sdma_queue_count >= CIK_SDMA_QUEUES) {
-			pr_err("kfd: over-subscription is not allowed for SDMA.\n");
-			retval = -EPERM;
-			goto err_create_queue;
-		}
-
-		retval = create_cp_queue(pqm, dev, &q, properties, f, *qid);
-		if (retval != 0)
-			goto err_create_queue;
-		pqn->q = q;
-		pqn->kq = NULL;
-		retval = dev->dqm->ops.create_queue(dev->dqm, q, &pdd->qpd,
-						&q->properties.vmid);
-		pr_debug("DQM returned %d for create_queue\n", retval);
-		print_queue(q);
-		break;
-
 	case KFD_QUEUE_TYPE_COMPUTE:
 		/* check if there is over subscription */
-		if ((dev->dqm->sched_policy == KFD_SCHED_POLICY_HWS_NO_OVERSUBSCRIPTION) &&
-		((dev->dqm->processes_count >= dev->vm_info.vmid_num_kfd) ||
+		if ((sched_policy == KFD_SCHED_POLICY_HWS_NO_OVERSUBSCRIPTION) &&
+		((dev->dqm->processes_count >= VMID_PER_DEVICE) ||
 		(dev->dqm->queue_count >= PIPE_PER_ME_CP_SCHEDULING * QUEUES_PER_PIPE))) {
 			pr_err("kfd: over-subscription is not allowed in radeon_kfd.sched_policy == 1\n");
 			retval = -EPERM;
 			goto err_create_queue;
 		}
 
-		retval = create_cp_queue(pqm, dev, &q, properties, f, *qid);
+		retval = create_cp_queue(pqm, dev, &q, &q_properties, f, *qid);
 		if (retval != 0)
 			goto err_create_queue;
 		pqn->q = q;
@@ -288,8 +252,9 @@ int pqm_create_queue(struct process_queue_manager *pqm,
 	list_add(&pqn->process_queue_list, &pqm->queues);
 
 	if (q) {
+		*properties = q->properties;
 		pr_debug("kfd: PQM done creating queue\n");
-		print_queue_properties(&q->properties);
+		print_queue_properties(properties);
 	}
 
 	return retval;
@@ -299,8 +264,7 @@ err_create_queue:
 err_allocate_pqn:
 	/* check if queues list is empty unregister process from device */
 	clear_bit(*qid, pqm->queue_slot_bitmap);
-	if (list_empty(&pdd->qpd.queues_list) &&
-			list_empty(&pdd->qpd.priv_queue_list))
+	if (list_empty(&pqm->queues))
 		dev->dqm->ops.unregister_process(dev->dqm, &pdd->qpd);
 	return retval;
 }
@@ -348,13 +312,10 @@ int pqm_destroy_queue(struct process_queue_manager *pqm, unsigned int qid)
 
 	if (pqn->q) {
 		dqm = pqn->q->device->dqm;
-		kfree(pqn->q->properties.cu_mask);
 		retval = dqm->ops.destroy_queue(dqm, &pdd->qpd, pqn->q);
-		if (retval != 0) {
-			if (retval == -ETIME)
-				pdd->reset_wavefronts = true;
+		if (retval != 0)
 			return retval;
-		}
+
 		uninit_queue(pqn->q);
 	}
 
@@ -362,8 +323,7 @@ int pqm_destroy_queue(struct process_queue_manager *pqm, unsigned int qid)
 	kfree(pqn);
 	clear_bit(qid, pqm->queue_slot_bitmap);
 
-	if (list_empty(&pdd->qpd.queues_list) &&
-			list_empty(&pdd->qpd.priv_queue_list))
+	if (list_empty(&pqm->queues))
 		dqm->ops.unregister_process(dqm, &pdd->qpd);
 
 	return retval;
@@ -397,37 +357,6 @@ int pqm_update_queue(struct process_queue_manager *pqm, unsigned int qid,
 	return 0;
 }
 
-int pqm_set_cu_mask(struct process_queue_manager *pqm, unsigned int qid,
-			struct queue_properties *p)
-{
-	int retval;
-	struct process_queue_node *pqn;
-
-	BUG_ON(!pqm);
-
-	pqn = get_queue_by_qid(pqm, qid);
-	if (!pqn) {
-		pr_debug("amdkfd: No queue %d exists for update operation\n",
-				qid);
-		return -EFAULT;
-	}
-
-	/* Free the old CU mask memory if it is already allocated, then
-	 * allocate memory for the new CU mask.
-	 */
-	kfree(pqn->q->properties.cu_mask);
-
-	pqn->q->properties.cu_mask_count = p->cu_mask_count;
-	pqn->q->properties.cu_mask = p->cu_mask;
-
-	retval = pqn->q->device->dqm->ops.update_queue(pqn->q->device->dqm,
-							pqn->q);
-	if (retval != 0)
-		return retval;
-
-	return 0;
-}
-
 struct kernel_queue *pqm_get_kernel_queue(
 					struct process_queue_manager *pqm,
 					unsigned int qid)
@@ -443,67 +372,4 @@ struct kernel_queue *pqm_get_kernel_queue(
 	return NULL;
 }
 
-#if defined(CONFIG_DEBUG_FS)
-
-int pqm_debugfs_mqds(struct seq_file *m, void *data)
-{
-	struct process_queue_manager *pqm = data;
-	struct process_queue_node *pqn;
-	struct queue *q;
-	enum KFD_MQD_TYPE mqd_type;
-	struct mqd_manager *mqd_manager;
-	int r = 0;
-
-	list_for_each_entry(pqn, &pqm->queues, process_queue_list) {
-		if (pqn->q) {
-			q = pqn->q;
-			switch (q->properties.type) {
-			case KFD_QUEUE_TYPE_SDMA:
-				seq_printf(m, "  SDMA queue on device %x\n",
-					   q->device->id);
-				mqd_type = KFD_MQD_TYPE_SDMA;
-				break;
-			case KFD_QUEUE_TYPE_COMPUTE:
-				seq_printf(m, "  Compute queue on device %x\n",
-					   q->device->id);
-				mqd_type = KFD_MQD_TYPE_CP;
-				break;
-			default:
-				seq_printf(m,
-				"  Bad user queue type %d on device %x\n",
-					   q->properties.type, q->device->id);
-				continue;
-			}
-			mqd_manager = q->device->dqm->ops.get_mqd_manager(
-				q->device->dqm, mqd_type);
-		} else if (pqn->kq) {
-			q = pqn->kq->queue;
-			mqd_manager = pqn->kq->mqd;
-			switch (q->properties.type) {
-			case KFD_QUEUE_TYPE_DIQ:
-				seq_printf(m, "  DIQ on device %x\n",
-					   pqn->kq->dev->id);
-				mqd_type = KFD_MQD_TYPE_HIQ;
-				break;
-			default:
-				seq_printf(m,
-				"  Bad kernel queue type %d on device %x\n",
-					   q->properties.type,
-					   pqn->kq->dev->id);
-				continue;
-			}
-		} else {
-			seq_printf(m,
-		"  Weird: Queue node with neither kernel nor user queue\n");
-			continue;
-		}
-
-		r = mqd_manager->debugfs_show_mqd(m, q->mqd);
-		if (r != 0)
-			break;
-	}
-
-	return r;
-}
 
-#endif
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c b/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c
deleted file mode 100644
index 56bf9a2..0000000
--- a/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c
+++ /dev/null
@@ -1,298 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- */
-
-#include <linux/device.h>
-#include <linux/export.h>
-#include <linux/pid.h>
-#include <linux/err.h>
-#include <linux/slab.h>
-#include "amd_rdma.h"
-#include "kfd_priv.h"
-
-
-struct rdma_cb {
-	struct list_head node;
-	struct amd_p2p_info amd_p2p_data;
-	void  (*free_callback)(void *client_priv);
-	void  *client_priv;
-};
-
-/**
- * This function makes the pages underlying a range of GPU virtual memory
- * accessible for DMA operations from another PCIe device
- *
- * \param   address       - The start address in the Unified Virtual Address
- *			    space in the specified process
- * \param   length        - The length of requested mapping
- * \param   pid           - Pointer to structure pid to which address belongs.
- *			    Could be NULL for current process address space.
- * \param   p2p_data    - On return: Pointer to structure describing
- *			    underlying pages/locations
- * \param   free_callback - Pointer to callback which will be called when access
- *			    to such memory must be stopped immediately: Memory
- *			    was freed, GECC events, etc.
- *			    Client should  immediately stop any transfer
- *			    operations and returned as soon as possible.
- *			    After return all resources associated with address
- *			    will be release and no access will be allowed.
- * \param   client_priv   - Pointer to be passed as parameter on
- *			    'free_callback;
- *
- * \return  0 if operation was successful
- */
-static int get_pages(uint64_t address, uint64_t length, struct pid *pid,
-		struct amd_p2p_info **amd_p2p_data,
-		void  (*free_callback)(void *client_priv),
-		void  *client_priv)
-{
-	struct kfd_bo *buf_obj;
-	struct kgd_mem *mem;
-	struct sg_table *sg_table_tmp;
-	struct kfd_dev *dev;
-	uint64_t last = address + length - 1;
-	uint64_t offset;
-	struct kfd_process *p;
-	struct rdma_cb *rdma_cb_data;
-	int ret = 0;
-
-	p = kfd_lookup_process_by_pid(pid);
-	if (!p) {
-		pr_err("could not find the process in %s.\n",
-				__func__);
-		return -EINVAL;
-	}
-	down_read(&p->lock);
-
-	buf_obj = kfd_process_find_bo_from_interval(p, address, last);
-	if (!buf_obj) {
-		pr_err("can not find a kfd_bo for the range\n");
-		ret = -EINVAL;
-		goto out;
-	}
-
-	rdma_cb_data = kmalloc(sizeof(*rdma_cb_data), GFP_KERNEL);
-	if (!rdma_cb_data) {
-		*amd_p2p_data = NULL;
-		ret = -ENOMEM;
-		goto out;
-	}
-
-	mem = buf_obj->mem;
-	dev = buf_obj->dev;
-	offset = address - buf_obj->it.start;
-
-	ret = dev->kfd2kgd->pin_get_sg_table_bo(dev->kgd, mem,
-			offset, length, &sg_table_tmp);
-
-	if (ret) {
-		pr_err("pin_get_sg_table_bo failed.\n");
-		*amd_p2p_data = NULL;
-		goto free_mem;
-	}
-
-	rdma_cb_data->amd_p2p_data.va = address;
-	rdma_cb_data->amd_p2p_data.size = length;
-	rdma_cb_data->amd_p2p_data.pid = pid;
-	rdma_cb_data->amd_p2p_data.priv = buf_obj;
-	rdma_cb_data->amd_p2p_data.pages = sg_table_tmp;
-
-	rdma_cb_data->free_callback = free_callback;
-	rdma_cb_data->client_priv = client_priv;
-
-	list_add(&rdma_cb_data->node, &buf_obj->cb_data_head);
-
-	*amd_p2p_data = &rdma_cb_data->amd_p2p_data;
-
-	goto out;
-
-free_mem:
-	kfree(rdma_cb_data);
-out:
-	up_read(&p->lock);
-	kfd_unref_process(p);
-
-	return ret;
-}
-
-static int put_pages_helper(struct amd_p2p_info *p2p_data)
-{
-	struct kfd_bo *buf_obj;
-	struct kfd_dev *dev;
-	struct sg_table *sg_table_tmp;
-	struct rdma_cb *rdma_cb_data;
-
-	if (!p2p_data) {
-		pr_err("amd_p2p_info pointer is invalid.\n");
-		return -EINVAL;
-	}
-
-	rdma_cb_data = container_of(p2p_data, struct rdma_cb, amd_p2p_data);
-
-	buf_obj = p2p_data->priv;
-	dev = buf_obj->dev;
-	sg_table_tmp = p2p_data->pages;
-
-	list_del(&rdma_cb_data->node);
-	kfree(rdma_cb_data);
-
-	dev->kfd2kgd->unpin_put_sg_table_bo(buf_obj->mem, sg_table_tmp);
-
-
-	return 0;
-}
-
-void run_rdma_free_callback(struct kfd_bo *buf_obj)
-{
-	struct rdma_cb *tmp, *rdma_cb_data;
-
-	list_for_each_entry_safe(rdma_cb_data, tmp,
-			&buf_obj->cb_data_head, node) {
-		if (rdma_cb_data->free_callback)
-			rdma_cb_data->free_callback(
-					rdma_cb_data->client_priv);
-
-		put_pages_helper(&rdma_cb_data->amd_p2p_data);
-	}
-}
-
-/**
- *
- * This function release resources previously allocated by get_pages() call.
- *
- * \param   p_p2p_data - A pointer to pointer to amd_p2p_info entries
- * 			allocated by get_pages() call.
- *
- * \return  0 if operation was successful
- */
-static int put_pages(struct amd_p2p_info **p_p2p_data)
-{
-	struct kfd_process *p = NULL;
-	int ret = 0;
-
-	if (!(*p_p2p_data)) {
-		pr_err("amd_p2p_info pointer is invalid.\n");
-		return -EINVAL;
-	}
-
-	p = kfd_lookup_process_by_pid((*p_p2p_data)->pid);
-	if (!p) {
-		pr_err("could not find the process in %s\n",
-				__func__);
-		return -EINVAL;
-	}
-
-	ret = put_pages_helper(*p_p2p_data);
-
-	if (!ret)
-		*p_p2p_data = NULL;
-
-	kfd_unref_process(p);
-
-	return ret;
-}
-
-/**
- * Check if given address belongs to GPU address space.
- *
- * \param   address - Address to check
- * \param   pid     - Process to which given address belongs.
- *		      Could be NULL if current one.
- *
- * \return  0  - This is not GPU address managed by AMD driver
- *	    1  - This is GPU address managed by AMD driver
- */
-static int is_gpu_address(uint64_t address, struct pid *pid)
-{
-	struct kfd_bo *buf_obj;
-	struct kfd_process *p;
-
-	p = kfd_lookup_process_by_pid(pid);
-	if (!p) {
-		pr_debug("could not find the process in %s.\n",
-				__func__);
-		return 0;
-	}
-
-	buf_obj = kfd_process_find_bo_from_interval(p, address, address);
-
-	kfd_unref_process(p);
-	if (!buf_obj)
-		return 0;
-	else
-		return 1;
-}
-
-/**
- * Return the single page size to be used when building scatter/gather table
- * for given range.
- *
- * \param   address   - Address
- * \param   length    - Range length
- * \param   pid       - Process id structure. Could be NULL if current one.
- * \param   page_size - On return: Page size
- *
- * \return  0 if operation was successful
- */
-static int get_page_size(uint64_t address, uint64_t length, struct pid *pid,
-			unsigned long *page_size)
-{
-	/*
-	 * As local memory is always consecutive, we can assume the local
-	 * memory page size to be arbitrary.
-	 * Currently we assume the local memory page size to be the same
-	 * as system memory, which is 4KB.
-	 */
-	*page_size = PAGE_SIZE;
-
-	return 0;
-}
-
-
-/**
- * Singleton object: rdma interface function pointers
- */
-static const struct amd_rdma_interface  rdma_ops = {
-	.get_pages = get_pages,
-	.put_pages = put_pages,
-	.is_gpu_address = is_gpu_address,
-	.get_page_size = get_page_size,
-};
-
-/**
- * amdkfd_query_rdma_interface - Return interface (function pointers table) for
- *				 rdma interface
- *
- *
- * \param interace     - OUT: Pointer to interface
- *
- * \return 0 if operation was successful.
- */
-int amdkfd_query_rdma_interface(const struct amd_rdma_interface **ops)
-{
-	*ops  = &rdma_ops;
-
-	return 0;
-}
-EXPORT_SYMBOL(amdkfd_query_rdma_interface);
-
-
-
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_topology.c b/drivers/gpu/drm/amd/amdkfd/kfd_topology.c
index 1aa6463..1e50647 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_topology.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_topology.c
@@ -28,20 +28,16 @@
 #include <linux/hash.h>
 #include <linux/cpufreq.h>
 #include <linux/log2.h>
-#include <linux/dmi.h>
-#include <linux/atomic.h>
 
 #include "kfd_priv.h"
 #include "kfd_crat.h"
 #include "kfd_topology.h"
-#include "kfd_device_queue_manager.h"
 
-/* topology_device_list - Master list of all topology devices */
-struct list_head topology_device_list;
+static struct list_head topology_device_list;
+static int topology_crat_parsed;
 static struct kfd_system_properties sys_props;
 
 static DECLARE_RWSEM(topology_lock);
-static atomic_t topology_crat_proximity_domain;
 
 struct kfd_dev *kfd_device_by_id(uint32_t gpu_id)
 {
@@ -61,61 +57,311 @@ struct kfd_dev *kfd_device_by_id(uint32_t gpu_id)
 	return device;
 }
 
-uint32_t kfd_get_gpu_id(struct kfd_dev *dev)
+struct kfd_dev *kfd_device_by_pci_dev(const struct pci_dev *pdev)
 {
 	struct kfd_topology_device *top_dev;
-	uint32_t gpu_id = 0;
+	struct kfd_dev *device = NULL;
 
 	down_read(&topology_lock);
 
 	list_for_each_entry(top_dev, &topology_device_list, list)
-		if (top_dev->gpu == dev) {
-			gpu_id = top_dev->gpu_id;
+		if (top_dev->gpu->pdev == pdev) {
+			device = top_dev->gpu;
 			break;
 		}
 
 	up_read(&topology_lock);
 
-	return gpu_id;
+	return device;
 }
 
-struct kfd_dev *kfd_device_by_pci_dev(const struct pci_dev *pdev)
+static int kfd_topology_get_crat_acpi(void *crat_image, size_t *size)
 {
-	struct kfd_topology_device *top_dev;
-	struct kfd_dev *device = NULL;
+	struct acpi_table_header *crat_table;
+	acpi_status status;
 
-	down_read(&topology_lock);
+	if (!size)
+		return -EINVAL;
 
-	list_for_each_entry(top_dev, &topology_device_list, list)
-		if (top_dev->gpu && top_dev->gpu->pdev == pdev) {
-			device = top_dev->gpu;
+	/*
+	 * Fetch the CRAT table from ACPI
+	 */
+	status = acpi_get_table(CRAT_SIGNATURE, 0, &crat_table);
+	if (status == AE_NOT_FOUND) {
+		pr_warn("CRAT table not found\n");
+		return -ENODATA;
+	} else if (ACPI_FAILURE(status)) {
+		const char *err = acpi_format_exception(status);
+
+		pr_err("CRAT table error: %s\n", err);
+		return -EINVAL;
+	}
+
+	if (*size >= crat_table->length && crat_image != NULL)
+		memcpy(crat_image, crat_table, crat_table->length);
+
+	*size = crat_table->length;
+
+	return 0;
+}
+
+static void kfd_populated_cu_info_cpu(struct kfd_topology_device *dev,
+		struct crat_subtype_computeunit *cu)
+{
+	BUG_ON(!dev);
+	BUG_ON(!cu);
+
+	dev->node_props.cpu_cores_count = cu->num_cpu_cores;
+	dev->node_props.cpu_core_id_base = cu->processor_id_low;
+	if (cu->hsa_capability & CRAT_CU_FLAGS_IOMMU_PRESENT)
+		dev->node_props.capability |= HSA_CAP_ATS_PRESENT;
+
+	pr_info("CU CPU: cores=%d id_base=%d\n", cu->num_cpu_cores,
+			cu->processor_id_low);
+}
+
+static void kfd_populated_cu_info_gpu(struct kfd_topology_device *dev,
+		struct crat_subtype_computeunit *cu)
+{
+	BUG_ON(!dev);
+	BUG_ON(!cu);
+
+	dev->node_props.simd_id_base = cu->processor_id_low;
+	dev->node_props.simd_count = cu->num_simd_cores;
+	dev->node_props.lds_size_in_kb = cu->lds_size_in_kb;
+	dev->node_props.max_waves_per_simd = cu->max_waves_simd;
+	dev->node_props.wave_front_size = cu->wave_front_size;
+	dev->node_props.mem_banks_count = cu->num_banks;
+	dev->node_props.array_count = cu->num_arrays;
+	dev->node_props.cu_per_simd_array = cu->num_cu_per_array;
+	dev->node_props.simd_per_cu = cu->num_simd_per_cu;
+	dev->node_props.max_slots_scratch_cu = cu->max_slots_scatch_cu;
+	if (cu->hsa_capability & CRAT_CU_FLAGS_HOT_PLUGGABLE)
+		dev->node_props.capability |= HSA_CAP_HOT_PLUGGABLE;
+	pr_info("CU GPU: simds=%d id_base=%d\n", cu->num_simd_cores,
+				cu->processor_id_low);
+}
+
+/* kfd_parse_subtype_cu is called when the topology mutex is already acquired */
+static int kfd_parse_subtype_cu(struct crat_subtype_computeunit *cu)
+{
+	struct kfd_topology_device *dev;
+	int i = 0;
+
+	BUG_ON(!cu);
+
+	pr_info("Found CU entry in CRAT table with proximity_domain=%d caps=%x\n",
+			cu->proximity_domain, cu->hsa_capability);
+	list_for_each_entry(dev, &topology_device_list, list) {
+		if (cu->proximity_domain == i) {
+			if (cu->flags & CRAT_CU_FLAGS_CPU_PRESENT)
+				kfd_populated_cu_info_cpu(dev, cu);
+
+			if (cu->flags & CRAT_CU_FLAGS_GPU_PRESENT)
+				kfd_populated_cu_info_gpu(dev, cu);
 			break;
 		}
+		i++;
+	}
 
-	up_read(&topology_lock);
+	return 0;
+}
 
-	return device;
+/*
+ * kfd_parse_subtype_mem is called when the topology mutex is
+ * already acquired
+ */
+static int kfd_parse_subtype_mem(struct crat_subtype_memory *mem)
+{
+	struct kfd_mem_properties *props;
+	struct kfd_topology_device *dev;
+	int i = 0;
+
+	BUG_ON(!mem);
+
+	pr_info("Found memory entry in CRAT table with proximity_domain=%d\n",
+			mem->promixity_domain);
+	list_for_each_entry(dev, &topology_device_list, list) {
+		if (mem->promixity_domain == i) {
+			props = kfd_alloc_struct(props);
+			if (props == NULL)
+				return -ENOMEM;
+
+			if (dev->node_props.cpu_cores_count == 0)
+				props->heap_type = HSA_MEM_HEAP_TYPE_FB_PRIVATE;
+			else
+				props->heap_type = HSA_MEM_HEAP_TYPE_SYSTEM;
+
+			if (mem->flags & CRAT_MEM_FLAGS_HOT_PLUGGABLE)
+				props->flags |= HSA_MEM_FLAGS_HOT_PLUGGABLE;
+			if (mem->flags & CRAT_MEM_FLAGS_NON_VOLATILE)
+				props->flags |= HSA_MEM_FLAGS_NON_VOLATILE;
+
+			props->size_in_bytes =
+				((uint64_t)mem->length_high << 32) +
+							mem->length_low;
+			props->width = mem->width;
+
+			dev->mem_bank_count++;
+			list_add_tail(&props->list, &dev->mem_props);
+
+			break;
+		}
+		i++;
+	}
+
+	return 0;
 }
 
-struct kfd_dev *kfd_device_by_kgd(const struct kgd_dev *kgd)
+/*
+ * kfd_parse_subtype_cache is called when the topology mutex
+ * is already acquired
+ */
+static int kfd_parse_subtype_cache(struct crat_subtype_cache *cache)
 {
-	struct kfd_topology_device *top_dev;
-	struct kfd_dev *device = NULL;
+	struct kfd_cache_properties *props;
+	struct kfd_topology_device *dev;
+	uint32_t id;
 
-	down_read(&topology_lock);
+	BUG_ON(!cache);
+
+	id = cache->processor_id_low;
+
+	pr_info("Found cache entry in CRAT table with processor_id=%d\n", id);
+	list_for_each_entry(dev, &topology_device_list, list)
+		if (id == dev->node_props.cpu_core_id_base ||
+		    id == dev->node_props.simd_id_base) {
+			props = kfd_alloc_struct(props);
+			if (props == NULL)
+				return -ENOMEM;
+
+			props->processor_id_low = id;
+			props->cache_level = cache->cache_level;
+			props->cache_size = cache->cache_size;
+			props->cacheline_size = cache->cache_line_size;
+			props->cachelines_per_tag = cache->lines_per_tag;
+			props->cache_assoc = cache->associativity;
+			props->cache_latency = cache->cache_latency;
+
+			if (cache->flags & CRAT_CACHE_FLAGS_DATA_CACHE)
+				props->cache_type |= HSA_CACHE_TYPE_DATA;
+			if (cache->flags & CRAT_CACHE_FLAGS_INST_CACHE)
+				props->cache_type |= HSA_CACHE_TYPE_INSTRUCTION;
+			if (cache->flags & CRAT_CACHE_FLAGS_CPU_CACHE)
+				props->cache_type |= HSA_CACHE_TYPE_CPU;
+			if (cache->flags & CRAT_CACHE_FLAGS_SIMD_CACHE)
+				props->cache_type |= HSA_CACHE_TYPE_HSACU;
+
+			dev->cache_count++;
+			dev->node_props.caches_count++;
+			list_add_tail(&props->list, &dev->cache_props);
 
-	list_for_each_entry(top_dev, &topology_device_list, list)
-		if (top_dev->gpu && top_dev->gpu->kgd == kgd) {
-			device = top_dev->gpu;
 			break;
 		}
 
-	up_read(&topology_lock);
+	return 0;
+}
 
-	return device;
+/*
+ * kfd_parse_subtype_iolink is called when the topology mutex
+ * is already acquired
+ */
+static int kfd_parse_subtype_iolink(struct crat_subtype_iolink *iolink)
+{
+	struct kfd_iolink_properties *props;
+	struct kfd_topology_device *dev;
+	uint32_t i = 0;
+	uint32_t id_from;
+	uint32_t id_to;
+
+	BUG_ON(!iolink);
+
+	id_from = iolink->proximity_domain_from;
+	id_to = iolink->proximity_domain_to;
+
+	pr_info("Found IO link entry in CRAT table with id_from=%d\n", id_from);
+	list_for_each_entry(dev, &topology_device_list, list) {
+		if (id_from == i) {
+			props = kfd_alloc_struct(props);
+			if (props == NULL)
+				return -ENOMEM;
+
+			props->node_from = id_from;
+			props->node_to = id_to;
+			props->ver_maj = iolink->version_major;
+			props->ver_min = iolink->version_minor;
+
+			/*
+			 * weight factor (derived from CDIR), currently always 1
+			 */
+			props->weight = 1;
+
+			props->min_latency = iolink->minimum_latency;
+			props->max_latency = iolink->maximum_latency;
+			props->min_bandwidth = iolink->minimum_bandwidth_mbs;
+			props->max_bandwidth = iolink->maximum_bandwidth_mbs;
+			props->rec_transfer_size =
+					iolink->recommended_transfer_size;
+
+			dev->io_link_count++;
+			dev->node_props.io_links_count++;
+			list_add_tail(&props->list, &dev->io_link_props);
+
+			break;
+		}
+		i++;
+	}
+
+	return 0;
+}
+
+static int kfd_parse_subtype(struct crat_subtype_generic *sub_type_hdr)
+{
+	struct crat_subtype_computeunit *cu;
+	struct crat_subtype_memory *mem;
+	struct crat_subtype_cache *cache;
+	struct crat_subtype_iolink *iolink;
+	int ret = 0;
+
+	BUG_ON(!sub_type_hdr);
+
+	switch (sub_type_hdr->type) {
+	case CRAT_SUBTYPE_COMPUTEUNIT_AFFINITY:
+		cu = (struct crat_subtype_computeunit *)sub_type_hdr;
+		ret = kfd_parse_subtype_cu(cu);
+		break;
+	case CRAT_SUBTYPE_MEMORY_AFFINITY:
+		mem = (struct crat_subtype_memory *)sub_type_hdr;
+		ret = kfd_parse_subtype_mem(mem);
+		break;
+	case CRAT_SUBTYPE_CACHE_AFFINITY:
+		cache = (struct crat_subtype_cache *)sub_type_hdr;
+		ret = kfd_parse_subtype_cache(cache);
+		break;
+	case CRAT_SUBTYPE_TLB_AFFINITY:
+		/*
+		 * For now, nothing to do here
+		 */
+		pr_info("Found TLB entry in CRAT table (not processing)\n");
+		break;
+	case CRAT_SUBTYPE_CCOMPUTE_AFFINITY:
+		/*
+		 * For now, nothing to do here
+		 */
+		pr_info("Found CCOMPUTE entry in CRAT table (not processing)\n");
+		break;
+	case CRAT_SUBTYPE_IOLINK_AFFINITY:
+		iolink = (struct crat_subtype_iolink *)sub_type_hdr;
+		ret = kfd_parse_subtype_iolink(iolink);
+		break;
+	default:
+		pr_warn("Unknown subtype (%d) in CRAT\n",
+				sub_type_hdr->type);
+	}
+
+	return ret;
 }
 
-/* Called with write topology_lock acquired */
 static void kfd_release_topology_device(struct kfd_topology_device *dev)
 {
 	struct kfd_mem_properties *mem;
@@ -152,22 +398,20 @@ static void kfd_release_topology_device(struct kfd_topology_device *dev)
 	sys_props.num_devices--;
 }
 
-void kfd_release_live_view(void)
+static void kfd_release_live_view(void)
 {
 	struct kfd_topology_device *dev;
 
-	down_write(&topology_lock);
 	while (topology_device_list.next != &topology_device_list) {
 		dev = container_of(topology_device_list.next,
 				 struct kfd_topology_device, list);
 		kfd_release_topology_device(dev);
-	}
-	up_write(&topology_lock);
+}
+
 	memset(&sys_props, 0, sizeof(sys_props));
 }
 
-struct kfd_topology_device *kfd_create_topology_device(
-				struct list_head *device_list)
+static struct kfd_topology_device *kfd_create_topology_device(void)
 {
 	struct kfd_topology_device *dev;
 
@@ -181,12 +425,65 @@ struct kfd_topology_device *kfd_create_topology_device(
 	INIT_LIST_HEAD(&dev->cache_props);
 	INIT_LIST_HEAD(&dev->io_link_props);
 
-	list_add_tail(&dev->list, device_list);
+	list_add_tail(&dev->list, &topology_device_list);
 	sys_props.num_devices++;
 
 	return dev;
 }
 
+static int kfd_parse_crat_table(void *crat_image)
+{
+	struct kfd_topology_device *top_dev;
+	struct crat_subtype_generic *sub_type_hdr;
+	uint16_t node_id;
+	int ret;
+	struct crat_header *crat_table = (struct crat_header *)crat_image;
+	uint16_t num_nodes;
+	uint32_t image_len;
+
+	if (!crat_image)
+		return -EINVAL;
+
+	num_nodes = crat_table->num_domains;
+	image_len = crat_table->length;
+
+	pr_info("Parsing CRAT table with %d nodes\n", num_nodes);
+
+	for (node_id = 0; node_id < num_nodes; node_id++) {
+		top_dev = kfd_create_topology_device();
+		if (!top_dev) {
+			kfd_release_live_view();
+			return -ENOMEM;
+		}
+	}
+
+	sys_props.platform_id =
+		(*((uint64_t *)crat_table->oem_id)) & CRAT_OEMID_64BIT_MASK;
+	sys_props.platform_oem = *((uint64_t *)crat_table->oem_table_id);
+	sys_props.platform_rev = crat_table->revision;
+
+	sub_type_hdr = (struct crat_subtype_generic *)(crat_table+1);
+	while ((char *)sub_type_hdr + sizeof(struct crat_subtype_generic) <
+			((char *)crat_image) + image_len) {
+		if (sub_type_hdr->flags & CRAT_SUBTYPE_FLAGS_ENABLED) {
+			ret = kfd_parse_subtype(sub_type_hdr);
+			if (ret != 0) {
+				kfd_release_live_view();
+				return ret;
+			}
+		}
+
+		sub_type_hdr = (typeof(sub_type_hdr))((char *)sub_type_hdr +
+				sub_type_hdr->length);
+	}
+
+	sys_props.generation_count++;
+	topology_crat_parsed = 1;
+
+	return 0;
+}
+
+
 #define sysfs_show_gen_prop(buffer, fmt, ...) \
 		snprintf(buffer, PAGE_SIZE, "%s"fmt, buffer, __VA_ARGS__)
 #define sysfs_show_32bit_prop(buffer, name, value) \
@@ -222,17 +519,11 @@ static ssize_t sysprops_show(struct kobject *kobj, struct attribute *attr,
 	return ret;
 }
 
-static void kfd_topology_kobj_release(struct kobject *kobj)
-{
-	kfree(kobj);
-}
-
 static const struct sysfs_ops sysprops_ops = {
 	.show = sysprops_show,
 };
 
 static struct kobj_type sysprops_type = {
-	.release = kfd_topology_kobj_release,
 	.sysfs_ops = &sysprops_ops,
 };
 
@@ -268,7 +559,6 @@ static const struct sysfs_ops iolink_ops = {
 };
 
 static struct kobj_type iolink_type = {
-	.release = kfd_topology_kobj_release,
 	.sysfs_ops = &iolink_ops,
 };
 
@@ -296,7 +586,6 @@ static const struct sysfs_ops mem_ops = {
 };
 
 static struct kobj_type mem_type = {
-	.release = kfd_topology_kobj_release,
 	.sysfs_ops = &mem_ops,
 };
 
@@ -304,7 +593,7 @@ static ssize_t kfd_cache_show(struct kobject *kobj, struct attribute *attr,
 		char *buffer)
 {
 	ssize_t ret;
-	uint32_t i, j;
+	uint32_t i;
 	struct kfd_cache_properties *cache;
 
 	/* Making sure that the buffer is an empty string */
@@ -322,18 +611,12 @@ static ssize_t kfd_cache_show(struct kobject *kobj, struct attribute *attr,
 	sysfs_show_32bit_prop(buffer, "latency", cache->cache_latency);
 	sysfs_show_32bit_prop(buffer, "type", cache->cache_type);
 	snprintf(buffer, PAGE_SIZE, "%ssibling_map ", buffer);
-	for (i = 0; i < CRAT_SIBLINGMAP_SIZE; i++)
-		for (j = 0; j < sizeof(cache->sibling_map[0])*8; j++) {
-			/* Check each bit */
-			if (cache->sibling_map[i] & (1 << j))
-				ret = snprintf(buffer, PAGE_SIZE,
-					 "%s%d%s", buffer, 1, ",");
-			else
-				ret = snprintf(buffer, PAGE_SIZE,
-					 "%s%d%s", buffer, 0, ",");
-		}
-	/* Replace the last "," with end of line */
-	*(buffer + strlen(buffer) - 1) = 0xA;
+	for (i = 0; i < KFD_TOPOLOGY_CPU_SIBLINGS; i++)
+		ret = snprintf(buffer, PAGE_SIZE, "%s%d%s",
+				buffer, cache->sibling_map[i],
+				(i == KFD_TOPOLOGY_CPU_SIBLINGS-1) ?
+						"\n" : ",");
+
 	return ret;
 }
 
@@ -342,7 +625,6 @@ static const struct sysfs_ops cache_ops = {
 };
 
 static struct kobj_type cache_type = {
-	.release = kfd_topology_kobj_release,
 	.sysfs_ops = &cache_ops,
 };
 
@@ -353,7 +635,6 @@ static ssize_t node_show(struct kobject *kobj, struct attribute *attr,
 	char public_name[KFD_TOPOLOGY_PUBLIC_NAME_SIZE];
 	uint32_t i;
 	uint32_t log_max_watch_addr;
-	struct kfd_local_mem_info local_mem_info;
 
 	/* Making sure that the buffer is an empty string */
 	buffer[0] = 0;
@@ -384,8 +665,16 @@ static ssize_t node_show(struct kobject *kobj, struct attribute *attr,
 	sysfs_show_32bit_prop(buffer, "simd_count",
 			dev->node_props.simd_count);
 
-	sysfs_show_32bit_prop(buffer, "mem_banks_count",
+	if (dev->mem_bank_count < dev->node_props.mem_banks_count) {
+		pr_info_once("kfd: mem_banks_count truncated from %d to %d\n",
+				dev->node_props.mem_banks_count,
+				dev->mem_bank_count);
+		sysfs_show_32bit_prop(buffer, "mem_banks_count",
+				dev->mem_bank_count);
+	} else {
+		sysfs_show_32bit_prop(buffer, "mem_banks_count",
 				dev->node_props.mem_banks_count);
+	}
 
 	sysfs_show_32bit_prop(buffer, "caches_count",
 			dev->node_props.caches_count);
@@ -434,30 +723,17 @@ static ssize_t node_show(struct kobject *kobj, struct attribute *attr,
 				HSA_CAP_WATCH_POINTS_TOTALBITS_MASK);
 		}
 
-		if (dev->gpu->device_info->asic_family == CHIP_TONGA)
-			dev->node_props.capability |=
-					HSA_CAP_AQL_QUEUE_DOUBLE_MAP;
-
 		sysfs_show_32bit_prop(buffer, "max_engine_clk_fcompute",
-			dev->node_props.max_engine_clk_fcompute);
+			dev->gpu->kfd2kgd->get_max_engine_clock_in_mhz(
+					dev->gpu->kgd));
 
-		/*
-		 * If the ASIC is CZ, set local memory size to 0 to disable
-		 * local memory support
-		 */
-		if (dev->gpu->device_info->asic_family != CHIP_CARRIZO) {
-			dev->gpu->kfd2kgd->get_local_mem_info(dev->gpu->kgd,
-				&local_mem_info);
-			sysfs_show_64bit_prop(buffer, "local_mem_size",
-					local_mem_info.local_mem_size_private +
-					local_mem_info.local_mem_size_public);
-		}
-		else
-			sysfs_show_64bit_prop(buffer, "local_mem_size",
-					(unsigned long long int) 0);
+		sysfs_show_64bit_prop(buffer, "local_mem_size",
+				(unsigned long long int) 0);
 
 		sysfs_show_32bit_prop(buffer, "fw_version",
-				dev->gpu->mec_fw_version);
+			dev->gpu->kfd2kgd->get_fw_version(
+						dev->gpu->kgd,
+						KGD_ENGINE_MEC1));
 		sysfs_show_32bit_prop(buffer, "capability",
 				dev->node_props.capability);
 	}
@@ -471,7 +747,6 @@ static const struct sysfs_ops node_ops = {
 };
 
 static struct kobj_type node_type = {
-	.release = kfd_topology_kobj_release,
 	.sysfs_ops = &node_ops,
 };
 
@@ -653,7 +928,6 @@ static int kfd_build_sysfs_node_entry(struct kfd_topology_device *dev,
 	return 0;
 }
 
-/* Called with write topology lock acquired */
 static int kfd_build_sysfs_node_tree(void)
 {
 	struct kfd_topology_device *dev;
@@ -670,7 +944,6 @@ static int kfd_build_sysfs_node_tree(void)
 	return 0;
 }
 
-/* Called with write topology lock acquired */
 static void kfd_remove_sysfs_node_tree(void)
 {
 	struct kfd_topology_device *dev;
@@ -742,200 +1015,88 @@ static void kfd_topology_release_sysfs(void)
 	}
 }
 
-/* Called with write topology_lock acquired */
-static int kfd_topology_update_device_list(struct list_head *temp_list,
-					struct list_head *master_list)
-{
-	int num = 0;
-
-	while (!list_empty(temp_list)) {
-		list_move_tail(temp_list->next, master_list);
-		num++;
-	}
-	return num;
-}
-
-static void kfd_debug_print_topology(void)
-{
-	struct kfd_topology_device *dev;
-
-	down_read(&topology_lock);
-
-	dev = list_last_entry(&topology_device_list, struct kfd_topology_device, list);
-	if (dev) {
-		if (dev->node_props.cpu_cores_count && dev->node_props.simd_count) {
-			pr_info("Topology: Add APU node [0x%0x:0x%0x]\n",
-				dev->node_props.device_id, dev->node_props.vendor_id);
-		}
-		else if (dev->node_props.cpu_cores_count)
-			pr_info("Topology: Add CPU node\n");
-		else if (dev->node_props.simd_count)
-			pr_info("Topology: Add dGPU node [0x%0x:0x%0x]\n",
-				dev->node_props.device_id, dev->node_props.vendor_id);
-	}
-	up_read(&topology_lock);
-}
-
-/* Helper function for intializing platform_xx members of kfd_system_properties
- */
-static void kfd_update_system_properties(void)
-{
-	struct kfd_topology_device *dev;
-
-	down_read(&topology_lock);
-	dev = list_last_entry(&topology_device_list, struct kfd_topology_device, list);
-	if (dev) {
-		sys_props.platform_id =
-			(*((uint64_t *)dev->oem_id)) & CRAT_OEMID_64BIT_MASK;
-		sys_props.platform_oem = *((uint64_t *)dev->oem_table_id);
-		sys_props.platform_rev = dev->oem_revision;
-	}
-	up_read(&topology_lock);
-}
-
-static void find_system_memory(const struct dmi_header *dm,
-	void *private)
-{
-	struct kfd_mem_properties *mem;
-	u16 mem_width, mem_clock;
-	struct kfd_topology_device *kdev =
-		(struct kfd_topology_device *)private;
-	const u8 *dmi_data = (const u8 *)(dm + 1);
-
-	if (dm->type == DMI_ENTRY_MEM_DEVICE && dm->length >= 0x15) {
-		mem_width = (u16)(*(const u16 *)(dmi_data + 0x6));
-		mem_clock = (u16)(*(const u16 *)(dmi_data + 0x11));
-		list_for_each_entry(mem, &kdev->mem_props, list) {
-			if (mem_width != 0xFFFF && mem_width != 0)
-				mem->width = mem_width;
-			if (mem_clock != 0)
-				mem->mem_clk_max = mem_clock;
-		}
-	}
-}
-/* kfd_add_non_crat_information - Add information that is not currently
- *	defined in CRAT but is necessary for KFD topology
- * @dev - topology device to which addition info is added
- */
-static void kfd_add_non_crat_information(struct kfd_topology_device *kdev)
-{
-	/* Check if CPU only node. */
-	if (kdev->gpu == NULL) {
-		/* Add system memory information */
-		dmi_walk(find_system_memory, kdev);
-	}
-	/* TODO: For GPU node, rearrange code from kfd_topology_add_device */
-}
-
 int kfd_topology_init(void)
 {
 	void *crat_image = NULL;
 	size_t image_size = 0;
 	int ret;
-	struct list_head temp_topology_device_list;
-	int cpu_only_node = 0;
-	struct kfd_topology_device *kdev;
-	int proximity_domain;
-	int num_nodes;
-
-	/* topology_device_list - Master list of all topology devices
-	 * temp_topology_device_list - temporary list created while parsing CRAT
-	 * or VCRAT. Once parsing is complete the contents of list is moved to
-	 * topology_device_list
-	 */
 
-	 /* Initialize the head for the both the lists
+	/*
+	 * Initialize the head for the topology device list
 	 */
 	INIT_LIST_HEAD(&topology_device_list);
-	INIT_LIST_HEAD(&temp_topology_device_list);
 	init_rwsem(&topology_lock);
+	topology_crat_parsed = 0;
 
 	memset(&sys_props, 0, sizeof(sys_props));
 
-	/* Proximity domains in ACPI CRAT tables start counting at
-	 * 0. The same should be true for virtual CRAT tables created
-	 * at this stage. GPUs added later in kfd_topology_add_device
-	 * use a counter. */
-	proximity_domain = 0;
-
 	/*
-	 * Get the CRAT image from the ACPI. If ACPI doesn't have one
-	 * create a virtual CRAT.
-	 * NOTE: The current implementation expects all AMD APUs to have
-	 *	CRAT. If no CRAT is available, it is assumed to be a CPU
+	 * Get the CRAT image from the ACPI
 	 */
-	ret = kfd_create_crat_image_acpi(&crat_image, &image_size);
-	if (ret != 0) {
-		ret = kfd_create_crat_image_virtual(&crat_image, &image_size,
-				COMPUTE_UNIT_CPU, NULL,
-				proximity_domain);
-		cpu_only_node = 1;
-	}
-
-	if (ret == 0)
-		ret = kfd_parse_crat_table(crat_image,
-				&temp_topology_device_list,
-				proximity_domain);
-	else {
-		pr_err("Error getting/creating CRAT table\n");
-		goto err;
-	}
-
-	down_write(&topology_lock);
-	num_nodes = kfd_topology_update_device_list(&temp_topology_device_list,
-						    &topology_device_list);
-	atomic_set(&topology_crat_proximity_domain, num_nodes-1);
-	ret = kfd_topology_update_sysfs();
-	up_write(&topology_lock);
-
-	if (ret == 0) {
-		sys_props.generation_count++;
-		kfd_update_system_properties();
-		kfd_debug_print_topology();
-		pr_info("Finished initializing topology\n");
-	}
-	else
-		pr_err("Failed to update topology in sysfs ret=%d\n", ret);
-
-	/* For nodes with GPU, this information gets added
-	 * when GPU is detected (kfd_topology_add_device). */
-	if (cpu_only_node) {
-		/* Add additional information to CPU only node created above */
-		down_write(&topology_lock);
-		kdev = list_first_entry(&topology_device_list,
-				struct kfd_topology_device, list);
-		up_write(&topology_lock);
-		kfd_add_non_crat_information(kdev);
+	ret = kfd_topology_get_crat_acpi(crat_image, &image_size);
+	if (ret == 0 && image_size > 0) {
+		pr_info("Found CRAT image with size=%zd\n", image_size);
+		crat_image = kmalloc(image_size, GFP_KERNEL);
+		if (!crat_image) {
+			ret = -ENOMEM;
+			pr_err("No memory for allocating CRAT image\n");
+			goto err;
+		}
+		ret = kfd_topology_get_crat_acpi(crat_image, &image_size);
+
+		if (ret == 0) {
+			down_write(&topology_lock);
+			ret = kfd_parse_crat_table(crat_image);
+			if (ret == 0)
+				ret = kfd_topology_update_sysfs();
+			up_write(&topology_lock);
+		} else {
+			pr_err("Couldn't get CRAT table size from ACPI\n");
+		}
+		kfree(crat_image);
+	} else if (ret == -ENODATA) {
+		ret = 0;
+	} else {
+		pr_err("Couldn't get CRAT table size from ACPI\n");
 	}
 
 err:
-	kfd_destroy_crat_image(crat_image);
+	pr_info("Finished initializing topology ret=%d\n", ret);
 	return ret;
 }
 
 void kfd_topology_shutdown(void)
 {
-	down_write(&topology_lock);
 	kfd_topology_release_sysfs();
-	up_write(&topology_lock);
 	kfd_release_live_view();
 }
 
+static void kfd_debug_print_topology(void)
+{
+	struct kfd_topology_device *dev;
+	uint32_t i = 0;
+
+	pr_info("DEBUG PRINT OF TOPOLOGY:");
+	list_for_each_entry(dev, &topology_device_list, list) {
+		pr_info("Node: %d\n", i);
+		pr_info("\tGPU assigned: %s\n", (dev->gpu ? "yes" : "no"));
+		pr_info("\tCPU count: %d\n", dev->node_props.cpu_cores_count);
+		pr_info("\tSIMD count: %d", dev->node_props.simd_count);
+		i++;
+	}
+}
+
 static uint32_t kfd_generate_gpu_id(struct kfd_dev *gpu)
 {
 	uint32_t hashout;
 	uint32_t buf[7];
 	uint64_t local_mem_size;
 	int i;
-	struct kfd_local_mem_info local_mem_info;
 
 	if (!gpu)
 		return 0;
 
-	gpu->kfd2kgd->get_local_mem_info(gpu->kgd, &local_mem_info);
-
-	local_mem_size = local_mem_info.local_mem_size_private +
-			local_mem_info.local_mem_size_public;
+	local_mem_size = gpu->kfd2kgd->get_vmem_size(gpu->kgd);
 
 	buf[0] = gpu->pdev->devfn;
 	buf[1] = gpu->pdev->subsystem_vendor;
@@ -950,13 +1111,7 @@ static uint32_t kfd_generate_gpu_id(struct kfd_dev *gpu)
 
 	return hashout;
 }
-/* kfd_assign_gpu - Attach @gpu to the correct kfd topology device. If
- *		the GPU device is not already present in the topology device list
- *		then return NULL. This means a new topology device has to be
- *		created for this GPU.
- * TODO: Rather than assiging @gpu to first topology device withtout
- *		gpu attached, it will better to have more stringent check.
- */
+
 static struct kfd_topology_device *kfd_assign_gpu(struct kfd_dev *gpu)
 {
 	struct kfd_topology_device *dev;
@@ -964,14 +1119,13 @@ static struct kfd_topology_device *kfd_assign_gpu(struct kfd_dev *gpu)
 
 	BUG_ON(!gpu);
 
-	down_write(&topology_lock);
 	list_for_each_entry(dev, &topology_device_list, list)
 		if (dev->gpu == NULL && dev->node_props.simd_count > 0) {
 			dev->gpu = gpu;
 			out_dev = dev;
 			break;
 		}
-	up_write(&topology_lock);
+
 	return out_dev;
 }
 
@@ -983,165 +1137,70 @@ static void kfd_notify_gpu_change(uint32_t gpu_id, int arrival)
 	 */
 }
 
-/* kfd_fill_mem_clk_max_info - Since CRAT doesn't have memory clock info,
- *		patch this after CRAT parsing.
- */
-static void kfd_fill_mem_clk_max_info(struct kfd_topology_device *dev)
-{
-	struct kfd_mem_properties *mem;
-	struct kfd_local_mem_info local_mem_info;
-
-	if (dev == NULL)
-		return;
-
-	/* Currently, amdgpu driver (amdgpu_mc) deals only with GPUs with
-	 * single bank of VRAM local memory.
-	 * for dGPUs - VCRAT reports only one bank of Local Memory
-	 * for APUs - If CRAT from ACPI reports more than one bank, then
-	 *	all the banks will report the same mem_clk_max information
-	 */
-	dev->gpu->kfd2kgd->get_local_mem_info(dev->gpu->kgd,
-		&local_mem_info);
-
-	list_for_each_entry(mem, &dev->mem_props, list)
-		mem->mem_clk_max = local_mem_info.mem_clk_max;
-}
-
-static void kfd_fill_iolink_non_crat_info(struct kfd_topology_device *dev)
-{
-	struct kfd_iolink_properties *link;
-
-	if ((dev == NULL) || (dev->gpu == NULL))
-		return;
-
-	/* GPU only creates direck links so apply flags setting to all */
-	if (dev->gpu->device_info->asic_family == CHIP_HAWAII)
-		list_for_each_entry(link, &dev->io_link_props, list)
-			link->flags = CRAT_IOLINK_FLAGS_ENABLED |
-				CRAT_IOLINK_FLAGS_NO_ATOMICS_32_BIT |
-				CRAT_IOLINK_FLAGS_NO_ATOMICS_64_BIT;
-}
-
 int kfd_topology_add_device(struct kfd_dev *gpu)
 {
 	uint32_t gpu_id;
 	struct kfd_topology_device *dev;
-	struct kfd_cu_info cu_info;
-	int res = 0;
-	struct list_head temp_topology_device_list;
-	void *crat_image = NULL;
-	size_t image_size = 0;
-	int proximity_domain;
+	int res;
 
 	BUG_ON(!gpu);
 
-	INIT_LIST_HEAD(&temp_topology_device_list);
-
 	gpu_id = kfd_generate_gpu_id(gpu);
 
 	pr_debug("kfd: Adding new GPU (ID: 0x%x) to topology\n", gpu_id);
 
-	proximity_domain = atomic_inc_return(&
-				topology_crat_proximity_domain);
-
-	/* Check to see if this gpu device exists in the topology_device_list.
-	 * If so, assign the gpu to that device,
-	 * else create a Virtual CRAT for this gpu device and then parse that CRAT
-	 * to create a new topology device. Once created assign the gpu to that
-	 * topology device
+	down_write(&topology_lock);
+	/*
+	 * Try to assign the GPU to existing topology device (generated from
+	 * CRAT table
 	 */
 	dev = kfd_assign_gpu(gpu);
 	if (!dev) {
-		res = kfd_create_crat_image_virtual(&crat_image, &image_size,
-				COMPUTE_UNIT_GPU,
-				gpu, proximity_domain);
-		if (res == 0)
-			res = kfd_parse_crat_table(crat_image,
-				&temp_topology_device_list, proximity_domain);
-		else {
-			pr_err("Error in VCRAT for GPU (ID: 0x%x)\n", gpu_id);
+		pr_info("GPU was not found in the current topology. Extending.\n");
+		kfd_debug_print_topology();
+		dev = kfd_create_topology_device();
+		if (!dev) {
+			res = -ENOMEM;
 			goto err;
 		}
+		dev->gpu = gpu;
 
-		down_write(&topology_lock);
-		kfd_topology_update_device_list(&temp_topology_device_list,
-			&topology_device_list);
+		/*
+		 * TODO: Make a call to retrieve topology information from the
+		 * GPU vBIOS
+		 */
 
 		/*
 		 * Update the SYSFS tree, since we added another topology device
 		 */
-		res = kfd_topology_update_sysfs();
-		up_write(&topology_lock);
-
-		if (res == 0)
-			sys_props.generation_count++;
-		else
-			pr_err("Failed to update GPU (ID: 0x%x) to sysfs topology. res=%d\n",
-						gpu_id, res);
-		dev = kfd_assign_gpu(gpu);
-		BUG_ON(!dev);
+		if (kfd_topology_update_sysfs() < 0)
+			kfd_topology_release_sysfs();
+
 	}
 
 	dev->gpu_id = gpu_id;
 	gpu->id = gpu_id;
-
-	/* TODO: Move the following lines to function
-	 *	kfd_add_non_crat_information */
-
-	/* Fill-in additional information that is not available in CRAT but
-	 * needed for the topology */
-
-	dev->gpu->kfd2kgd->get_cu_info(dev->gpu->kgd, &cu_info);
-	dev->node_props.simd_arrays_per_engine = cu_info.num_shader_arrays_per_engine;
-
 	dev->node_props.vendor_id = gpu->pdev->vendor;
 	dev->node_props.device_id = gpu->pdev->device;
-	dev->node_props.location_id = PCI_DEVID(gpu->pdev->bus->number,
-		gpu->pdev->devfn);
-	dev->node_props.max_engine_clk_fcompute =
-		dev->gpu->kfd2kgd->get_max_engine_clock_in_mhz(dev->gpu->kgd);
-	dev->node_props.max_engine_clk_ccompute =
-		cpufreq_quick_get_max(0) / 1000;
-
-	kfd_fill_mem_clk_max_info(dev);
-	kfd_fill_iolink_non_crat_info(dev);
-
-	switch (dev->gpu->device_info->asic_family) {
-	case CHIP_KAVERI:
-	case CHIP_HAWAII:
-	case CHIP_TONGA:
-		dev->node_props.capability |= ((HSA_CAP_DOORBELL_TYPE_PRE_1_0 <<
-			HSA_CAP_DOORBELL_TYPE_TOTALBITS_SHIFT) &
-			HSA_CAP_DOORBELL_TYPE_TOTALBITS_MASK);
-		break;
-	case CHIP_CARRIZO:
-	case CHIP_FIJI:
-	case CHIP_POLARIS10:
-	case CHIP_POLARIS11:
-		pr_debug("amdkfd: adding doorbell packet type capability\n");
-		dev->node_props.capability |= ((HSA_CAP_DOORBELL_TYPE_1_0 <<
-			HSA_CAP_DOORBELL_TYPE_TOTALBITS_SHIFT) &
-			HSA_CAP_DOORBELL_TYPE_TOTALBITS_MASK);
-		break;
-	}
-
-	/* Fix errors in CZ CRAT.
-	 * simd_count: Carrizo CRAT reports wrong simd_count, probably because it
-	 *		doesn't consider masked out CUs
-	 * capability flag: Carrizo CRAT doesn't report IOMMU flags.
+	dev->node_props.location_id = (gpu->pdev->bus->number << 24) +
+			(gpu->pdev->devfn & 0xffffff);
+	/*
+	 * TODO: Retrieve max engine clock values from KGD
 	 */
+
 	if (dev->gpu->device_info->asic_family == CHIP_CARRIZO) {
-		dev->node_props.simd_count =
-			cu_info.simd_per_cu * cu_info.cu_active_number;
-		dev->node_props.capability |= HSA_CAP_ATS_PRESENT;
-        }
+		dev->node_props.capability |= HSA_CAP_DOORBELL_PACKET_TYPE;
+		pr_info("amdkfd: adding doorbell packet type capability\n");
+	}
+
+	res = 0;
 
-	kfd_debug_print_topology();
 err:
+	up_write(&topology_lock);
+
 	if (res == 0)
 		kfd_notify_gpu_change(gpu_id, 1);
 
-	kfd_destroy_crat_image(crat_image);
 	return res;
 }
 
@@ -1174,140 +1233,29 @@ int kfd_topology_remove_device(struct kfd_dev *gpu)
 	return res;
 }
 
-/* kfd_topology_enum_kfd_devices - Enumerate through all devices in KFD
- *	topology. If GPU device is found @idx, then valid kfd_dev pointer is
- *	returned through @kdev
- * Return -	0: On success (@kdev will be NULL for non GPU nodes)
- *		-1: If end of list
+/*
+ * When idx is out of bounds, the function will return NULL
  */
-int kfd_topology_enum_kfd_devices(uint8_t idx, struct kfd_dev **kdev)
+struct kfd_dev *kfd_topology_enum_kfd_devices(uint8_t idx)
 {
 
 	struct kfd_topology_device *top_dev;
+	struct kfd_dev *device = NULL;
 	uint8_t device_idx = 0;
 
-	*kdev = NULL;
 	down_read(&topology_lock);
 
 	list_for_each_entry(top_dev, &topology_device_list, list) {
 		if (device_idx == idx) {
-			*kdev = top_dev->gpu;
-			up_read(&topology_lock);
-			return 0;
-		}
-
-		device_idx++;
-	}
-
-	up_read(&topology_lock);
-
-	return -1;
-
-}
-
-static int kfd_cpumask_to_apic_id(const struct cpumask *cpumask)
-{
-	const struct cpuinfo_x86 *cpuinfo;
-	int first_cpu_of_numa_node;
-
-	if (cpumask == NULL || cpumask == cpu_none_mask)
-		return -1;
-	first_cpu_of_numa_node = cpumask_first(cpumask);
-	if (first_cpu_of_numa_node >= nr_cpu_ids)
-		return -1;
-	cpuinfo = &cpu_data(first_cpu_of_numa_node);
-
-	return cpuinfo->apicid;
-}
-
-/* kfd_numa_node_to_apic_id - Returns the APIC ID of the first logical processor
- *	of the given NUMA node (numa_node_id)
- * Return -1 on failure
- */
-int kfd_numa_node_to_apic_id(int numa_node_id)
-{
-	if (numa_node_id == -1) {
-		pr_warn("Invalid NUMA Node. Use online CPU mask\n");
-		return kfd_cpumask_to_apic_id(cpu_online_mask);
-	}
-	return kfd_cpumask_to_apic_id(cpumask_of_node(numa_node_id));
-}
-
-/* kfd_get_proximity_domain - Find proximity_domain (node id) to which
- *  given PCI bus belongs to. CRAT table contains only the APIC ID
- *  of the parent NUMA node. So use that as the search parameter.
- * Return -1 on failure
- */
-int kfd_get_proximity_domain(const struct pci_bus *bus)
-{
-	struct kfd_topology_device *dev;
-	int proximity_domain = -1;
-
-	down_read(&topology_lock);
-
-	list_for_each_entry(dev, &topology_device_list, list)
-		if (dev->node_props.cpu_cores_count &&
-			dev->node_props.cpu_core_id_base ==
-			kfd_cpumask_to_apic_id(cpumask_of_pcibus(bus))) {
-				proximity_domain = dev->proximity_domain;
+			device = top_dev->gpu;
 			break;
 		}
 
-	up_read(&topology_lock);
-
-	return proximity_domain;
-}
-
-#if defined(CONFIG_DEBUG_FS)
-
-int kfd_debugfs_hqds_by_device(struct seq_file *m, void *data)
-{
-	struct kfd_topology_device *dev;
-	unsigned i = 0;
-	int r = 0;
-
-	down_read(&topology_lock);
-
-	list_for_each_entry(dev, &topology_device_list, list) {
-		if (!dev->gpu) {
-			i++;
-			continue;
-		}
-
-		seq_printf(m, "Node %u, gpu_id %x:\n", i++, dev->gpu->id);
-		r = device_queue_manager_debugfs_hqds(m, dev->gpu->dqm);
-		if (r != 0)
-			break;
+		device_idx++;
 	}
 
 	up_read(&topology_lock);
 
-	return r;
-}
-
-int kfd_debugfs_rls_by_device(struct seq_file *m, void *data)
-{
-	struct kfd_topology_device *dev;
-	unsigned i = 0;
-	int r = 0;
-
-	down_read(&topology_lock);
-
-	list_for_each_entry(dev, &topology_device_list, list) {
-		if (!dev->gpu) {
-			i++;
-			continue;
-		}
-
-		seq_printf(m, "Node %u, gpu_id %x:\n", i++, dev->gpu->id);
-		r = pm_debugfs_runlist(m, &dev->gpu->dqm->packets);
-		if (r != 0)
-			break;
-	}
-
-	up_read(&topology_lock);
+	return device;
 
-	return r;
 }
-
-#endif
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_topology.h b/drivers/gpu/drm/amd/amdkfd/kfd_topology.h
index ab28188..c3ddb9b 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_topology.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_topology.h
@@ -39,16 +39,8 @@
 #define HSA_CAP_WATCH_POINTS_SUPPORTED		0x00000080
 #define HSA_CAP_WATCH_POINTS_TOTALBITS_MASK	0x00000f00
 #define HSA_CAP_WATCH_POINTS_TOTALBITS_SHIFT	8
-#define HSA_CAP_DOORBELL_TYPE_TOTALBITS_MASK	0x00003000
-#define HSA_CAP_DOORBELL_TYPE_TOTALBITS_SHIFT	12
-#define HSA_CAP_RESERVED			0xffffc000
-
-#define HSA_CAP_DOORBELL_TYPE_PRE_1_0		0x0
-#define HSA_CAP_DOORBELL_TYPE_1_0		0x1
-#define HSA_CAP_WATCH_POINTS_TOTALBITS_MASK	0x00000f00
-#define HSA_CAP_WATCH_POINTS_TOTALBITS_SHIFT	8
+#define HSA_CAP_RESERVED			0xfffff000
 #define HSA_CAP_DOORBELL_PACKET_TYPE		0x00001000
-#define HSA_CAP_AQL_QUEUE_DOUBLE_MAP		0x00004000
 
 struct kfd_node_properties {
 	uint32_t cpu_cores_count;
@@ -99,6 +91,8 @@ struct kfd_mem_properties {
 	struct attribute	attr;
 };
 
+#define KFD_TOPOLOGY_CPU_SIBLINGS 256
+
 #define HSA_CACHE_TYPE_DATA		0x00000001
 #define HSA_CACHE_TYPE_INSTRUCTION	0x00000002
 #define HSA_CACHE_TYPE_CPU		0x00000004
@@ -115,7 +109,7 @@ struct kfd_cache_properties {
 	uint32_t		cache_assoc;
 	uint32_t		cache_latency;
 	uint32_t		cache_type;
-	uint8_t			sibling_map[CRAT_SIBLINGMAP_SIZE];
+	uint8_t			sibling_map[KFD_TOPOLOGY_CPU_SIBLINGS];
 	struct kobject		*kobj;
 	struct attribute	attr;
 };
@@ -141,8 +135,8 @@ struct kfd_iolink_properties {
 struct kfd_topology_device {
 	struct list_head		list;
 	uint32_t			gpu_id;
-	uint32_t			proximity_domain;
 	struct kfd_node_properties	node_props;
+	uint32_t			mem_bank_count;
 	struct list_head		mem_props;
 	uint32_t			cache_count;
 	struct list_head		cache_props;
@@ -156,9 +150,6 @@ struct kfd_topology_device {
 	struct attribute		attr_gpuid;
 	struct attribute		attr_name;
 	struct attribute		attr_props;
-	uint8_t		oem_id[CRAT_OEMID_LENGTH];
-	uint8_t		oem_table_id[CRAT_OEMTABLEID_LENGTH];
-	uint32_t	oem_revision;
 };
 
 struct kfd_system_properties {
@@ -173,8 +164,6 @@ struct kfd_system_properties {
 	struct attribute	attr_props;
 };
 
-struct kfd_topology_device *kfd_create_topology_device(
-		struct list_head *device_list);
-void kfd_release_live_view(void);
+
 
 #endif /* __KFD_TOPOLOGY_H__ */
diff --git a/drivers/gpu/drm/amd/include/amd_shared.h b/drivers/gpu/drm/amd/include/amd_shared.h
index 7a793fb..c92532c 100644
--- a/drivers/gpu/drm/amd/include/amd_shared.h
+++ b/drivers/gpu/drm/amd/include/amd_shared.h
@@ -207,8 +207,6 @@ struct amd_ip_funcs {
 	int (*soft_reset)(void *handle);
 	/* post soft reset the IP block */
 	int (*post_soft_reset)(void *handle);
- 	/* dump the IP block status registers */
-	void (*print_status)(void *handle);
 	/* enable/disable cg for the IP block */
 	int (*set_clockgating_state)(void *handle,
 				     enum amd_clockgating_state state);
diff --git a/drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_d.h b/drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_d.h
old mode 100644
new mode 100755
diff --git a/drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_sh_mask.h b/drivers/gpu/drm/amd/include/asic_reg/dce/dce_11_2_sh_mask.h
old mode 100644
new mode 100755
diff --git a/drivers/gpu/drm/amd/include/cgs_common.h b/drivers/gpu/drm/amd/include/cgs_common.h
old mode 100644
new mode 100755
diff --git a/drivers/gpu/drm/amd/include/kgd_kfd_interface.h b/drivers/gpu/drm/amd/include/kgd_kfd_interface.h
index 359a056..a09d9f3 100644
--- a/drivers/gpu/drm/amd/include/kgd_kfd_interface.h
+++ b/drivers/gpu/drm/amd/include/kgd_kfd_interface.h
@@ -29,10 +29,6 @@
 #define KGD_KFD_INTERFACE_H_INCLUDED
 
 #include <linux/types.h>
-#include <linux/mm_types.h>
-#include <linux/scatterlist.h>
-#include <linux/fence.h>
-#include <linux/dma-buf.h>
 
 struct pci_dev;
 
@@ -42,46 +38,6 @@ struct kfd_dev;
 struct kgd_dev;
 
 struct kgd_mem;
-struct kfd_process_device;
-struct amdgpu_bo;
-
-enum kfd_preempt_type {
-	KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN = 0,
-	KFD_PREEMPT_TYPE_WAVEFRONT_RESET,
-};
-
-struct kfd_vm_fault_info {
-	uint64_t	page_addr;
-	uint32_t	vmid;
-	uint32_t	mc_id;
-	uint32_t	status;
-	bool		prot_valid;
-	bool		prot_read;
-	bool		prot_write;
-	bool		prot_exec;
-};
-
-struct kfd_cu_info {
-	uint32_t num_shader_engines;
-	uint32_t num_shader_arrays_per_engine;
-	uint32_t num_cu_per_sh;
-	uint32_t cu_active_number;
-	uint32_t cu_ao_mask;
-	uint32_t simd_per_cu;
-	uint32_t max_waves_per_simd;
-	uint32_t wave_front_size;
-	uint32_t max_scratch_slots_per_cu;
-	uint32_t lds_size;
-	uint32_t cu_bitmap[4][4];
-};
-
-/* For getting GPU local memory information from KGD */
-struct kfd_local_mem_info {
-	uint64_t local_mem_size_private;
-	uint64_t local_mem_size_public;
-	uint32_t vram_width;
-	uint32_t mem_clk_max;
-};
 
 enum kgd_memory_pool {
 	KGD_POOL_SYSTEM_CACHEABLE = 1,
@@ -119,40 +75,8 @@ struct kgd2kfd_shared_resources {
 
 	/* Number of bytes at start of aperture reserved for KGD. */
 	size_t doorbell_start_offset;
-
-	/* GPUVM address space size in bytes */
-	uint64_t gpuvm_size;
-};
-
-struct tile_config {
-	uint32_t *tile_config_ptr;
-	uint32_t *macro_tile_config_ptr;
-	uint32_t num_tile_configs;
-	uint32_t num_macro_tile_configs;
-
-	uint32_t gb_addr_config;
-	uint32_t num_banks;
-	uint32_t num_ranks;
 };
 
-/*
- * Allocation flag domains currently only VRAM and GTT domain supported
- */
-#define ALLOC_MEM_FLAGS_VRAM			(1 << 0)
-#define ALLOC_MEM_FLAGS_GTT				(1 << 1)
-#define ALLOC_MEM_FLAGS_USERPTR			(1 << 2)
-#define ALLOC_MEM_FLAGS_DOORBELL		(1 << 3)
-
-/*
- * Allocation flags attributes/access options.
- */
-#define ALLOC_MEM_FLAGS_NONPAGED		(1 << 31)
-#define ALLOC_MEM_FLAGS_READONLY		(1 << 30)
-#define ALLOC_MEM_FLAGS_PUBLIC			(1 << 29)
-#define ALLOC_MEM_FLAGS_NO_SUBSTITUTE	(1 << 28)
-#define ALLOC_MEM_FLAGS_AQL_QUEUE_MEM	(1 << 27)
-#define ALLOC_MEM_FLAGS_EXECUTE_ACCESS	(1 << 26)
-
 /**
  * struct kfd2kgd_calls
  *
@@ -161,7 +85,7 @@ struct tile_config {
  *
  * @free_gtt_mem: Frees a buffer that was allocated on the gart aperture
  *
- * @get_local_mem_info: Retrieves information about GPU local memory
+ * @get_vmem_size: Retrieves (physical) size of VRAM
  *
  * @get_gpu_clock_counter: Retrieves GPU clock counter
  *
@@ -183,12 +107,6 @@ struct tile_config {
  * @hqd_sdma_load: Loads the SDMA mqd structure to a H/W SDMA hqd slot.
  * used only for no HWS mode.
  *
- * @hqd_dump: Dumps CPC HQD registers to an array of address-value pairs.
- * Array is allocated with kmalloc, needs to be freed with kfree by caller.
- *
- * @hqd_sdma_dump: Dumps SDMA HQD registers to an array of address-value pairs.
- * Array is allocated with kmalloc, needs to be freed with kfree by caller.
- *
  * @hqd_is_occupies: Checks if a hqd slot is occupied.
  *
  * @hqd_destroy: Destructs and preempts the queue assigned to that hqd slot.
@@ -198,31 +116,8 @@ struct tile_config {
  * @hqd_sdma_destroy: Destructs and preempts the SDMA queue assigned to that
  * SDMA hqd slot.
  *
- * @map_memory_to_gpu: Allocates and pins BO, PD and all related PTs
- *
- * @unmap_memory_to_gpu: Releases and unpins BO, PD and all related PTs
- *
  * @get_fw_version: Returns FW versions from the header
  *
- * @set_num_of_requests: Sets number of Peripheral Page Request (PPR) sent to
- * IOMMU when address translation failed
- *
- * @get_cu_info: Retrieves activated cu info
- *
- * @get_dmabuf_info: Returns information about a dmabuf if it was
- * created by the GPU driver
- *
- * @import_dmabuf: Imports a DMA buffer, creating a new kgd_mem object
- * Supports only DMA buffers created by GPU driver on the same GPU
- *
- * @export_dmabuf: Emports a KFD BO for sharing with other process
- *
- * @submit_ib: Submits an IB to the engine specified by inserting the IB to
- * the corresonded ring (ring type).
- *
- * @restore_process_bos: Restore all BOs that belongs to the process identified
- * by master_vm.
- *
  * This structure contains function pointers to services that the kgd driver
  * provides to amdkfd driver.
  *
@@ -234,23 +129,11 @@ struct kfd2kgd_calls {
 
 	void (*free_gtt_mem)(struct kgd_dev *kgd, void *mem_obj);
 
-	void(*get_local_mem_info)(struct kgd_dev *kgd,
-			struct kfd_local_mem_info *mem_info);
+	uint64_t (*get_vmem_size)(struct kgd_dev *kgd);
 	uint64_t (*get_gpu_clock_counter)(struct kgd_dev *kgd);
 
 	uint32_t (*get_max_engine_clock_in_mhz)(struct kgd_dev *kgd);
 
-	int (*create_process_vm)(struct kgd_dev *kgd, void **vm,
-				 void *master_vm);
-	void (*destroy_process_vm)(struct kgd_dev *kgd, void *vm);
-
-	int (*create_process_gpumem)(struct kgd_dev *kgd, uint64_t va, size_t size, void *vm, struct kgd_mem **mem);
-	void (*destroy_process_gpumem)(struct kgd_dev *kgd, struct kgd_mem *mem);
-
-	uint32_t (*get_process_page_dir)(void *vm);
-
-	int (*open_graphic_handle)(struct kgd_dev *kgd, uint64_t va, void *vm, int fd, uint32_t handle, struct kgd_mem **mem);
-
 	/* Register access functions */
 	void (*program_sh_mem_settings)(struct kgd_dev *kgd, uint32_t vmid,
 			uint32_t sh_mem_config,	uint32_t sh_mem_ape1_base,
@@ -263,23 +146,11 @@ struct kfd2kgd_calls {
 				uint32_t hpd_size, uint64_t hpd_gpu_addr);
 
 	int (*init_interrupts)(struct kgd_dev *kgd, uint32_t pipe_id);
-	
 
 	int (*hqd_load)(struct kgd_dev *kgd, void *mqd, uint32_t pipe_id,
-			uint32_t queue_id, uint32_t __user *wptr,
-			uint32_t wptr_shift, uint32_t wptr_mask,
-			struct mm_struct *mm);
-
-	int (*hqd_sdma_load)(struct kgd_dev *kgd, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm);
-
-	int (*hqd_dump)(struct kgd_dev *kgd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs);
+			uint32_t queue_id, uint32_t __user *wptr);
 
-	int (*hqd_sdma_dump)(struct kgd_dev *kgd,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs);
+	int (*hqd_sdma_load)(struct kgd_dev *kgd, void *mqd);
 
 	bool (*hqd_is_occupied)(struct kgd_dev *kgd, uint64_t queue_address,
 				uint32_t pipe_id, uint32_t queue_id);
@@ -292,7 +163,7 @@ struct kfd2kgd_calls {
 
 	int (*hqd_sdma_destroy)(struct kgd_dev *kgd, void *mqd,
 				unsigned int timeout);
-				
+
 	int (*address_watch_disable)(struct kgd_dev *kgd);
 	int (*address_watch_execute)(struct kgd_dev *kgd,
 					unsigned int watch_point_id,
@@ -311,62 +182,11 @@ struct kfd2kgd_calls {
 	uint16_t (*get_atc_vmid_pasid_mapping_pasid)(
 					struct kgd_dev *kgd,
 					uint8_t vmid);
-	uint32_t (*read_vmid_from_vmfault_reg)(struct kgd_dev *kgd);
 	void (*write_vmid_invalidate_request)(struct kgd_dev *kgd,
 					uint8_t vmid);
-	int (*alloc_memory_of_gpu)(struct kgd_dev *kgd, uint64_t va,
-			uint64_t size, void *vm,
-			struct kgd_mem **mem, uint64_t *offset,
-			void **kptr, uint32_t flags);
-	int (*free_memory_of_gpu)(struct kgd_dev *kgd, struct kgd_mem *mem,
-			void *vm);
-	int (*map_memory_to_gpu)(struct kgd_dev *kgd, struct kgd_mem *mem,
-			void *vm);
-	int (*unmap_memory_to_gpu)(struct kgd_dev *kgd, struct kgd_mem *mem,
-			void *vm);
 
 	uint16_t (*get_fw_version)(struct kgd_dev *kgd,
 				enum kgd_engine_type type);
-
-	void (*set_num_of_requests)(struct kgd_dev *kgd,
-			uint8_t num_of_requests);
-	int (*alloc_memory_of_scratch)(struct kgd_dev *kgd,
-			uint64_t va, uint32_t vmid);
-	int (*write_config_static_mem)(struct kgd_dev *kgd, bool swizzle_enable,
-		uint8_t element_size, uint8_t index_stride, uint8_t mtype);
-	void (*get_cu_info)(struct kgd_dev *kgd,
-			struct kfd_cu_info *cu_info);
-	int (*mmap_bo)(struct kgd_dev *kgd, struct vm_area_struct *vma);
-	int (*map_gtt_bo_to_kernel)(struct kgd_dev *kgd,
-			struct kgd_mem *mem, void **kptr);
-	void (*set_vm_context_page_table_base)(struct kgd_dev *kgd, uint32_t vmid,
-			uint32_t page_table_base);
-
-	int (*pin_get_sg_table_bo)(struct kgd_dev *kgd,
-			struct kgd_mem *mem, uint64_t offset,
-			uint64_t size, struct sg_table **ret_sg);
-	void (*unpin_put_sg_table_bo)(struct kgd_mem *mem,
-			struct sg_table *sg);
-
-	int (*get_dmabuf_info)(struct kgd_dev *kgd, int dma_buf_fd,
-			       struct kgd_dev **dma_buf_kgd, uint64_t *bo_size,
-			       void *metadata_buffer, size_t buffer_size,
-			       uint32_t *metadata_size, uint32_t *flags);
-	int (*import_dmabuf)(struct kgd_dev *kgd, struct dma_buf *dmabuf,
-			     uint64_t va, void *vm, struct kgd_mem **mem,
-			     uint64_t *size, uint64_t *mmap_offset);
-	int (*export_dmabuf)(struct kgd_dev *kgd, void *vm, struct kgd_mem *mem,
-				struct dma_buf **dmabuf);
-
-	int (*get_vm_fault_info)(struct kgd_dev *kgd,
-			struct kfd_vm_fault_info *info);
-	int (*submit_ib)(struct kgd_dev *kgd, enum kgd_engine_type engine,
-			uint32_t vmid, uint64_t gpu_addr,
-			uint32_t *ib_cmd, uint32_t ib_len);
-	int (*get_tile_config)(struct kgd_dev *kgd,
-			struct tile_config *config);
-
-	int (*restore_process_bos)(void *master_vm);
 };
 
 /**
@@ -385,13 +205,6 @@ struct kfd2kgd_calls {
  *
  * @resume: Notifies amdkfd about a resume action done to a kgd device
  *
- * @quiesce_mm: Quiesce all user queue access to specified MM address space
- *
- * @resume_mm: Resume user queue access to specified MM address space
- *
- * @schedule_evict_and_restore_process: Schedules work queue that will prepare
- * for safe eviction of KFD BOs that belong to the specified process.
- *
  * This structure contains function callback pointers so the kgd driver
  * will notify to the amdkfd about certain status changes.
  *
@@ -406,13 +219,9 @@ struct kgd2kfd_calls {
 	void (*interrupt)(struct kfd_dev *kfd, const void *ih_ring_entry);
 	void (*suspend)(struct kfd_dev *kfd);
 	int (*resume)(struct kfd_dev *kfd);
-	int (*quiesce_mm)(struct kfd_dev *kfd, struct mm_struct *mm);
-	int (*resume_mm)(struct kfd_dev *kfd, struct mm_struct *mm);
-	int (*schedule_evict_and_restore_process)(struct mm_struct *mm,
-			struct fence *fence);
 };
 
 int kgd2kfd_init(unsigned interface_version,
 		const struct kgd2kfd_calls **g2f);
 
-#endif /* KGD_KFD_INTERFACE_H_INCLUDED */
+#endif	/* KGD_KFD_INTERFACE_H_INCLUDED */
diff --git a/drivers/gpu/drm/amd/include/vi_structs.h b/drivers/gpu/drm/amd/include/vi_structs.h
index 5829ae8..65cfacd 100644
--- a/drivers/gpu/drm/amd/include/vi_structs.h
+++ b/drivers/gpu/drm/amd/include/vi_structs.h
@@ -153,8 +153,6 @@ struct vi_sdma_mqd {
 	uint32_t reserved_125;
 	uint32_t reserved_126;
 	uint32_t reserved_127;
-	uint32_t sdma_engine_id;
-	uint32_t sdma_queue_id;
 };
 
 struct vi_mqd {
diff --git a/drivers/gpu/drm/amd/powerplay/amd_powerplay.c b/drivers/gpu/drm/amd/powerplay/amd_powerplay.c
index c20c86c..fbbac6b 100644
--- a/drivers/gpu/drm/amd/powerplay/amd_powerplay.c
+++ b/drivers/gpu/drm/amd/powerplay/amd_powerplay.c
@@ -840,143 +840,6 @@ static int pp_dpm_set_mclk_od(void *handle, uint32_t value)
 	return hwmgr->hwmgr_func->set_mclk_od(hwmgr, value);
 }
 
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static int pp_dpm_reset_power_profile_state(void *handle,
-		struct pp_profile *request)
-{
-	struct pp_hwmgr *hwmgr;
-
-	if (!handle || !request)
-		return -EINVAL;
-
-	hwmgr = ((struct pp_instance *)handle)->hwmgr;
-
-	PP_CHECK_HW(hwmgr);
-
-	if (hwmgr->hwmgr_func->set_power_profile_state == NULL) {
-		pr_info("%s was not implemented.\n", __func__);
-		return 0;
-	}
-
-	if (request->type == PP_GFX_PROFILE) {
-		hwmgr->gfx_power_profile = hwmgr->default_gfx_power_profile;
-		return hwmgr->hwmgr_func->set_power_profile_state(hwmgr,
-					  &hwmgr->gfx_power_profile);
-	} else if (request->type == PP_COMPUTE_PROFILE) {
-		hwmgr->compute_power_profile =
-			hwmgr->default_compute_power_profile;
-		return hwmgr->hwmgr_func->set_power_profile_state(hwmgr,
-					  &hwmgr->compute_power_profile);
-	} else
-		return -EINVAL;
-}
-
-static int pp_dpm_get_power_profile_state(void *handle,
-		struct pp_profile *query)
-{
-	struct pp_hwmgr *hwmgr;
-
-	if (!handle || !query)
-		return -EINVAL;
-
-	hwmgr = ((struct pp_instance *)handle)->hwmgr;
-
-	PP_CHECK_HW(hwmgr);
-
-	if (query->type == PP_GFX_PROFILE)
-		memcpy(query, &hwmgr->gfx_power_profile,
-				sizeof(struct pp_profile));
-	else if (query->type == PP_COMPUTE_PROFILE)
-		memcpy(query, &hwmgr->compute_power_profile,
-				sizeof(struct pp_profile));
-	else
-		return -EINVAL;
-
-	return 0;
-}
-
-static int pp_dpm_set_power_profile_state(void *handle,
-		struct pp_profile *request)
-{
-	struct pp_hwmgr *hwmgr;
-	int ret = -1;
-
-	if (!handle || !request)
-		return -EINVAL;
-
-	hwmgr = ((struct pp_instance *)handle)->hwmgr;
-
-	PP_CHECK_HW(hwmgr);
-
-	if (hwmgr->hwmgr_func->set_power_profile_state == NULL) {
-		pr_info("%s was not implemented.\n", __func__);
-		return 0;
-	}
-
-	if (request->min_sclk ||
-		request->min_mclk ||
-		request->activity_threshold ||
-		request->up_hyst ||
-		request->down_hyst) {
-		if (request->type == PP_GFX_PROFILE)
-			memcpy(&hwmgr->gfx_power_profile, request,
-					sizeof(struct pp_profile));
-		else if (request->type == PP_COMPUTE_PROFILE)
-			memcpy(&hwmgr->compute_power_profile, request,
-					sizeof(struct pp_profile));
-		else
-			return -EINVAL;
-
-		if (request->type == hwmgr->current_power_profile)
-			ret = hwmgr->hwmgr_func->set_power_profile_state(
-					hwmgr,
-					request);
-	} else {
-		/* set power profile if it exists */
-		switch (request->type) {
-		case PP_GFX_PROFILE:
-			ret = hwmgr->hwmgr_func->set_power_profile_state(
-				hwmgr,
-				&hwmgr->gfx_power_profile);
-			break;
-		case PP_COMPUTE_PROFILE:
-			ret = hwmgr->hwmgr_func->set_power_profile_state(
-				hwmgr,
-				&hwmgr->compute_power_profile);
-			break;
-		default:
-			return -EINVAL;
-		}
-	}
-
-	if (!ret)
-		hwmgr->current_power_profile = request->type;
-
-	return 0;
-}
-
-static int pp_dpm_switch_power_profile(void *handle,
-		enum pp_profile_type type)
-{
-	struct pp_hwmgr *hwmgr;
-	struct pp_profile request = {0};
-
-	if (!handle)
-		return -EINVAL;
-
-	hwmgr = ((struct pp_instance *)handle)->hwmgr;
-
-	PP_CHECK_HW(hwmgr);
-
-	if (hwmgr->current_power_profile != type) {
-		request.type = type;
-		pp_dpm_set_power_profile_state(handle, &request);
-	}
-
-	return 0;
-}
-#endif
-
 static int pp_dpm_read_sensor(void *handle, int idx, int32_t *value)
 {
 	struct pp_hwmgr *hwmgr;
@@ -1037,12 +900,6 @@ const struct amd_powerplay_funcs pp_dpm_funcs = {
 	.set_sclk_od = pp_dpm_set_sclk_od,
 	.get_mclk_od = pp_dpm_get_mclk_od,
 	.set_mclk_od = pp_dpm_set_mclk_od,
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	.reset_power_profile_state = pp_dpm_reset_power_profile_state,
-	.get_power_profile_state = pp_dpm_get_power_profile_state,
-	.set_power_profile_state = pp_dpm_set_power_profile_state,
-	.switch_power_profile = pp_dpm_switch_power_profile,	
-#endif
 	.read_sensor = pp_dpm_read_sensor,
 	.get_vce_clock_state = pp_dpm_get_vce_clock_state,
 };
diff --git a/drivers/gpu/drm/amd/powerplay/eventmgr/eventtasks.c b/drivers/gpu/drm/amd/powerplay/eventmgr/eventtasks.c
index d47f693..e04216e 100644
--- a/drivers/gpu/drm/amd/powerplay/eventmgr/eventtasks.c
+++ b/drivers/gpu/drm/amd/powerplay/eventmgr/eventtasks.c
@@ -37,13 +37,14 @@
 
 int pem_task_update_allowed_performance_levels(struct pp_eventmgr *eventmgr, struct pem_event_data *event_data)
 {
-        if (eventmgr == NULL || eventmgr->hwmgr == NULL)
-                return -EINVAL;
- 
+
+	if (eventmgr == NULL || eventmgr->hwmgr == NULL)
+		return -EINVAL;
+
 	if (pem_is_hw_access_blocked(eventmgr))
 		return 0;
 
-        phm_force_dpm_levels(eventmgr->hwmgr, eventmgr->hwmgr->dpm_level);
+	phm_force_dpm_levels(eventmgr->hwmgr, eventmgr->hwmgr->dpm_level);
 
 	return 0;
 }
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_clockpowergating.c b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_clockpowergating.c
deleted file mode 100644
index 5afe820..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_clockpowergating.c
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#include "hwmgr.h"
-#include "fiji_clockpowergating.h"
-#include "fiji_ppsmc.h"
-#include "fiji_hwmgr.h"
-
-int fiji_phm_disable_clock_power_gating(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	data->uvd_power_gated = false;
-	data->vce_power_gated = false;
-	data->samu_power_gated = false;
-	data->acp_power_gated = false;
-
-	return 0;
-}
-
-int fiji_phm_powergate_uvd(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (data->uvd_power_gated == bgate)
-		return 0;
-
-	data->uvd_power_gated = bgate;
-
-	if (bgate) {
-		cgs_set_clockgating_state(hwmgr->device,
-					  AMD_IP_BLOCK_TYPE_UVD,
-					  AMD_CG_STATE_GATE);
-		fiji_update_uvd_dpm(hwmgr, true);
-	} else {
-		fiji_update_uvd_dpm(hwmgr, false);
-		cgs_set_clockgating_state(hwmgr->device,
-					  AMD_IP_BLOCK_TYPE_UVD,
-					  AMD_CG_STATE_UNGATE);
-	}
-
-	return 0;
-}
-
-int fiji_phm_powergate_vce(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_set_power_state_input states;
-	const struct pp_power_state  *pcurrent;
-	struct pp_power_state  *requested;
-
-	if (data->vce_power_gated == bgate)
-		return 0;
-
-	data->vce_power_gated = bgate;
-
-	pcurrent = hwmgr->current_ps;
-	requested = hwmgr->request_ps;
-
-	states.pcurrent_state = &(pcurrent->hardware);
-	states.pnew_state = &(requested->hardware);
-
-	fiji_update_vce_dpm(hwmgr, &states);
-	fiji_enable_disable_vce_dpm(hwmgr, !bgate);
-
-	return 0;
-}
-
-int fiji_phm_powergate_samu(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (data->samu_power_gated == bgate)
-		return 0;
-
-	data->samu_power_gated = bgate;
-
-	if (bgate)
-		fiji_update_samu_dpm(hwmgr, true);
-	else
-		fiji_update_samu_dpm(hwmgr, false);
-
-	return 0;
-}
-
-int fiji_phm_powergate_acp(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (data->acp_power_gated == bgate)
-		return 0;
-
-	data->acp_power_gated = bgate;
-
-	if (bgate)
-		fiji_update_acp_dpm(hwmgr, true);
-	else
-		fiji_update_acp_dpm(hwmgr, false);
-
-	return 0;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_clockpowergating.h b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_clockpowergating.h
deleted file mode 100644
index 33af5f5..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_clockpowergating.h
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef _FIJI_CLOCK_POWER_GATING_H_
-#define _FIJI_CLOCK_POWER_GATING_H_
-
-#include "fiji_hwmgr.h"
-#include "pp_asicblocks.h"
-
-extern int fiji_phm_powergate_vce(struct pp_hwmgr *hwmgr, bool bgate);
-extern int fiji_phm_powergate_uvd(struct pp_hwmgr *hwmgr, bool bgate);
-extern int fiji_phm_powergate_samu(struct pp_hwmgr *hwmgr, bool bgate);
-extern int fiji_phm_powergate_acp(struct pp_hwmgr *hwmgr, bool bgate);
-extern int fiji_phm_disable_clock_power_gating(struct pp_hwmgr *hwmgr);
-#endif /* _TONGA_CLOCK_POWER_GATING_H_ */
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_dyn_defaults.h b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_dyn_defaults.h
deleted file mode 100644
index 32d43e8..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_dyn_defaults.h
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef FIJI_DYN_DEFAULTS_H
-#define FIJI_DYN_DEFAULTS_H
-
-/** \file
-* Volcanic Islands Dynamic default parameters.
-*/
-
-enum FIJIdpm_TrendDetection
-{
-    FIJIAdpm_TrendDetection_AUTO,
-    FIJIAdpm_TrendDetection_UP,
-    FIJIAdpm_TrendDetection_DOWN
-};
-typedef enum FIJIdpm_TrendDetection FIJIdpm_TrendDetection;
-
-/* We need to fill in the default values!!!!!!!!!!!!!!!!!!!!!!! */
-
-/* Bit vector representing same fields as hardware register. */
-#define PPFIJI_VOTINGRIGHTSCLIENTS_DFLT0    0x3FFFC102  /* CP_Gfx_busy ????
-                                                         * HDP_busy
-                                                         * IH_busy
-                                                         * UVD_busy
-                                                         * VCE_busy
-                                                         * ACP_busy
-                                                         * SAMU_busy
-                                                         * SDMA enabled */
-#define PPFIJI_VOTINGRIGHTSCLIENTS_DFLT1    0x000400  /* FE_Gfx_busy  - Intended for primary usage.   Rest are for flexibility. ????
-                                                       * SH_Gfx_busy
-                                                       * RB_Gfx_busy
-                                                       * VCE_busy */
-
-#define PPFIJI_VOTINGRIGHTSCLIENTS_DFLT2    0xC00080  /* SH_Gfx_busy - Intended for primary usage.   Rest are for flexibility.
-                                                       * FE_Gfx_busy
-                                                       * RB_Gfx_busy
-                                                       * ACP_busy */
-
-#define PPFIJI_VOTINGRIGHTSCLIENTS_DFLT3    0xC00200  /* RB_Gfx_busy - Intended for primary usage.   Rest are for flexibility.
-                                                       * FE_Gfx_busy
-                                                       * SH_Gfx_busy
-                                                       * UVD_busy */
-
-#define PPFIJI_VOTINGRIGHTSCLIENTS_DFLT4    0xC01680  /* UVD_busy
-                                                       * VCE_busy
-                                                       * ACP_busy
-                                                       * SAMU_busy */
-
-#define PPFIJI_VOTINGRIGHTSCLIENTS_DFLT5    0xC00033  /* GFX, HDP */
-#define PPFIJI_VOTINGRIGHTSCLIENTS_DFLT6    0xC00033  /* GFX, HDP */
-#define PPFIJI_VOTINGRIGHTSCLIENTS_DFLT7    0x3FFFC000  /* GFX, HDP */
-
-
-/* thermal protection counter (units). */
-#define PPFIJI_THERMALPROTECTCOUNTER_DFLT            0x200 /* ~19us */
-
-/* static screen threshold unit */
-#define PPFIJI_STATICSCREENTHRESHOLDUNIT_DFLT    0
-
-/* static screen threshold */
-#define PPFIJI_STATICSCREENTHRESHOLD_DFLT        0x00C8
-
-/* gfx idle clock stop threshold */
-#define PPFIJI_GFXIDLECLOCKSTOPTHRESHOLD_DFLT        0x200 /* ~19us with static screen threshold unit of 0 */
-
-/* Fixed reference divider to use when building baby stepping tables. */
-#define PPFIJI_REFERENCEDIVIDER_DFLT                  4
-
-/* ULV voltage change delay time
- * Used to be delay_vreg in N.I. split for S.I.
- * Using N.I. delay_vreg value as default
- * ReferenceClock = 2700
- * VoltageResponseTime = 1000
- * VDDCDelayTime = (VoltageResponseTime * ReferenceClock) / 1600 = 1687
- */
-#define PPFIJI_ULVVOLTAGECHANGEDELAY_DFLT             1687
-
-#define PPFIJI_CGULVPARAMETER_DFLT			0x00040035
-#define PPFIJI_CGULVCONTROL_DFLT			0x00007450
-#define PPFIJI_TARGETACTIVITY_DFLT			30 /* 30%*/
-#define PPFIJI_MCLK_TARGETACTIVITY_DFLT		10 /* 10% */
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c
deleted file mode 100644
index 9044949..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.c
+++ /dev/null
@@ -1,5744 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-#include <linux/module.h>
-#include <linux/slab.h>
-#include <linux/fb.h>
-#include "linux/delay.h"
-
-#include "hwmgr.h"
-#include "fiji_smumgr.h"
-#include "atombios.h"
-#include "hardwaremanager.h"
-#include "ppatomctrl.h"
-#include "atombios.h"
-#include "cgs_common.h"
-#include "fiji_dyn_defaults.h"
-#include "fiji_powertune.h"
-#include "smu73.h"
-#include "smu/smu_7_1_3_d.h"
-#include "smu/smu_7_1_3_sh_mask.h"
-#include "gmc/gmc_8_1_d.h"
-#include "gmc/gmc_8_1_sh_mask.h"
-#include "bif/bif_5_0_d.h"
-#include "bif/bif_5_0_sh_mask.h"
-#include "dce/dce_10_0_d.h"
-#include "dce/dce_10_0_sh_mask.h"
-#include "pppcielanes.h"
-#include "fiji_hwmgr.h"
-#include "process_pptables_v1_0.h"
-#include "pptable_v1_0.h"
-#include "pp_debug.h"
-#include "pp_acpi.h"
-#include "amd_pcie_helpers.h"
-#include "cgs_linux.h"
-#include "ppinterrupt.h"
-
-#include "fiji_clockpowergating.h"
-#include "fiji_thermal.h"
-
-#define VOLTAGE_SCALE	4
-#define SMC_RAM_END		0x40000
-#define VDDC_VDDCI_DELTA	300
-
-#define MC_SEQ_MISC0_GDDR5_SHIFT 28
-#define MC_SEQ_MISC0_GDDR5_MASK  0xf0000000
-#define MC_SEQ_MISC0_GDDR5_VALUE 5
-
-#define MC_CG_ARB_FREQ_F0           0x0a /* boot-up default */
-#define MC_CG_ARB_FREQ_F1           0x0b
-#define MC_CG_ARB_FREQ_F2           0x0c
-#define MC_CG_ARB_FREQ_F3           0x0d
-
-/* From smc_reg.h */
-#define SMC_CG_IND_START            0xc0030000
-#define SMC_CG_IND_END              0xc0040000  /* First byte after SMC_CG_IND */
-
-#define VOLTAGE_SCALE               4
-#define VOLTAGE_VID_OFFSET_SCALE1   625
-#define VOLTAGE_VID_OFFSET_SCALE2   100
-
-#define VDDC_VDDCI_DELTA            300
-
-#define ixSWRST_COMMAND_1           0x1400103
-#define MC_SEQ_CNTL__CAC_EN_MASK    0x40000000
-
-/** Values for the CG_THERMAL_CTRL::DPM_EVENT_SRC field. */
-enum DPM_EVENT_SRC {
-    DPM_EVENT_SRC_ANALOG = 0,               /* Internal analog trip point */
-    DPM_EVENT_SRC_EXTERNAL = 1,             /* External (GPIO 17) signal */
-    DPM_EVENT_SRC_DIGITAL = 2,              /* Internal digital trip point (DIG_THERM_DPM) */
-    DPM_EVENT_SRC_ANALOG_OR_EXTERNAL = 3,   /* Internal analog or external */
-    DPM_EVENT_SRC_DIGITAL_OR_EXTERNAL = 4   /* Internal digital or external */
-};
-
-
-/* [2.5%,~2.5%] Clock stretched is multiple of 2.5% vs
- * not and [Fmin, Fmax, LDO_REFSEL, USE_FOR_LOW_FREQ]
- */
-static const uint16_t fiji_clock_stretcher_lookup_table[2][4] =
-{ {600, 1050, 3, 0}, {600, 1050, 6, 1} };
-
-/* [FF, SS] type, [] 4 voltage ranges, and
- * [Floor Freq, Boundary Freq, VID min , VID max]
- */
-static const uint32_t fiji_clock_stretcher_ddt_table[2][4][4] =
-{ { {265, 529, 120, 128}, {325, 650, 96, 119}, {430, 860, 32, 95}, {0, 0, 0, 31} },
-  { {275, 550, 104, 112}, {319, 638, 96, 103}, {360, 720, 64, 95}, {384, 768, 32, 63} } };
-
-/* [Use_For_Low_freq] value, [0%, 5%, 10%, 7.14%, 14.28%, 20%]
- * (coming from PWR_CKS_CNTL.stretch_amount reg spec)
- */
-static const uint8_t fiji_clock_stretch_amount_conversion[2][6] =
-{ {0, 1, 3, 2, 4, 5}, {0, 2, 4, 5, 6, 5} };
-
-static const unsigned long PhwFiji_Magic = (unsigned long)(PHM_VIslands_Magic);
-
-static struct fiji_power_state *cast_phw_fiji_power_state(
-				  struct pp_hw_power_state *hw_ps)
-{
-	PP_ASSERT_WITH_CODE((PhwFiji_Magic == hw_ps->magic),
-				"Invalid Powerstate Type!",
-				 return NULL;);
-
-	return (struct fiji_power_state *)hw_ps;
-}
-
-static const struct
-fiji_power_state *cast_const_phw_fiji_power_state(
-				 const struct pp_hw_power_state *hw_ps)
-{
-	PP_ASSERT_WITH_CODE((PhwFiji_Magic == hw_ps->magic),
-				"Invalid Powerstate Type!",
-				 return NULL;);
-
-	return (const struct fiji_power_state *)hw_ps;
-}
-
-static bool fiji_is_dpm_running(struct pp_hwmgr *hwmgr)
-{
-	return (1 == PHM_READ_INDIRECT_FIELD(hwmgr->device,
-			CGS_IND_REG__SMC, FEATURE_STATUS, VOLTAGE_CONTROLLER_ON))
-			? true : false;
-}
-
-static void fiji_init_dpm_defaults(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_ulv_parm *ulv = &data->ulv;
-
-	ulv->cg_ulv_parameter = PPFIJI_CGULVPARAMETER_DFLT;
-	data->voting_rights_clients0 = PPFIJI_VOTINGRIGHTSCLIENTS_DFLT0;
-	data->voting_rights_clients1 = PPFIJI_VOTINGRIGHTSCLIENTS_DFLT1;
-	data->voting_rights_clients2 = PPFIJI_VOTINGRIGHTSCLIENTS_DFLT2;
-	data->voting_rights_clients3 = PPFIJI_VOTINGRIGHTSCLIENTS_DFLT3;
-	data->voting_rights_clients4 = PPFIJI_VOTINGRIGHTSCLIENTS_DFLT4;
-	data->voting_rights_clients5 = PPFIJI_VOTINGRIGHTSCLIENTS_DFLT5;
-	data->voting_rights_clients6 = PPFIJI_VOTINGRIGHTSCLIENTS_DFLT6;
-	data->voting_rights_clients7 = PPFIJI_VOTINGRIGHTSCLIENTS_DFLT7;
-
-	data->static_screen_threshold_unit =
-			PPFIJI_STATICSCREENTHRESHOLDUNIT_DFLT;
-	data->static_screen_threshold =
-			PPFIJI_STATICSCREENTHRESHOLD_DFLT;
-
-	/* Unset ABM cap as it moved to DAL.
-	 * Add PHM_PlatformCaps_NonABMSupportInPPLib
-	 * for re-direct ABM related request to DAL
-	 */
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ABM);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_NonABMSupportInPPLib);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DynamicACTiming);
-
-	fiji_initialize_power_tune_defaults(hwmgr);
-
-	data->mclk_stutter_mode_threshold = 60000;
-	data->pcie_gen_performance.max = PP_PCIEGen1;
-	data->pcie_gen_performance.min = PP_PCIEGen3;
-	data->pcie_gen_power_saving.max = PP_PCIEGen1;
-	data->pcie_gen_power_saving.min = PP_PCIEGen3;
-	data->pcie_lane_performance.max = 0;
-	data->pcie_lane_performance.min = 16;
-	data->pcie_lane_power_saving.max = 0;
-	data->pcie_lane_power_saving.min = 16;
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DynamicUVDState);
-}
-
-static int fiji_get_sclk_for_voltage_evv(struct pp_hwmgr *hwmgr,
-	phm_ppt_v1_voltage_lookup_table *lookup_table,
-	uint16_t virtual_voltage_id, int32_t *sclk)
-{
-	uint8_t entryId;
-	uint8_t voltageId;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	PP_ASSERT_WITH_CODE(lookup_table->count != 0, "Lookup table is empty", return -EINVAL);
-
-	/* search for leakage voltage ID 0xff01 ~ 0xff08 and sckl */
-	for (entryId = 0; entryId < table_info->vdd_dep_on_sclk->count; entryId++) {
-		voltageId = table_info->vdd_dep_on_sclk->entries[entryId].vddInd;
-		if (lookup_table->entries[voltageId].us_vdd == virtual_voltage_id)
-			break;
-	}
-
-	PP_ASSERT_WITH_CODE(entryId < table_info->vdd_dep_on_sclk->count,
-			"Can't find requested voltage id in vdd_dep_on_sclk table!",
-			return -EINVAL;
-			);
-
-	*sclk = table_info->vdd_dep_on_sclk->entries[entryId].clk;
-
-	return 0;
-}
-
-/**
-* Get Leakage VDDC based on leakage ID.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int fiji_get_evv_voltages(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint16_t    vv_id;
-	uint16_t    vddc = 0;
-	uint16_t    evv_default = 1150;
-	uint16_t    i, j;
-	uint32_t  sclk = 0;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)hwmgr->pptable;
-	struct phm_ppt_v1_clock_voltage_dependency_table *sclk_table =
-			table_info->vdd_dep_on_sclk;
-	int result;
-
-	for (i = 0; i < FIJI_MAX_LEAKAGE_COUNT; i++) {
-		vv_id = ATOM_VIRTUAL_VOLTAGE_ID0 + i;
-		if (!fiji_get_sclk_for_voltage_evv(hwmgr,
-				table_info->vddc_lookup_table, vv_id, &sclk)) {
-			if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_ClockStretcher)) {
-				for (j = 1; j < sclk_table->count; j++) {
-					if (sclk_table->entries[j].clk == sclk &&
-							sclk_table->entries[j].cks_enable == 0) {
-						sclk += 5000;
-						break;
-					}
-				}
-			}
-
-			if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_EnableDriverEVV))
-				result = atomctrl_calculate_voltage_evv_on_sclk(hwmgr,
-						VOLTAGE_TYPE_VDDC, sclk, vv_id, &vddc, i, true);
-			else
-				result = -EINVAL;
-
-			if (result)
-				result = atomctrl_get_voltage_evv_on_sclk(hwmgr,
-						VOLTAGE_TYPE_VDDC, sclk,vv_id, &vddc);
-
-			/* need to make sure vddc is less than 2v or else, it could burn the ASIC. */
-			PP_ASSERT_WITH_CODE((vddc < 2000),
-					"Invalid VDDC value, greater than 2v!", result = -EINVAL;);
-
-			if (result)
-				/* 1.15V is the default safe value for Fiji */
-				vddc = evv_default;
-
-			/* the voltage should not be zero nor equal to leakage ID */
-			if (vddc != 0 && vddc != vv_id) {
-				data->vddc_leakage.actual_voltage
-				[data->vddc_leakage.count] = vddc;
-				data->vddc_leakage.leakage_id
-				[data->vddc_leakage.count] = vv_id;
-				data->vddc_leakage.count++;
-			}
-		}
-	}
-	return 0;
-}
-
-/**
- * Change virtual leakage voltage to actual value.
- *
- * @param     hwmgr  the address of the powerplay hardware manager.
- * @param     pointer to changing voltage
- * @param     pointer to leakage table
- */
-static void fiji_patch_with_vdd_leakage(struct pp_hwmgr *hwmgr,
-		uint16_t *voltage, struct fiji_leakage_voltage *leakage_table)
-{
-	uint32_t index;
-
-	/* search for leakage voltage ID 0xff01 ~ 0xff08 */
-	for (index = 0; index < leakage_table->count; index++) {
-		/* if this voltage matches a leakage voltage ID */
-		/* patch with actual leakage voltage */
-		if (leakage_table->leakage_id[index] == *voltage) {
-			*voltage = leakage_table->actual_voltage[index];
-			break;
-		}
-	}
-
-	if (*voltage > ATOM_VIRTUAL_VOLTAGE_ID0)
-		printk(KERN_ERR "Voltage value looks like a Leakage ID but it's not patched \n");
-}
-
-/**
-* Patch voltage lookup table by EVV leakages.
-*
-* @param     hwmgr  the address of the powerplay hardware manager.
-* @param     pointer to voltage lookup table
-* @param     pointer to leakage table
-* @return     always 0
-*/
-static int fiji_patch_lookup_table_with_leakage(struct pp_hwmgr *hwmgr,
-		phm_ppt_v1_voltage_lookup_table *lookup_table,
-		struct fiji_leakage_voltage *leakage_table)
-{
-	uint32_t i;
-
-	for (i = 0; i < lookup_table->count; i++)
-		fiji_patch_with_vdd_leakage(hwmgr,
-				&lookup_table->entries[i].us_vdd, leakage_table);
-
-	return 0;
-}
-
-static int fiji_patch_clock_voltage_limits_with_vddc_leakage(
-		struct pp_hwmgr *hwmgr, struct fiji_leakage_voltage *leakage_table,
-		uint16_t *vddc)
-{
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	fiji_patch_with_vdd_leakage(hwmgr, (uint16_t *)vddc, leakage_table);
-	hwmgr->dyn_state.max_clock_voltage_on_dc.vddc =
-			table_info->max_clock_voltage_on_dc.vddc;
-	return 0;
-}
-
-static int fiji_patch_voltage_dependency_tables_with_lookup_table(
-		struct pp_hwmgr *hwmgr)
-{
-	uint8_t entryId;
-	uint8_t voltageId;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	struct phm_ppt_v1_clock_voltage_dependency_table *sclk_table =
-			table_info->vdd_dep_on_sclk;
-	struct phm_ppt_v1_clock_voltage_dependency_table *mclk_table =
-			table_info->vdd_dep_on_mclk;
-	struct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =
-			table_info->mm_dep_table;
-
-	for (entryId = 0; entryId < sclk_table->count; ++entryId) {
-		voltageId = sclk_table->entries[entryId].vddInd;
-		sclk_table->entries[entryId].vddc =
-				table_info->vddc_lookup_table->entries[voltageId].us_vdd;
-	}
-
-	for (entryId = 0; entryId < mclk_table->count; ++entryId) {
-		voltageId = mclk_table->entries[entryId].vddInd;
-		mclk_table->entries[entryId].vddc =
-			table_info->vddc_lookup_table->entries[voltageId].us_vdd;
-	}
-
-	for (entryId = 0; entryId < mm_table->count; ++entryId) {
-		voltageId = mm_table->entries[entryId].vddcInd;
-		mm_table->entries[entryId].vddc =
-			table_info->vddc_lookup_table->entries[voltageId].us_vdd;
-	}
-
-	return 0;
-
-}
-
-static int fiji_calc_voltage_dependency_tables(struct pp_hwmgr *hwmgr)
-{
-	/* Need to determine if we need calculated voltage. */
-	return 0;
-}
-
-static int fiji_calc_mm_voltage_dependency_table(struct pp_hwmgr *hwmgr)
-{
-	/* Need to determine if we need calculated voltage from mm table. */
-	return 0;
-}
-
-static int fiji_sort_lookup_table(struct pp_hwmgr *hwmgr,
-		struct phm_ppt_v1_voltage_lookup_table *lookup_table)
-{
-	uint32_t table_size, i, j;
-	struct phm_ppt_v1_voltage_lookup_record tmp_voltage_lookup_record;
-	table_size = lookup_table->count;
-
-	PP_ASSERT_WITH_CODE(0 != lookup_table->count,
-		"Lookup table is empty", return -EINVAL);
-
-	/* Sorting voltages */
-	for (i = 0; i < table_size - 1; i++) {
-		for (j = i + 1; j > 0; j--) {
-			if (lookup_table->entries[j].us_vdd <
-					lookup_table->entries[j - 1].us_vdd) {
-				tmp_voltage_lookup_record = lookup_table->entries[j - 1];
-				lookup_table->entries[j - 1] = lookup_table->entries[j];
-				lookup_table->entries[j] = tmp_voltage_lookup_record;
-			}
-		}
-	}
-
-	return 0;
-}
-
-static int fiji_complete_dependency_tables(struct pp_hwmgr *hwmgr)
-{
-	int result = 0;
-	int tmp_result;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	tmp_result = fiji_patch_lookup_table_with_leakage(hwmgr,
-			table_info->vddc_lookup_table, &(data->vddc_leakage));
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = fiji_patch_clock_voltage_limits_with_vddc_leakage(hwmgr,
-			&(data->vddc_leakage), &table_info->max_clock_voltage_on_dc.vddc);
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = fiji_patch_voltage_dependency_tables_with_lookup_table(hwmgr);
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = fiji_calc_voltage_dependency_tables(hwmgr);
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = fiji_calc_mm_voltage_dependency_table(hwmgr);
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = fiji_sort_lookup_table(hwmgr, table_info->vddc_lookup_table);
-	if(tmp_result)
-		result = tmp_result;
-
-	return result;
-}
-
-static int fiji_set_private_data_based_on_pptable(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	struct phm_ppt_v1_clock_voltage_dependency_table *allowed_sclk_vdd_table =
-			table_info->vdd_dep_on_sclk;
-	struct phm_ppt_v1_clock_voltage_dependency_table *allowed_mclk_vdd_table =
-			table_info->vdd_dep_on_mclk;
-
-	PP_ASSERT_WITH_CODE(allowed_sclk_vdd_table != NULL,
-		"VDD dependency on SCLK table is missing.	\
-		This table is mandatory", return -EINVAL);
-	PP_ASSERT_WITH_CODE(allowed_sclk_vdd_table->count >= 1,
-		"VDD dependency on SCLK table has to have is missing.	\
-		This table is mandatory", return -EINVAL);
-
-	PP_ASSERT_WITH_CODE(allowed_mclk_vdd_table != NULL,
-		"VDD dependency on MCLK table is missing.	\
-		This table is mandatory", return -EINVAL);
-	PP_ASSERT_WITH_CODE(allowed_mclk_vdd_table->count >= 1,
-		"VDD dependency on MCLK table has to have is missing.	 \
-		This table is mandatory", return -EINVAL);
-
-	data->min_vddc_in_pptable = (uint16_t)allowed_sclk_vdd_table->entries[0].vddc;
-	data->max_vddc_in_pptable =	(uint16_t)allowed_sclk_vdd_table->
-			entries[allowed_sclk_vdd_table->count - 1].vddc;
-
-	table_info->max_clock_voltage_on_ac.sclk =
-		allowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].clk;
-	table_info->max_clock_voltage_on_ac.mclk =
-		allowed_mclk_vdd_table->entries[allowed_mclk_vdd_table->count - 1].clk;
-	table_info->max_clock_voltage_on_ac.vddc =
-		allowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].vddc;
-	table_info->max_clock_voltage_on_ac.vddci =
-		allowed_mclk_vdd_table->entries[allowed_mclk_vdd_table->count - 1].vddci;
-
-	hwmgr->dyn_state.max_clock_voltage_on_ac.sclk =
-		table_info->max_clock_voltage_on_ac.sclk;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.mclk =
-		table_info->max_clock_voltage_on_ac.mclk;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.vddc =
-		table_info->max_clock_voltage_on_ac.vddc;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.vddci =
-		table_info->max_clock_voltage_on_ac.vddci;
-
-	return 0;
-}
-
-static uint16_t fiji_get_current_pcie_speed(struct pp_hwmgr *hwmgr)
-{
-	uint32_t speedCntl = 0;
-
-	/* mmPCIE_PORT_INDEX rename as mmPCIE_INDEX */
-	speedCntl = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__PCIE,
-			ixPCIE_LC_SPEED_CNTL);
-	return((uint16_t)PHM_GET_FIELD(speedCntl,
-			PCIE_LC_SPEED_CNTL, LC_CURRENT_DATA_RATE));
-}
-
-static int fiji_get_current_pcie_lane_number(struct pp_hwmgr *hwmgr)
-{
-	uint32_t link_width;
-
-	/* mmPCIE_PORT_INDEX rename as mmPCIE_INDEX */
-	link_width = PHM_READ_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__PCIE,
-			PCIE_LC_LINK_WIDTH_CNTL, LC_LINK_WIDTH_RD);
-
-	PP_ASSERT_WITH_CODE((7 >= link_width),
-			"Invalid PCIe lane width!", return 0);
-
-	return decode_pcie_lane_width(link_width);
-}
-
-/** Patch the Boot State to match VBIOS boot clocks and voltage.
-*
-* @param hwmgr Pointer to the hardware manager.
-* @param pPowerState The address of the PowerState instance being created.
-*
-*/
-static int fiji_patch_boot_state(struct pp_hwmgr *hwmgr,
-		struct pp_hw_power_state *hw_ps)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_power_state *ps = (struct fiji_power_state *)hw_ps;
-	ATOM_FIRMWARE_INFO_V2_2 *fw_info;
-	uint16_t size;
-	uint8_t frev, crev;
-	int index = GetIndexIntoMasterTable(DATA, FirmwareInfo);
-
-	/* First retrieve the Boot clocks and VDDC from the firmware info table.
-	 * We assume here that fw_info is unchanged if this call fails.
-	 */
-	fw_info = (ATOM_FIRMWARE_INFO_V2_2 *)cgs_atom_get_data_table(
-			hwmgr->device, index,
-			&size, &frev, &crev);
-	if (!fw_info)
-		/* During a test, there is no firmware info table. */
-		return 0;
-
-	/* Patch the state. */
-	data->vbios_boot_state.sclk_bootup_value =
-			le32_to_cpu(fw_info->ulDefaultEngineClock);
-	data->vbios_boot_state.mclk_bootup_value =
-			le32_to_cpu(fw_info->ulDefaultMemoryClock);
-	data->vbios_boot_state.mvdd_bootup_value =
-			le16_to_cpu(fw_info->usBootUpMVDDCVoltage);
-	data->vbios_boot_state.vddc_bootup_value =
-			le16_to_cpu(fw_info->usBootUpVDDCVoltage);
-	data->vbios_boot_state.vddci_bootup_value =
-			le16_to_cpu(fw_info->usBootUpVDDCIVoltage);
-	data->vbios_boot_state.pcie_gen_bootup_value =
-			fiji_get_current_pcie_speed(hwmgr);
-	data->vbios_boot_state.pcie_lane_bootup_value =
-			(uint16_t)fiji_get_current_pcie_lane_number(hwmgr);
-
-	/* set boot power state */
-	ps->performance_levels[0].memory_clock = data->vbios_boot_state.mclk_bootup_value;
-	ps->performance_levels[0].engine_clock = data->vbios_boot_state.sclk_bootup_value;
-	ps->performance_levels[0].pcie_gen = data->vbios_boot_state.pcie_gen_bootup_value;
-	ps->performance_levels[0].pcie_lane = data->vbios_boot_state.pcie_lane_bootup_value;
-
-	return 0;
-}
-
-static int fiji_hwmgr_backend_fini(struct pp_hwmgr *hwmgr)
-{
-	return phm_hwmgr_backend_fini(hwmgr);
-}
-
-static int fiji_hwmgr_backend_init(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data;
-	uint32_t i;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	bool stay_in_boot;
-	int result;
-
-	data = kzalloc(sizeof(struct fiji_hwmgr), GFP_KERNEL);
-	if (data == NULL)
-		return -ENOMEM;
-
-	hwmgr->backend = data;
-
-	data->dll_default_on = false;
-	data->sram_end = SMC_RAM_END;
-
-	for (i = 0; i < SMU73_MAX_LEVELS_GRAPHICS; i++)
-		data->activity_target[i] = FIJI_AT_DFLT;
-
-	data->vddc_vddci_delta = VDDC_VDDCI_DELTA;
-
-	data->mclk_activity_target = PPFIJI_MCLK_TARGETACTIVITY_DFLT;
-	data->mclk_dpm0_activity_target = 0xa;
-
-	data->sclk_dpm_key_disabled = 0;
-	data->mclk_dpm_key_disabled = 0;
-	data->pcie_dpm_key_disabled = 0;
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_UnTabledHardwareInterface);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_TablelessHardwareInterface);
-
-	data->gpio_debug = 0;
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DynamicPatchPowerState);
-
-	/* need to set voltage control types before EVV patching */
-	data->voltage_control = FIJI_VOLTAGE_CONTROL_NONE;
-	data->vddci_control = FIJI_VOLTAGE_CONTROL_NONE;
-	data->mvdd_control = FIJI_VOLTAGE_CONTROL_NONE;
-
-	data->force_pcie_gen = PP_PCIEGenInvalid;
-
-	if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-			VOLTAGE_TYPE_VDDC, VOLTAGE_OBJ_SVID2))
-		data->voltage_control = FIJI_VOLTAGE_CONTROL_BY_SVID2;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EnableMVDDControl))
-		if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-				VOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_GPIO_LUT))
-			data->mvdd_control = FIJI_VOLTAGE_CONTROL_BY_GPIO;
-
-	if (data->mvdd_control == FIJI_VOLTAGE_CONTROL_NONE)
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EnableMVDDControl);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ControlVDDCI)) {
-		if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-				VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_GPIO_LUT))
-			data->vddci_control = FIJI_VOLTAGE_CONTROL_BY_GPIO;
-		else if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-				VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_SVID2))
-			data->vddci_control = FIJI_VOLTAGE_CONTROL_BY_SVID2;
-	}
-
-	if (data->vddci_control == FIJI_VOLTAGE_CONTROL_NONE)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_ControlVDDCI);
-
-	if (table_info && table_info->cac_dtp_table->usClockStretchAmount)
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_ClockStretcher);
-
-	fiji_init_dpm_defaults(hwmgr);
-
-	/* Get leakage voltage based on leakage ID. */
-	fiji_get_evv_voltages(hwmgr);
-
-	/* Patch our voltage dependency table with actual leakage voltage
-	 * We need to perform leakage translation before it's used by other functions
-	 */
-	fiji_complete_dependency_tables(hwmgr);
-
-	/* Parse pptable data read from VBIOS */
-	fiji_set_private_data_based_on_pptable(hwmgr);
-
-	/* ULV Support */
-	data->ulv.ulv_supported = true; /* ULV feature is enabled by default */
-
-	/* Initalize Dynamic State Adjustment Rule Settings */
-	result = tonga_initializa_dynamic_state_adjustment_rule_settings(hwmgr);
-
-	if (!result) {
-		data->uvd_enabled = false;
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_EnableSMU7ThermalManagement);
-		data->vddc_phase_shed_control = false;
-	}
-
-	stay_in_boot = phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_StayInBootState);
-
-	if (0 == result) {
-		struct cgs_system_info sys_info = {0};
-
-		data->is_tlu_enabled = false;
-		hwmgr->platform_descriptor.hardwareActivityPerformanceLevels =
-				FIJI_MAX_HARDWARE_POWERLEVELS;
-		hwmgr->platform_descriptor.hardwarePerformanceLevels = 2;
-		hwmgr->platform_descriptor.minimumClocksReductionPercentage  = 50;
-
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_FanSpeedInTableIsRPM);
-
-		if (table_info->cac_dtp_table->usDefaultTargetOperatingTemp &&
-				hwmgr->thermal_controller.
-				advanceFanControlParameters.ucFanControlMode) {
-			hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanPWM =
-					hwmgr->thermal_controller.advanceFanControlParameters.usDefaultMaxFanPWM;
-			hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanRPM =
-					hwmgr->thermal_controller.advanceFanControlParameters.usDefaultMaxFanRPM;
-			hwmgr->dyn_state.cac_dtp_table->usOperatingTempMinLimit =
-					table_info->cac_dtp_table->usOperatingTempMinLimit;
-			hwmgr->dyn_state.cac_dtp_table->usOperatingTempMaxLimit =
-					table_info->cac_dtp_table->usOperatingTempMaxLimit;
-			hwmgr->dyn_state.cac_dtp_table->usDefaultTargetOperatingTemp =
-					table_info->cac_dtp_table->usDefaultTargetOperatingTemp;
-			hwmgr->dyn_state.cac_dtp_table->usOperatingTempStep =
-					table_info->cac_dtp_table->usOperatingTempStep;
-			hwmgr->dyn_state.cac_dtp_table->usTargetOperatingTemp =
-					table_info->cac_dtp_table->usTargetOperatingTemp;
-
-			phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_ODFuzzyFanControlSupport);
-		}
-
-		sys_info.size = sizeof(struct cgs_system_info);
-		sys_info.info_id = CGS_SYSTEM_INFO_PCIE_GEN_INFO;
-		result = cgs_query_system_info(hwmgr->device, &sys_info);
-		if (result)
-			data->pcie_gen_cap = AMDGPU_DEFAULT_PCIE_GEN_MASK;
-		else
-			data->pcie_gen_cap = (uint32_t)sys_info.value;
-		if (data->pcie_gen_cap & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)
-			data->pcie_spc_cap = 20;
-		sys_info.size = sizeof(struct cgs_system_info);
-		sys_info.info_id = CGS_SYSTEM_INFO_PCIE_MLW;
-		result = cgs_query_system_info(hwmgr->device, &sys_info);
-		if (result)
-			data->pcie_lane_cap = AMDGPU_DEFAULT_PCIE_MLW_MASK;
-		else
-			data->pcie_lane_cap = (uint32_t)sys_info.value;
-	} else {
-		/* Ignore return value in here, we are cleaning up a mess. */
-		fiji_hwmgr_backend_fini(hwmgr);
-	}
-
-	return 0;
-}
-
-/**
- * Read clock related registers.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int fiji_read_clock_registers(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	data->clock_registers.vCG_SPLL_FUNC_CNTL =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixCG_SPLL_FUNC_CNTL);
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_2 =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixCG_SPLL_FUNC_CNTL_2);
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_3 =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixCG_SPLL_FUNC_CNTL_3);
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_4 =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixCG_SPLL_FUNC_CNTL_4);
-	data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixCG_SPLL_SPREAD_SPECTRUM);
-	data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2 =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixCG_SPLL_SPREAD_SPECTRUM_2);
-
-	return 0;
-}
-
-/**
- * Find out if memory is GDDR5.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int fiji_get_memory_type(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t temp;
-
-	temp = cgs_read_register(hwmgr->device, mmMC_SEQ_MISC0);
-
-	data->is_memory_gddr5 = (MC_SEQ_MISC0_GDDR5_VALUE ==
-			((temp & MC_SEQ_MISC0_GDDR5_MASK) >>
-			 MC_SEQ_MISC0_GDDR5_SHIFT));
-
-	return 0;
-}
-
-/**
- * Enables Dynamic Power Management by SMC
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int fiji_enable_acpi_power_management(struct pp_hwmgr *hwmgr)
-{
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			GENERAL_PWRMGT, STATIC_PM_EN, 1);
-
-	return 0;
-}
-
-/**
- * Initialize PowerGating States for different engines
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int fiji_init_power_gate_state(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	data->uvd_power_gated = false;
-	data->vce_power_gated = false;
-	data->samu_power_gated = false;
-	data->acp_power_gated = false;
-	data->pg_acp_init = true;
-
-	return 0;
-}
-
-static int fiji_init_sclk_threshold(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	data->low_sclk_interrupt_threshold = 0;
-
-	return 0;
-}
-
-static int fiji_setup_asic_task(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = fiji_read_clock_registers(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to read clock registers!", result = tmp_result);
-
-	tmp_result = fiji_get_memory_type(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to get memory type!", result = tmp_result);
-
-	tmp_result = fiji_enable_acpi_power_management(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable ACPI power management!", result = tmp_result);
-
-	tmp_result = fiji_init_power_gate_state(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to init power gate state!", result = tmp_result);
-
-	tmp_result = tonga_get_mc_microcode_version(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to get MC microcode version!", result = tmp_result);
-
-	tmp_result = fiji_init_sclk_threshold(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to init sclk threshold!", result = tmp_result);
-
-	return result;
-}
-
-/**
-* Checks if we want to support voltage control
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-*/
-static bool fiji_voltage_control(const struct pp_hwmgr *hwmgr)
-{
-	const struct fiji_hwmgr *data =
-			(const struct fiji_hwmgr *)(hwmgr->backend);
-
-	return (FIJI_VOLTAGE_CONTROL_NONE != data->voltage_control);
-}
-
-/**
-* Enable voltage control
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int fiji_enable_voltage_control(struct pp_hwmgr *hwmgr)
-{
-	/* enable voltage control */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			GENERAL_PWRMGT, VOLT_PWRMGT_EN, 1);
-
-	return 0;
-}
-
-/**
-* Remove repeated voltage values and create table with unique values.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    vol_table  the pointer to changing voltage table
-* @return    0 in success
-*/
-
-static int fiji_trim_voltage_table(struct pp_hwmgr *hwmgr,
-		struct pp_atomctrl_voltage_table *vol_table)
-{
-	uint32_t i, j;
-	uint16_t vvalue;
-	bool found = false;
-	struct pp_atomctrl_voltage_table *table;
-
-	PP_ASSERT_WITH_CODE((NULL != vol_table),
-			"Voltage Table empty.", return -EINVAL);
-	table = kzalloc(sizeof(struct pp_atomctrl_voltage_table),
-			GFP_KERNEL);
-
-	if (NULL == table)
-		return -ENOMEM;
-
-	table->mask_low = vol_table->mask_low;
-	table->phase_delay = vol_table->phase_delay;
-
-	for (i = 0; i < vol_table->count; i++) {
-		vvalue = vol_table->entries[i].value;
-		found = false;
-
-		for (j = 0; j < table->count; j++) {
-			if (vvalue == table->entries[j].value) {
-				found = true;
-				break;
-			}
-		}
-
-		if (!found) {
-			table->entries[table->count].value = vvalue;
-			table->entries[table->count].smio_low =
-					vol_table->entries[i].smio_low;
-			table->count++;
-		}
-	}
-
-	memcpy(vol_table, table, sizeof(struct pp_atomctrl_voltage_table));
-	kfree(table);
-
-	return 0;
-}
-
-static int fiji_get_svi2_mvdd_voltage_table(struct pp_hwmgr *hwmgr,
-		phm_ppt_v1_clock_voltage_dependency_table *dep_table)
-{
-	uint32_t i;
-	int result;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct pp_atomctrl_voltage_table *vol_table = &(data->mvdd_voltage_table);
-
-	PP_ASSERT_WITH_CODE((0 != dep_table->count),
-			"Voltage Dependency Table empty.", return -EINVAL);
-
-	vol_table->mask_low = 0;
-	vol_table->phase_delay = 0;
-	vol_table->count = dep_table->count;
-
-	for (i = 0; i < dep_table->count; i++) {
-		vol_table->entries[i].value = dep_table->entries[i].mvdd;
-		vol_table->entries[i].smio_low = 0;
-	}
-
-	result = fiji_trim_voltage_table(hwmgr, vol_table);
-	PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to trim MVDD table.", return result);
-
-	return 0;
-}
-
-static int fiji_get_svi2_vddci_voltage_table(struct pp_hwmgr *hwmgr,
-		phm_ppt_v1_clock_voltage_dependency_table *dep_table)
-{
-	uint32_t i;
-	int result;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct pp_atomctrl_voltage_table *vol_table = &(data->vddci_voltage_table);
-
-	PP_ASSERT_WITH_CODE((0 != dep_table->count),
-			"Voltage Dependency Table empty.", return -EINVAL);
-
-	vol_table->mask_low = 0;
-	vol_table->phase_delay = 0;
-	vol_table->count = dep_table->count;
-
-	for (i = 0; i < dep_table->count; i++) {
-		vol_table->entries[i].value = dep_table->entries[i].vddci;
-		vol_table->entries[i].smio_low = 0;
-	}
-
-	result = fiji_trim_voltage_table(hwmgr, vol_table);
-	PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to trim VDDCI table.", return result);
-
-	return 0;
-}
-
-static int fiji_get_svi2_vdd_voltage_table(struct pp_hwmgr *hwmgr,
-		phm_ppt_v1_voltage_lookup_table *lookup_table)
-{
-	int i = 0;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct pp_atomctrl_voltage_table *vol_table = &(data->vddc_voltage_table);
-
-	PP_ASSERT_WITH_CODE((0 != lookup_table->count),
-			"Voltage Lookup Table empty.", return -EINVAL);
-
-	vol_table->mask_low = 0;
-	vol_table->phase_delay = 0;
-
-	vol_table->count = lookup_table->count;
-
-	for (i = 0; i < vol_table->count; i++) {
-		vol_table->entries[i].value = lookup_table->entries[i].us_vdd;
-		vol_table->entries[i].smio_low = 0;
-	}
-
-	return 0;
-}
-
-/* ---- Voltage Tables ----
- * If the voltage table would be bigger than
- * what will fit into the state table on
- * the SMC keep only the higher entries.
- */
-static void fiji_trim_voltage_table_to_fit_state_table(struct pp_hwmgr *hwmgr,
-		uint32_t max_vol_steps, struct pp_atomctrl_voltage_table *vol_table)
-{
-	unsigned int i, diff;
-
-	if (vol_table->count <= max_vol_steps)
-		return;
-
-	diff = vol_table->count - max_vol_steps;
-
-	for (i = 0; i < max_vol_steps; i++)
-		vol_table->entries[i] = vol_table->entries[i + diff];
-
-	vol_table->count = max_vol_steps;
-
-	return;
-}
-
-/**
-* Create Voltage Tables.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int fiji_construct_voltage_tables(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)hwmgr->pptable;
-	int result;
-
-	if (FIJI_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control) {
-		result = atomctrl_get_voltage_table_v3(hwmgr,
-				VOLTAGE_TYPE_MVDDC,	VOLTAGE_OBJ_GPIO_LUT,
-				&(data->mvdd_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve MVDD table.",
-				return result);
-	} else if (FIJI_VOLTAGE_CONTROL_BY_SVID2 == data->mvdd_control) {
-		result = fiji_get_svi2_mvdd_voltage_table(hwmgr,
-				table_info->vdd_dep_on_mclk);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve SVI2 MVDD table from dependancy table.",
-				return result;);
-	}
-
-	if (FIJI_VOLTAGE_CONTROL_BY_GPIO == data->vddci_control) {
-		result = atomctrl_get_voltage_table_v3(hwmgr,
-				VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_GPIO_LUT,
-				&(data->vddci_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve VDDCI table.",
-				return result);
-	} else if (FIJI_VOLTAGE_CONTROL_BY_SVID2 == data->vddci_control) {
-		result = fiji_get_svi2_vddci_voltage_table(hwmgr,
-				table_info->vdd_dep_on_mclk);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve SVI2 VDDCI table from dependancy table.",
-				return result);
-	}
-
-	if(FIJI_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {
-		result = fiji_get_svi2_vdd_voltage_table(hwmgr,
-				table_info->vddc_lookup_table);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve SVI2 VDDC table from lookup table.",
-				return result);
-	}
-
-	PP_ASSERT_WITH_CODE(
-			(data->vddc_voltage_table.count <= (SMU73_MAX_LEVELS_VDDC)),
-			"Too many voltage values for VDDC. Trimming to fit state table.",
-			fiji_trim_voltage_table_to_fit_state_table(hwmgr,
-					SMU73_MAX_LEVELS_VDDC, &(data->vddc_voltage_table)));
-
-	PP_ASSERT_WITH_CODE(
-			(data->vddci_voltage_table.count <= (SMU73_MAX_LEVELS_VDDCI)),
-			"Too many voltage values for VDDCI. Trimming to fit state table.",
-			fiji_trim_voltage_table_to_fit_state_table(hwmgr,
-					SMU73_MAX_LEVELS_VDDCI, &(data->vddci_voltage_table)));
-
-	PP_ASSERT_WITH_CODE(
-			(data->mvdd_voltage_table.count <= (SMU73_MAX_LEVELS_MVDD)),
-			"Too many voltage values for MVDD. Trimming to fit state table.",
-			fiji_trim_voltage_table_to_fit_state_table(hwmgr,
-					SMU73_MAX_LEVELS_MVDD, &(data->mvdd_voltage_table)));
-
-	return 0;
-}
-
-static int fiji_initialize_mc_reg_table(struct pp_hwmgr *hwmgr)
-{
-	/* Program additional LP registers
-	 * that are no longer programmed by VBIOS
-	 */
-	cgs_write_register(hwmgr->device, mmMC_SEQ_RAS_TIMING_LP,
-			cgs_read_register(hwmgr->device, mmMC_SEQ_RAS_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_CAS_TIMING_LP,
-			cgs_read_register(hwmgr->device, mmMC_SEQ_CAS_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_MISC_TIMING2_LP,
-			cgs_read_register(hwmgr->device, mmMC_SEQ_MISC_TIMING2));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_WR_CTL_D1_LP,
-			cgs_read_register(hwmgr->device, mmMC_SEQ_WR_CTL_D1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_RD_CTL_D0_LP,
-			cgs_read_register(hwmgr->device, mmMC_SEQ_RD_CTL_D0));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_RD_CTL_D1_LP,
-			cgs_read_register(hwmgr->device, mmMC_SEQ_RD_CTL_D1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_TIMING_LP,
-			cgs_read_register(hwmgr->device, mmMC_SEQ_PMG_TIMING));
-
-	return 0;
-}
-
-/**
-* Programs static screed detection parameters
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int fiji_program_static_screen_threshold_parameters(
-		struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	/* Set static screen threshold unit */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD_UNIT,
-			data->static_screen_threshold_unit);
-	/* Set static screen threshold */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD,
-			data->static_screen_threshold);
-
-	return 0;
-}
-
-/**
-* Setup display gap for glitch free memory clock switching.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always  0
-*/
-static int fiji_enable_display_gap(struct pp_hwmgr *hwmgr)
-{
-	uint32_t displayGap =
-			cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixCG_DISPLAY_GAP_CNTL);
-
-	displayGap = PHM_SET_FIELD(displayGap, CG_DISPLAY_GAP_CNTL,
-			DISP_GAP, DISPLAY_GAP_IGNORE);
-
-	displayGap = PHM_SET_FIELD(displayGap, CG_DISPLAY_GAP_CNTL,
-			DISP_GAP_MCHG, DISPLAY_GAP_VBLANK);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_DISPLAY_GAP_CNTL, displayGap);
-
-	return 0;
-}
-
-/**
-* Programs activity state transition voting clients
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always  0
-*/
-static int fiji_program_voting_clients(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	/* Clear reset for voting clients before enabling DPM */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			SCLK_PWRMGT_CNTL, RESET_SCLK_CNT, 0);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			SCLK_PWRMGT_CNTL, RESET_BUSY_CNT, 0);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_0, data->voting_rights_clients0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_1, data->voting_rights_clients1);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_2, data->voting_rights_clients2);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_3, data->voting_rights_clients3);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_4, data->voting_rights_clients4);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_5, data->voting_rights_clients5);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_6, data->voting_rights_clients6);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_7, data->voting_rights_clients7);
-
-	return 0;
-}
-
-static int fiji_clear_voting_clients(struct pp_hwmgr *hwmgr)
-{
-	/* Reset voting clients before disabling DPM */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			SCLK_PWRMGT_CNTL, RESET_SCLK_CNT, 1);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			SCLK_PWRMGT_CNTL, RESET_BUSY_CNT, 1);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_0, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_1, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_2, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_3, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_4, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_5, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_6, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_7, 0);
-
-	return 0;
-}
-
-/**
-* Get the location of various tables inside the FW image.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always  0
-*/
-static int fiji_process_firmware_header(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_smumgr *smu_data = (struct fiji_smumgr *)(hwmgr->smumgr->backend);
-	uint32_t tmp;
-	int result;
-	bool error = false;
-
-	result = fiji_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU73_Firmware_Header, DpmTable),
-			&tmp, data->sram_end);
-
-	if (0 == result)
-		data->dpm_table_start = tmp;
-
-	error |= (0 != result);
-
-	result = fiji_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU73_Firmware_Header, SoftRegisters),
-			&tmp, data->sram_end);
-
-	if (!result) {
-		data->soft_regs_start = tmp;
-		smu_data->soft_regs_start = tmp;
-	}
-
-	error |= (0 != result);
-
-	result = fiji_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU73_Firmware_Header, mcRegisterTable),
-			&tmp, data->sram_end);
-
-	if (!result)
-		data->mc_reg_table_start = tmp;
-
-	result = fiji_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU73_Firmware_Header, FanTable),
-			&tmp, data->sram_end);
-
-	if (!result)
-		data->fan_table_start = tmp;
-
-	error |= (0 != result);
-
-	result = fiji_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU73_Firmware_Header, mcArbDramTimingTable),
-			&tmp, data->sram_end);
-
-	if (!result)
-		data->arb_table_start = tmp;
-
-	error |= (0 != result);
-
-	result = fiji_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU73_Firmware_Header, Version),
-			&tmp, data->sram_end);
-
-	if (!result)
-		hwmgr->microcode_version_info.SMC = tmp;
-
-	error |= (0 != result);
-
-	return error ? -1 : 0;
-}
-
-/* Copy one arb setting to another and then switch the active set.
- * arb_src and arb_dest is one of the MC_CG_ARB_FREQ_Fx constants.
- */
-static int fiji_copy_and_switch_arb_sets(struct pp_hwmgr *hwmgr,
-		uint32_t arb_src, uint32_t arb_dest)
-{
-	uint32_t mc_arb_dram_timing;
-	uint32_t mc_arb_dram_timing2;
-	uint32_t burst_time;
-	uint32_t mc_cg_config;
-
-	switch (arb_src) {
-	case MC_CG_ARB_FREQ_F0:
-		mc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING);
-		mc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2);
-		burst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0);
-		break;
-	case MC_CG_ARB_FREQ_F1:
-		mc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1);
-		mc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1);
-		burst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1);
-		break;
-	default:
-		return -EINVAL;
-	}
-
-	switch (arb_dest) {
-	case MC_CG_ARB_FREQ_F0:
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING, mc_arb_dram_timing);
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2, mc_arb_dram_timing2);
-		PHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0, burst_time);
-		break;
-	case MC_CG_ARB_FREQ_F1:
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1, mc_arb_dram_timing);
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1, mc_arb_dram_timing2);
-		PHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1, burst_time);
-		break;
-	default:
-		return -EINVAL;
-	}
-
-	mc_cg_config = cgs_read_register(hwmgr->device, mmMC_CG_CONFIG);
-	mc_cg_config |= 0x0000000F;
-	cgs_write_register(hwmgr->device, mmMC_CG_CONFIG, mc_cg_config);
-	PHM_WRITE_FIELD(hwmgr->device, MC_ARB_CG, CG_ARB_REQ, arb_dest);
-
-	return 0;
-}
-
-/**
-* Call SMC to reset S0/S1 to S1 and Reset SMIO to initial value
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   if success then 0;
-*/
-static int fiji_reset_to_default(struct pp_hwmgr *hwmgr)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_ResetToDefaults);
-}
-
-/**
-* Initial switch from ARB F0->F1
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-* This function is to be called from the SetPowerState table.
-*/
-static int fiji_initial_switch_from_arbf0_to_f1(struct pp_hwmgr *hwmgr)
-{
-	return fiji_copy_and_switch_arb_sets(hwmgr,
-			MC_CG_ARB_FREQ_F0, MC_CG_ARB_FREQ_F1);
-}
-
-static int fiji_force_switch_to_arbf0(struct pp_hwmgr *hwmgr)
-{
-	uint32_t tmp;
-
-	tmp = (cgs_read_ind_register(hwmgr->device,
-			CGS_IND_REG__SMC, ixSMC_SCRATCH9) &
-			0x0000ff00) >> 8;
-
-	if (tmp == MC_CG_ARB_FREQ_F0)
-		return 0;
-
-	return fiji_copy_and_switch_arb_sets(hwmgr,
-			tmp, MC_CG_ARB_FREQ_F0);
-}
-
-static int fiji_reset_single_dpm_table(struct pp_hwmgr *hwmgr,
-		struct fiji_single_dpm_table *dpm_table, uint32_t count)
-{
-	int i;
-	PP_ASSERT_WITH_CODE(count <= MAX_REGULAR_DPM_NUMBER,
-			"Fatal error, can not set up single DPM table entries "
-			"to exceed max number!",);
-
-	dpm_table->count = count;
-	for (i = 0; i < MAX_REGULAR_DPM_NUMBER; i++)
-		dpm_table->dpm_levels[i].enabled = false;
-
-	return 0;
-}
-
-static void fiji_setup_pcie_table_entry(
-	struct fiji_single_dpm_table *dpm_table,
-	uint32_t index, uint32_t pcie_gen,
-	uint32_t pcie_lanes)
-{
-	dpm_table->dpm_levels[index].value = pcie_gen;
-	dpm_table->dpm_levels[index].param1 = pcie_lanes;
-	dpm_table->dpm_levels[index].enabled = true;
-}
-
-static int fiji_setup_default_pcie_table(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_pcie_table *pcie_table = table_info->pcie_table;
-	uint32_t i, max_entry;
-
-	PP_ASSERT_WITH_CODE((data->use_pcie_performance_levels ||
-			data->use_pcie_power_saving_levels), "No pcie performance levels!",
-			return -EINVAL);
-
-	if (data->use_pcie_performance_levels &&
-			!data->use_pcie_power_saving_levels) {
-		data->pcie_gen_power_saving = data->pcie_gen_performance;
-		data->pcie_lane_power_saving = data->pcie_lane_performance;
-	} else if (!data->use_pcie_performance_levels &&
-			data->use_pcie_power_saving_levels) {
-		data->pcie_gen_performance = data->pcie_gen_power_saving;
-		data->pcie_lane_performance = data->pcie_lane_power_saving;
-	}
-
-	fiji_reset_single_dpm_table(hwmgr,
-			&data->dpm_table.pcie_speed_table, SMU73_MAX_LEVELS_LINK);
-
-	if (pcie_table != NULL) {
-		/* max_entry is used to make sure we reserve one PCIE level
-		 * for boot level (fix for A+A PSPP issue).
-		 * If PCIE table from PPTable have ULV entry + 8 entries,
-		 * then ignore the last entry.*/
-		max_entry = (SMU73_MAX_LEVELS_LINK < pcie_table->count) ?
-				SMU73_MAX_LEVELS_LINK : pcie_table->count;
-		for (i = 1; i < max_entry; i++) {
-			fiji_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, i - 1,
-					get_pcie_gen_support(data->pcie_gen_cap,
-							pcie_table->entries[i].gen_speed),
-					get_pcie_lane_support(data->pcie_lane_cap,
-							pcie_table->entries[i].lane_width));
-		}
-		data->dpm_table.pcie_speed_table.count = max_entry - 1;
-	} else {
-		/* Hardcode Pcie Table */
-		fiji_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 0,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Min_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		fiji_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 1,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Min_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		fiji_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 2,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Max_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		fiji_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 3,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Max_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		fiji_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 4,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Max_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		fiji_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 5,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Max_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-
-		data->dpm_table.pcie_speed_table.count = 6;
-	}
-	/* Populate last level for boot PCIE level, but do not increment count. */
-	fiji_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table,
-			data->dpm_table.pcie_speed_table.count,
-			get_pcie_gen_support(data->pcie_gen_cap,
-					PP_Min_PCIEGen),
-			get_pcie_lane_support(data->pcie_lane_cap,
-					PP_Max_PCIELane));
-
-	return 0;
-}
-
-/*
- * This function is to initalize all DPM state tables
- * for SMU7 based on the dependency table.
- * Dynamic state patching function will then trim these
- * state tables to the allowed range based
- * on the power policy or external client requests,
- * such as UVD request, etc.
- */
-static int fiji_setup_default_dpm_tables(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint32_t i;
-
-	struct phm_ppt_v1_clock_voltage_dependency_table *dep_sclk_table =
-			table_info->vdd_dep_on_sclk;
-	struct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table =
-			table_info->vdd_dep_on_mclk;
-
-	PP_ASSERT_WITH_CODE(dep_sclk_table != NULL,
-			"SCLK dependency table is missing. This table is mandatory",
-			return -EINVAL);
-	PP_ASSERT_WITH_CODE(dep_sclk_table->count >= 1,
-			"SCLK dependency table has to have is missing. "
-			"This table is mandatory",
-			return -EINVAL);
-
-	PP_ASSERT_WITH_CODE(dep_mclk_table != NULL,
-			"MCLK dependency table is missing. This table is mandatory",
-			return -EINVAL);
-	PP_ASSERT_WITH_CODE(dep_mclk_table->count >= 1,
-			"MCLK dependency table has to have is missing. "
-			"This table is mandatory",
-			return -EINVAL);
-
-	/* clear the state table to reset everything to default */
-	fiji_reset_single_dpm_table(hwmgr,
-			&data->dpm_table.sclk_table, SMU73_MAX_LEVELS_GRAPHICS);
-	fiji_reset_single_dpm_table(hwmgr,
-			&data->dpm_table.mclk_table, SMU73_MAX_LEVELS_MEMORY);
-
-	/* Initialize Sclk DPM table based on allow Sclk values */
-	data->dpm_table.sclk_table.count = 0;
-	for (i = 0; i < dep_sclk_table->count; i++) {
-		if (i == 0 || data->dpm_table.sclk_table.dpm_levels
-				[data->dpm_table.sclk_table.count - 1].value !=
-						dep_sclk_table->entries[i].clk) {
-			data->dpm_table.sclk_table.dpm_levels
-			[data->dpm_table.sclk_table.count].value =
-					dep_sclk_table->entries[i].clk;
-			data->dpm_table.sclk_table.dpm_levels
-			[data->dpm_table.sclk_table.count].enabled =
-					(i == 0) ? true : false;
-			data->dpm_table.sclk_table.count++;
-		}
-	}
-
-	/* Initialize Mclk DPM table based on allow Mclk values */
-	data->dpm_table.mclk_table.count = 0;
-	for (i=0; i<dep_mclk_table->count; i++) {
-		if ( i==0 || data->dpm_table.mclk_table.dpm_levels
-				[data->dpm_table.mclk_table.count - 1].value !=
-						dep_mclk_table->entries[i].clk) {
-			data->dpm_table.mclk_table.dpm_levels
-			[data->dpm_table.mclk_table.count].value =
-					dep_mclk_table->entries[i].clk;
-			data->dpm_table.mclk_table.dpm_levels
-			[data->dpm_table.mclk_table.count].enabled =
-					(i == 0) ? true : false;
-			data->dpm_table.mclk_table.count++;
-		}
-	}
-
-	/* setup PCIE gen speed levels */
-	fiji_setup_default_pcie_table(hwmgr);
-
-	/* save a copy of the default DPM table */
-	memcpy(&(data->golden_dpm_table), &(data->dpm_table),
-			sizeof(struct fiji_dpm_table));
-
-	return 0;
-}
-
-/**
- * @brief PhwFiji_GetVoltageOrder
- *  Returns index of requested voltage record in lookup(table)
- * @param lookup_table - lookup list to search in
- * @param voltage - voltage to look for
- * @return 0 on success
- */
-static uint8_t fiji_get_voltage_index(
-		struct phm_ppt_v1_voltage_lookup_table *lookup_table, uint16_t voltage)
-{
-	uint8_t count = (uint8_t) (lookup_table->count);
-	uint8_t i;
-
-	PP_ASSERT_WITH_CODE((NULL != lookup_table),
-			"Lookup Table empty.", return 0);
-	PP_ASSERT_WITH_CODE((0 != count),
-			"Lookup Table empty.", return 0);
-
-	for (i = 0; i < lookup_table->count; i++) {
-		/* find first voltage equal or bigger than requested */
-		if (lookup_table->entries[i].us_vdd >= voltage)
-			return i;
-	}
-	/* voltage is bigger than max voltage in the table */
-	return i - 1;
-}
-
-/**
-* Preparation of vddc and vddgfx CAC tables for SMC.
-*
-* @param    hwmgr  the address of the hardware manager
-* @param    table  the SMC DPM table structure to be populated
-* @return   always 0
-*/
-static int fiji_populate_cac_table(struct pp_hwmgr *hwmgr,
-		struct SMU73_Discrete_DpmTable *table)
-{
-	uint32_t count;
-	uint8_t index;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_voltage_lookup_table *lookup_table =
-			table_info->vddc_lookup_table;
-	/* tables is already swapped, so in order to use the value from it,
-	 * we need to swap it back.
-	 * We are populating vddc CAC data to BapmVddc table
-	 * in split and merged mode
-	 */
-	for( count = 0; count<lookup_table->count; count++) {
-		index = fiji_get_voltage_index(lookup_table,
-				data->vddc_voltage_table.entries[count].value);
-		table->BapmVddcVidLoSidd[count] = (uint8_t) ((6200 -
-				(lookup_table->entries[index].us_cac_low *
-						VOLTAGE_SCALE)) / 25);
-		table->BapmVddcVidHiSidd[count] = (uint8_t) ((6200 -
-				(lookup_table->entries[index].us_cac_high *
-						VOLTAGE_SCALE)) / 25);
-	}
-
-	return 0;
-}
-
-/**
-* Preparation of voltage tables for SMC.
-*
-* @param    hwmgr   the address of the hardware manager
-* @param    table   the SMC DPM table structure to be populated
-* @return   always  0
-*/
-
-static int fiji_populate_smc_voltage_tables(struct pp_hwmgr *hwmgr,
-		struct SMU73_Discrete_DpmTable *table)
-{
-	int result;
-
-	result = fiji_populate_cac_table(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"can not populate CAC voltage tables to SMC",
-			return -EINVAL);
-
-	return 0;
-}
-
-static int fiji_populate_ulv_level(struct pp_hwmgr *hwmgr,
-		struct SMU73_Discrete_Ulv *state)
-{
-	int result = 0;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	state->CcPwrDynRm = 0;
-	state->CcPwrDynRm1 = 0;
-
-	state->VddcOffset = (uint16_t) table_info->us_ulv_voltage_offset;
-	state->VddcOffsetVid = (uint8_t)( table_info->us_ulv_voltage_offset *
-			VOLTAGE_VID_OFFSET_SCALE2 / VOLTAGE_VID_OFFSET_SCALE1 );
-
-	state->VddcPhase = (data->vddc_phase_shed_control) ? 0 : 1;
-
-	if (!result) {
-		CONVERT_FROM_HOST_TO_SMC_UL(state->CcPwrDynRm);
-		CONVERT_FROM_HOST_TO_SMC_UL(state->CcPwrDynRm1);
-		CONVERT_FROM_HOST_TO_SMC_US(state->VddcOffset);
-	}
-	return result;
-}
-
-static int fiji_populate_ulv_state(struct pp_hwmgr *hwmgr,
-		struct SMU73_Discrete_DpmTable *table)
-{
-	return fiji_populate_ulv_level(hwmgr, &table->Ulv);
-}
-
-static int32_t fiji_get_dpm_level_enable_mask_value(
-		struct fiji_single_dpm_table* dpm_table)
-{
-	int32_t i;
-	int32_t mask = 0;
-
-	for (i = dpm_table->count; i > 0; i--) {
-		mask = mask << 1;
-		if (dpm_table->dpm_levels[i - 1].enabled)
-			mask |= 0x1;
-		else
-			mask &= 0xFFFFFFFE;
-	}
-	return mask;
-}
-
-static int fiji_populate_smc_link_level(struct pp_hwmgr *hwmgr,
-		struct SMU73_Discrete_DpmTable *table)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_dpm_table *dpm_table = &data->dpm_table;
-	int i;
-
-	/* Index (dpm_table->pcie_speed_table.count)
-	 * is reserved for PCIE boot level. */
-	for (i = 0; i <= dpm_table->pcie_speed_table.count; i++) {
-		table->LinkLevel[i].PcieGenSpeed  =
-				(uint8_t)dpm_table->pcie_speed_table.dpm_levels[i].value;
-		table->LinkLevel[i].PcieLaneCount = (uint8_t)encode_pcie_lane_width(
-				dpm_table->pcie_speed_table.dpm_levels[i].param1);
-		table->LinkLevel[i].EnabledForActivity = 1;
-		table->LinkLevel[i].SPC = (uint8_t)(data->pcie_spc_cap & 0xff);
-		table->LinkLevel[i].DownThreshold = PP_HOST_TO_SMC_UL(5);
-		table->LinkLevel[i].UpThreshold = PP_HOST_TO_SMC_UL(30);
-	}
-
-	data->smc_state_table.LinkLevelCount =
-			(uint8_t)dpm_table->pcie_speed_table.count;
-	data->dpm_level_enable_mask.pcie_dpm_enable_mask =
-			fiji_get_dpm_level_enable_mask_value(&dpm_table->pcie_speed_table);
-
-	return 0;
-}
-
-/**
-* Calculates the SCLK dividers using the provided engine clock
-*
-* @param    hwmgr  the address of the hardware manager
-* @param    clock  the engine clock to use to populate the structure
-* @param    sclk   the SMC SCLK structure to be populated
-*/
-static int fiji_calculate_sclk_params(struct pp_hwmgr *hwmgr,
-		uint32_t clock, struct SMU73_Discrete_GraphicsLevel *sclk)
-{
-	const struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct pp_atomctrl_clock_dividers_vi dividers;
-	uint32_t spll_func_cntl            = data->clock_registers.vCG_SPLL_FUNC_CNTL;
-	uint32_t spll_func_cntl_3          = data->clock_registers.vCG_SPLL_FUNC_CNTL_3;
-	uint32_t spll_func_cntl_4          = data->clock_registers.vCG_SPLL_FUNC_CNTL_4;
-	uint32_t cg_spll_spread_spectrum   = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM;
-	uint32_t cg_spll_spread_spectrum_2 = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2;
-	uint32_t ref_clock;
-	uint32_t ref_divider;
-	uint32_t fbdiv;
-	int result;
-
-	/* get the engine clock dividers for this clock value */
-	result = atomctrl_get_engine_pll_dividers_vi(hwmgr, clock,  &dividers);
-
-	PP_ASSERT_WITH_CODE(result == 0,
-			"Error retrieving Engine Clock dividers from VBIOS.",
-			return result);
-
-	/* To get FBDIV we need to multiply this by 16384 and divide it by Fref. */
-	ref_clock = atomctrl_get_reference_clock(hwmgr);
-	ref_divider = 1 + dividers.uc_pll_ref_div;
-
-	/* low 14 bits is fraction and high 12 bits is divider */
-	fbdiv = dividers.ul_fb_div.ul_fb_divider & 0x3FFFFFF;
-
-	/* SPLL_FUNC_CNTL setup */
-	spll_func_cntl = PHM_SET_FIELD(spll_func_cntl, CG_SPLL_FUNC_CNTL,
-			SPLL_REF_DIV, dividers.uc_pll_ref_div);
-	spll_func_cntl = PHM_SET_FIELD(spll_func_cntl, CG_SPLL_FUNC_CNTL,
-			SPLL_PDIV_A,  dividers.uc_pll_post_div);
-
-	/* SPLL_FUNC_CNTL_3 setup*/
-	spll_func_cntl_3 = PHM_SET_FIELD(spll_func_cntl_3, CG_SPLL_FUNC_CNTL_3,
-			SPLL_FB_DIV, fbdiv);
-
-	/* set to use fractional accumulation*/
-	spll_func_cntl_3 = PHM_SET_FIELD(spll_func_cntl_3, CG_SPLL_FUNC_CNTL_3,
-			SPLL_DITHEN, 1);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_EngineSpreadSpectrumSupport)) {
-		struct pp_atomctrl_internal_ss_info ssInfo;
-
-		uint32_t vco_freq = clock * dividers.uc_pll_post_div;
-		if (!atomctrl_get_engine_clock_spread_spectrum(hwmgr,
-				vco_freq, &ssInfo)) {
-			/*
-			 * ss_info.speed_spectrum_percentage -- in unit of 0.01%
-			 * ss_info.speed_spectrum_rate -- in unit of khz
-			 *
-			 * clks = reference_clock * 10 / (REFDIV + 1) / speed_spectrum_rate / 2
-			 */
-			uint32_t clk_s = ref_clock * 5 /
-					(ref_divider * ssInfo.speed_spectrum_rate);
-			/* clkv = 2 * D * fbdiv / NS */
-			uint32_t clk_v = 4 * ssInfo.speed_spectrum_percentage *
-					fbdiv / (clk_s * 10000);
-
-			cg_spll_spread_spectrum = PHM_SET_FIELD(cg_spll_spread_spectrum,
-					CG_SPLL_SPREAD_SPECTRUM, CLKS, clk_s);
-			cg_spll_spread_spectrum = PHM_SET_FIELD(cg_spll_spread_spectrum,
-					CG_SPLL_SPREAD_SPECTRUM, SSEN, 1);
-			cg_spll_spread_spectrum_2 = PHM_SET_FIELD(cg_spll_spread_spectrum_2,
-					CG_SPLL_SPREAD_SPECTRUM_2, CLKV, clk_v);
-		}
-	}
-
-	sclk->SclkFrequency        = clock;
-	sclk->CgSpllFuncCntl3      = spll_func_cntl_3;
-	sclk->CgSpllFuncCntl4      = spll_func_cntl_4;
-	sclk->SpllSpreadSpectrum   = cg_spll_spread_spectrum;
-	sclk->SpllSpreadSpectrum2  = cg_spll_spread_spectrum_2;
-	sclk->SclkDid              = (uint8_t)dividers.pll_post_divider;
-
-	return 0;
-}
-
-static uint16_t fiji_find_closest_vddci(struct pp_hwmgr *hwmgr, uint16_t vddci)
-{
-	uint32_t  i;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct pp_atomctrl_voltage_table *vddci_table =
-			&(data->vddci_voltage_table);
-
-	for (i = 0; i < vddci_table->count; i++) {
-		if (vddci_table->entries[i].value >= vddci)
-			return vddci_table->entries[i].value;
-	}
-
-	PP_ASSERT_WITH_CODE(false,
-			"VDDCI is larger than max VDDCI in VDDCI Voltage Table!",
-			return vddci_table->entries[i-1].value);
-}
-
-static int fiji_get_dependency_volt_by_clk(struct pp_hwmgr *hwmgr,
-		struct phm_ppt_v1_clock_voltage_dependency_table* dep_table,
-		uint32_t clock, SMU_VoltageLevel *voltage, uint32_t *mvdd)
-{
-	uint32_t i;
-	uint16_t vddci;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	*voltage = *mvdd = 0;
-
-	/* clock - voltage dependency table is empty table */
-	if (dep_table->count == 0)
-		return -EINVAL;
-
-	for (i = 0; i < dep_table->count; i++) {
-		/* find first sclk bigger than request */
-		if (dep_table->entries[i].clk >= clock) {
-			*voltage |= (dep_table->entries[i].vddc *
-					VOLTAGE_SCALE) << VDDC_SHIFT;
-			if (FIJI_VOLTAGE_CONTROL_NONE == data->vddci_control)
-				*voltage |= (data->vbios_boot_state.vddci_bootup_value *
-						VOLTAGE_SCALE) << VDDCI_SHIFT;
-			else if (dep_table->entries[i].vddci)
-				*voltage |= (dep_table->entries[i].vddci *
-						VOLTAGE_SCALE) << VDDCI_SHIFT;
-			else {
-				vddci = fiji_find_closest_vddci(hwmgr,
-						(dep_table->entries[i].vddc -
-								(uint16_t)data->vddc_vddci_delta));
-				*voltage |= (vddci * VOLTAGE_SCALE) <<	VDDCI_SHIFT;
-			}
-
-			if (FIJI_VOLTAGE_CONTROL_NONE == data->mvdd_control)
-				*mvdd = data->vbios_boot_state.mvdd_bootup_value *
-					VOLTAGE_SCALE;
-			else if (dep_table->entries[i].mvdd)
-				*mvdd = (uint32_t) dep_table->entries[i].mvdd *
-					VOLTAGE_SCALE;
-
-			*voltage |= 1 << PHASES_SHIFT;
-			return 0;
-		}
-	}
-
-	/* sclk is bigger than max sclk in the dependence table */
-	*voltage |= (dep_table->entries[i - 1].vddc * VOLTAGE_SCALE) << VDDC_SHIFT;
-
-	if (FIJI_VOLTAGE_CONTROL_NONE == data->vddci_control)
-		*voltage |= (data->vbios_boot_state.vddci_bootup_value *
-				VOLTAGE_SCALE) << VDDCI_SHIFT;
-	else if (dep_table->entries[i-1].vddci) {
-		vddci = fiji_find_closest_vddci(hwmgr,
-				(dep_table->entries[i].vddc -
-						(uint16_t)data->vddc_vddci_delta));
-		*voltage |= (vddci * VOLTAGE_SCALE) << VDDCI_SHIFT;
-	}
-
-	if (FIJI_VOLTAGE_CONTROL_NONE == data->mvdd_control)
-		*mvdd = data->vbios_boot_state.mvdd_bootup_value * VOLTAGE_SCALE;
-	else if (dep_table->entries[i].mvdd)
-		*mvdd = (uint32_t) dep_table->entries[i - 1].mvdd * VOLTAGE_SCALE;
-
-	return 0;
-}
-
-static uint8_t fiji_get_sleep_divider_id_from_clock(uint32_t clock,
-		uint32_t clock_insr)
-{
-	uint8_t i;
-	uint32_t temp;
-	uint32_t min = max(clock_insr, (uint32_t)FIJI_MINIMUM_ENGINE_CLOCK);
-
-	PP_ASSERT_WITH_CODE((clock >= min), "Engine clock can't satisfy stutter requirement!", return 0);
-	for (i = FIJI_MAX_DEEPSLEEP_DIVIDER_ID;  ; i--) {
-		temp = clock >> i;
-
-		if (temp >= min || i == 0)
-			break;
-	}
-	return i;
-}
-/**
-* Populates single SMC SCLK structure using the provided engine clock
-*
-* @param    hwmgr      the address of the hardware manager
-* @param    clock the engine clock to use to populate the structure
-* @param    sclk        the SMC SCLK structure to be populated
-*/
-
-static int fiji_populate_single_graphic_level(struct pp_hwmgr *hwmgr,
-		uint32_t clock, uint16_t sclk_al_threshold,
-		struct SMU73_Discrete_GraphicsLevel *level)
-{
-	int result;
-	/* PP_Clocks minClocks; */
-	uint32_t threshold, mvdd;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	result = fiji_calculate_sclk_params(hwmgr, clock, level);
-
-	/* populate graphics levels */
-	result = fiji_get_dependency_volt_by_clk(hwmgr,
-			table_info->vdd_dep_on_sclk, clock,
-			&level->MinVoltage, &mvdd);
-	PP_ASSERT_WITH_CODE((0 == result),
-			"can not find VDDC voltage value for "
-			"VDDC engine clock dependency table",
-			return result);
-
-	level->SclkFrequency = clock;
-	level->ActivityLevel = sclk_al_threshold;
-	level->CcPwrDynRm = 0;
-	level->CcPwrDynRm1 = 0;
-	level->EnabledForActivity = 0;
-	level->EnabledForThrottle = 1;
-	level->UpHyst = 10;
-	level->DownHyst = 0;
-	level->VoltageDownHyst = 0;
-	level->PowerThrottle = 0;
-
-	threshold = clock * data->fast_watermark_threshold / 100;
-
-
-	data->display_timing.min_clock_in_sr = hwmgr->display_config.min_core_set_clock_in_sr;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_SclkDeepSleep))
-		level->DeepSleepDivId = fiji_get_sleep_divider_id_from_clock(clock,
-								hwmgr->display_config.min_core_set_clock_in_sr);
-
-
-	/* Default to slow, highest DPM level will be
-	 * set to PPSMC_DISPLAY_WATERMARK_LOW later.
-	 */
-	level->DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-
-	CONVERT_FROM_HOST_TO_SMC_UL(level->MinVoltage);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->SclkFrequency);
-	CONVERT_FROM_HOST_TO_SMC_US(level->ActivityLevel);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->CgSpllFuncCntl3);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->CgSpllFuncCntl4);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->SpllSpreadSpectrum);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->SpllSpreadSpectrum2);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->CcPwrDynRm);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->CcPwrDynRm1);
-
-	return 0;
-}
-/**
-* Populates all SMC SCLK levels' structure based on the trimmed allowed dpm engine clock states
-*
-* @param    hwmgr      the address of the hardware manager
-*/
-static int fiji_populate_all_graphic_levels(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_dpm_table *dpm_table = &data->dpm_table;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_pcie_table *pcie_table = table_info->pcie_table;
-	uint8_t pcie_entry_cnt = (uint8_t) data->dpm_table.pcie_speed_table.count;
-	int result = 0;
-	uint32_t array = data->dpm_table_start +
-			offsetof(SMU73_Discrete_DpmTable, GraphicsLevel);
-	uint32_t array_size = sizeof(struct SMU73_Discrete_GraphicsLevel) *
-			SMU73_MAX_LEVELS_GRAPHICS;
-	struct SMU73_Discrete_GraphicsLevel *levels =
-			data->smc_state_table.GraphicsLevel;
-	uint32_t i, max_entry;
-	uint8_t hightest_pcie_level_enabled = 0,
-			lowest_pcie_level_enabled = 0,
-			mid_pcie_level_enabled = 0,
-			count = 0;
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		result = fiji_populate_single_graphic_level(hwmgr,
-				dpm_table->sclk_table.dpm_levels[i].value,
-				(uint16_t)data->activity_target[i],
-				&levels[i]);
-		if (result)
-			return result;
-
-		/* Making sure only DPM level 0-1 have Deep Sleep Div ID populated. */
-		if (i > 1)
-			levels[i].DeepSleepDivId = 0;
-	}
-
-	/* Only enable level 0 for now.*/
-	levels[0].EnabledForActivity = 1;
-
-	/* set highest level watermark to high */
-	levels[dpm_table->sclk_table.count - 1].DisplayWatermark =
-			PPSMC_DISPLAY_WATERMARK_HIGH;
-
-	data->smc_state_table.GraphicsDpmLevelCount =
-			(uint8_t)dpm_table->sclk_table.count;
-	data->dpm_level_enable_mask.sclk_dpm_enable_mask =
-			fiji_get_dpm_level_enable_mask_value(&dpm_table->sclk_table);
-
-	if (pcie_table != NULL) {
-		PP_ASSERT_WITH_CODE((1 <= pcie_entry_cnt),
-				"There must be 1 or more PCIE levels defined in PPTable.",
-				return -EINVAL);
-		max_entry = pcie_entry_cnt - 1;
-		for (i = 0; i < dpm_table->sclk_table.count; i++)
-			levels[i].pcieDpmLevel =
-					(uint8_t) ((i < max_entry)? i : max_entry);
-	} else {
-		while (data->dpm_level_enable_mask.pcie_dpm_enable_mask &&
-				((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-						(1 << (hightest_pcie_level_enabled + 1))) != 0 ))
-			hightest_pcie_level_enabled++;
-
-		while (data->dpm_level_enable_mask.pcie_dpm_enable_mask &&
-				((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-						(1 << lowest_pcie_level_enabled)) == 0 ))
-			lowest_pcie_level_enabled++;
-
-		while ((count < hightest_pcie_level_enabled) &&
-				((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-						(1 << (lowest_pcie_level_enabled + 1 + count))) == 0 ))
-			count++;
-
-		mid_pcie_level_enabled = (lowest_pcie_level_enabled + 1+ count) <
-				hightest_pcie_level_enabled?
-						(lowest_pcie_level_enabled + 1 + count) :
-						hightest_pcie_level_enabled;
-
-		/* set pcieDpmLevel to hightest_pcie_level_enabled */
-		for(i = 2; i < dpm_table->sclk_table.count; i++)
-			levels[i].pcieDpmLevel = hightest_pcie_level_enabled;
-
-		/* set pcieDpmLevel to lowest_pcie_level_enabled */
-		levels[0].pcieDpmLevel = lowest_pcie_level_enabled;
-
-		/* set pcieDpmLevel to mid_pcie_level_enabled */
-		levels[1].pcieDpmLevel = mid_pcie_level_enabled;
-	}
-	/* level count will send to smc once at init smc table and never change */
-	result = fiji_copy_bytes_to_smc(hwmgr->smumgr, array, (uint8_t *)levels,
-			(uint32_t)array_size, data->sram_end);
-
-	return result;
-}
-
-/**
- * MCLK Frequency Ratio
- * SEQ_CG_RESP  Bit[31:24] - 0x0
- * Bit[27:24] \96 DDR3 Frequency ratio
- * 0x0 <= 100MHz,       450 < 0x8 <= 500MHz
- * 100 < 0x1 <= 150MHz,       500 < 0x9 <= 550MHz
- * 150 < 0x2 <= 200MHz,       550 < 0xA <= 600MHz
- * 200 < 0x3 <= 250MHz,       600 < 0xB <= 650MHz
- * 250 < 0x4 <= 300MHz,       650 < 0xC <= 700MHz
- * 300 < 0x5 <= 350MHz,       700 < 0xD <= 750MHz
- * 350 < 0x6 <= 400MHz,       750 < 0xE <= 800MHz
- * 400 < 0x7 <= 450MHz,       800 < 0xF
- */
-static uint8_t fiji_get_mclk_frequency_ratio(uint32_t mem_clock)
-{
-	if (mem_clock <= 10000) return 0x0;
-	if (mem_clock <= 15000) return 0x1;
-	if (mem_clock <= 20000) return 0x2;
-	if (mem_clock <= 25000) return 0x3;
-	if (mem_clock <= 30000) return 0x4;
-	if (mem_clock <= 35000) return 0x5;
-	if (mem_clock <= 40000) return 0x6;
-	if (mem_clock <= 45000) return 0x7;
-	if (mem_clock <= 50000) return 0x8;
-	if (mem_clock <= 55000) return 0x9;
-	if (mem_clock <= 60000) return 0xa;
-	if (mem_clock <= 65000) return 0xb;
-	if (mem_clock <= 70000) return 0xc;
-	if (mem_clock <= 75000) return 0xd;
-	if (mem_clock <= 80000) return 0xe;
-	/* mem_clock > 800MHz */
-	return 0xf;
-}
-
-/**
-* Populates the SMC MCLK structure using the provided memory clock
-*
-* @param    hwmgr   the address of the hardware manager
-* @param    clock   the memory clock to use to populate the structure
-* @param    sclk    the SMC SCLK structure to be populated
-*/
-static int fiji_calculate_mclk_params(struct pp_hwmgr *hwmgr,
-    uint32_t clock, struct SMU73_Discrete_MemoryLevel *mclk)
-{
-	struct pp_atomctrl_memory_clock_param mem_param;
-	int result;
-
-	result = atomctrl_get_memory_pll_dividers_vi(hwmgr, clock, &mem_param);
-	PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to get Memory PLL Dividers.",);
-
-	/* Save the result data to outpupt memory level structure */
-	mclk->MclkFrequency   = clock;
-	mclk->MclkDivider     = (uint8_t)mem_param.mpll_post_divider;
-	mclk->FreqRange       = fiji_get_mclk_frequency_ratio(clock);
-
-	return result;
-}
-
-static int fiji_populate_single_memory_level(struct pp_hwmgr *hwmgr,
-		uint32_t clock, struct SMU73_Discrete_MemoryLevel *mem_level)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	int result = 0;
-
-	if (table_info->vdd_dep_on_mclk) {
-		result = fiji_get_dependency_volt_by_clk(hwmgr,
-				table_info->vdd_dep_on_mclk, clock,
-				&mem_level->MinVoltage, &mem_level->MinMvdd);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find MinVddc voltage value from memory "
-				"VDDC voltage dependency table", return result);
-	}
-
-	mem_level->EnabledForThrottle = 1;
-	mem_level->EnabledForActivity = 0;
-	mem_level->UpHyst = 0;
-	mem_level->DownHyst = 100;
-	mem_level->VoltageDownHyst = 0;
-	mem_level->ActivityLevel = (uint16_t)data->mclk_activity_target;
-	mem_level->StutterEnable = false;
-
-	mem_level->DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-
-	/* enable stutter mode if all the follow condition applied
-	 * PECI_GetNumberOfActiveDisplays(hwmgr->pPECI,
-	 * &(data->DisplayTiming.numExistingDisplays));
-	 */
-	data->display_timing.num_existing_displays = 1;
-
-	if ((data->mclk_stutter_mode_threshold) &&
-		(clock <= data->mclk_stutter_mode_threshold) &&
-		(!data->is_uvd_enabled) &&
-		(PHM_READ_FIELD(hwmgr->device, DPG_PIPE_STUTTER_CONTROL,
-				STUTTER_ENABLE) & 0x1))
-		mem_level->StutterEnable = true;
-
-	result = fiji_calculate_mclk_params(hwmgr, clock, mem_level);
-	if (!result) {
-		CONVERT_FROM_HOST_TO_SMC_UL(mem_level->MinMvdd);
-		CONVERT_FROM_HOST_TO_SMC_UL(mem_level->MclkFrequency);
-		CONVERT_FROM_HOST_TO_SMC_US(mem_level->ActivityLevel);
-		CONVERT_FROM_HOST_TO_SMC_UL(mem_level->MinVoltage);
-	}
-	return result;
-}
-
-/**
-* Populates all SMC MCLK levels' structure based on the trimmed allowed dpm memory clock states
-*
-* @param    hwmgr      the address of the hardware manager
-*/
-static int fiji_populate_all_memory_levels(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_dpm_table *dpm_table = &data->dpm_table;
-	int result;
-	/* populate MCLK dpm table to SMU7 */
-	uint32_t array = data->dpm_table_start +
-			offsetof(SMU73_Discrete_DpmTable, MemoryLevel);
-	uint32_t array_size = sizeof(SMU73_Discrete_MemoryLevel) *
-			SMU73_MAX_LEVELS_MEMORY;
-	struct SMU73_Discrete_MemoryLevel *levels =
-			data->smc_state_table.MemoryLevel;
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->mclk_table.count; i++) {
-		PP_ASSERT_WITH_CODE((0 != dpm_table->mclk_table.dpm_levels[i].value),
-				"can not populate memory level as memory clock is zero",
-				return -EINVAL);
-		result = fiji_populate_single_memory_level(hwmgr,
-				dpm_table->mclk_table.dpm_levels[i].value,
-				&levels[i]);
-		if (result)
-			return result;
-	}
-
-	/* Only enable level 0 for now. */
-	levels[0].EnabledForActivity = 1;
-
-	/* in order to prevent MC activity from stutter mode to push DPM up.
-	 * the UVD change complements this by putting the MCLK in
-	 * a higher state by default such that we are not effected by
-	 * up threshold or and MCLK DPM latency.
-	 */
-	levels[0].ActivityLevel = (uint16_t)data->mclk_dpm0_activity_target;
-	CONVERT_FROM_HOST_TO_SMC_US(levels[0].ActivityLevel);
-
-	data->smc_state_table.MemoryDpmLevelCount =
-			(uint8_t)dpm_table->mclk_table.count;
-	data->dpm_level_enable_mask.mclk_dpm_enable_mask =
-			fiji_get_dpm_level_enable_mask_value(&dpm_table->mclk_table);
-	/* set highest level watermark to high */
-	levels[dpm_table->mclk_table.count - 1].DisplayWatermark =
-			PPSMC_DISPLAY_WATERMARK_HIGH;
-
-	/* level count will send to smc once at init smc table and never change */
-	result = fiji_copy_bytes_to_smc(hwmgr->smumgr, array, (uint8_t *)levels,
-			(uint32_t)array_size, data->sram_end);
-
-	return result;
-}
-
-/**
-* Populates the SMC MVDD structure using the provided memory clock.
-*
-* @param    hwmgr      the address of the hardware manager
-* @param    mclk        the MCLK value to be used in the decision if MVDD should be high or low.
-* @param    voltage     the SMC VOLTAGE structure to be populated
-*/
-static int fiji_populate_mvdd_value(struct pp_hwmgr *hwmgr,
-		uint32_t mclk, SMIO_Pattern *smio_pat)
-{
-	const struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint32_t i = 0;
-
-	if (FIJI_VOLTAGE_CONTROL_NONE != data->mvdd_control) {
-		/* find mvdd value which clock is more than request */
-		for (i = 0; i < table_info->vdd_dep_on_mclk->count; i++) {
-			if (mclk <= table_info->vdd_dep_on_mclk->entries[i].clk) {
-				smio_pat->Voltage = data->mvdd_voltage_table.entries[i].value;
-				break;
-			}
-		}
-		PP_ASSERT_WITH_CODE(i < table_info->vdd_dep_on_mclk->count,
-				"MVDD Voltage is outside the supported range.",
-				return -EINVAL);
-	} else
-		return -EINVAL;
-
-	return 0;
-}
-
-static int fiji_populate_smc_acpi_level(struct pp_hwmgr *hwmgr,
-		SMU73_Discrete_DpmTable *table)
-{
-	int result = 0;
-	const struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct pp_atomctrl_clock_dividers_vi dividers;
-	SMIO_Pattern vol_level;
-	uint32_t mvdd;
-	uint16_t us_mvdd;
-	uint32_t spll_func_cntl    = data->clock_registers.vCG_SPLL_FUNC_CNTL;
-	uint32_t spll_func_cntl_2  = data->clock_registers.vCG_SPLL_FUNC_CNTL_2;
-
-	table->ACPILevel.Flags &= ~PPSMC_SWSTATE_FLAG_DC;
-
-	if (!data->sclk_dpm_key_disabled) {
-		/* Get MinVoltage and Frequency from DPM0,
-		 * already converted to SMC_UL */
-		table->ACPILevel.SclkFrequency =
-				data->dpm_table.sclk_table.dpm_levels[0].value;
-		result = fiji_get_dependency_volt_by_clk(hwmgr,
-				table_info->vdd_dep_on_sclk,
-				table->ACPILevel.SclkFrequency,
-				&table->ACPILevel.MinVoltage, &mvdd);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Cannot find ACPI VDDC voltage value "
-				"in Clock Dependency Table",);
-	} else {
-		table->ACPILevel.SclkFrequency =
-				data->vbios_boot_state.sclk_bootup_value;
-		table->ACPILevel.MinVoltage =
-				data->vbios_boot_state.vddc_bootup_value * VOLTAGE_SCALE;
-	}
-
-	/* get the engine clock dividers for this clock value */
-	result = atomctrl_get_engine_pll_dividers_vi(hwmgr,
-			table->ACPILevel.SclkFrequency,  &dividers);
-	PP_ASSERT_WITH_CODE(result == 0,
-			"Error retrieving Engine Clock dividers from VBIOS.",
-			return result);
-
-	table->ACPILevel.SclkDid = (uint8_t)dividers.pll_post_divider;
-	table->ACPILevel.DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-	table->ACPILevel.DeepSleepDivId = 0;
-
-	spll_func_cntl = PHM_SET_FIELD(spll_func_cntl, CG_SPLL_FUNC_CNTL,
-			SPLL_PWRON, 0);
-	spll_func_cntl = PHM_SET_FIELD(spll_func_cntl, CG_SPLL_FUNC_CNTL,
-			SPLL_RESET, 1);
-	spll_func_cntl_2 = PHM_SET_FIELD(spll_func_cntl_2, CG_SPLL_FUNC_CNTL_2,
-			SCLK_MUX_SEL, 4);
-
-	table->ACPILevel.CgSpllFuncCntl = spll_func_cntl;
-	table->ACPILevel.CgSpllFuncCntl2 = spll_func_cntl_2;
-	table->ACPILevel.CgSpllFuncCntl3 = data->clock_registers.vCG_SPLL_FUNC_CNTL_3;
-	table->ACPILevel.CgSpllFuncCntl4 = data->clock_registers.vCG_SPLL_FUNC_CNTL_4;
-	table->ACPILevel.SpllSpreadSpectrum = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM;
-	table->ACPILevel.SpllSpreadSpectrum2 = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2;
-	table->ACPILevel.CcPwrDynRm = 0;
-	table->ACPILevel.CcPwrDynRm1 = 0;
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.Flags);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SclkFrequency);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.MinVoltage);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl2);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl3);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl4);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SpllSpreadSpectrum);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SpllSpreadSpectrum2);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CcPwrDynRm);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CcPwrDynRm1);
-
-	if (!data->mclk_dpm_key_disabled) {
-		/* Get MinVoltage and Frequency from DPM0, already converted to SMC_UL */
-		table->MemoryACPILevel.MclkFrequency =
-				data->dpm_table.mclk_table.dpm_levels[0].value;
-		result = fiji_get_dependency_volt_by_clk(hwmgr,
-				table_info->vdd_dep_on_mclk,
-				table->MemoryACPILevel.MclkFrequency,
-				&table->MemoryACPILevel.MinVoltage, &mvdd);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Cannot find ACPI VDDCI voltage value "
-				"in Clock Dependency Table",);
-	} else {
-		table->MemoryACPILevel.MclkFrequency =
-				data->vbios_boot_state.mclk_bootup_value;
-		table->MemoryACPILevel.MinVoltage =
-				data->vbios_boot_state.vddci_bootup_value * VOLTAGE_SCALE;
-	}
-
-	us_mvdd = 0;
-	if ((FIJI_VOLTAGE_CONTROL_NONE == data->mvdd_control) ||
-			(data->mclk_dpm_key_disabled))
-		us_mvdd = data->vbios_boot_state.mvdd_bootup_value;
-	else {
-		if (!fiji_populate_mvdd_value(hwmgr,
-				data->dpm_table.mclk_table.dpm_levels[0].value,
-				&vol_level))
-			us_mvdd = vol_level.Voltage;
-	}
-
-	table->MemoryACPILevel.MinMvdd =
-			PP_HOST_TO_SMC_UL(us_mvdd * VOLTAGE_SCALE);
-
-	table->MemoryACPILevel.EnabledForThrottle = 0;
-	table->MemoryACPILevel.EnabledForActivity = 0;
-	table->MemoryACPILevel.UpHyst = 0;
-	table->MemoryACPILevel.DownHyst = 100;
-	table->MemoryACPILevel.VoltageDownHyst = 0;
-	table->MemoryACPILevel.ActivityLevel =
-			PP_HOST_TO_SMC_US((uint16_t)data->mclk_activity_target);
-
-	table->MemoryACPILevel.StutterEnable = false;
-	CONVERT_FROM_HOST_TO_SMC_UL(table->MemoryACPILevel.MclkFrequency);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->MemoryACPILevel.MinVoltage);
-
-	return result;
-}
-
-static int fiji_populate_smc_vce_level(struct pp_hwmgr *hwmgr,
-		SMU73_Discrete_DpmTable *table)
-{
-	int result = -EINVAL;
-	uint8_t count;
-	struct pp_atomctrl_clock_dividers_vi dividers;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =
-			table_info->mm_dep_table;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	table->VceLevelCount = (uint8_t)(mm_table->count);
-	table->VceBootLevel = 0;
-
-	for(count = 0; count < table->VceLevelCount; count++) {
-		table->VceLevel[count].Frequency = mm_table->entries[count].eclk;
-		table->VceLevel[count].MinVoltage = 0;
-		table->VceLevel[count].MinVoltage |=
-				(mm_table->entries[count].vddc * VOLTAGE_SCALE) << VDDC_SHIFT;
-		table->VceLevel[count].MinVoltage |=
-				((mm_table->entries[count].vddc - data->vddc_vddci_delta) *
-						VOLTAGE_SCALE) << VDDCI_SHIFT;
-		table->VceLevel[count].MinVoltage |= 1 << PHASES_SHIFT;
-
-		/*retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-				table->VceLevel[count].Frequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for VCE engine clock",
-				return result);
-
-		table->VceLevel[count].Divider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->VceLevel[count].Frequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->VceLevel[count].MinVoltage);
-	}
-	return result;
-}
-
-static int fiji_populate_smc_acp_level(struct pp_hwmgr *hwmgr,
-		SMU73_Discrete_DpmTable *table)
-{
-	int result = -EINVAL;
-	uint8_t count;
-	struct pp_atomctrl_clock_dividers_vi dividers;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =
-			table_info->mm_dep_table;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	table->AcpLevelCount = (uint8_t)(mm_table->count);
-	table->AcpBootLevel = 0;
-
-	for (count = 0; count < table->AcpLevelCount; count++) {
-		table->AcpLevel[count].Frequency = mm_table->entries[count].aclk;
-		table->AcpLevel[count].MinVoltage |= (mm_table->entries[count].vddc *
-				VOLTAGE_SCALE) << VDDC_SHIFT;
-		table->AcpLevel[count].MinVoltage |= ((mm_table->entries[count].vddc -
-				data->vddc_vddci_delta) * VOLTAGE_SCALE) << VDDCI_SHIFT;
-		table->AcpLevel[count].MinVoltage |= 1 << PHASES_SHIFT;
-
-		/* retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-				table->AcpLevel[count].Frequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for engine clock", return result);
-
-		table->AcpLevel[count].Divider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->AcpLevel[count].Frequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->AcpLevel[count].MinVoltage);
-	}
-	return result;
-}
-
-static int fiji_populate_smc_samu_level(struct pp_hwmgr *hwmgr,
-		SMU73_Discrete_DpmTable *table)
-{
-	int result = -EINVAL;
-	uint8_t count;
-	struct pp_atomctrl_clock_dividers_vi dividers;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =
-			table_info->mm_dep_table;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	table->SamuBootLevel = 0;
-	table->SamuLevelCount = (uint8_t)(mm_table->count);
-
-	for (count = 0; count < table->SamuLevelCount; count++) {
-		/* not sure whether we need evclk or not */
-		table->SamuLevel[count].MinVoltage = 0;
-		table->SamuLevel[count].Frequency = mm_table->entries[count].samclock;
-		table->SamuLevel[count].MinVoltage |= (mm_table->entries[count].vddc *
-				VOLTAGE_SCALE) << VDDC_SHIFT;
-		table->SamuLevel[count].MinVoltage |= ((mm_table->entries[count].vddc -
-				data->vddc_vddci_delta) * VOLTAGE_SCALE) << VDDCI_SHIFT;
-		table->SamuLevel[count].MinVoltage |= 1 << PHASES_SHIFT;
-
-		/* retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-				table->SamuLevel[count].Frequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for samu clock", return result);
-
-		table->SamuLevel[count].Divider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->SamuLevel[count].Frequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->SamuLevel[count].MinVoltage);
-	}
-	return result;
-}
-
-static int fiji_populate_memory_timing_parameters(struct pp_hwmgr *hwmgr,
-		int32_t eng_clock, int32_t mem_clock,
-		struct SMU73_Discrete_MCArbDramTimingTableEntry *arb_regs)
-{
-	uint32_t dram_timing;
-	uint32_t dram_timing2;
-	uint32_t burstTime;
-	ULONG state, trrds, trrdl;
-	int result;
-
-	result = atomctrl_set_engine_dram_timings_rv770(hwmgr,
-			eng_clock, mem_clock);
-	PP_ASSERT_WITH_CODE(result == 0,
-			"Error calling VBIOS to set DRAM_TIMING.", return result);
-
-	dram_timing = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING);
-	dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2);
-	burstTime = cgs_read_register(hwmgr->device, mmMC_ARB_BURST_TIME);
-
-	state = PHM_GET_FIELD(burstTime, MC_ARB_BURST_TIME, STATE0);
-	trrds = PHM_GET_FIELD(burstTime, MC_ARB_BURST_TIME, TRRDS0);
-	trrdl = PHM_GET_FIELD(burstTime, MC_ARB_BURST_TIME, TRRDL0);
-
-	arb_regs->McArbDramTiming  = PP_HOST_TO_SMC_UL(dram_timing);
-	arb_regs->McArbDramTiming2 = PP_HOST_TO_SMC_UL(dram_timing2);
-	arb_regs->McArbBurstTime   = (uint8_t)burstTime;
-	arb_regs->TRRDS            = (uint8_t)trrds;
-	arb_regs->TRRDL            = (uint8_t)trrdl;
-
-	return 0;
-}
-
-static int fiji_program_memory_timing_parameters(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct SMU73_Discrete_MCArbDramTimingTable arb_regs;
-	uint32_t i, j;
-	int result = 0;
-
-	for (i = 0; i < data->dpm_table.sclk_table.count; i++) {
-		for (j = 0; j < data->dpm_table.mclk_table.count; j++) {
-			result = fiji_populate_memory_timing_parameters(hwmgr,
-					data->dpm_table.sclk_table.dpm_levels[i].value,
-					data->dpm_table.mclk_table.dpm_levels[j].value,
-					&arb_regs.entries[i][j]);
-			if (result)
-				break;
-		}
-	}
-
-	if (!result)
-		result = fiji_copy_bytes_to_smc(
-				hwmgr->smumgr,
-				data->arb_table_start,
-				(uint8_t *)&arb_regs,
-				sizeof(SMU73_Discrete_MCArbDramTimingTable),
-				data->sram_end);
-	return result;
-}
-
-static int fiji_populate_smc_uvd_level(struct pp_hwmgr *hwmgr,
-		struct SMU73_Discrete_DpmTable *table)
-{
-	int result = -EINVAL;
-	uint8_t count;
-	struct pp_atomctrl_clock_dividers_vi dividers;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =
-			table_info->mm_dep_table;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	table->UvdLevelCount = (uint8_t)(mm_table->count);
-	table->UvdBootLevel = 0;
-
-	for (count = 0; count < table->UvdLevelCount; count++) {
-		table->UvdLevel[count].MinVoltage = 0;
-		table->UvdLevel[count].VclkFrequency = mm_table->entries[count].vclk;
-		table->UvdLevel[count].DclkFrequency = mm_table->entries[count].dclk;
-		table->UvdLevel[count].MinVoltage |= (mm_table->entries[count].vddc *
-				VOLTAGE_SCALE) << VDDC_SHIFT;
-		table->UvdLevel[count].MinVoltage |= ((mm_table->entries[count].vddc -
-				data->vddc_vddci_delta) * VOLTAGE_SCALE) << VDDCI_SHIFT;
-		table->UvdLevel[count].MinVoltage |= 1 << PHASES_SHIFT;
-
-		/* retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-				table->UvdLevel[count].VclkFrequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for Vclk clock", return result);
-
-		table->UvdLevel[count].VclkDivider = (uint8_t)dividers.pll_post_divider;
-
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-				table->UvdLevel[count].DclkFrequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for Dclk clock", return result);
-
-		table->UvdLevel[count].DclkDivider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->UvdLevel[count].VclkFrequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->UvdLevel[count].DclkFrequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->UvdLevel[count].MinVoltage);
-
-	}
-	return result;
-}
-
-static int fiji_find_boot_level(struct fiji_single_dpm_table *table,
-		uint32_t value, uint32_t *boot_level)
-{
-	int result = -EINVAL;
-	uint32_t i;
-
-	for (i = 0; i < table->count; i++) {
-		if (value == table->dpm_levels[i].value) {
-			*boot_level = i;
-			result = 0;
-		}
-	}
-	return result;
-}
-
-static int fiji_populate_smc_boot_level(struct pp_hwmgr *hwmgr,
-		struct SMU73_Discrete_DpmTable *table)
-{
-	int result = 0;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	table->GraphicsBootLevel = 0;
-	table->MemoryBootLevel = 0;
-
-	/* find boot level from dpm table */
-	result = fiji_find_boot_level(&(data->dpm_table.sclk_table),
-			data->vbios_boot_state.sclk_bootup_value,
-			(uint32_t *)&(table->GraphicsBootLevel));
-
-	result = fiji_find_boot_level(&(data->dpm_table.mclk_table),
-			data->vbios_boot_state.mclk_bootup_value,
-			(uint32_t *)&(table->MemoryBootLevel));
-
-	table->BootVddc  = data->vbios_boot_state.vddc_bootup_value *
-			VOLTAGE_SCALE;
-	table->BootVddci = data->vbios_boot_state.vddci_bootup_value *
-			VOLTAGE_SCALE;
-	table->BootMVdd  = data->vbios_boot_state.mvdd_bootup_value *
-			VOLTAGE_SCALE;
-
-	CONVERT_FROM_HOST_TO_SMC_US(table->BootVddc);
-	CONVERT_FROM_HOST_TO_SMC_US(table->BootVddci);
-	CONVERT_FROM_HOST_TO_SMC_US(table->BootMVdd);
-
-	return 0;
-}
-
-static int fiji_populate_smc_initailial_state(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint8_t count, level;
-
-	count = (uint8_t)(table_info->vdd_dep_on_sclk->count);
-	for (level = 0; level < count; level++) {
-		if(table_info->vdd_dep_on_sclk->entries[level].clk >=
-				data->vbios_boot_state.sclk_bootup_value) {
-			data->smc_state_table.GraphicsBootLevel = level;
-			break;
-		}
-	}
-
-	count = (uint8_t)(table_info->vdd_dep_on_mclk->count);
-	for (level = 0; level < count; level++) {
-		if(table_info->vdd_dep_on_mclk->entries[level].clk >=
-				data->vbios_boot_state.mclk_bootup_value) {
-			data->smc_state_table.MemoryBootLevel = level;
-			break;
-		}
-	}
-
-	return 0;
-}
-
-static int fiji_populate_clock_stretcher_data_table(struct pp_hwmgr *hwmgr)
-{
-	uint32_t ro, efuse, efuse2, clock_freq, volt_without_cks,
-			volt_with_cks, value;
-	uint16_t clock_freq_u16;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint8_t type, i, j, cks_setting, stretch_amount, stretch_amount2,
-			volt_offset = 0;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_clock_voltage_dependency_table *sclk_table =
-			table_info->vdd_dep_on_sclk;
-
-	stretch_amount = (uint8_t)table_info->cac_dtp_table->usClockStretchAmount;
-
-	/* Read SMU_Eefuse to read and calculate RO and determine
-	 * if the part is SS or FF. if RO >= 1660MHz, part is FF.
-	 */
-	efuse = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixSMU_EFUSE_0 + (146 * 4));
-	efuse2 = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixSMU_EFUSE_0 + (148 * 4));
-	efuse &= 0xFF000000;
-	efuse = efuse >> 24;
-	efuse2 &= 0xF;
-
-	if (efuse2 == 1)
-		ro = (2300 - 1350) * efuse / 255 + 1350;
-	else
-		ro = (2500 - 1000) * efuse / 255 + 1000;
-
-	if (ro >= 1660)
-		type = 0;
-	else
-		type = 1;
-
-	/* Populate Stretch amount */
-	data->smc_state_table.ClockStretcherAmount = stretch_amount;
-
-	/* Populate Sclk_CKS_masterEn0_7 and Sclk_voltageOffset */
-	for (i = 0; i < sclk_table->count; i++) {
-		data->smc_state_table.Sclk_CKS_masterEn0_7 |=
-				sclk_table->entries[i].cks_enable << i;
-		volt_without_cks = (uint32_t)((14041 *
-			(sclk_table->entries[i].clk/100) / 10000 + 3571 + 75 - ro) * 1000 /
-			(4026 - (13924 * (sclk_table->entries[i].clk/100) / 10000)));
-		volt_with_cks = (uint32_t)((13946 *
-			(sclk_table->entries[i].clk/100) / 10000 + 3320 + 45 - ro) * 1000 /
-			(3664 - (11454 * (sclk_table->entries[i].clk/100) / 10000)));
-		if (volt_without_cks >= volt_with_cks)
-			volt_offset = (uint8_t)(((volt_without_cks - volt_with_cks +
-					sclk_table->entries[i].cks_voffset) * 100 / 625) + 1);
-		data->smc_state_table.Sclk_voltageOffset[i] = volt_offset;
-	}
-
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, PWR_CKS_ENABLE,
-			STRETCH_ENABLE, 0x0);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, PWR_CKS_ENABLE,
-			masterReset, 0x1);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, PWR_CKS_ENABLE,
-			staticEnable, 0x1);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, PWR_CKS_ENABLE,
-			masterReset, 0x0);
-
-	/* Populate CKS Lookup Table */
-	if (stretch_amount == 1 || stretch_amount == 2 || stretch_amount == 5)
-		stretch_amount2 = 0;
-	else if (stretch_amount == 3 || stretch_amount == 4)
-		stretch_amount2 = 1;
-	else {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_ClockStretcher);
-		PP_ASSERT_WITH_CODE(false,
-				"Stretch Amount in PPTable not supported\n",
-				return -EINVAL);
-	}
-
-	value = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixPWR_CKS_CNTL);
-	value &= 0xFFC2FF87;
-	data->smc_state_table.CKS_LOOKUPTable.CKS_LOOKUPTableEntry[0].minFreq =
-			fiji_clock_stretcher_lookup_table[stretch_amount2][0];
-	data->smc_state_table.CKS_LOOKUPTable.CKS_LOOKUPTableEntry[0].maxFreq =
-			fiji_clock_stretcher_lookup_table[stretch_amount2][1];
-	clock_freq_u16 = (uint16_t)(PP_SMC_TO_HOST_UL(data->smc_state_table.
-			GraphicsLevel[data->smc_state_table.GraphicsDpmLevelCount - 1].
-			SclkFrequency) / 100);
-	if (fiji_clock_stretcher_lookup_table[stretch_amount2][0] <
-			clock_freq_u16 &&
-	    fiji_clock_stretcher_lookup_table[stretch_amount2][1] >
-			clock_freq_u16) {
-		/* Program PWR_CKS_CNTL. CKS_USE_FOR_LOW_FREQ */
-		value |= (fiji_clock_stretcher_lookup_table[stretch_amount2][3]) << 16;
-		/* Program PWR_CKS_CNTL. CKS_LDO_REFSEL */
-		value |= (fiji_clock_stretcher_lookup_table[stretch_amount2][2]) << 18;
-		/* Program PWR_CKS_CNTL. CKS_STRETCH_AMOUNT */
-		value |= (fiji_clock_stretch_amount_conversion
-				[fiji_clock_stretcher_lookup_table[stretch_amount2][3]]
-				 [stretch_amount]) << 3;
-	}
-	CONVERT_FROM_HOST_TO_SMC_US(data->smc_state_table.CKS_LOOKUPTable.
-			CKS_LOOKUPTableEntry[0].minFreq);
-	CONVERT_FROM_HOST_TO_SMC_US(data->smc_state_table.CKS_LOOKUPTable.
-			CKS_LOOKUPTableEntry[0].maxFreq);
-	data->smc_state_table.CKS_LOOKUPTable.CKS_LOOKUPTableEntry[0].setting =
-			fiji_clock_stretcher_lookup_table[stretch_amount2][2] & 0x7F;
-	data->smc_state_table.CKS_LOOKUPTable.CKS_LOOKUPTableEntry[0].setting |=
-			(fiji_clock_stretcher_lookup_table[stretch_amount2][3]) << 7;
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixPWR_CKS_CNTL, value);
-
-	/* Populate DDT Lookup Table */
-	for (i = 0; i < 4; i++) {
-		/* Assign the minimum and maximum VID stored
-		 * in the last row of Clock Stretcher Voltage Table.
-		 */
-		data->smc_state_table.ClockStretcherDataTable.
-		ClockStretcherDataTableEntry[i].minVID =
-				(uint8_t) fiji_clock_stretcher_ddt_table[type][i][2];
-		data->smc_state_table.ClockStretcherDataTable.
-		ClockStretcherDataTableEntry[i].maxVID =
-				(uint8_t) fiji_clock_stretcher_ddt_table[type][i][3];
-		/* Loop through each SCLK and check the frequency
-		 * to see if it lies within the frequency for clock stretcher.
-		 */
-		for (j = 0; j < data->smc_state_table.GraphicsDpmLevelCount; j++) {
-			cks_setting = 0;
-			clock_freq = PP_SMC_TO_HOST_UL(
-					data->smc_state_table.GraphicsLevel[j].SclkFrequency);
-			/* Check the allowed frequency against the sclk level[j].
-			 *  Sclk's endianness has already been converted,
-			 *  and it's in 10Khz unit,
-			 *  as opposed to Data table, which is in Mhz unit.
-			 */
-			if (clock_freq >=
-					(fiji_clock_stretcher_ddt_table[type][i][0]) * 100) {
-				cks_setting |= 0x2;
-				if (clock_freq <
-						(fiji_clock_stretcher_ddt_table[type][i][1]) * 100)
-					cks_setting |= 0x1;
-			}
-			data->smc_state_table.ClockStretcherDataTable.
-			ClockStretcherDataTableEntry[i].setting |= cks_setting << (j * 2);
-		}
-		CONVERT_FROM_HOST_TO_SMC_US(data->smc_state_table.
-				ClockStretcherDataTable.
-				ClockStretcherDataTableEntry[i].setting);
-	}
-
-	value = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixPWR_CKS_CNTL);
-	value &= 0xFFFFFFFE;
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixPWR_CKS_CNTL, value);
-
-	return 0;
-}
-
-/**
-* Populates the SMC VRConfig field in DPM table.
-*
-* @param    hwmgr   the address of the hardware manager
-* @param    table   the SMC DPM table structure to be populated
-* @return   always 0
-*/
-static int fiji_populate_vr_config(struct pp_hwmgr *hwmgr,
-		struct SMU73_Discrete_DpmTable *table)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint16_t config;
-
-	config = VR_MERGED_WITH_VDDC;
-	table->VRConfig |= (config << VRCONF_VDDGFX_SHIFT);
-
-	/* Set Vddc Voltage Controller */
-	if(FIJI_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {
-		config = VR_SVI2_PLANE_1;
-		table->VRConfig |= config;
-	} else {
-		PP_ASSERT_WITH_CODE(false,
-				"VDDC should be on SVI2 control in merged mode!",);
-	}
-	/* Set Vddci Voltage Controller */
-	if(FIJI_VOLTAGE_CONTROL_BY_SVID2 == data->vddci_control) {
-		config = VR_SVI2_PLANE_2;  /* only in merged mode */
-		table->VRConfig |= (config << VRCONF_VDDCI_SHIFT);
-	} else if (FIJI_VOLTAGE_CONTROL_BY_GPIO == data->vddci_control) {
-		config = VR_SMIO_PATTERN_1;
-		table->VRConfig |= (config << VRCONF_VDDCI_SHIFT);
-	} else {
-		config = VR_STATIC_VOLTAGE;
-		table->VRConfig |= (config << VRCONF_VDDCI_SHIFT);
-	}
-	/* Set Mvdd Voltage Controller */
-	if(FIJI_VOLTAGE_CONTROL_BY_SVID2 == data->mvdd_control) {
-		config = VR_SVI2_PLANE_2;
-		table->VRConfig |= (config << VRCONF_MVDD_SHIFT);
-	} else if(FIJI_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control) {
-		config = VR_SMIO_PATTERN_2;
-		table->VRConfig |= (config << VRCONF_MVDD_SHIFT);
-	} else {
-		config = VR_STATIC_VOLTAGE;
-		table->VRConfig |= (config << VRCONF_MVDD_SHIFT);
-	}
-
-	return 0;
-}
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static int fiji_save_default_power_profile(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct SMU73_Discrete_GraphicsLevel *levels =
-				data->smc_state_table.GraphicsLevel;
-	unsigned min_level = 1;
-
-	hwmgr->default_gfx_power_profile.activity_threshold =
-			be16_to_cpu(levels[0].ActivityLevel);
-	hwmgr->default_gfx_power_profile.up_hyst = levels[0].UpHyst;
-	hwmgr->default_gfx_power_profile.down_hyst = levels[0].DownHyst;
-	hwmgr->default_gfx_power_profile.type = PP_GFX_PROFILE;
-
-	hwmgr->default_compute_power_profile = hwmgr->default_gfx_power_profile;
-	hwmgr->default_compute_power_profile.type = PP_COMPUTE_PROFILE;
-
-	/* Workaround compute SDMA instability: disable lowest SCLK
-	 * DPM level. Optimize compute power profile: Use only highest
-	 * 2 power levels (if more than 2 are available), Hysteresis:
-	 * 0ms up, 5ms down
-	 */
-	if (data->smc_state_table.GraphicsDpmLevelCount > 2)
-		min_level = data->smc_state_table.GraphicsDpmLevelCount - 2;
-	else if (data->smc_state_table.GraphicsDpmLevelCount == 2)
-		min_level = 1;
-	else
-		min_level = 0;
-	hwmgr->default_compute_power_profile.min_sclk =
-			be32_to_cpu(levels[min_level].SclkFrequency);
-	hwmgr->default_compute_power_profile.up_hyst = 0;
-	hwmgr->default_compute_power_profile.down_hyst = 5;
-
-	hwmgr->gfx_power_profile = hwmgr->default_gfx_power_profile;
-	hwmgr->compute_power_profile = hwmgr->default_compute_power_profile;
-
-	return 0;
-}
-#endif
-
-/**
-* Initializes the SMC table and uploads it
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput  the pointer to input data (PowerState)
-* @return   always 0
-*/
-static int fiji_init_smc_table(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct SMU73_Discrete_DpmTable *table = &(data->smc_state_table);
-	const struct fiji_ulv_parm *ulv = &(data->ulv);
-	uint8_t i;
-	struct pp_atomctrl_gpio_pin_assignment gpio_pin;
-
-	result = fiji_setup_default_dpm_tables(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to setup default DPM tables!", return result);
-
-	if(FIJI_VOLTAGE_CONTROL_NONE != data->voltage_control)
-		fiji_populate_smc_voltage_tables(hwmgr, table);
-
-	table->SystemFlags = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_AutomaticDCTransition))
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_GPIO_DC;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_StepVddc))
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_STEPVDDC;
-
-	if (data->is_memory_gddr5)
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_GDDR5;
-
-	if (ulv->ulv_supported && table_info->us_ulv_voltage_offset) {
-		result = fiji_populate_ulv_state(hwmgr, table);
-		PP_ASSERT_WITH_CODE(0 == result,
-				"Failed to initialize ULV state!", return result);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixCG_ULV_PARAMETER, ulv->cg_ulv_parameter);
-	}
-
-	result = fiji_populate_smc_link_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Link Level!", return result);
-
-	result = fiji_populate_all_graphic_levels(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Graphics Level!", return result);
-
-	result = fiji_populate_all_memory_levels(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Memory Level!", return result);
-
-	result = fiji_populate_smc_acpi_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize ACPI Level!", return result);
-
-	result = fiji_populate_smc_vce_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize VCE Level!", return result);
-
-	result = fiji_populate_smc_acp_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize ACP Level!", return result);
-
-	result = fiji_populate_smc_samu_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize SAMU Level!", return result);
-
-	/* Since only the initial state is completely set up at this point
-	 * (the other states are just copies of the boot state) we only
-	 * need to populate the  ARB settings for the initial state.
-	 */
-	result = fiji_program_memory_timing_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to Write ARB settings for the initial state.", return result);
-
-	result = fiji_populate_smc_uvd_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize UVD Level!", return result);
-
-	result = fiji_populate_smc_boot_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Boot Level!", return result);
-
-	result = fiji_populate_smc_initailial_state(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Boot State!", return result);
-
-	result = fiji_populate_bapm_parameters_in_dpm_table(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to populate BAPM Parameters!", return result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ClockStretcher)) {
-		result = fiji_populate_clock_stretcher_data_table(hwmgr);
-		PP_ASSERT_WITH_CODE(0 == result,
-				"Failed to populate Clock Stretcher Data Table!",
-				return result);
-	}
-
-	table->GraphicsVoltageChangeEnable  = 1;
-	table->GraphicsThermThrottleEnable  = 1;
-	table->GraphicsInterval = 1;
-	table->VoltageInterval  = 1;
-	table->ThermalInterval  = 1;
-	table->TemperatureLimitHigh =
-			table_info->cac_dtp_table->usTargetOperatingTemp *
-			FIJI_Q88_FORMAT_CONVERSION_UNIT;
-	table->TemperatureLimitLow  =
-			(table_info->cac_dtp_table->usTargetOperatingTemp - 1) *
-			FIJI_Q88_FORMAT_CONVERSION_UNIT;
-	table->MemoryVoltageChangeEnable = 1;
-	table->MemoryInterval = 1;
-	table->VoltageResponseTime = 0;
-	table->PhaseResponseTime = 0;
-	table->MemoryThermThrottleEnable = 1;
-	table->PCIeBootLinkLevel = 0;      /* 0:Gen1 1:Gen2 2:Gen3*/
-	table->PCIeGenInterval = 1;
-	table->VRConfig = 0;
-
-	result = fiji_populate_vr_config(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to populate VRConfig setting!", return result);
-
-	table->ThermGpio = 17;
-	table->SclkStepSize = 0x4000;
-
-	if (atomctrl_get_pp_assign_pin(hwmgr, VDDC_VRHOT_GPIO_PINID, &gpio_pin)) {
-		table->VRHotGpio = gpio_pin.uc_gpio_pin_bit_shift;
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_RegulatorHot);
-	} else {
-		table->VRHotGpio = FIJI_UNUSED_GPIO_PIN;
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_RegulatorHot);
-	}
-
-	if (atomctrl_get_pp_assign_pin(hwmgr, PP_AC_DC_SWITCH_GPIO_PINID,
-			&gpio_pin)) {
-		table->AcDcGpio = gpio_pin.uc_gpio_pin_bit_shift;
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_AutomaticDCTransition);
-	} else {
-		table->AcDcGpio = FIJI_UNUSED_GPIO_PIN;
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_AutomaticDCTransition);
-	}
-
-	/* Thermal Output GPIO */
-	if (atomctrl_get_pp_assign_pin(hwmgr, THERMAL_INT_OUTPUT_GPIO_PINID,
-			&gpio_pin)) {
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_ThermalOutGPIO);
-
-		table->ThermOutGpio = gpio_pin.uc_gpio_pin_bit_shift;
-
-		/* For porlarity read GPIOPAD_A with assigned Gpio pin
-		 * since VBIOS will program this register to set 'inactive state',
-		 * driver can then determine 'active state' from this and
-		 * program SMU with correct polarity
-		 */
-		table->ThermOutPolarity = (0 == (cgs_read_register(hwmgr->device, mmGPIOPAD_A) &
-				(1 << gpio_pin.uc_gpio_pin_bit_shift))) ? 1:0;
-		table->ThermOutMode = SMU7_THERM_OUT_MODE_THERM_ONLY;
-
-		/* if required, combine VRHot/PCC with thermal out GPIO */
-		if(phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_RegulatorHot) &&
-			phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_CombinePCCWithThermalSignal))
-			table->ThermOutMode = SMU7_THERM_OUT_MODE_THERM_VRHOT;
-	} else {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_ThermalOutGPIO);
-		table->ThermOutGpio = 17;
-		table->ThermOutPolarity = 1;
-		table->ThermOutMode = SMU7_THERM_OUT_MODE_DISABLE;
-	}
-
-	for (i = 0; i < SMU73_MAX_ENTRIES_SMIO; i++)
-		table->Smio[i] = PP_HOST_TO_SMC_UL(table->Smio[i]);
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SystemFlags);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->VRConfig);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMask1);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMask2);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SclkStepSize);
-	CONVERT_FROM_HOST_TO_SMC_US(table->TemperatureLimitHigh);
-	CONVERT_FROM_HOST_TO_SMC_US(table->TemperatureLimitLow);
-	CONVERT_FROM_HOST_TO_SMC_US(table->VoltageResponseTime);
-	CONVERT_FROM_HOST_TO_SMC_US(table->PhaseResponseTime);
-
-	/* Upload all dpm data to SMC memory.(dpm level, dpm level count etc) */
-	result = fiji_copy_bytes_to_smc(hwmgr->smumgr,
-			data->dpm_table_start +
-			offsetof(SMU73_Discrete_DpmTable, SystemFlags),
-			(uint8_t *)&(table->SystemFlags),
-			sizeof(SMU73_Discrete_DpmTable) - 3 * sizeof(SMU73_PIDController),
-			data->sram_end);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to upload dpm data to SMC memory!", return result);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	fiji_save_default_power_profile(hwmgr);
-#enduf
-	return 0;
-}
-
-/**
-* Initialize the ARB DRAM timing table's index field.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int fiji_init_arb_table_index(struct pp_hwmgr *hwmgr)
-{
-	const struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t tmp;
-	int result;
-
-	/* This is a read-modify-write on the first byte of the ARB table.
-	 * The first byte in the SMU73_Discrete_MCArbDramTimingTable structure
-	 * is the field 'current'.
-	 * This solution is ugly, but we never write the whole table only
-	 * individual fields in it.
-	 * In reality this field should not be in that structure
-	 * but in a soft register.
-	 */
-	result = fiji_read_smc_sram_dword(hwmgr->smumgr,
-			data->arb_table_start, &tmp, data->sram_end);
-
-	if (result)
-		return result;
-
-	tmp &= 0x00FFFFFF;
-	tmp |= ((uint32_t)MC_CG_ARB_FREQ_F1) << 24;
-
-	return fiji_write_smc_sram_dword(hwmgr->smumgr,
-			data->arb_table_start,  tmp, data->sram_end);
-}
-
-static int fiji_enable_vrhot_gpio_interrupt(struct pp_hwmgr *hwmgr)
-{
-	if(phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_RegulatorHot))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_EnableVRHotGPIOInterrupt);
-
-	return 0;
-}
-
-static int fiji_enable_sclk_control(struct pp_hwmgr *hwmgr)
-{
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL,
-			SCLK_PWRMGT_OFF, 0);
-	return 0;
-}
-
-static int fiji_enable_ulv(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_ulv_parm *ulv = &(data->ulv);
-
-	if (ulv->ulv_supported)
-		return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_EnableULV);
-
-	return 0;
-}
-
-static int fiji_disable_ulv(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_ulv_parm *ulv = &(data->ulv);
-
-	if (ulv->ulv_supported)
-		return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_DisableULV);
-
-	return 0;
-}
-
-static int fiji_enable_deep_sleep_master_switch(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkDeepSleep)) {
-		if (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_MASTER_DeepSleep_ON))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to enable Master Deep Sleep switch failed!",
-					return -1);
-	} else {
-		if (smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_MASTER_DeepSleep_OFF)) {
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to disable Master Deep Sleep switch failed!",
-					return -1);
-		}
-	}
-
-	return 0;
-}
-
-static int fiji_disable_deep_sleep_master_switch(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkDeepSleep)) {
-		if (smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_MASTER_DeepSleep_OFF)) {
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to disable Master Deep Sleep switch failed!",
-					return -1);
-		}
-	}
-
-	return 0;
-}
-
-static int fiji_enable_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t   val, val0, val2;
-	uint32_t   i, cpl_cntl, cpl_threshold, mc_threshold;
-
-	/* enable SCLK dpm */
-	if(!data->sclk_dpm_key_disabled)
-		PP_ASSERT_WITH_CODE(
-		(0 == smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_DPM_Enable)),
-		"Failed to enable SCLK DPM during DPM Start Function!",
-		return -1);
-
-	/* enable MCLK dpm */
-	if(0 == data->mclk_dpm_key_disabled) {
-		cpl_threshold = 0;
-		mc_threshold = 0;
-
-		/* Read per MCD tile (0 - 7) */
-		for (i = 0; i < 8; i++) {
-			PHM_WRITE_FIELD(hwmgr->device, MC_CONFIG_MCD, MC_RD_ENABLE, i);
-			val = cgs_read_register(hwmgr->device, mmMC_SEQ_RESERVE_0_S) & 0xf0000000;
-			if (0xf0000000 != val) {
-				/* count number of MCQ that has channel(s) enabled */
-				cpl_threshold++;
-				/* only harvest 3 or full 4 supported */
-				mc_threshold = val ? 3 : 4;
-			}
-		}
-		PP_ASSERT_WITH_CODE(0 != cpl_threshold,
-				"Number of MCQ is zero!", return -EINVAL;);
-
-		mc_threshold = ((mc_threshold & LCAC_MC0_CNTL__MC0_THRESHOLD_MASK) <<
-				LCAC_MC0_CNTL__MC0_THRESHOLD__SHIFT) |
-						LCAC_MC0_CNTL__MC0_ENABLE_MASK;
-		cpl_cntl = ((cpl_threshold & LCAC_CPL_CNTL__CPL_THRESHOLD_MASK) <<
-				LCAC_CPL_CNTL__CPL_THRESHOLD__SHIFT) |
-						LCAC_CPL_CNTL__CPL_ENABLE_MASK;
-		cpl_cntl = (cpl_cntl | (8 << LCAC_CPL_CNTL__CPL_BLOCK_ID__SHIFT));
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixLCAC_MC0_CNTL, mc_threshold);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixLCAC_MC1_CNTL, mc_threshold);
-		if (8 == cpl_threshold) {
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC2_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC3_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC4_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC5_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC6_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC7_CNTL, mc_threshold);
-		}
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixLCAC_CPL_CNTL, cpl_cntl);
-
-		udelay(5);
-
-		mc_threshold = mc_threshold |
-				(1 << LCAC_MC0_CNTL__MC0_SIGNAL_ID__SHIFT);
-		cpl_cntl = cpl_cntl | (1 << LCAC_CPL_CNTL__CPL_SIGNAL_ID__SHIFT);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixLCAC_MC0_CNTL, mc_threshold);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixLCAC_MC1_CNTL, mc_threshold);
-		if (8 == cpl_threshold) {
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC2_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC3_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC4_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC5_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC6_CNTL, mc_threshold);
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixLCAC_MC7_CNTL, mc_threshold);
-		}
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixLCAC_CPL_CNTL, cpl_cntl);
-
-		/* Program CAC_EN per MCD (0-7) Tile */
-		val0 = val = cgs_read_register(hwmgr->device, mmMC_CONFIG_MCD);
-		val &= ~(MC_CONFIG_MCD__MCD0_WR_ENABLE_MASK |
-				MC_CONFIG_MCD__MCD1_WR_ENABLE_MASK |
-				MC_CONFIG_MCD__MCD2_WR_ENABLE_MASK |
-				MC_CONFIG_MCD__MCD3_WR_ENABLE_MASK |
-				MC_CONFIG_MCD__MCD4_WR_ENABLE_MASK |
-				MC_CONFIG_MCD__MCD5_WR_ENABLE_MASK |
-				MC_CONFIG_MCD__MCD6_WR_ENABLE_MASK |
-				MC_CONFIG_MCD__MCD7_WR_ENABLE_MASK |
-				MC_CONFIG_MCD__MC_RD_ENABLE_MASK);
-
-		for (i = 0; i < 8; i++) {
-			/* Enable MCD i Tile read & write */
-			val2  = (val | (i << MC_CONFIG_MCD__MC_RD_ENABLE__SHIFT) |
-					(1 << i));
-			cgs_write_register(hwmgr->device, mmMC_CONFIG_MCD, val2);
-			/* Enbale CAC_ON MCD i Tile */
-			val2 = cgs_read_register(hwmgr->device, mmMC_SEQ_CNTL);
-			val2 |= MC_SEQ_CNTL__CAC_EN_MASK;
-			cgs_write_register(hwmgr->device, mmMC_SEQ_CNTL, val2);
-		}
-		/* Set MC_CONFIG_MCD back to its default setting val0 */
-		cgs_write_register(hwmgr->device, mmMC_CONFIG_MCD, val0);
-
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_MCLKDPM_Enable)),
-				"Failed to enable MCLK DPM during DPM Start Function!",
-				return -1);
-	}
-	return 0;
-}
-
-static int fiji_start_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	/*enable general power management */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-			GLOBAL_PWRMGT_EN, 1);
-	/* enable sclk deep sleep */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL,
-			DYNAMIC_PM_EN, 1);
-	/* prepare for PCIE DPM */
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			data->soft_regs_start + offsetof(SMU73_SoftRegisters,
-					VoltageChangeTimeout), 0x1000);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__PCIE,
-			SWRST_COMMAND_1, RESETLC, 0x0);
-
-	PP_ASSERT_WITH_CODE(
-			(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					PPSMC_MSG_Voltage_Cntl_Enable)),
-			"Failed to enable voltage DPM during DPM Start Function!",
-			return -1);
-
-	if (fiji_enable_sclk_mclk_dpm(hwmgr)) {
-		printk(KERN_ERR "Failed to enable Sclk DPM and Mclk DPM!");
-		return -1;
-	}
-
-	/* enable PCIE dpm */
-	if(!data->pcie_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_PCIeDPM_Enable)),
-				"Failed to enable pcie DPM during DPM Start Function!",
-				return -1);
-	}
-
-	return 0;
-}
-
-static int fiji_disable_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	/* disable SCLK dpm */
-	if (!data->sclk_dpm_key_disabled)
-		PP_ASSERT_WITH_CODE(
-				(smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_DPM_Disable) == 0),
-				"Failed to disable SCLK DPM!",
-				return -1);
-
-	/* disable MCLK dpm */
-	if (!data->mclk_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_MCLKDPM_SetEnabledMask, 1) == 0),
-				"Failed to force MCLK DPM0!",
-				return -1);
-
-		PP_ASSERT_WITH_CODE(
-				(smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_MCLKDPM_Disable) == 0),
-				"Failed to disable MCLK DPM!",
-				return -1);
-	}
-
-	return 0;
-}
-
-static int fiji_stop_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	/* disable general power management */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-			GLOBAL_PWRMGT_EN, 0);
-	/* disable sclk deep sleep */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL,
-			DYNAMIC_PM_EN, 0);
-
-	/* disable PCIE dpm */
-	if (!data->pcie_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_PCIeDPM_Disable) == 0),
-				"Failed to disable pcie DPM during DPM Stop Function!",
-				return -1);
-	}
-
-	if (fiji_disable_sclk_mclk_dpm(hwmgr)) {
-		printk(KERN_ERR "Failed to disable Sclk DPM and Mclk DPM!");
-		return -1;
-	}
-
-	PP_ASSERT_WITH_CODE(
-			(smum_send_msg_to_smc(hwmgr->smumgr,
-					PPSMC_MSG_Voltage_Cntl_Disable) == 0),
-			"Failed to disable voltage DPM during DPM Stop Function!",
-			return -1);
-
-	return 0;
-}
-
-static void fiji_set_dpm_event_sources(struct pp_hwmgr *hwmgr,
-		uint32_t sources)
-{
-	bool protection;
-	enum DPM_EVENT_SRC src;
-
-	switch (sources) {
-	default:
-		printk(KERN_ERR "Unknown throttling event sources.");
-		/* fall through */
-	case 0:
-		protection = false;
-		/* src is unused */
-		break;
-	case (1 << PHM_AutoThrottleSource_Thermal):
-		protection = true;
-		src = DPM_EVENT_SRC_DIGITAL;
-		break;
-	case (1 << PHM_AutoThrottleSource_External):
-		protection = true;
-		src = DPM_EVENT_SRC_EXTERNAL;
-		break;
-	case (1 << PHM_AutoThrottleSource_External) |
-			(1 << PHM_AutoThrottleSource_Thermal):
-		protection = true;
-		src = DPM_EVENT_SRC_DIGITAL_OR_EXTERNAL;
-		break;
-	}
-	/* Order matters - don't enable thermal protection for the wrong source. */
-	if (protection) {
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_CTRL,
-				DPM_EVENT_SRC, src);
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-				THERMAL_PROTECTION_DIS,
-				!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_ThermalController));
-	} else
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-				THERMAL_PROTECTION_DIS, 1);
-}
-
-static int fiji_enable_auto_throttle_source(struct pp_hwmgr *hwmgr,
-		PHM_AutoThrottleSource source)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (!(data->active_auto_throttle_sources & (1 << source))) {
-		data->active_auto_throttle_sources |= 1 << source;
-		fiji_set_dpm_event_sources(hwmgr, data->active_auto_throttle_sources);
-	}
-	return 0;
-}
-
-static int fiji_enable_thermal_auto_throttle(struct pp_hwmgr *hwmgr)
-{
-	return fiji_enable_auto_throttle_source(hwmgr, PHM_AutoThrottleSource_Thermal);
-}
-
-static int fiji_disable_auto_throttle_source(struct pp_hwmgr *hwmgr,
-		PHM_AutoThrottleSource source)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (data->active_auto_throttle_sources & (1 << source)) {
-		data->active_auto_throttle_sources &= ~(1 << source);
-		fiji_set_dpm_event_sources(hwmgr, data->active_auto_throttle_sources);
-	}
-	return 0;
-}
-
-static int fiji_disable_thermal_auto_throttle(struct pp_hwmgr *hwmgr)
-{
-	return fiji_disable_auto_throttle_source(hwmgr, PHM_AutoThrottleSource_Thermal);
-}
-
-static int fiji_enable_dpm_tasks(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = (!fiji_is_dpm_running(hwmgr))? 0 : -1;
-	PP_ASSERT_WITH_CODE(result == 0,
-			"DPM is already running right now, no need to enable DPM!",
-			return 0);
-
-	if (fiji_voltage_control(hwmgr)) {
-		tmp_result = fiji_enable_voltage_control(hwmgr);
-		PP_ASSERT_WITH_CODE(tmp_result == 0,
-				"Failed to enable voltage control!",
-				result = tmp_result);
-	}
-
-	if (fiji_voltage_control(hwmgr)) {
-		tmp_result = fiji_construct_voltage_tables(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-				"Failed to contruct voltage tables!",
-				result = tmp_result);
-	}
-
-	tmp_result = fiji_initialize_mc_reg_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to initialize MC reg table!", result = tmp_result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EngineSpreadSpectrumSupport))
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				GENERAL_PWRMGT, DYN_SPREAD_SPECTRUM_EN, 1);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ThermalController))
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				GENERAL_PWRMGT, THERMAL_PROTECTION_DIS, 0);
-
-	tmp_result = fiji_program_static_screen_threshold_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to program static screen threshold parameters!",
-			result = tmp_result);
-
-	tmp_result = fiji_enable_display_gap(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable display gap!", result = tmp_result);
-
-	tmp_result = fiji_program_voting_clients(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to program voting clients!", result = tmp_result);
-
-	tmp_result = fiji_process_firmware_header(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to process firmware header!", result = tmp_result);
-
-	tmp_result = fiji_initial_switch_from_arbf0_to_f1(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to initialize switch from ArbF0 to F1!",
-			result = tmp_result);
-
-	tmp_result = fiji_init_smc_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to initialize SMC table!", result = tmp_result);
-
-	tmp_result = fiji_init_arb_table_index(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to initialize ARB table index!", result = tmp_result);
-
-	tmp_result = fiji_populate_pm_fuses(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to populate PM fuses!", result = tmp_result);
-
-	tmp_result = fiji_enable_vrhot_gpio_interrupt(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable VR hot GPIO interrupt!", result = tmp_result);
-
-	tmp_result = tonga_notify_smc_display_change(hwmgr, false);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to notify no display!", result = tmp_result);
-
-	tmp_result = fiji_enable_sclk_control(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable SCLK control!", result = tmp_result);
-
-	tmp_result = fiji_enable_ulv(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable ULV!", result = tmp_result);
-
-	tmp_result = fiji_enable_deep_sleep_master_switch(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable deep sleep master switch!", result = tmp_result);
-
-	tmp_result = fiji_start_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to start DPM!", result = tmp_result);
-
-	tmp_result = fiji_enable_smc_cac(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable SMC CAC!", result = tmp_result);
-
-	tmp_result = fiji_enable_power_containment(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable power containment!", result = tmp_result);
-
-	tmp_result = fiji_power_control_set_level(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to power control set level!", result = tmp_result);
-
-	tmp_result = fiji_enable_thermal_auto_throttle(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable thermal auto throttle!", result = tmp_result);
-
-	return result;
-}
-
-static int fiji_disable_dpm_tasks(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = (fiji_is_dpm_running(hwmgr)) ? 0 : -1;
-	PP_ASSERT_WITH_CODE(tmp_result == 0,
-			"DPM is not running right now, no need to disable DPM!",
-			return 0);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ThermalController))
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				GENERAL_PWRMGT, THERMAL_PROTECTION_DIS, 1);
-
-	tmp_result = fiji_disable_power_containment(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable power containment!", result = tmp_result);
-
-	tmp_result = fiji_disable_smc_cac(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable SMC CAC!", result = tmp_result);
-
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_SPLL_SPREAD_SPECTRUM, SSEN, 0);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			GENERAL_PWRMGT, DYN_SPREAD_SPECTRUM_EN, 0);
-
-	tmp_result = fiji_disable_thermal_auto_throttle(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable thermal auto throttle!", result = tmp_result);
-
-	tmp_result = fiji_stop_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to stop DPM!", result = tmp_result);
-
-	tmp_result = fiji_disable_deep_sleep_master_switch(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable deep sleep master switch!", result = tmp_result);
-
-	tmp_result = fiji_disable_ulv(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable ULV!", result = tmp_result);
-
-	tmp_result = fiji_clear_voting_clients(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to clear voting clients!", result = tmp_result);
-
-	tmp_result = fiji_reset_to_default(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to reset to default!", result = tmp_result);
-
-	tmp_result = fiji_force_switch_to_arbf0(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to force to switch arbf0!", result = tmp_result);
-
-	return result;
-}
-
-static int fiji_force_dpm_highest(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t level, tmp;
-
-	if (!data->sclk_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.sclk_dpm_enable_mask) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.sclk_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++;
-			if (level)
-				smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-						PPSMC_MSG_SCLKDPM_SetEnabledMask,
-						(1 << level));
-		}
-	}
-
-	if (!data->mclk_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.mclk_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++;
-			if (level)
-				smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-						PPSMC_MSG_MCLKDPM_SetEnabledMask,
-						(1 << level));
-		}
-	}
-
-	if (!data->pcie_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.pcie_dpm_enable_mask) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.pcie_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++;
-			if (level)
-				smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-						PPSMC_MSG_PCIeDPM_ForceLevel,
-						(1 << level));
-		}
-	}
-	return 0;
-}
-
-static int fiji_upload_dpmlevel_enable_mask(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	phm_apply_dal_min_voltage_request(hwmgr);
-
-	if (!data->sclk_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.sclk_dpm_enable_mask)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_SCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.sclk_dpm_enable_mask);
-	}
-	return 0;
-}
-
-static int fiji_unforce_dpm_levels(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (!fiji_is_dpm_running(hwmgr))
-		return -EINVAL;
-
-	if (!data->pcie_dpm_key_disabled) {
-		smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_PCIeDPM_UnForceLevel);
-	}
-
-	return fiji_upload_dpmlevel_enable_mask(hwmgr);
-}
-
-static uint32_t fiji_get_lowest_enabled_level(
-		struct pp_hwmgr *hwmgr, uint32_t mask)
-{
-	uint32_t level = 0;
-
-	while(0 == (mask & (1 << level)))
-		level++;
-
-	return level;
-}
-
-static int fiji_force_dpm_lowest(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data =
-			(struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t level;
-
-	if (!data->sclk_dpm_key_disabled)
-		if (data->dpm_level_enable_mask.sclk_dpm_enable_mask) {
-			level = fiji_get_lowest_enabled_level(hwmgr,
-							      data->dpm_level_enable_mask.sclk_dpm_enable_mask);
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-							    PPSMC_MSG_SCLKDPM_SetEnabledMask,
-							    (1 << level));
-
-	}
-
-	if (!data->mclk_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask) {
-			level = fiji_get_lowest_enabled_level(hwmgr,
-							      data->dpm_level_enable_mask.mclk_dpm_enable_mask);
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-							    PPSMC_MSG_MCLKDPM_SetEnabledMask,
-							    (1 << level));
-		}
-	}
-
-	if (!data->pcie_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.pcie_dpm_enable_mask) {
-			level = fiji_get_lowest_enabled_level(hwmgr,
-							      data->dpm_level_enable_mask.pcie_dpm_enable_mask);
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-							    PPSMC_MSG_PCIeDPM_ForceLevel,
-							    (1 << level));
-		}
-	}
-
-	return 0;
-
-}
-static int fiji_dpm_force_dpm_level(struct pp_hwmgr *hwmgr,
-				enum amd_dpm_forced_level level)
-{
-	int ret = 0;
-
-	switch (level) {
-	case AMD_DPM_FORCED_LEVEL_HIGH:
-		ret = fiji_force_dpm_highest(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	case AMD_DPM_FORCED_LEVEL_LOW:
-		ret = fiji_force_dpm_lowest(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	case AMD_DPM_FORCED_LEVEL_AUTO:
-		ret = fiji_unforce_dpm_levels(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	default:
-		break;
-	}
-
-	hwmgr->dpm_level = level;
-
-	return ret;
-}
-
-static int fiji_get_power_state_size(struct pp_hwmgr *hwmgr)
-{
-	return sizeof(struct fiji_power_state);
-}
-
-static int fiji_get_pp_table_entry_callback_func(struct pp_hwmgr *hwmgr,
-		void *state, struct pp_power_state *power_state,
-		void *pp_table, uint32_t classification_flag)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_power_state  *fiji_power_state =
-			(struct fiji_power_state *)(&(power_state->hardware));
-	struct fiji_performance_level *performance_level;
-	ATOM_Tonga_State *state_entry = (ATOM_Tonga_State *)state;
-	ATOM_Tonga_POWERPLAYTABLE *powerplay_table =
-			(ATOM_Tonga_POWERPLAYTABLE *)pp_table;
-	ATOM_Tonga_SCLK_Dependency_Table *sclk_dep_table =
-			(ATOM_Tonga_SCLK_Dependency_Table *)
-			(((unsigned long)powerplay_table) +
-				le16_to_cpu(powerplay_table->usSclkDependencyTableOffset));
-	ATOM_Tonga_MCLK_Dependency_Table *mclk_dep_table =
-			(ATOM_Tonga_MCLK_Dependency_Table *)
-			(((unsigned long)powerplay_table) +
-				le16_to_cpu(powerplay_table->usMclkDependencyTableOffset));
-
-	/* The following fields are not initialized here: id orderedList allStatesList */
-	power_state->classification.ui_label =
-			(le16_to_cpu(state_entry->usClassification) &
-			ATOM_PPLIB_CLASSIFICATION_UI_MASK) >>
-			ATOM_PPLIB_CLASSIFICATION_UI_SHIFT;
-	power_state->classification.flags = classification_flag;
-	/* NOTE: There is a classification2 flag in BIOS that is not being used right now */
-
-	power_state->classification.temporary_state = false;
-	power_state->classification.to_be_deleted = false;
-
-	power_state->validation.disallowOnDC =
-			(0 != (le32_to_cpu(state_entry->ulCapsAndSettings) &
-					ATOM_Tonga_DISALLOW_ON_DC));
-
-	power_state->pcie.lanes = 0;
-
-	power_state->display.disableFrameModulation = false;
-	power_state->display.limitRefreshrate = false;
-	power_state->display.enableVariBright =
-			(0 != (le32_to_cpu(state_entry->ulCapsAndSettings) &
-					ATOM_Tonga_ENABLE_VARIBRIGHT));
-
-	power_state->validation.supportedPowerLevels = 0;
-	power_state->uvd_clocks.VCLK = 0;
-	power_state->uvd_clocks.DCLK = 0;
-	power_state->temperatures.min = 0;
-	power_state->temperatures.max = 0;
-
-	performance_level = &(fiji_power_state->performance_levels
-			[fiji_power_state->performance_level_count++]);
-
-	PP_ASSERT_WITH_CODE(
-			(fiji_power_state->performance_level_count < SMU73_MAX_LEVELS_GRAPHICS),
-			"Performance levels exceeds SMC limit!",
-			return -1);
-
-	PP_ASSERT_WITH_CODE(
-			(fiji_power_state->performance_level_count <=
-					hwmgr->platform_descriptor.hardwareActivityPerformanceLevels),
-			"Performance levels exceeds Driver limit!",
-			return -1);
-
-	/* Performance levels are arranged from low to high. */
-	performance_level->memory_clock = mclk_dep_table->entries
-			[state_entry->ucMemoryClockIndexLow].ulMclk;
-	performance_level->engine_clock = sclk_dep_table->entries
-			[state_entry->ucEngineClockIndexLow].ulSclk;
-	performance_level->pcie_gen = get_pcie_gen_support(data->pcie_gen_cap,
-			state_entry->ucPCIEGenLow);
-	performance_level->pcie_lane = get_pcie_lane_support(data->pcie_lane_cap,
-			state_entry->ucPCIELaneHigh);
-
-	performance_level = &(fiji_power_state->performance_levels
-			[fiji_power_state->performance_level_count++]);
-	performance_level->memory_clock = mclk_dep_table->entries
-			[state_entry->ucMemoryClockIndexHigh].ulMclk;
-	performance_level->engine_clock = sclk_dep_table->entries
-			[state_entry->ucEngineClockIndexHigh].ulSclk;
-	performance_level->pcie_gen = get_pcie_gen_support(data->pcie_gen_cap,
-			state_entry->ucPCIEGenHigh);
-	performance_level->pcie_lane = get_pcie_lane_support(data->pcie_lane_cap,
-			state_entry->ucPCIELaneHigh);
-
-	return 0;
-}
-
-static int fiji_get_pp_table_entry(struct pp_hwmgr *hwmgr,
-		unsigned long entry_index, struct pp_power_state *state)
-{
-	int result;
-	struct fiji_power_state *ps;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table =
-			table_info->vdd_dep_on_mclk;
-
-	state->hardware.magic = PHM_VIslands_Magic;
-
-	ps = (struct fiji_power_state *)(&state->hardware);
-
-	result = get_powerplay_table_entry_v1_0(hwmgr, entry_index, state,
-			fiji_get_pp_table_entry_callback_func);
-
-	/* This is the earliest time we have all the dependency table and the VBIOS boot state
-	 * as PP_Tables_GetPowerPlayTableEntry retrieves the VBIOS boot state
-	 * if there is only one VDDCI/MCLK level, check if it's the same as VBIOS boot state
-	 */
-	if (dep_mclk_table != NULL && dep_mclk_table->count == 1) {
-		if (dep_mclk_table->entries[0].clk !=
-				data->vbios_boot_state.mclk_bootup_value)
-			printk(KERN_ERR "Single MCLK entry VDDCI/MCLK dependency table "
-					"does not match VBIOS boot MCLK level");
-		if (dep_mclk_table->entries[0].vddci !=
-				data->vbios_boot_state.vddci_bootup_value)
-			printk(KERN_ERR "Single VDDCI entry VDDCI/MCLK dependency table "
-					"does not match VBIOS boot VDDCI level");
-	}
-
-	/* set DC compatible flag if this state supports DC */
-	if (!state->validation.disallowOnDC)
-		ps->dc_compatible = true;
-
-	if (state->classification.flags & PP_StateClassificationFlag_ACPI)
-		data->acpi_pcie_gen = ps->performance_levels[0].pcie_gen;
-
-	ps->uvd_clks.vclk = state->uvd_clocks.VCLK;
-	ps->uvd_clks.dclk = state->uvd_clocks.DCLK;
-
-	if (!result) {
-		uint32_t i;
-
-		switch (state->classification.ui_label) {
-		case PP_StateUILabel_Performance:
-			data->use_pcie_performance_levels = true;
-
-			for (i = 0; i < ps->performance_level_count; i++) {
-				if (data->pcie_gen_performance.max <
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_performance.max =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_gen_performance.min >
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_performance.min =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_lane_performance.max <
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_performance.max =
-							ps->performance_levels[i].pcie_lane;
-
-				if (data->pcie_lane_performance.min >
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_performance.min =
-							ps->performance_levels[i].pcie_lane;
-			}
-			break;
-		case PP_StateUILabel_Battery:
-			data->use_pcie_power_saving_levels = true;
-
-			for (i = 0; i < ps->performance_level_count; i++) {
-				if (data->pcie_gen_power_saving.max <
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_power_saving.max =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_gen_power_saving.min >
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_power_saving.min =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_lane_power_saving.max <
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_power_saving.max =
-							ps->performance_levels[i].pcie_lane;
-
-				if (data->pcie_lane_power_saving.min >
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_power_saving.min =
-							ps->performance_levels[i].pcie_lane;
-			}
-			break;
-		default:
-			break;
-		}
-	}
-	return 0;
-}
-
-static int fiji_apply_state_adjust_rules(struct pp_hwmgr *hwmgr,
-				struct pp_power_state  *request_ps,
-			const struct pp_power_state *current_ps)
-{
-	struct fiji_power_state *fiji_ps =
-				cast_phw_fiji_power_state(&request_ps->hardware);
-	uint32_t sclk;
-	uint32_t mclk;
-	struct PP_Clocks minimum_clocks = {0};
-	bool disable_mclk_switching;
-	bool disable_mclk_switching_for_frame_lock;
-	struct cgs_display_info info = {0};
-	const struct phm_clock_and_voltage_limits *max_limits;
-	uint32_t i;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	int32_t count;
-	int32_t stable_pstate_sclk = 0, stable_pstate_mclk = 0;
-
-	data->battery_state = (PP_StateUILabel_Battery ==
-			request_ps->classification.ui_label);
-
-	PP_ASSERT_WITH_CODE(fiji_ps->performance_level_count == 2,
-				 "VI should always have 2 performance levels",);
-
-	max_limits = (PP_PowerSource_AC == hwmgr->power_source) ?
-			&(hwmgr->dyn_state.max_clock_voltage_on_ac) :
-			&(hwmgr->dyn_state.max_clock_voltage_on_dc);
-
-	/* Cap clock DPM tables at DC MAX if it is in DC. */
-	if (PP_PowerSource_DC == hwmgr->power_source) {
-		for (i = 0; i < fiji_ps->performance_level_count; i++) {
-			if (fiji_ps->performance_levels[i].memory_clock > max_limits->mclk)
-				fiji_ps->performance_levels[i].memory_clock = max_limits->mclk;
-			if (fiji_ps->performance_levels[i].engine_clock > max_limits->sclk)
-				fiji_ps->performance_levels[i].engine_clock = max_limits->sclk;
-		}
-	}
-
-	fiji_ps->vce_clks.evclk = hwmgr->vce_arbiter.evclk;
-	fiji_ps->vce_clks.ecclk = hwmgr->vce_arbiter.ecclk;
-
-	fiji_ps->acp_clk = hwmgr->acp_arbiter.acpclk;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	/*TO DO result = PHM_CheckVBlankTime(hwmgr, &vblankTooShort);*/
-
-	/* TO DO GetMinClockSettings(hwmgr->pPECI, &minimum_clocks); */
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_StablePState)) {
-		max_limits = &(hwmgr->dyn_state.max_clock_voltage_on_ac);
-		stable_pstate_sclk = (max_limits->sclk * 75) / 100;
-
-		for (count = table_info->vdd_dep_on_sclk->count - 1;
-				count >= 0; count--) {
-			if (stable_pstate_sclk >=
-					table_info->vdd_dep_on_sclk->entries[count].clk) {
-				stable_pstate_sclk =
-						table_info->vdd_dep_on_sclk->entries[count].clk;
-				break;
-			}
-		}
-
-		if (count < 0)
-			stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[0].clk;
-
-		stable_pstate_mclk = max_limits->mclk;
-
-		minimum_clocks.engineClock = stable_pstate_sclk;
-		minimum_clocks.memoryClock = stable_pstate_mclk;
-	}
-
-	if (minimum_clocks.engineClock < hwmgr->gfx_arbiter.sclk)
-		minimum_clocks.engineClock = hwmgr->gfx_arbiter.sclk;
-
-	if (minimum_clocks.memoryClock < hwmgr->gfx_arbiter.mclk)
-		minimum_clocks.memoryClock = hwmgr->gfx_arbiter.mclk;
-
-	fiji_ps->sclk_threshold = hwmgr->gfx_arbiter.sclk_threshold;
-
-	if (0 != hwmgr->gfx_arbiter.sclk_over_drive) {
-		PP_ASSERT_WITH_CODE((hwmgr->gfx_arbiter.sclk_over_drive <=
-				hwmgr->platform_descriptor.overdriveLimit.engineClock),
-				"Overdrive sclk exceeds limit",
-				hwmgr->gfx_arbiter.sclk_over_drive =
-						hwmgr->platform_descriptor.overdriveLimit.engineClock);
-
-		if (hwmgr->gfx_arbiter.sclk_over_drive >= hwmgr->gfx_arbiter.sclk)
-			fiji_ps->performance_levels[1].engine_clock =
-					hwmgr->gfx_arbiter.sclk_over_drive;
-	}
-
-	if (0 != hwmgr->gfx_arbiter.mclk_over_drive) {
-		PP_ASSERT_WITH_CODE((hwmgr->gfx_arbiter.mclk_over_drive <=
-				hwmgr->platform_descriptor.overdriveLimit.memoryClock),
-				"Overdrive mclk exceeds limit",
-				hwmgr->gfx_arbiter.mclk_over_drive =
-						hwmgr->platform_descriptor.overdriveLimit.memoryClock);
-
-		if (hwmgr->gfx_arbiter.mclk_over_drive >= hwmgr->gfx_arbiter.mclk)
-			fiji_ps->performance_levels[1].memory_clock =
-					hwmgr->gfx_arbiter.mclk_over_drive;
-	}
-
-	disable_mclk_switching_for_frame_lock = phm_cap_enabled(
-				    hwmgr->platform_descriptor.platformCaps,
-				    PHM_PlatformCaps_DisableMclkSwitchingForFrameLock);
-
-	disable_mclk_switching = (1 < info.display_count) ||
-				    disable_mclk_switching_for_frame_lock;
-
-	sclk = fiji_ps->performance_levels[0].engine_clock;
-	mclk = fiji_ps->performance_levels[0].memory_clock;
-
-	if (disable_mclk_switching)
-		mclk = fiji_ps->performance_levels
-		[fiji_ps->performance_level_count - 1].memory_clock;
-
-	if (sclk < minimum_clocks.engineClock)
-		sclk = (minimum_clocks.engineClock > max_limits->sclk) ?
-				max_limits->sclk : minimum_clocks.engineClock;
-
-	if (mclk < minimum_clocks.memoryClock)
-		mclk = (minimum_clocks.memoryClock > max_limits->mclk) ?
-				max_limits->mclk : minimum_clocks.memoryClock;
-
-	fiji_ps->performance_levels[0].engine_clock = sclk;
-	fiji_ps->performance_levels[0].memory_clock = mclk;
-
-	fiji_ps->performance_levels[1].engine_clock =
-		(fiji_ps->performance_levels[1].engine_clock >=
-				fiji_ps->performance_levels[0].engine_clock) ?
-						fiji_ps->performance_levels[1].engine_clock :
-						fiji_ps->performance_levels[0].engine_clock;
-
-	if (disable_mclk_switching) {
-		if (mclk < fiji_ps->performance_levels[1].memory_clock)
-			mclk = fiji_ps->performance_levels[1].memory_clock;
-
-		fiji_ps->performance_levels[0].memory_clock = mclk;
-		fiji_ps->performance_levels[1].memory_clock = mclk;
-	} else {
-		if (fiji_ps->performance_levels[1].memory_clock <
-				fiji_ps->performance_levels[0].memory_clock)
-			fiji_ps->performance_levels[1].memory_clock =
-					fiji_ps->performance_levels[0].memory_clock;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_StablePState)) {
-		for (i = 0; i < fiji_ps->performance_level_count; i++) {
-			fiji_ps->performance_levels[i].engine_clock = stable_pstate_sclk;
-			fiji_ps->performance_levels[i].memory_clock = stable_pstate_mclk;
-			fiji_ps->performance_levels[i].pcie_gen = data->pcie_gen_performance.max;
-			fiji_ps->performance_levels[i].pcie_lane = data->pcie_gen_performance.max;
-		}
-	}
-
-	return 0;
-}
-
-static int fiji_find_dpm_states_clocks_in_dpm_table(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	const struct fiji_power_state *fiji_ps =
-			cast_const_phw_fiji_power_state(states->pnew_state);
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	uint32_t sclk = fiji_ps->performance_levels
-			[fiji_ps->performance_level_count - 1].engine_clock;
-	struct fiji_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	uint32_t mclk = fiji_ps->performance_levels
-			[fiji_ps->performance_level_count - 1].memory_clock;
-	uint32_t i;
-	struct cgs_display_info info = {0};
-
-	data->need_update_smu7_dpm_table = 0;
-
-	for (i = 0; i < sclk_table->count; i++) {
-		if (sclk == sclk_table->dpm_levels[i].value)
-			break;
-	}
-
-	if (i >= sclk_table->count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_SCLK;
-	else {
-		if(data->display_timing.min_clock_in_sr !=
-			hwmgr->display_config.min_core_set_clock_in_sr)
-			data->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_SCLK;
-	}
-
-	for (i = 0; i < mclk_table->count; i++) {
-		if (mclk == mclk_table->dpm_levels[i].value)
-			break;
-	}
-
-	if (i >= mclk_table->count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_MCLK;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (data->display_timing.num_existing_displays != info.display_count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_MCLK;
-
-	return 0;
-}
-
-static uint16_t fiji_get_maximum_link_speed(struct pp_hwmgr *hwmgr,
-		const struct fiji_power_state *fiji_ps)
-{
-	uint32_t i;
-	uint32_t sclk, max_sclk = 0;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_dpm_table *dpm_table = &data->dpm_table;
-
-	for (i = 0; i < fiji_ps->performance_level_count; i++) {
-		sclk = fiji_ps->performance_levels[i].engine_clock;
-		if (max_sclk < sclk)
-			max_sclk = sclk;
-	}
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		if (dpm_table->sclk_table.dpm_levels[i].value == max_sclk)
-			return (uint16_t) ((i >= dpm_table->pcie_speed_table.count) ?
-					dpm_table->pcie_speed_table.dpm_levels
-					[dpm_table->pcie_speed_table.count - 1].value :
-					dpm_table->pcie_speed_table.dpm_levels[i].value);
-	}
-
-	return 0;
-}
-
-static int fiji_request_link_speed_change_before_state_change(
-		struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	const struct fiji_power_state *fiji_nps =
-			cast_const_phw_fiji_power_state(states->pnew_state);
-	const struct fiji_power_state *fiji_cps =
-			cast_const_phw_fiji_power_state(states->pcurrent_state);
-
-	uint16_t target_link_speed = fiji_get_maximum_link_speed(hwmgr, fiji_nps);
-	uint16_t current_link_speed;
-
-	if (data->force_pcie_gen == PP_PCIEGenInvalid)
-		current_link_speed = fiji_get_maximum_link_speed(hwmgr, fiji_cps);
-	else
-		current_link_speed = data->force_pcie_gen;
-
-	data->force_pcie_gen = PP_PCIEGenInvalid;
-	data->pspp_notify_required = false;
-	if (target_link_speed > current_link_speed) {
-		switch(target_link_speed) {
-		case PP_PCIEGen3:
-			if (0 == acpi_pcie_perf_request(hwmgr->device, PCIE_PERF_REQ_GEN3, false))
-				break;
-			data->force_pcie_gen = PP_PCIEGen2;
-			if (current_link_speed == PP_PCIEGen2)
-				break;
-		case PP_PCIEGen2:
-			if (0 == acpi_pcie_perf_request(hwmgr->device, PCIE_PERF_REQ_GEN2, false))
-				break;
-		default:
-			data->force_pcie_gen = fiji_get_current_pcie_speed(hwmgr);
-			break;
-		}
-	} else {
-		if (target_link_speed < current_link_speed)
-			data->pspp_notify_required = true;
-	}
-
-	return 0;
-}
-
-static int fiji_freeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if ((0 == data->sclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-			(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK))) {
-		PP_ASSERT_WITH_CODE(fiji_is_dpm_running(hwmgr),
-				    "Trying to freeze SCLK DPM when DPM is disabled",
-				    );
-		PP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_SCLKDPM_FreezeLevel),
-				"Failed to freeze SCLK DPM during FreezeSclkMclkDPM Function!",
-				return -1);
-	}
-
-	if ((0 == data->mclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		 DPMTABLE_OD_UPDATE_MCLK)) {
-		PP_ASSERT_WITH_CODE(fiji_is_dpm_running(hwmgr),
-				    "Trying to freeze MCLK DPM when DPM is disabled",
-				    );
-		PP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_MCLKDPM_FreezeLevel),
-				"Failed to freeze MCLK DPM during FreezeSclkMclkDPM Function!",
-				return -1);
-	}
-
-	return 0;
-}
-
-static int fiji_populate_and_upload_sclk_mclk_dpm_levels(
-		struct pp_hwmgr *hwmgr, const void *input)
-{
-	int result = 0;
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	const struct fiji_power_state *fiji_ps =
-			cast_const_phw_fiji_power_state(states->pnew_state);
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t sclk = fiji_ps->performance_levels
-			[fiji_ps->performance_level_count - 1].engine_clock;
-	uint32_t mclk = fiji_ps->performance_levels
-			[fiji_ps->performance_level_count - 1].memory_clock;
-	struct fiji_dpm_table *dpm_table = &data->dpm_table;
-
-	struct fiji_dpm_table *golden_dpm_table = &data->golden_dpm_table;
-	uint32_t dpm_count, clock_percent;
-	uint32_t i;
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_SCLK) {
-		dpm_table->sclk_table.dpm_levels
-		[dpm_table->sclk_table.count - 1].value = sclk;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_OD6PlusinACSupport) ||
-			phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_OD6PlusinDCSupport)) {
-		/* Need to do calculation based on the golden DPM table
-		 * as the Heatmap GPU Clock axis is also based on the default values
-		 */
-			PP_ASSERT_WITH_CODE(
-				(golden_dpm_table->sclk_table.dpm_levels
-						[golden_dpm_table->sclk_table.count - 1].value != 0),
-				"Divide by 0!",
-				return -1);
-			dpm_count = dpm_table->sclk_table.count < 2 ?
-					0 : dpm_table->sclk_table.count - 2;
-			for (i = dpm_count; i > 1; i--) {
-				if (sclk > golden_dpm_table->sclk_table.dpm_levels
-						[golden_dpm_table->sclk_table.count-1].value) {
-					clock_percent =
-						((sclk - golden_dpm_table->sclk_table.dpm_levels
-							[golden_dpm_table->sclk_table.count-1].value) * 100) /
-						golden_dpm_table->sclk_table.dpm_levels
-							[golden_dpm_table->sclk_table.count-1].value;
-
-					dpm_table->sclk_table.dpm_levels[i].value =
-							golden_dpm_table->sclk_table.dpm_levels[i].value +
-							(golden_dpm_table->sclk_table.dpm_levels[i].value *
-								clock_percent)/100;
-
-				} else if (golden_dpm_table->sclk_table.dpm_levels
-						[dpm_table->sclk_table.count-1].value > sclk) {
-					clock_percent =
-						((golden_dpm_table->sclk_table.dpm_levels
-						[golden_dpm_table->sclk_table.count - 1].value - sclk) *
-								100) /
-						golden_dpm_table->sclk_table.dpm_levels
-							[golden_dpm_table->sclk_table.count-1].value;
-
-					dpm_table->sclk_table.dpm_levels[i].value =
-							golden_dpm_table->sclk_table.dpm_levels[i].value -
-							(golden_dpm_table->sclk_table.dpm_levels[i].value *
-									clock_percent) / 100;
-				} else
-					dpm_table->sclk_table.dpm_levels[i].value =
-							golden_dpm_table->sclk_table.dpm_levels[i].value;
-			}
-		}
-	}
-
-	if (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK) {
-		dpm_table->mclk_table.dpm_levels
-			[dpm_table->mclk_table.count - 1].value = mclk;
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_OD6PlusinACSupport) ||
-			phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_OD6PlusinDCSupport)) {
-
-			PP_ASSERT_WITH_CODE(
-					(golden_dpm_table->mclk_table.dpm_levels
-						[golden_dpm_table->mclk_table.count-1].value != 0),
-					"Divide by 0!",
-					return -1);
-			dpm_count = dpm_table->mclk_table.count < 2 ?
-					0 : dpm_table->mclk_table.count - 2;
-			for (i = dpm_count; i > 1; i--) {
-				if (mclk > golden_dpm_table->mclk_table.dpm_levels
-						[golden_dpm_table->mclk_table.count-1].value) {
-					clock_percent = ((mclk -
-							golden_dpm_table->mclk_table.dpm_levels
-							[golden_dpm_table->mclk_table.count-1].value) * 100) /
-							golden_dpm_table->mclk_table.dpm_levels
-							[golden_dpm_table->mclk_table.count-1].value;
-
-					dpm_table->mclk_table.dpm_levels[i].value =
-							golden_dpm_table->mclk_table.dpm_levels[i].value +
-							(golden_dpm_table->mclk_table.dpm_levels[i].value *
-									clock_percent) / 100;
-
-				} else if (golden_dpm_table->mclk_table.dpm_levels
-						[dpm_table->mclk_table.count-1].value > mclk) {
-					clock_percent = ((golden_dpm_table->mclk_table.dpm_levels
-							[golden_dpm_table->mclk_table.count-1].value - mclk) * 100) /
-									golden_dpm_table->mclk_table.dpm_levels
-									[golden_dpm_table->mclk_table.count-1].value;
-
-					dpm_table->mclk_table.dpm_levels[i].value =
-							golden_dpm_table->mclk_table.dpm_levels[i].value -
-							(golden_dpm_table->mclk_table.dpm_levels[i].value *
-									clock_percent) / 100;
-				} else
-					dpm_table->mclk_table.dpm_levels[i].value =
-							golden_dpm_table->mclk_table.dpm_levels[i].value;
-			}
-		}
-	}
-
-	if (data->need_update_smu7_dpm_table &
-			(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK)) {
-		result = fiji_populate_all_graphic_levels(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to populate SCLK during PopulateNewDPMClocksStates Function!",
-				return result);
-	}
-
-	if (data->need_update_smu7_dpm_table &
-			(DPMTABLE_OD_UPDATE_MCLK + DPMTABLE_UPDATE_MCLK)) {
-		/*populate MCLK dpm table to SMU7 */
-		result = fiji_populate_all_memory_levels(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to populate MCLK during PopulateNewDPMClocksStates Function!",
-				return result);
-	}
-
-	return result;
-}
-
-static int fiji_trim_single_dpm_states(struct pp_hwmgr *hwmgr,
-			  struct fiji_single_dpm_table * dpm_table,
-			     uint32_t low_limit, uint32_t high_limit)
-{
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->count; i++) {
-		if ((dpm_table->dpm_levels[i].value < low_limit) ||
-		    (dpm_table->dpm_levels[i].value > high_limit))
-			dpm_table->dpm_levels[i].enabled = false;
-		else
-			dpm_table->dpm_levels[i].enabled = true;
-	}
-	return 0;
-}
-
-static int fiji_trim_dpm_states(struct pp_hwmgr *hwmgr,
-		const struct fiji_power_state *fiji_ps)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t high_limit_count;
-
-	PP_ASSERT_WITH_CODE((fiji_ps->performance_level_count >= 1),
-			"power state did not have any performance level",
-			return -1);
-
-	high_limit_count = (1 == fiji_ps->performance_level_count) ? 0 : 1;
-
-	fiji_trim_single_dpm_states(hwmgr,
-			&(data->dpm_table.sclk_table),
-			fiji_ps->performance_levels[0].engine_clock,
-			fiji_ps->performance_levels[high_limit_count].engine_clock);
-
-	fiji_trim_single_dpm_states(hwmgr,
-			&(data->dpm_table.mclk_table),
-			fiji_ps->performance_levels[0].memory_clock,
-			fiji_ps->performance_levels[high_limit_count].memory_clock);
-
-	return 0;
-}
-
-static int fiji_generate_dpm_level_enable_mask(
-		struct pp_hwmgr *hwmgr, const void *input)
-{
-	int result;
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	const struct fiji_power_state *fiji_ps =
-			cast_const_phw_fiji_power_state(states->pnew_state);
-
-	result = fiji_trim_dpm_states(hwmgr, fiji_ps);
-	if (result)
-		return result;
-
-	data->dpm_level_enable_mask.sclk_dpm_enable_mask =
-			fiji_get_dpm_level_enable_mask_value(&data->dpm_table.sclk_table);
-	data->dpm_level_enable_mask.mclk_dpm_enable_mask =
-			fiji_get_dpm_level_enable_mask_value(&data->dpm_table.mclk_table);
-	data->last_mclk_dpm_enable_mask =
-			data->dpm_level_enable_mask.mclk_dpm_enable_mask;
-
-	if (data->uvd_enabled) {
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask & 1)
-			data->dpm_level_enable_mask.mclk_dpm_enable_mask &= 0xFFFFFFFE;
-	}
-
-	data->dpm_level_enable_mask.pcie_dpm_enable_mask =
-			fiji_get_dpm_level_enable_mask_value(&data->dpm_table.pcie_speed_table);
-
-	return 0;
-}
-
-static int fiji_enable_disable_uvd_dpm(struct pp_hwmgr *hwmgr, bool enable)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, enable ?
-				  (PPSMC_Msg)PPSMC_MSG_UVDDPM_Enable :
-				  (PPSMC_Msg)PPSMC_MSG_UVDDPM_Disable);
-}
-
-int fiji_enable_disable_vce_dpm(struct pp_hwmgr *hwmgr, bool enable)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, enable?
-			PPSMC_MSG_VCEDPM_Enable :
-			PPSMC_MSG_VCEDPM_Disable);
-}
-
-static int fiji_enable_disable_samu_dpm(struct pp_hwmgr *hwmgr, bool enable)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, enable?
-			PPSMC_MSG_SAMUDPM_Enable :
-			PPSMC_MSG_SAMUDPM_Disable);
-}
-
-static int fiji_enable_disable_acp_dpm(struct pp_hwmgr *hwmgr, bool enable)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, enable?
-			PPSMC_MSG_ACPDPM_Enable :
-			PPSMC_MSG_ACPDPM_Disable);
-}
-
-int fiji_update_uvd_dpm(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t mm_boot_level_offset, mm_boot_level_value;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (!bgate) {
-		data->smc_state_table.UvdBootLevel = 0;
-		if (table_info->mm_dep_table->count > 0)
-			data->smc_state_table.UvdBootLevel =
-					(uint8_t) (table_info->mm_dep_table->count - 1);
-		mm_boot_level_offset = data->dpm_table_start +
-				offsetof(SMU73_Discrete_DpmTable, UvdBootLevel);
-		mm_boot_level_offset /= 4;
-		mm_boot_level_offset *= 4;
-		mm_boot_level_value = cgs_read_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset);
-		mm_boot_level_value &= 0x00FFFFFF;
-		mm_boot_level_value |= data->smc_state_table.UvdBootLevel << 24;
-		cgs_write_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset, mm_boot_level_value);
-
-		if (!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_UVDDPM) ||
-			phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_StablePState))
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_UVDDPM_SetEnabledMask,
-					(uint32_t)(1 << data->smc_state_table.UvdBootLevel));
-	}
-
-	return fiji_enable_disable_uvd_dpm(hwmgr, !bgate);
-}
-
-int fiji_update_vce_dpm(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	const struct fiji_power_state *fiji_nps =
-			cast_const_phw_fiji_power_state(states->pnew_state);
-	const struct fiji_power_state *fiji_cps =
-			cast_const_phw_fiji_power_state(states->pcurrent_state);
-
-	uint32_t mm_boot_level_offset, mm_boot_level_value;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (fiji_nps->vce_clks.evclk >0 &&
-	(fiji_cps == NULL || fiji_cps->vce_clks.evclk == 0)) {
-		data->smc_state_table.VceBootLevel =
-				(uint8_t) (table_info->mm_dep_table->count - 1);
-
-		mm_boot_level_offset = data->dpm_table_start +
-				offsetof(SMU73_Discrete_DpmTable, VceBootLevel);
-		mm_boot_level_offset /= 4;
-		mm_boot_level_offset *= 4;
-		mm_boot_level_value = cgs_read_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset);
-		mm_boot_level_value &= 0xFF00FFFF;
-		mm_boot_level_value |= data->smc_state_table.VceBootLevel << 16;
-		cgs_write_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset, mm_boot_level_value);
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_StablePState)) {
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_VCEDPM_SetEnabledMask,
-					(uint32_t)1 << data->smc_state_table.VceBootLevel);
-
-			fiji_enable_disable_vce_dpm(hwmgr, true);
-		} else if (fiji_nps->vce_clks.evclk == 0 &&
-				fiji_cps != NULL &&
-				fiji_cps->vce_clks.evclk > 0)
-			fiji_enable_disable_vce_dpm(hwmgr, false);
-	}
-
-	return 0;
-}
-
-int fiji_update_samu_dpm(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t mm_boot_level_offset, mm_boot_level_value;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (!bgate) {
-		data->smc_state_table.SamuBootLevel =
-				(uint8_t) (table_info->mm_dep_table->count - 1);
-		mm_boot_level_offset = data->dpm_table_start +
-				offsetof(SMU73_Discrete_DpmTable, SamuBootLevel);
-		mm_boot_level_offset /= 4;
-		mm_boot_level_offset *= 4;
-		mm_boot_level_value = cgs_read_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset);
-		mm_boot_level_value &= 0xFFFFFF00;
-		mm_boot_level_value |= data->smc_state_table.SamuBootLevel << 0;
-		cgs_write_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset, mm_boot_level_value);
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_StablePState))
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_SAMUDPM_SetEnabledMask,
-					(uint32_t)(1 << data->smc_state_table.SamuBootLevel));
-	}
-
-	return fiji_enable_disable_samu_dpm(hwmgr, !bgate);
-}
-
-int fiji_update_acp_dpm(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t mm_boot_level_offset, mm_boot_level_value;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (!bgate) {
-		data->smc_state_table.AcpBootLevel =
-				(uint8_t) (table_info->mm_dep_table->count - 1);
-		mm_boot_level_offset = data->dpm_table_start +
-				offsetof(SMU73_Discrete_DpmTable, AcpBootLevel);
-		mm_boot_level_offset /= 4;
-		mm_boot_level_offset *= 4;
-		mm_boot_level_value = cgs_read_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset);
-		mm_boot_level_value &= 0xFFFF00FF;
-		mm_boot_level_value |= data->smc_state_table.AcpBootLevel << 8;
-		cgs_write_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset, mm_boot_level_value);
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_StablePState))
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-						PPSMC_MSG_ACPDPM_SetEnabledMask,
-						(uint32_t)(1 << data->smc_state_table.AcpBootLevel));
-	}
-
-	return fiji_enable_disable_acp_dpm(hwmgr, !bgate);
-}
-
-static int fiji_update_sclk_threshold(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	int result = 0;
-	uint32_t low_sclk_interrupt_threshold = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkThrottleLowNotification)
-		&& (hwmgr->gfx_arbiter.sclk_threshold !=
-				data->low_sclk_interrupt_threshold)) {
-		data->low_sclk_interrupt_threshold =
-				hwmgr->gfx_arbiter.sclk_threshold;
-		low_sclk_interrupt_threshold =
-				data->low_sclk_interrupt_threshold;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(low_sclk_interrupt_threshold);
-
-		result = fiji_copy_bytes_to_smc(
-				hwmgr->smumgr,
-				data->dpm_table_start +
-				offsetof(SMU73_Discrete_DpmTable,
-					LowSclkInterruptThreshold),
-				(uint8_t *)&low_sclk_interrupt_threshold,
-				sizeof(uint32_t),
-				data->sram_end);
-	}
-
-	return result;
-}
-
-static int fiji_program_mem_timing_parameters(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_OD_UPDATE_MCLK))
-		return fiji_program_memory_timing_parameters(hwmgr);
-
-	return 0;
-}
-
-static int fiji_unfreeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if ((0 == data->sclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK))) {
-
-		PP_ASSERT_WITH_CODE(fiji_is_dpm_running(hwmgr),
-				    "Trying to Unfreeze SCLK DPM when DPM is disabled",
-				    );
-		PP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_SCLKDPM_UnfreezeLevel),
-			"Failed to unfreeze SCLK DPM during UnFreezeSclkMclkDPM Function!",
-			return -1);
-	}
-
-	if ((0 == data->mclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK)) {
-
-		PP_ASSERT_WITH_CODE(fiji_is_dpm_running(hwmgr),
-				    "Trying to Unfreeze MCLK DPM when DPM is disabled",
-				    );
-		PP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_SCLKDPM_UnfreezeLevel),
-		    "Failed to unfreeze MCLK DPM during UnFreezeSclkMclkDPM Function!",
-		    return -1);
-	}
-
-	data->need_update_smu7_dpm_table = 0;
-
-	return 0;
-}
-
-/* Look up the voltaged based on DAL's requested level.
- * and then send the requested VDDC voltage to SMC
- */
-static void fiji_apply_dal_minimum_voltage_request(struct pp_hwmgr *hwmgr)
-{
-	return;
-}
-
-static int fiji_upload_dpm_level_enable_mask(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	/* Apply minimum voltage based on DAL's request level */
-	fiji_apply_dal_minimum_voltage_request(hwmgr);
-
-	if (0 == data->sclk_dpm_key_disabled) {
-		/* Checking if DPM is running.  If we discover hang because of this,
-		 *  we should skip this message.
-		 */
-		if (!fiji_is_dpm_running(hwmgr))
-			printk(KERN_ERR "[ powerplay ] "
-					"Trying to set Enable Mask when DPM is disabled \n");
-
-		if (data->dpm_level_enable_mask.sclk_dpm_enable_mask) {
-			result = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_SCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.sclk_dpm_enable_mask);
-			PP_ASSERT_WITH_CODE((0 == result),
-				"Set Sclk Dpm enable Mask failed", return -1);
-		}
-	}
-
-	if (0 == data->mclk_dpm_key_disabled) {
-		/* Checking if DPM is running.  If we discover hang because of this,
-		 *  we should skip this message.
-		 */
-		if (!fiji_is_dpm_running(hwmgr))
-			printk(KERN_ERR "[ powerplay ]"
-					" Trying to set Enable Mask when DPM is disabled \n");
-
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask) {
-			result = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_MCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.mclk_dpm_enable_mask);
-			PP_ASSERT_WITH_CODE((0 == result),
-				"Set Mclk Dpm enable Mask failed", return -1);
-		}
-	}
-
-	return 0;
-}
-
-static int fiji_notify_link_speed_change_after_state_change(
-		struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	const struct fiji_power_state *fiji_ps =
-			cast_const_phw_fiji_power_state(states->pnew_state);
-	uint16_t target_link_speed = fiji_get_maximum_link_speed(hwmgr, fiji_ps);
-	uint8_t  request;
-
-	if (data->pspp_notify_required) {
-		if (target_link_speed == PP_PCIEGen3)
-			request = PCIE_PERF_REQ_GEN3;
-		else if (target_link_speed == PP_PCIEGen2)
-			request = PCIE_PERF_REQ_GEN2;
-		else
-			request = PCIE_PERF_REQ_GEN1;
-
-		if(request == PCIE_PERF_REQ_GEN1 &&
-				fiji_get_current_pcie_speed(hwmgr) > 0)
-			return 0;
-
-		if (acpi_pcie_perf_request(hwmgr->device, request, false)) {
-			if (PP_PCIEGen2 == target_link_speed)
-				printk("PSPP request to switch to Gen2 from Gen3 Failed!");
-			else
-				printk("PSPP request to switch to Gen1 from Gen2 Failed!");
-		}
-	}
-
-	return 0;
-}
-
-static int fiji_set_power_state_tasks(struct pp_hwmgr *hwmgr,
-		const void *input)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = fiji_find_dpm_states_clocks_in_dpm_table(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to find DPM states clocks in DPM table!",
-			result = tmp_result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PCIEPerformanceRequest)) {
-		tmp_result =
-			fiji_request_link_speed_change_before_state_change(hwmgr, input);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-				"Failed to request link speed change before state change!",
-				result = tmp_result);
-	}
-
-	tmp_result = fiji_freeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to freeze SCLK MCLK DPM!", result = tmp_result);
-
-	tmp_result = fiji_populate_and_upload_sclk_mclk_dpm_levels(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to populate and upload SCLK MCLK DPM levels!",
-			result = tmp_result);
-
-	tmp_result = fiji_generate_dpm_level_enable_mask(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to generate DPM level enabled mask!",
-			result = tmp_result);
-
-	tmp_result = fiji_update_vce_dpm(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to update VCE DPM!",
-			result = tmp_result);
-
-	tmp_result = fiji_update_sclk_threshold(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to update SCLK threshold!",
-			result = tmp_result);
-
-	tmp_result = fiji_program_mem_timing_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to program memory timing parameters!",
-			result = tmp_result);
-
-	tmp_result = fiji_unfreeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to unfreeze SCLK MCLK DPM!",
-			result = tmp_result);
-
-	tmp_result = fiji_upload_dpm_level_enable_mask(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to upload DPM level enabled mask!",
-			result = tmp_result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PCIEPerformanceRequest)) {
-		tmp_result =
-			fiji_notify_link_speed_change_after_state_change(hwmgr, input);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-				"Failed to notify link speed change after state change!",
-				result = tmp_result);
-	}
-
-	return result;
-}
-
-static int fiji_dpm_get_sclk(struct pp_hwmgr *hwmgr, bool low)
-{
-	struct pp_power_state  *ps;
-	struct fiji_power_state  *fiji_ps;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	fiji_ps = cast_phw_fiji_power_state(&ps->hardware);
-
-	if (low)
-		return fiji_ps->performance_levels[0].engine_clock;
-	else
-		return fiji_ps->performance_levels
-				[fiji_ps->performance_level_count-1].engine_clock;
-}
-
-static int fiji_dpm_get_mclk(struct pp_hwmgr *hwmgr, bool low)
-{
-	struct pp_power_state  *ps;
-	struct fiji_power_state  *fiji_ps;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	fiji_ps = cast_phw_fiji_power_state(&ps->hardware);
-
-	if (low)
-		return fiji_ps->performance_levels[0].memory_clock;
-	else
-		return fiji_ps->performance_levels
-				[fiji_ps->performance_level_count-1].memory_clock;
-}
-
-static void fiji_print_current_perforce_level(
-		struct pp_hwmgr *hwmgr, struct seq_file *m)
-{
-	uint32_t sclk, mclk, activity_percent = 0;
-	uint32_t offset;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetSclkFrequency);
-
-	sclk = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-	smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetMclkFrequency);
-
-	mclk = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-	seq_printf(m, "\n [  mclk  ]: %u MHz\n\n [  sclk  ]: %u MHz\n",
-			mclk / 100, sclk / 100);
-
-	offset = data->soft_regs_start + offsetof(SMU73_SoftRegisters, AverageGraphicsActivity);
-	activity_percent = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, offset);
-	activity_percent += 0x80;
-	activity_percent >>= 8;
-
-	seq_printf(m, "\n [GPU load]: %u%%\n\n", activity_percent > 100 ? 100 : activity_percent);
-
-	seq_printf(m, "uvd    %sabled\n", data->uvd_power_gated ? "dis" : "en");
-
-	seq_printf(m, "vce    %sabled\n", data->vce_power_gated ? "dis" : "en");
-}
-
-static int fiji_program_display_gap(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t num_active_displays = 0;
-	uint32_t display_gap = cgs_read_ind_register(hwmgr->device,
-			CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL);
-	uint32_t display_gap2;
-	uint32_t pre_vbi_time_in_us;
-	uint32_t frame_time_in_us;
-	uint32_t ref_clock;
-	uint32_t refresh_rate = 0;
-	struct cgs_display_info info = {0};
-	struct cgs_mode_info mode_info;
-
-	info.mode_info = &mode_info;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-	num_active_displays = info.display_count;
-
-	display_gap = PHM_SET_FIELD(display_gap, CG_DISPLAY_GAP_CNTL,
-			DISP_GAP, (num_active_displays > 0)?
-			DISPLAY_GAP_VBLANK_OR_WM : DISPLAY_GAP_IGNORE);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_DISPLAY_GAP_CNTL, display_gap);
-
-	ref_clock = mode_info.ref_clock;
-	refresh_rate = mode_info.refresh_rate;
-
-	if (refresh_rate == 0)
-		refresh_rate = 60;
-
-	frame_time_in_us = 1000000 / refresh_rate;
-
-	pre_vbi_time_in_us = frame_time_in_us - 200 - mode_info.vblank_time_us;
-	display_gap2 = pre_vbi_time_in_us * (ref_clock / 100);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_DISPLAY_GAP_CNTL2, display_gap2);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			data->soft_regs_start +
-			offsetof(SMU73_SoftRegisters, PreVBlankGap), 0x64);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			data->soft_regs_start +
-			offsetof(SMU73_SoftRegisters, VBlankTimeout),
-			(frame_time_in_us - pre_vbi_time_in_us));
-
-	if (num_active_displays == 1)
-		tonga_notify_smc_display_change(hwmgr, true);
-
-	return 0;
-}
-
-static int fiji_display_configuration_changed_task(struct pp_hwmgr *hwmgr)
-{
-	return fiji_program_display_gap(hwmgr);
-}
-
-static int fiji_set_max_fan_pwm_output(struct pp_hwmgr *hwmgr,
-		uint16_t us_max_fan_pwm)
-{
-	hwmgr->thermal_controller.
-	advanceFanControlParameters.usMaxFanPWM = us_max_fan_pwm;
-
-	if (phm_is_hw_access_blocked(hwmgr))
-		return 0;
-
-	return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-			PPSMC_MSG_SetFanPwmMax, us_max_fan_pwm);
-}
-
-static int fiji_set_max_fan_rpm_output(struct pp_hwmgr *hwmgr,
-		uint16_t us_max_fan_rpm)
-{
-	hwmgr->thermal_controller.
-	advanceFanControlParameters.usMaxFanRPM = us_max_fan_rpm;
-
-	if (phm_is_hw_access_blocked(hwmgr))
-		return 0;
-
-	return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-			PPSMC_MSG_SetFanRpmMax, us_max_fan_rpm);
-}
-
-static int fiji_dpm_set_interrupt_state(void *private_data,
-					 unsigned src_id, unsigned type,
-					 int enabled)
-{
-	uint32_t cg_thermal_int;
-	struct pp_hwmgr *hwmgr = ((struct pp_eventmgr *)private_data)->hwmgr;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	switch (type) {
-	case AMD_THERMAL_IRQ_LOW_TO_HIGH:
-		if (enabled) {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device,
-					CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int |= CG_THERMAL_INT_CTRL__THERM_INTH_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device,
-					CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		} else {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device,
-					CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int &= ~CG_THERMAL_INT_CTRL__THERM_INTH_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device,
-					CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		}
-		break;
-
-	case AMD_THERMAL_IRQ_HIGH_TO_LOW:
-		if (enabled) {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device,
-					CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int |= CG_THERMAL_INT_CTRL__THERM_INTL_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device,
-					CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		} else {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device,
-					CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int &= ~CG_THERMAL_INT_CTRL__THERM_INTL_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device,
-					CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		}
-		break;
-	default:
-		break;
-	}
-	return 0;
-}
-
-static int fiji_register_internal_thermal_interrupt(struct pp_hwmgr *hwmgr,
-					const void *thermal_interrupt_info)
-{
-	int result;
-	const struct pp_interrupt_registration_info *info =
-			(const struct pp_interrupt_registration_info *)
-			thermal_interrupt_info;
-
-	if (info == NULL)
-		return -EINVAL;
-
-	result = cgs_add_irq_source(hwmgr->device, 230, AMD_THERMAL_IRQ_LAST,
-				fiji_dpm_set_interrupt_state,
-				info->call_back, info->context);
-
-	if (result)
-		return -EINVAL;
-
-	result = cgs_add_irq_source(hwmgr->device, 231, AMD_THERMAL_IRQ_LAST,
-				fiji_dpm_set_interrupt_state,
-				info->call_back, info->context);
-
-	if (result)
-		return -EINVAL;
-
-	return 0;
-}
-
-static int fiji_set_fan_control_mode(struct pp_hwmgr *hwmgr, uint32_t mode)
-{
-	if (mode) {
-		/* stop auto-manage */
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl))
-			fiji_fan_ctrl_stop_smc_fan_control(hwmgr);
-		fiji_fan_ctrl_set_static_mode(hwmgr, mode);
-	} else
-		/* restart auto-manage */
-		fiji_fan_ctrl_reset_fan_speed_to_default(hwmgr);
-
-	return 0;
-}
-
-static int fiji_get_fan_control_mode(struct pp_hwmgr *hwmgr)
-{
-	if (hwmgr->fan_ctrl_is_in_default_mode)
-		return hwmgr->fan_ctrl_default_mode;
-	else
-		return PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_FDO_CTRL2, FDO_PWM_MODE);
-}
-
-static int fiji_force_clock_level(struct pp_hwmgr *hwmgr,
-		enum pp_clock_type type, uint32_t mask)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if (hwmgr->dpm_level != AMD_DPM_FORCED_LEVEL_MANUAL)
-		return -EINVAL;
-
-	switch (type) {
-	case PP_SCLK:
-		if (!data->sclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_SCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.sclk_dpm_enable_mask & mask);
-		break;
-
-	case PP_MCLK:
-		if (!data->mclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_MCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.mclk_dpm_enable_mask & mask);
-		break;
-
-	case PP_PCIE:
-	{
-		uint32_t tmp = mask & data->dpm_level_enable_mask.pcie_dpm_enable_mask;
-		uint32_t level = 0;
-
-		while (tmp >>= 1)
-			level++;
-
-		if (!data->pcie_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_PCIeDPM_ForceLevel,
-					level);
-		break;
-	}
-	default:
-		break;
-	}
-
-	return 0;
-}
-
-static int fiji_print_clock_levels(struct pp_hwmgr *hwmgr,
-		enum pp_clock_type type, char *buf)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	struct fiji_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	struct fiji_single_dpm_table *pcie_table = &(data->dpm_table.pcie_speed_table);
-	int i, now, size = 0;
-	uint32_t clock, pcie_speed;
-
-	switch (type) {
-	case PP_SCLK:
-		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetSclkFrequency);
-		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-		for (i = 0; i < sclk_table->count; i++) {
-			if (clock > sclk_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < sclk_table->count; i++)
-			size += sprintf(buf + size, "%d: %uMhz %s\n",
-					i, sclk_table->dpm_levels[i].value / 100,
-					(i == now) ? "*" : "");
-		break;
-	case PP_MCLK:
-		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetMclkFrequency);
-		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-		for (i = 0; i < mclk_table->count; i++) {
-			if (clock > mclk_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < mclk_table->count; i++)
-			size += sprintf(buf + size, "%d: %uMhz %s\n",
-					i, mclk_table->dpm_levels[i].value / 100,
-					(i == now) ? "*" : "");
-		break;
-	case PP_PCIE:
-		pcie_speed = fiji_get_current_pcie_speed(hwmgr);
-		for (i = 0; i < pcie_table->count; i++) {
-			if (pcie_speed != pcie_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < pcie_table->count; i++)
-			size += sprintf(buf + size, "%d: %s %s\n", i,
-					(pcie_table->dpm_levels[i].value == 0) ? "2.5GB, x1" :
-					(pcie_table->dpm_levels[i].value == 1) ? "5.0GB, x16" :
-					(pcie_table->dpm_levels[i].value == 2) ? "8.0GB, x16" : "",
-					(i == now) ? "*" : "");
-		break;
-	default:
-		break;
-	}
-	return size;
-}
-
-static inline bool fiji_are_power_levels_equal(const struct fiji_performance_level *pl1,
-							   const struct fiji_performance_level *pl2)
-{
-	return ((pl1->memory_clock == pl2->memory_clock) &&
-		  (pl1->engine_clock == pl2->engine_clock) &&
-		  (pl1->pcie_gen == pl2->pcie_gen) &&
-		  (pl1->pcie_lane == pl2->pcie_lane));
-}
-
-static int
-fiji_check_states_equal(struct pp_hwmgr *hwmgr,
-		const struct pp_hw_power_state *pstate1,
-		const struct pp_hw_power_state *pstate2, bool *equal)
-{
-	const struct fiji_power_state *psa = cast_const_phw_fiji_power_state(pstate1);
-	const struct fiji_power_state *psb = cast_const_phw_fiji_power_state(pstate2);
-	int i;
-
-	if (equal == NULL || psa == NULL || psb == NULL)
-		return -EINVAL;
-
-	/* If the two states don't even have the same number of performance levels they cannot be the same state. */
-	if (psa->performance_level_count != psb->performance_level_count) {
-		*equal = false;
-		return 0;
-	}
-
-	for (i = 0; i < psa->performance_level_count; i++) {
-		if (!fiji_are_power_levels_equal(&(psa->performance_levels[i]), &(psb->performance_levels[i]))) {
-			/* If we have found even one performance level pair that is different the states are different. */
-			*equal = false;
-			return 0;
-		}
-	}
-
-	/* If all performance levels are the same try to use the UVD clocks to break the tie.*/
-	*equal = ((psa->uvd_clks.vclk == psb->uvd_clks.vclk) && (psa->uvd_clks.dclk == psb->uvd_clks.dclk));
-	*equal &= ((psa->vce_clks.evclk == psb->vce_clks.evclk) && (psa->vce_clks.ecclk == psb->vce_clks.ecclk));
-	*equal &= (psa->sclk_threshold == psb->sclk_threshold);
-	*equal &= (psa->acp_clk == psb->acp_clk);
-
-	return 0;
-}
-
-static bool
-fiji_check_smc_update_required_for_display_configuration(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	bool is_update_required = false;
-	struct cgs_display_info info = {0,0,NULL};
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (data->display_timing.num_existing_displays != info.display_count)
-		is_update_required = true;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_SclkDeepSleep)) {
-		if(hwmgr->display_config.min_core_set_clock_in_sr != data->display_timing.min_clock_in_sr)
-			is_update_required = true;
-	}
-
-	return is_update_required;
-}
-
-static int fiji_get_sclk_od(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	struct fiji_single_dpm_table *golden_sclk_table =
-			&(data->golden_dpm_table.sclk_table);
-	int value;
-
-	value = (sclk_table->dpm_levels[sclk_table->count - 1].value -
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value) *
-			100 /
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
-
-	return value;
-}
-
-static int fiji_set_sclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_single_dpm_table *golden_sclk_table =
-			&(data->golden_dpm_table.sclk_table);
-	struct pp_power_state  *ps;
-	struct fiji_power_state  *fiji_ps;
-
-	if (value > 20)
-		value = 20;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	fiji_ps = cast_phw_fiji_power_state(&ps->hardware);
-
-	fiji_ps->performance_levels[fiji_ps->performance_level_count - 1].engine_clock =
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value *
-			value / 100 +
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
-
-	return 0;
-}
-
-static int fiji_get_mclk_od(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	struct fiji_single_dpm_table *golden_mclk_table =
-			&(data->golden_dpm_table.mclk_table);
-	int value;
-
-	value = (mclk_table->dpm_levels[mclk_table->count - 1].value -
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value) *
-			100 /
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
-
-	return value;
-}
-
-static int fiji_set_mclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_single_dpm_table *golden_mclk_table =
-			&(data->golden_dpm_table.mclk_table);
-	struct pp_power_state  *ps;
-	struct fiji_power_state  *fiji_ps;
-
-	if (value > 20)
-		value = 20;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	fiji_ps = cast_phw_fiji_power_state(&ps->hardware);
-
-	fiji_ps->performance_levels[fiji_ps->performance_level_count - 1].memory_clock =
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value *
-			value / 100 +
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
-
-	return 0;
-}
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static int fiji_populate_requested_graphic_levels(struct pp_hwmgr *hwmgr,
-		struct pp_profile *request)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_dpm_table *dpm_table = &(data->dpm_table);
-	struct SMU73_Discrete_GraphicsLevel *levels =
-			data->smc_state_table.GraphicsLevel;
-	uint32_t array = data->dpm_table_start +
-			offsetof(SMU73_Discrete_DpmTable, GraphicsLevel);
-	uint32_t array_size = sizeof(struct SMU73_Discrete_GraphicsLevel) *
-			SMU73_MAX_LEVELS_GRAPHICS;
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		levels[i].ActivityLevel =
-				cpu_to_be16(request->activity_threshold);
-		levels[i].EnabledForActivity = 1;
-		levels[i].UpHyst = request->up_hyst;
-		levels[i].DownHyst = request->down_hyst;
-	}
-
-	return fiji_copy_bytes_to_smc(hwmgr->smumgr, array, (uint8_t *)levels,
-				array_size, data->sram_end);
-}
-
-static int fiji_find_min_clock_masks(struct pp_hwmgr *hwmgr,
-		uint32_t *sclk_mask, uint32_t *mclk_mask,
-		uint32_t min_sclk, uint32_t min_mclk)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct fiji_dpm_table *dpm_table = &(data->dpm_table);
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		if (dpm_table->sclk_table.dpm_levels[i].enabled &&
-			dpm_table->sclk_table.dpm_levels[i].value >= min_sclk)
-			*sclk_mask |= 1 << i;
-	}
-
-	for (i = 0; i < dpm_table->mclk_table.count; i++) {
-		if (dpm_table->mclk_table.dpm_levels[i].enabled &&
-			dpm_table->mclk_table.dpm_levels[i].value >= min_mclk)
-			*mclk_mask |= 1 << i;
-	}
-
-	return 0;
-}
-
-static int fiji_set_power_profile_state(struct pp_hwmgr *hwmgr,
-		struct pp_profile *request)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	int tmp_result, result = 0;
-	uint32_t sclk_mask = 0, mclk_mask = 0;
-
-	if (hwmgr->dpm_level != AMD_DPM_FORCED_LEVEL_AUTO)
-		return -EINVAL;
-
-	tmp_result = fiji_freeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE(!tmp_result,
-			"Failed to freeze SCLK MCLK DPM!",
-			result = tmp_result);
-
-	tmp_result = fiji_populate_requested_graphic_levels(hwmgr, request);
-	PP_ASSERT_WITH_CODE(!tmp_result,
-			"Failed to populate requested graphic levels!",
-			result = tmp_result);
-
-	tmp_result = fiji_unfreeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE(!tmp_result,
-			"Failed to unfreeze SCLK MCLK DPM!",
-			result = tmp_result);
-
-	fiji_find_min_clock_masks(hwmgr, &sclk_mask, &mclk_mask,
-			request->min_sclk, request->min_mclk);
-
-	if (sclk_mask) {
-		if (!data->sclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_SCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.
-				sclk_dpm_enable_mask &
-				sclk_mask);
-	}
-
-	if (mclk_mask) {
-		if (!data->mclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_MCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.
-				mclk_dpm_enable_mask &
-				mclk_mask);
-	}
-
-
-	return result;
-}
-#endif
-
-static const struct pp_hwmgr_func fiji_hwmgr_funcs = {
-	.backend_init = &fiji_hwmgr_backend_init,
-	.backend_fini = &fiji_hwmgr_backend_fini,
-	.asic_setup = &fiji_setup_asic_task,
-	.dynamic_state_management_enable = &fiji_enable_dpm_tasks,
-	.dynamic_state_management_disable = &fiji_disable_dpm_tasks,
-	.force_dpm_level = &fiji_dpm_force_dpm_level,
-	.get_num_of_pp_table_entries = &get_number_of_powerplay_table_entries_v1_0,
-	.get_power_state_size = &fiji_get_power_state_size,
-	.get_pp_table_entry = &fiji_get_pp_table_entry,
-	.patch_boot_state = &fiji_patch_boot_state,
-	.apply_state_adjust_rules = &fiji_apply_state_adjust_rules,
-	.power_state_set = &fiji_set_power_state_tasks,
-	.get_sclk = &fiji_dpm_get_sclk,
-	.get_mclk = &fiji_dpm_get_mclk,
-	.print_current_perforce_level = &fiji_print_current_perforce_level,
-	.powergate_uvd = &fiji_phm_powergate_uvd,
-	.powergate_vce = &fiji_phm_powergate_vce,
-	.disable_clock_power_gating = &fiji_phm_disable_clock_power_gating,
-	.notify_smc_display_config_after_ps_adjustment =
-			&tonga_notify_smc_display_config_after_ps_adjustment,
-	.display_config_changed = &fiji_display_configuration_changed_task,
-	.set_max_fan_pwm_output = fiji_set_max_fan_pwm_output,
-	.set_max_fan_rpm_output = fiji_set_max_fan_rpm_output,
-	.get_temperature = fiji_thermal_get_temperature,
-	.stop_thermal_controller = fiji_thermal_stop_thermal_controller,
-	.get_fan_speed_info = fiji_fan_ctrl_get_fan_speed_info,
-	.get_fan_speed_percent = fiji_fan_ctrl_get_fan_speed_percent,
-	.set_fan_speed_percent = fiji_fan_ctrl_set_fan_speed_percent,
-	.reset_fan_speed_to_default = fiji_fan_ctrl_reset_fan_speed_to_default,
-	.get_fan_speed_rpm = fiji_fan_ctrl_get_fan_speed_rpm,
-	.set_fan_speed_rpm = fiji_fan_ctrl_set_fan_speed_rpm,
-	.uninitialize_thermal_controller = fiji_thermal_ctrl_uninitialize_thermal_controller,
-	.register_internal_thermal_interrupt = fiji_register_internal_thermal_interrupt,
-	.set_fan_control_mode = fiji_set_fan_control_mode,
-	.get_fan_control_mode = fiji_get_fan_control_mode,
-	.check_states_equal = fiji_check_states_equal,
-	.check_smc_update_required_for_display_configuration = fiji_check_smc_update_required_for_display_configuration,
-	.force_clock_level = fiji_force_clock_level,
-	.print_clock_levels = fiji_print_clock_levels,
-	.get_sclk_od = fiji_get_sclk_od,
-	.set_sclk_od = fiji_set_sclk_od,
-	.get_mclk_od = fiji_get_mclk_od,
-	.set_mclk_od = fiji_set_mclk_od,
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
-	.set_power_profile_state = fiji_set_power_profile_state,
-#endif	
-};
-
-int fiji_hwmgr_init(struct pp_hwmgr *hwmgr)
-{
-	hwmgr->hwmgr_func = &fiji_hwmgr_funcs;
-	hwmgr->pptable_func = &pptable_v1_0_funcs;
-	pp_fiji_thermal_initialize(hwmgr);
-	return 0;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.h b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.h
deleted file mode 100644
index bf67c2a..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_hwmgr.h
+++ /dev/null
@@ -1,350 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef _FIJI_HWMGR_H_
-#define _FIJI_HWMGR_H_
-
-#include "hwmgr.h"
-#include "smu73.h"
-#include "smu73_discrete.h"
-#include "ppatomctrl.h"
-#include "fiji_ppsmc.h"
-#include "pp_endian.h"
-
-#define FIJI_MAX_HARDWARE_POWERLEVELS	2
-#define FIJI_AT_DFLT	30
-
-#define FIJI_VOLTAGE_CONTROL_NONE                   0x0
-#define FIJI_VOLTAGE_CONTROL_BY_GPIO                0x1
-#define FIJI_VOLTAGE_CONTROL_BY_SVID2               0x2
-#define FIJI_VOLTAGE_CONTROL_MERGED                 0x3
-
-#define DPMTABLE_OD_UPDATE_SCLK     0x00000001
-#define DPMTABLE_OD_UPDATE_MCLK     0x00000002
-#define DPMTABLE_UPDATE_SCLK        0x00000004
-#define DPMTABLE_UPDATE_MCLK        0x00000008
-
-struct fiji_performance_level {
-	uint32_t  memory_clock;
-	uint32_t  engine_clock;
-	uint16_t  pcie_gen;
-	uint16_t  pcie_lane;
-};
-
-struct fiji_uvd_clocks {
-	uint32_t  vclk;
-	uint32_t  dclk;
-};
-
-struct fiji_vce_clocks {
-	uint32_t  evclk;
-	uint32_t  ecclk;
-};
-
-struct fiji_power_state {
-    uint32_t                  magic;
-    struct fiji_uvd_clocks    uvd_clks;
-    struct fiji_vce_clocks    vce_clks;
-    uint32_t                  sam_clk;
-    uint32_t                  acp_clk;
-    uint16_t                  performance_level_count;
-    bool                      dc_compatible;
-    uint32_t                  sclk_threshold;
-    struct fiji_performance_level  performance_levels[FIJI_MAX_HARDWARE_POWERLEVELS];
-};
-
-struct fiji_dpm_level {
-	bool	enabled;
-    uint32_t	value;
-    uint32_t	param1;
-};
-
-#define FIJI_MAX_DEEPSLEEP_DIVIDER_ID 5
-#define MAX_REGULAR_DPM_NUMBER 8
-#define FIJI_MINIMUM_ENGINE_CLOCK 2500
-
-struct fiji_single_dpm_table {
-	uint32_t		count;
-	struct fiji_dpm_level	dpm_levels[MAX_REGULAR_DPM_NUMBER];
-};
-
-struct fiji_dpm_table {
-	struct fiji_single_dpm_table  sclk_table;
-	struct fiji_single_dpm_table  mclk_table;
-	struct fiji_single_dpm_table  pcie_speed_table;
-	struct fiji_single_dpm_table  vddc_table;
-	struct fiji_single_dpm_table  vddci_table;
-	struct fiji_single_dpm_table  mvdd_table;
-};
-
-struct fiji_clock_registers {
-	uint32_t  vCG_SPLL_FUNC_CNTL;
-	uint32_t  vCG_SPLL_FUNC_CNTL_2;
-	uint32_t  vCG_SPLL_FUNC_CNTL_3;
-	uint32_t  vCG_SPLL_FUNC_CNTL_4;
-	uint32_t  vCG_SPLL_SPREAD_SPECTRUM;
-	uint32_t  vCG_SPLL_SPREAD_SPECTRUM_2;
-	uint32_t  vDLL_CNTL;
-	uint32_t  vMCLK_PWRMGT_CNTL;
-	uint32_t  vMPLL_AD_FUNC_CNTL;
-	uint32_t  vMPLL_DQ_FUNC_CNTL;
-	uint32_t  vMPLL_FUNC_CNTL;
-	uint32_t  vMPLL_FUNC_CNTL_1;
-	uint32_t  vMPLL_FUNC_CNTL_2;
-	uint32_t  vMPLL_SS1;
-	uint32_t  vMPLL_SS2;
-};
-
-struct fiji_voltage_smio_registers {
-	uint32_t vS0_VID_LOWER_SMIO_CNTL;
-};
-
-#define FIJI_MAX_LEAKAGE_COUNT  8
-struct fiji_leakage_voltage {
-	uint16_t  count;
-	uint16_t  leakage_id[FIJI_MAX_LEAKAGE_COUNT];
-	uint16_t  actual_voltage[FIJI_MAX_LEAKAGE_COUNT];
-};
-
-struct fiji_vbios_boot_state {
-	uint16_t    mvdd_bootup_value;
-	uint16_t    vddc_bootup_value;
-	uint16_t    vddci_bootup_value;
-	uint32_t    sclk_bootup_value;
-	uint32_t    mclk_bootup_value;
-	uint16_t    pcie_gen_bootup_value;
-	uint16_t    pcie_lane_bootup_value;
-};
-
-struct fiji_bacos {
-	uint32_t                       best_match;
-	uint32_t                       baco_flags;
-	struct fiji_performance_level  performance_level;
-};
-
-/* Ultra Low Voltage parameter structure */
-struct fiji_ulv_parm {
-	bool                           ulv_supported;
-	uint32_t                       cg_ulv_parameter;
-	uint32_t                       ulv_volt_change_delay;
-	struct fiji_performance_level  ulv_power_level;
-};
-
-struct fiji_display_timing {
-	uint32_t  min_clock_in_sr;
-	uint32_t  num_existing_displays;
-};
-
-struct fiji_dpmlevel_enable_mask {
-	uint32_t  uvd_dpm_enable_mask;
-	uint32_t  vce_dpm_enable_mask;
-	uint32_t  acp_dpm_enable_mask;
-	uint32_t  samu_dpm_enable_mask;
-	uint32_t  sclk_dpm_enable_mask;
-	uint32_t  mclk_dpm_enable_mask;
-	uint32_t  pcie_dpm_enable_mask;
-};
-
-struct fiji_pcie_perf_range {
-	uint16_t  max;
-	uint16_t  min;
-};
-
-struct fiji_hwmgr {
-	struct fiji_dpm_table			dpm_table;
-	struct fiji_dpm_table			golden_dpm_table;
-
-	uint32_t						voting_rights_clients0;
-	uint32_t						voting_rights_clients1;
-	uint32_t						voting_rights_clients2;
-	uint32_t						voting_rights_clients3;
-	uint32_t						voting_rights_clients4;
-	uint32_t						voting_rights_clients5;
-	uint32_t						voting_rights_clients6;
-	uint32_t						voting_rights_clients7;
-	uint32_t						static_screen_threshold_unit;
-	uint32_t						static_screen_threshold;
-	uint32_t						voltage_control;
-	uint32_t						vddc_vddci_delta;
-
-	uint32_t						active_auto_throttle_sources;
-
-	struct fiji_clock_registers            clock_registers;
-	struct fiji_voltage_smio_registers      voltage_smio_registers;
-
-	bool                           is_memory_gddr5;
-	uint16_t                       acpi_vddc;
-	bool                           pspp_notify_required;
-	uint16_t                       force_pcie_gen;
-	uint16_t                       acpi_pcie_gen;
-	uint32_t                       pcie_gen_cap;
-	uint32_t                       pcie_lane_cap;
-	uint32_t                       pcie_spc_cap;
-	struct fiji_leakage_voltage          vddc_leakage;
-	struct fiji_leakage_voltage          Vddci_leakage;
-
-	uint32_t                             mvdd_control;
-	uint32_t                             vddc_mask_low;
-	uint32_t                             mvdd_mask_low;
-	uint16_t                            max_vddc_in_pptable;
-	uint16_t                            min_vddc_in_pptable;
-	uint16_t                            max_vddci_in_pptable;
-	uint16_t                            min_vddci_in_pptable;
-	uint32_t                             mclk_strobe_mode_threshold;
-	uint32_t                             mclk_stutter_mode_threshold;
-	uint32_t                             mclk_edc_enable_threshold;
-	uint32_t                             mclk_edcwr_enable_threshold;
-	bool                                is_uvd_enabled;
-	struct fiji_vbios_boot_state        vbios_boot_state;
-
-	bool                           battery_state;
-	bool                           is_tlu_enabled;
-
-	/* ---- SMC SRAM Address of firmware header tables ---- */
-	uint32_t                             sram_end;
-	uint32_t                             dpm_table_start;
-	uint32_t                             soft_regs_start;
-	uint32_t                             mc_reg_table_start;
-	uint32_t                             fan_table_start;
-	uint32_t                             arb_table_start;
-	struct SMU73_Discrete_DpmTable       smc_state_table;
-	struct SMU73_Discrete_Ulv            ulv_setting;
-
-	/* ---- Stuff originally coming from Evergreen ---- */
-	uint32_t                             vddci_control;
-	struct pp_atomctrl_voltage_table     vddc_voltage_table;
-	struct pp_atomctrl_voltage_table     vddci_voltage_table;
-	struct pp_atomctrl_voltage_table     mvdd_voltage_table;
-
-	uint32_t                             mgcg_cgtt_local2;
-	uint32_t                             mgcg_cgtt_local3;
-	uint32_t                             gpio_debug;
-	uint32_t                             mc_micro_code_feature;
-	uint32_t                             highest_mclk;
-	uint16_t                             acpi_vddci;
-	uint8_t                              mvdd_high_index;
-	uint8_t                              mvdd_low_index;
-	bool                                 dll_default_on;
-	bool                                 performance_request_registered;
-
-	/* ---- Low Power Features ---- */
-	struct fiji_bacos                    bacos;
-	struct fiji_ulv_parm                 ulv;
-
-	/* ---- CAC Stuff ---- */
-	uint32_t                       cac_table_start;
-	bool                           cac_configuration_required;
-	bool                           driver_calculate_cac_leakage;
-	bool                           cac_enabled;
-
-	/* ---- DPM2 Parameters ---- */
-	uint32_t                       power_containment_features;
-	bool                           enable_dte_feature;
-	bool                           enable_tdc_limit_feature;
-	bool                           enable_pkg_pwr_tracking_feature;
-	bool                           disable_uvd_power_tune_feature;
-	const struct fiji_pt_defaults  *power_tune_defaults;
-	struct SMU73_Discrete_PmFuses  power_tune_table;
-	uint32_t                       dte_tj_offset;
-	uint32_t                       fast_watermark_threshold;
-
-	/* ---- Phase Shedding ---- */
-	bool                           vddc_phase_shed_control;
-
-	/* ---- DI/DT ---- */
-	struct fiji_display_timing        display_timing;
-
-	/* ---- Thermal Temperature Setting ---- */
-	struct fiji_dpmlevel_enable_mask     dpm_level_enable_mask;
-	uint32_t                             need_update_smu7_dpm_table;
-	uint32_t                             sclk_dpm_key_disabled;
-	uint32_t                             mclk_dpm_key_disabled;
-	uint32_t                             pcie_dpm_key_disabled;
-	uint32_t                             min_engine_clocks;
-	struct fiji_pcie_perf_range          pcie_gen_performance;
-	struct fiji_pcie_perf_range          pcie_lane_performance;
-	struct fiji_pcie_perf_range          pcie_gen_power_saving;
-	struct fiji_pcie_perf_range          pcie_lane_power_saving;
-	bool                                 use_pcie_performance_levels;
-	bool                                 use_pcie_power_saving_levels;
-	uint32_t                             activity_target[SMU73_MAX_LEVELS_GRAPHICS];
-	uint32_t                             mclk_activity_target;
-	uint32_t                             mclk_dpm0_activity_target;
-	uint32_t                             low_sclk_interrupt_threshold;
-	uint32_t                             last_mclk_dpm_enable_mask;
-	bool                                 uvd_enabled;
-
-	/* ---- Power Gating States ---- */
-	bool                           uvd_power_gated;
-	bool                           vce_power_gated;
-	bool                           samu_power_gated;
-	bool                           acp_power_gated;
-	bool                           pg_acp_init;
-	bool                           frtc_enabled;
-	bool                           frtc_status_changed;
-};
-
-/* To convert to Q8.8 format for firmware */
-#define FIJI_Q88_FORMAT_CONVERSION_UNIT             256
-
-enum Fiji_I2CLineID {
-    Fiji_I2CLineID_DDC1 = 0x90,
-    Fiji_I2CLineID_DDC2 = 0x91,
-    Fiji_I2CLineID_DDC3 = 0x92,
-    Fiji_I2CLineID_DDC4 = 0x93,
-    Fiji_I2CLineID_DDC5 = 0x94,
-    Fiji_I2CLineID_DDC6 = 0x95,
-    Fiji_I2CLineID_SCLSDA = 0x96,
-    Fiji_I2CLineID_DDCVGA = 0x97
-};
-
-#define Fiji_I2C_DDC1DATA          0
-#define Fiji_I2C_DDC1CLK           1
-#define Fiji_I2C_DDC2DATA          2
-#define Fiji_I2C_DDC2CLK           3
-#define Fiji_I2C_DDC3DATA          4
-#define Fiji_I2C_DDC3CLK           5
-#define Fiji_I2C_SDA               40
-#define Fiji_I2C_SCL               41
-#define Fiji_I2C_DDC4DATA          65
-#define Fiji_I2C_DDC4CLK           66
-#define Fiji_I2C_DDC5DATA          0x48
-#define Fiji_I2C_DDC5CLK           0x49
-#define Fiji_I2C_DDC6DATA          0x4a
-#define Fiji_I2C_DDC6CLK           0x4b
-#define Fiji_I2C_DDCVGADATA        0x4c
-#define Fiji_I2C_DDCVGACLK         0x4d
-
-#define FIJI_UNUSED_GPIO_PIN       0x7F
-
-extern int tonga_initializa_dynamic_state_adjustment_rule_settings(struct pp_hwmgr *hwmgr);
-extern int tonga_get_mc_microcode_version (struct pp_hwmgr *hwmgr);
-extern int tonga_notify_smc_display_config_after_ps_adjustment(struct pp_hwmgr *hwmgr);
-extern int tonga_notify_smc_display_change(struct pp_hwmgr *hwmgr, bool has_display);
-int fiji_update_vce_dpm(struct pp_hwmgr *hwmgr, const void *input);
-int fiji_update_uvd_dpm(struct pp_hwmgr *hwmgr, bool bgate);
-int fiji_update_samu_dpm(struct pp_hwmgr *hwmgr, bool bgate);
-int fiji_update_acp_dpm(struct pp_hwmgr *hwmgr, bool bgate);
-int fiji_enable_disable_vce_dpm(struct pp_hwmgr *hwmgr, bool enable);
-
-#endif /* _FIJI_HWMGR_H_ */
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_powertune.c b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_powertune.c
deleted file mode 100644
index f5992ea..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_powertune.c
+++ /dev/null
@@ -1,610 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#include "hwmgr.h"
-#include "smumgr.h"
-#include "fiji_hwmgr.h"
-#include "fiji_powertune.h"
-#include "fiji_smumgr.h"
-#include "smu73_discrete.h"
-#include "pp_debug.h"
-
-#define VOLTAGE_SCALE  4
-#define POWERTUNE_DEFAULT_SET_MAX    1
-
-const struct fiji_pt_defaults fiji_power_tune_data_set_array[POWERTUNE_DEFAULT_SET_MAX] = {
-		/*sviLoadLIneEn,  SviLoadLineVddC, TDC_VDDC_ThrottleReleaseLimitPerc */
-		{1,               0xF,             0xFD,
-		/* TDC_MAWt, TdcWaterfallCtl, DTEAmbientTempBase */
-		0x19,        5,               45}
-};
-
-void fiji_initialize_power_tune_defaults(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *fiji_hwmgr = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct  phm_ppt_v1_information *table_info =
-			(struct  phm_ppt_v1_information *)(hwmgr->pptable);
-	uint32_t tmp = 0;
-
-	if(table_info &&
-			table_info->cac_dtp_table->usPowerTuneDataSetID <= POWERTUNE_DEFAULT_SET_MAX &&
-			table_info->cac_dtp_table->usPowerTuneDataSetID)
-		fiji_hwmgr->power_tune_defaults =
-				&fiji_power_tune_data_set_array
-				[table_info->cac_dtp_table->usPowerTuneDataSetID - 1];
-	else
-		fiji_hwmgr->power_tune_defaults = &fiji_power_tune_data_set_array[0];
-
-	/* Assume disabled */
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SQRamping);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DBRamping);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_TDRamping);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_TCPRamping);
-
-	fiji_hwmgr->dte_tj_offset = tmp;
-
-	if (!tmp) {
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_CAC);
-
-		fiji_hwmgr->fast_watermark_threshold = 100;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_PowerContainment)) {
-			tmp = 1;
-			fiji_hwmgr->enable_dte_feature = tmp ? false : true;
-			fiji_hwmgr->enable_tdc_limit_feature = tmp ? true : false;
-			fiji_hwmgr->enable_pkg_pwr_tracking_feature = tmp ? true : false;
-		}
-	}
-}
-
-/* PPGen has the gain setting generated in x * 100 unit
- * This function is to convert the unit to x * 4096(0x1000) unit.
- *  This is the unit expected by SMC firmware
- */
-static uint16_t scale_fan_gain_settings(uint16_t raw_setting)
-{
-	uint32_t tmp;
-	tmp = raw_setting * 4096 / 100;
-	return (uint16_t)tmp;
-}
-
-static void get_scl_sda_value(uint8_t line, uint8_t *scl, uint8_t* sda)
-{
-	switch (line) {
-	case Fiji_I2CLineID_DDC1 :
-		*scl = Fiji_I2C_DDC1CLK;
-		*sda = Fiji_I2C_DDC1DATA;
-		break;
-	case Fiji_I2CLineID_DDC2 :
-		*scl = Fiji_I2C_DDC2CLK;
-		*sda = Fiji_I2C_DDC2DATA;
-		break;
-	case Fiji_I2CLineID_DDC3 :
-		*scl = Fiji_I2C_DDC3CLK;
-		*sda = Fiji_I2C_DDC3DATA;
-		break;
-	case Fiji_I2CLineID_DDC4 :
-		*scl = Fiji_I2C_DDC4CLK;
-		*sda = Fiji_I2C_DDC4DATA;
-		break;
-	case Fiji_I2CLineID_DDC5 :
-		*scl = Fiji_I2C_DDC5CLK;
-		*sda = Fiji_I2C_DDC5DATA;
-		break;
-	case Fiji_I2CLineID_DDC6 :
-		*scl = Fiji_I2C_DDC6CLK;
-		*sda = Fiji_I2C_DDC6DATA;
-		break;
-	case Fiji_I2CLineID_SCLSDA :
-		*scl = Fiji_I2C_SCL;
-		*sda = Fiji_I2C_SDA;
-		break;
-	case Fiji_I2CLineID_DDCVGA :
-		*scl = Fiji_I2C_DDCVGACLK;
-		*sda = Fiji_I2C_DDCVGADATA;
-		break;
-	default:
-		*scl = 0;
-		*sda = 0;
-		break;
-	}
-}
-
-int fiji_populate_bapm_parameters_in_dpm_table(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	const struct fiji_pt_defaults *defaults = data->power_tune_defaults;
-	SMU73_Discrete_DpmTable  *dpm_table = &(data->smc_state_table);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_cac_tdp_table *cac_dtp_table = table_info->cac_dtp_table;
-	struct pp_advance_fan_control_parameters *fan_table=
-			&hwmgr->thermal_controller.advanceFanControlParameters;
-	uint8_t uc_scl, uc_sda;
-
-	/* TDP number of fraction bits are changed from 8 to 7 for Fiji
-	 * as requested by SMC team
-	 */
-	dpm_table->DefaultTdp = PP_HOST_TO_SMC_US(
-			(uint16_t)(cac_dtp_table->usTDP * 128));
-	dpm_table->TargetTdp = PP_HOST_TO_SMC_US(
-			(uint16_t)(cac_dtp_table->usTDP * 128));
-
-	PP_ASSERT_WITH_CODE(cac_dtp_table->usTargetOperatingTemp <= 255,
-			"Target Operating Temp is out of Range!",);
-
-	dpm_table->GpuTjMax = (uint8_t)(cac_dtp_table->usTargetOperatingTemp);
-	dpm_table->GpuTjHyst = 8;
-
-	dpm_table->DTEAmbientTempBase = defaults->DTEAmbientTempBase;
-
-	/* The following are for new Fiji Multi-input fan/thermal control */
-	dpm_table->TemperatureLimitEdge = PP_HOST_TO_SMC_US(
-			cac_dtp_table->usTargetOperatingTemp * 256);
-	dpm_table->TemperatureLimitHotspot = PP_HOST_TO_SMC_US(
-			cac_dtp_table->usTemperatureLimitHotspot * 256);
-	dpm_table->TemperatureLimitLiquid1 = PP_HOST_TO_SMC_US(
-			cac_dtp_table->usTemperatureLimitLiquid1 * 256);
-	dpm_table->TemperatureLimitLiquid2 = PP_HOST_TO_SMC_US(
-			cac_dtp_table->usTemperatureLimitLiquid2 * 256);
-	dpm_table->TemperatureLimitVrVddc = PP_HOST_TO_SMC_US(
-			cac_dtp_table->usTemperatureLimitVrVddc * 256);
-	dpm_table->TemperatureLimitVrMvdd = PP_HOST_TO_SMC_US(
-			cac_dtp_table->usTemperatureLimitVrMvdd * 256);
-	dpm_table->TemperatureLimitPlx = PP_HOST_TO_SMC_US(
-			cac_dtp_table->usTemperatureLimitPlx * 256);
-
-	dpm_table->FanGainEdge = PP_HOST_TO_SMC_US(
-			scale_fan_gain_settings(fan_table->usFanGainEdge));
-	dpm_table->FanGainHotspot = PP_HOST_TO_SMC_US(
-			scale_fan_gain_settings(fan_table->usFanGainHotspot));
-	dpm_table->FanGainLiquid = PP_HOST_TO_SMC_US(
-			scale_fan_gain_settings(fan_table->usFanGainLiquid));
-	dpm_table->FanGainVrVddc = PP_HOST_TO_SMC_US(
-			scale_fan_gain_settings(fan_table->usFanGainVrVddc));
-	dpm_table->FanGainVrMvdd = PP_HOST_TO_SMC_US(
-			scale_fan_gain_settings(fan_table->usFanGainVrMvdd));
-	dpm_table->FanGainPlx = PP_HOST_TO_SMC_US(
-			scale_fan_gain_settings(fan_table->usFanGainPlx));
-	dpm_table->FanGainHbm = PP_HOST_TO_SMC_US(
-			scale_fan_gain_settings(fan_table->usFanGainHbm));
-
-	dpm_table->Liquid1_I2C_address = cac_dtp_table->ucLiquid1_I2C_address;
-	dpm_table->Liquid2_I2C_address = cac_dtp_table->ucLiquid2_I2C_address;
-	dpm_table->Vr_I2C_address = cac_dtp_table->ucVr_I2C_address;
-	dpm_table->Plx_I2C_address = cac_dtp_table->ucPlx_I2C_address;
-
-	get_scl_sda_value(cac_dtp_table->ucLiquid_I2C_Line, &uc_scl, &uc_sda);
-	dpm_table->Liquid_I2C_LineSCL = uc_scl;
-	dpm_table->Liquid_I2C_LineSDA = uc_sda;
-
-	get_scl_sda_value(cac_dtp_table->ucVr_I2C_Line, &uc_scl, &uc_sda);
-	dpm_table->Vr_I2C_LineSCL = uc_scl;
-	dpm_table->Vr_I2C_LineSDA = uc_sda;
-
-	get_scl_sda_value(cac_dtp_table->ucPlx_I2C_Line, &uc_scl, &uc_sda);
-	dpm_table->Plx_I2C_LineSCL = uc_scl;
-	dpm_table->Plx_I2C_LineSDA = uc_sda;
-
-	return 0;
-}
-
-static int fiji_populate_svi_load_line(struct pp_hwmgr *hwmgr)
-{
-    struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-    const struct fiji_pt_defaults *defaults = data->power_tune_defaults;
-
-    data->power_tune_table.SviLoadLineEn = defaults->SviLoadLineEn;
-    data->power_tune_table.SviLoadLineVddC = defaults->SviLoadLineVddC;
-    data->power_tune_table.SviLoadLineTrimVddC = 3;
-    data->power_tune_table.SviLoadLineOffsetVddC = 0;
-
-    return 0;
-}
-
-static int fiji_populate_tdc_limit(struct pp_hwmgr *hwmgr)
-{
-	uint16_t tdc_limit;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	const struct fiji_pt_defaults *defaults = data->power_tune_defaults;
-
-	/* TDC number of fraction bits are changed from 8 to 7
-	 * for Fiji as requested by SMC team
-	 */
-	tdc_limit = (uint16_t)(table_info->cac_dtp_table->usTDC * 128);
-	data->power_tune_table.TDC_VDDC_PkgLimit =
-			CONVERT_FROM_HOST_TO_SMC_US(tdc_limit);
-	data->power_tune_table.TDC_VDDC_ThrottleReleaseLimitPerc =
-			defaults->TDC_VDDC_ThrottleReleaseLimitPerc;
-	data->power_tune_table.TDC_MAWt = defaults->TDC_MAWt;
-
-	return 0;
-}
-
-static int fiji_populate_dw8(struct pp_hwmgr *hwmgr, uint32_t fuse_table_offset)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	const struct fiji_pt_defaults *defaults = data->power_tune_defaults;
-	uint32_t temp;
-
-	if (fiji_read_smc_sram_dword(hwmgr->smumgr,
-			fuse_table_offset +
-			offsetof(SMU73_Discrete_PmFuses, TdcWaterfallCtl),
-			(uint32_t *)&temp, data->sram_end))
-		PP_ASSERT_WITH_CODE(false,
-				"Attempt to read PmFuses.DW6 (SviLoadLineEn) from SMC Failed!",
-				return -EINVAL);
-	else {
-		data->power_tune_table.TdcWaterfallCtl = defaults->TdcWaterfallCtl;
-		data->power_tune_table.LPMLTemperatureMin =
-				(uint8_t)((temp >> 16) & 0xff);
-		data->power_tune_table.LPMLTemperatureMax =
-				(uint8_t)((temp >> 8) & 0xff);
-		data->power_tune_table.Reserved = (uint8_t)(temp & 0xff);
-	}
-	return 0;
-}
-
-static int fiji_populate_temperature_scaler(struct pp_hwmgr *hwmgr)
-{
-	int i;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	/* Currently not used. Set all to zero. */
-	for (i = 0; i < 16; i++)
-		data->power_tune_table.LPMLTemperatureScaler[i] = 0;
-
-	return 0;
-}
-
-static int fiji_populate_fuzzy_fan(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if( (hwmgr->thermal_controller.advanceFanControlParameters.
-			usFanOutputSensitivity & (1 << 15)) ||
-			0 == hwmgr->thermal_controller.advanceFanControlParameters.
-			usFanOutputSensitivity )
-		hwmgr->thermal_controller.advanceFanControlParameters.
-		usFanOutputSensitivity = hwmgr->thermal_controller.
-			advanceFanControlParameters.usDefaultFanOutputSensitivity;
-
-	data->power_tune_table.FuzzyFan_PwmSetDelta =
-			PP_HOST_TO_SMC_US(hwmgr->thermal_controller.
-					advanceFanControlParameters.usFanOutputSensitivity);
-	return 0;
-}
-
-static int fiji_populate_gnb_lpml(struct pp_hwmgr *hwmgr)
-{
-	int i;
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	/* Currently not used. Set all to zero. */
-	for (i = 0; i < 16; i++)
-		data->power_tune_table.GnbLPML[i] = 0;
-
-	return 0;
-}
-
-static int fiji_min_max_vgnb_lpml_id_from_bapm_vddc(struct pp_hwmgr *hwmgr)
-{
- /*   int  i, min, max;
-    struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-    uint8_t * pHiVID = data->power_tune_table.BapmVddCVidHiSidd;
-    uint8_t * pLoVID = data->power_tune_table.BapmVddCVidLoSidd;
-
-    min = max = pHiVID[0];
-    for (i = 0; i < 8; i++) {
-        if (0 != pHiVID[i]) {
-            if (min > pHiVID[i])
-                min = pHiVID[i];
-            if (max < pHiVID[i])
-                max = pHiVID[i];
-        }
-
-        if (0 != pLoVID[i]) {
-            if (min > pLoVID[i])
-                min = pLoVID[i];
-            if (max < pLoVID[i])
-                max = pLoVID[i];
-        }
-    }
-
-    PP_ASSERT_WITH_CODE((0 != min) && (0 != max), "BapmVddcVidSidd table does not exist!", return int_Failed);
-    data->power_tune_table.GnbLPMLMaxVid = (uint8_t)max;
-    data->power_tune_table.GnbLPMLMinVid = (uint8_t)min;
-*/
-    return 0;
-}
-
-static int fiji_populate_bapm_vddc_base_leakage_sidd(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint16_t HiSidd = data->power_tune_table.BapmVddCBaseLeakageHiSidd;
-	uint16_t LoSidd = data->power_tune_table.BapmVddCBaseLeakageLoSidd;
-	struct phm_cac_tdp_table *cac_table = table_info->cac_dtp_table;
-
-	HiSidd = (uint16_t)(cac_table->usHighCACLeakage / 100 * 256);
-	LoSidd = (uint16_t)(cac_table->usLowCACLeakage / 100 * 256);
-
-	data->power_tune_table.BapmVddCBaseLeakageHiSidd =
-			CONVERT_FROM_HOST_TO_SMC_US(HiSidd);
-	data->power_tune_table.BapmVddCBaseLeakageLoSidd =
-			CONVERT_FROM_HOST_TO_SMC_US(LoSidd);
-
-	return 0;
-}
-
-int fiji_populate_pm_fuses(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	uint32_t pm_fuse_table_offset;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		if (fiji_read_smc_sram_dword(hwmgr->smumgr,
-				SMU7_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU73_Firmware_Header, PmFuseTable),
-				&pm_fuse_table_offset, data->sram_end))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to get pm_fuse_table_offset Failed!",
-					return -EINVAL);
-
-		/* DW6 */
-		if (fiji_populate_svi_load_line(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate SviLoadLine Failed!",
-					return -EINVAL);
-		/* DW7 */
-		if (fiji_populate_tdc_limit(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate TDCLimit Failed!", return -EINVAL);
-		/* DW8 */
-		if (fiji_populate_dw8(hwmgr, pm_fuse_table_offset))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate TdcWaterfallCtl, "
-					"LPMLTemperature Min and Max Failed!",
-					return -EINVAL);
-
-		/* DW9-DW12 */
-		if (0 != fiji_populate_temperature_scaler(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate LPMLTemperatureScaler Failed!",
-					return -EINVAL);
-
-		/* DW13-DW14 */
-		if(fiji_populate_fuzzy_fan(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate Fuzzy Fan Control parameters Failed!",
-					return -EINVAL);
-
-		/* DW15-DW18 */
-		if (fiji_populate_gnb_lpml(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate GnbLPML Failed!",
-					return -EINVAL);
-
-		/* DW19 */
-		if (fiji_min_max_vgnb_lpml_id_from_bapm_vddc(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate GnbLPML Min and Max Vid Failed!",
-					return -EINVAL);
-
-		/* DW20 */
-		if (fiji_populate_bapm_vddc_base_leakage_sidd(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate BapmVddCBaseLeakage Hi and Lo "
-					"Sidd Failed!", return -EINVAL);
-
-		if (fiji_copy_bytes_to_smc(hwmgr->smumgr, pm_fuse_table_offset,
-				(uint8_t *)&data->power_tune_table,
-				sizeof(struct SMU73_Discrete_PmFuses), data->sram_end))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to download PmFuseTable Failed!",
-					return -EINVAL);
-	}
-	return 0;
-}
-
-int fiji_enable_smc_cac(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC)) {
-		int smc_result;
-		smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-				(uint16_t)(PPSMC_MSG_EnableCac));
-		PP_ASSERT_WITH_CODE((0 == smc_result),
-				"Failed to enable CAC in SMC.", result = -1);
-
-        data->cac_enabled = (0 == smc_result) ? true : false;
-	}
-	return result;
-}
-
-int fiji_disable_smc_cac(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC) && data->cac_enabled) {
-		int smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-				(uint16_t)(PPSMC_MSG_DisableCac));
-		PP_ASSERT_WITH_CODE((smc_result == 0),
-				"Failed to disable CAC in SMC.", result = -1);
-
-		data->cac_enabled = false;
-	}
-	return result;
-}
-
-int fiji_set_power_limit(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-
-	if(data->power_containment_features &
-			POWERCONTAINMENT_FEATURE_PkgPwrLimit)
-		return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_PkgPwrSetLimit, n);
-	return 0;
-}
-
-static int fiji_set_overdriver_target_tdp(struct pp_hwmgr *pHwMgr, uint32_t target_tdp)
-{
-	return smum_send_msg_to_smc_with_parameter(pHwMgr->smumgr,
-			PPSMC_MSG_OverDriveSetTargetTdp, target_tdp);
-}
-
-int fiji_enable_power_containment(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	int smc_result;
-	int result = 0;
-
-	data->power_containment_features = 0;
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		if (data->enable_dte_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_EnableDTE));
-			PP_ASSERT_WITH_CODE((0 == smc_result),
-					"Failed to enable DTE in SMC.", result = -1;);
-			if (0 == smc_result)
-				data->power_containment_features |= POWERCONTAINMENT_FEATURE_DTE;
-		}
-
-		if (data->enable_tdc_limit_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_TDCLimitEnable));
-			PP_ASSERT_WITH_CODE((0 == smc_result),
-					"Failed to enable TDCLimit in SMC.", result = -1;);
-			if (0 == smc_result)
-				data->power_containment_features |=
-						POWERCONTAINMENT_FEATURE_TDCLimit;
-		}
-
-		if (data->enable_pkg_pwr_tracking_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_PkgPwrLimitEnable));
-			PP_ASSERT_WITH_CODE((0 == smc_result),
-					"Failed to enable PkgPwrTracking in SMC.", result = -1;);
-			if (0 == smc_result) {
-				struct phm_cac_tdp_table *cac_table =
-						table_info->cac_dtp_table;
-				uint32_t default_limit =
-					(uint32_t)(cac_table->usMaximumPowerDeliveryLimit * 256);
-
-				data->power_containment_features |=
-						POWERCONTAINMENT_FEATURE_PkgPwrLimit;
-
-				if (fiji_set_power_limit(hwmgr, default_limit))
-					printk(KERN_ERR "Failed to set Default Power Limit in SMC!");
-			}
-		}
-	}
-	return result;
-}
-
-int fiji_disable_power_containment(struct pp_hwmgr *hwmgr)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment) &&
-			data->power_containment_features) {
-		int smc_result;
-
-		if (data->power_containment_features &
-				POWERCONTAINMENT_FEATURE_TDCLimit) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_TDCLimitDisable));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to disable TDCLimit in SMC.",
-					result = smc_result);
-		}
-
-		if (data->power_containment_features &
-				POWERCONTAINMENT_FEATURE_DTE) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_DisableDTE));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to disable DTE in SMC.",
-					result = smc_result);
-		}
-
-		if (data->power_containment_features &
-				POWERCONTAINMENT_FEATURE_PkgPwrLimit) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_PkgPwrLimitDisable));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to disable PkgPwrTracking in SMC.",
-					result = smc_result);
-		}
-		data->power_containment_features = 0;
-	}
-
-	return result;
-}
-
-int fiji_power_control_set_level(struct pp_hwmgr *hwmgr)
-{
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_cac_tdp_table *cac_table = table_info->cac_dtp_table;
-	int adjust_percent, target_tdp;
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		/* adjustment percentage has already been validated */
-		adjust_percent = hwmgr->platform_descriptor.TDPAdjustmentPolarity ?
-				hwmgr->platform_descriptor.TDPAdjustment :
-				(-1 * hwmgr->platform_descriptor.TDPAdjustment);
-		/* SMC requested that target_tdp to be 7 bit fraction in DPM table
-		 * but message to be 8 bit fraction for messages
-		 */
-		target_tdp = ((100 + adjust_percent) * (int)(cac_table->usTDP * 256)) / 100;
-		result = fiji_set_overdriver_target_tdp(hwmgr, (uint32_t)target_tdp);
-	}
-
-	return result;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_powertune.h b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_powertune.h
deleted file mode 100644
index fec7724..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_powertune.h
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-#ifndef FIJI_POWERTUNE_H
-#define FIJI_POWERTUNE_H
-
-enum fiji_pt_config_reg_type {
-	FIJI_CONFIGREG_MMR = 0,
-	FIJI_CONFIGREG_SMC_IND,
-	FIJI_CONFIGREG_DIDT_IND,
-	FIJI_CONFIGREG_CACHE,
-	FIJI_CONFIGREG_MAX
-};
-
-/* PowerContainment Features */
-#define POWERCONTAINMENT_FEATURE_DTE             0x00000001
-#define POWERCONTAINMENT_FEATURE_TDCLimit        0x00000002
-#define POWERCONTAINMENT_FEATURE_PkgPwrLimit     0x00000004
-
-#define DIDT_SQ_CTRL0__UNUSED_0_MASK             0xffffffc0
-#define DIDT_SQ_CTRL0__UNUSED_0__SHIFT           0x6
-#define DIDT_TD_CTRL0__UNUSED_0_MASK             0xffffffc0
-#define DIDT_TD_CTRL0__UNUSED_0__SHIFT           0x6
-#define DIDT_TCP_CTRL0__UNUSED_0_MASK            0xffffffc0
-#define DIDT_TCP_CTRL0__UNUSED_0__SHIFT          0x6
-#define DIDT_SQ_TUNING_CTRL__UNUSED_0_MASK                 0xe0000000
-#define DIDT_SQ_TUNING_CTRL__UNUSED_0__SHIFT               0x0000001d
-#define DIDT_TD_TUNING_CTRL__UNUSED_0_MASK                 0xe0000000
-#define DIDT_TD_TUNING_CTRL__UNUSED_0__SHIFT               0x0000001d
-#define DIDT_TCP_TUNING_CTRL__UNUSED_0_MASK                0xe0000000
-#define DIDT_TCP_TUNING_CTRL__UNUSED_0__SHIFT              0x0000001d
-
-struct fiji_pt_config_reg {
-	uint32_t                           offset;
-	uint32_t                           mask;
-	uint32_t                           shift;
-	uint32_t                           value;
-	enum fiji_pt_config_reg_type       type;
-};
-
-struct fiji_pt_defaults
-{
-    uint8_t   SviLoadLineEn;
-    uint8_t   SviLoadLineVddC;
-    uint8_t   TDC_VDDC_ThrottleReleaseLimitPerc;
-    uint8_t   TDC_MAWt;
-    uint8_t   TdcWaterfallCtl;
-    uint8_t   DTEAmbientTempBase;
-};
-
-void fiji_initialize_power_tune_defaults(struct pp_hwmgr *hwmgr);
-int fiji_populate_bapm_parameters_in_dpm_table(struct pp_hwmgr *hwmgr);
-int fiji_populate_pm_fuses(struct pp_hwmgr *hwmgr);
-int fiji_enable_smc_cac(struct pp_hwmgr *hwmgr);
-int fiji_disable_smc_cac(struct pp_hwmgr *hwmgr);
-int fiji_enable_power_containment(struct pp_hwmgr *hwmgr);
-int fiji_disable_power_containment(struct pp_hwmgr *hwmgr);
-int fiji_set_power_limit(struct pp_hwmgr *hwmgr, uint32_t n);
-int fiji_power_control_set_level(struct pp_hwmgr *hwmgr);
-
-#endif  /* FIJI_POWERTUNE_H */
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.c b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.c
deleted file mode 100644
index 7f431e7..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.c
+++ /dev/null
@@ -1,687 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-#include <asm/div64.h>
-#include "fiji_thermal.h"
-#include "fiji_hwmgr.h"
-#include "fiji_smumgr.h"
-#include "fiji_ppsmc.h"
-#include "smu/smu_7_1_3_d.h"
-#include "smu/smu_7_1_3_sh_mask.h"
-
-int fiji_fan_ctrl_get_fan_speed_info(struct pp_hwmgr *hwmgr,
-		struct phm_fan_speed_info *fan_speed_info)
-{
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	fan_speed_info->supports_percent_read = true;
-	fan_speed_info->supports_percent_write = true;
-	fan_speed_info->min_percent = 0;
-	fan_speed_info->max_percent = 100;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_FanSpeedInTableIsRPM) &&
-		hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution) {
-		fan_speed_info->supports_rpm_read = true;
-		fan_speed_info->supports_rpm_write = true;
-		fan_speed_info->min_rpm = hwmgr->thermal_controller.fanInfo.ulMinRPM;
-		fan_speed_info->max_rpm = hwmgr->thermal_controller.fanInfo.ulMaxRPM;
-	} else {
-		fan_speed_info->min_rpm = 0;
-		fan_speed_info->max_rpm = 0;
-	}
-
-	return 0;
-}
-
-int fiji_fan_ctrl_get_fan_speed_percent(struct pp_hwmgr *hwmgr,
-		uint32_t *speed)
-{
-	uint32_t duty100;
-	uint32_t duty;
-	uint64_t tmp64;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL1, FMAX_DUTY100);
-	duty = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_STATUS, FDO_PWM_DUTY);
-
-	if (duty100 == 0)
-		return -EINVAL;
-
-
-	tmp64 = (uint64_t)duty * 100;
-	do_div(tmp64, duty100);
-	*speed = (uint32_t)tmp64;
-
-	if (*speed > 100)
-		*speed = 100;
-
-	return 0;
-}
-
-int fiji_fan_ctrl_get_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t *speed)
-{
-	uint32_t tach_period;
-	uint32_t crystal_clock_freq;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan ||
-			(hwmgr->thermal_controller.fanInfo.
-				ucTachometerPulsesPerRevolution == 0))
-		return 0;
-
-	tach_period = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_TACH_STATUS, TACH_PERIOD);
-
-	if (tach_period == 0)
-		return -EINVAL;
-
-	crystal_clock_freq = tonga_get_xclk(hwmgr);
-
-	*speed = 60 * crystal_clock_freq * 10000/ tach_period;
-
-	return 0;
-}
-
-/**
-* Set Fan Speed Control to static mode, so that the user can decide what speed to use.
-* @param    hwmgr  the address of the powerplay hardware manager.
-*           mode    the fan control mode, 0 default, 1 by percent, 5, by RPM
-* @exception Should always succeed.
-*/
-int fiji_fan_ctrl_set_static_mode(struct pp_hwmgr *hwmgr, uint32_t mode)
-{
-
-	if (hwmgr->fan_ctrl_is_in_default_mode) {
-		hwmgr->fan_ctrl_default_mode =
-				PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device,	CGS_IND_REG__SMC,
-						CG_FDO_CTRL2, FDO_PWM_MODE);
-		hwmgr->tmin =
-				PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-						CG_FDO_CTRL2, TMIN);
-		hwmgr->fan_ctrl_is_in_default_mode = false;
-	}
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL2, TMIN, 0);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL2, FDO_PWM_MODE, mode);
-
-	return 0;
-}
-
-/**
-* Reset Fan Speed Control to default mode.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @exception Should always succeed.
-*/
-int fiji_fan_ctrl_set_default_mode(struct pp_hwmgr *hwmgr)
-{
-	if (!hwmgr->fan_ctrl_is_in_default_mode) {
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_FDO_CTRL2, FDO_PWM_MODE, hwmgr->fan_ctrl_default_mode);
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_FDO_CTRL2, TMIN, hwmgr->tmin);
-		hwmgr->fan_ctrl_is_in_default_mode = true;
-	}
-
-	return 0;
-}
-
-static int fiji_fan_ctrl_start_smc_fan_control(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ODFuzzyFanControlSupport)) {
-		cgs_write_register(hwmgr->device, mmSMC_MSG_ARG_0, FAN_CONTROL_FUZZY);
-		result = smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StartFanControl);
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_FanSpeedInTableIsRPM))
-			hwmgr->hwmgr_func->set_max_fan_rpm_output(hwmgr,
-					hwmgr->thermal_controller.
-					advanceFanControlParameters.usMaxFanRPM);
-		else
-			hwmgr->hwmgr_func->set_max_fan_pwm_output(hwmgr,
-					hwmgr->thermal_controller.
-					advanceFanControlParameters.usMaxFanPWM);
-
-	} else {
-		cgs_write_register(hwmgr->device, mmSMC_MSG_ARG_0, FAN_CONTROL_TABLE);
-		result = smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StartFanControl);
-	}
-
-	if (!result && hwmgr->thermal_controller.
-			advanceFanControlParameters.ucTargetTemperature)
-		result = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_SetFanTemperatureTarget,
-				hwmgr->thermal_controller.
-				advanceFanControlParameters.ucTargetTemperature);
-
-	return result;
-}
-
-
-int fiji_fan_ctrl_stop_smc_fan_control(struct pp_hwmgr *hwmgr)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StopFanControl);
-}
-
-/**
-* Set Fan Speed in percent.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the percentage value (0% - 100%) to be set.
-* @exception Fails is the 100% setting appears to be 0.
-*/
-int fiji_fan_ctrl_set_fan_speed_percent(struct pp_hwmgr *hwmgr,
-		uint32_t speed)
-{
-	uint32_t duty100;
-	uint32_t duty;
-	uint64_t tmp64;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	if (speed > 100)
-		speed = 100;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MicrocodeFanControl))
-		fiji_fan_ctrl_stop_smc_fan_control(hwmgr);
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL1, FMAX_DUTY100);
-
-	if (duty100 == 0)
-		return -EINVAL;
-
-	tmp64 = (uint64_t)speed * duty100;
-	do_div(tmp64, 100);
-	duty = (uint32_t)tmp64;
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL0, FDO_STATIC_DUTY, duty);
-
-	return fiji_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-}
-
-/**
-* Reset Fan Speed to default.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @exception Always succeeds.
-*/
-int fiji_fan_ctrl_reset_fan_speed_to_default(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MicrocodeFanControl)) {
-		result = fiji_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-		if (!result)
-			result = fiji_fan_ctrl_start_smc_fan_control(hwmgr);
-	} else
-		result = fiji_fan_ctrl_set_default_mode(hwmgr);
-
-	return result;
-}
-
-/**
-* Set Fan Speed in RPM.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the percentage value (min - max) to be set.
-* @exception Fails is the speed not lie between min and max.
-*/
-int fiji_fan_ctrl_set_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t speed)
-{
-	uint32_t tach_period;
-	uint32_t crystal_clock_freq;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan ||
-			(hwmgr->thermal_controller.fanInfo.
-			ucTachometerPulsesPerRevolution == 0) ||
-			(speed < hwmgr->thermal_controller.fanInfo.ulMinRPM) ||
-			(speed > hwmgr->thermal_controller.fanInfo.ulMaxRPM))
-		return 0;
-
-	crystal_clock_freq = tonga_get_xclk(hwmgr);
-
-	tach_period = 60 * crystal_clock_freq * 10000 / (8 * speed);
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_TACH_STATUS, TACH_PERIOD, tach_period);
-
-	return fiji_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-}
-
-/**
-* Reads the remote temperature from the SIslands thermal controller.
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-int fiji_thermal_get_temperature(struct pp_hwmgr *hwmgr)
-{
-	int temp;
-
-	temp = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_MULT_THERMAL_STATUS, CTF_TEMP);
-
-	/* Bit 9 means the reading is lower than the lowest usable value. */
-	if (temp & 0x200)
-		temp = FIJI_THERMAL_MAXIMUM_TEMP_READING;
-	else
-		temp = temp & 0x1ff;
-
-	temp *= PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-
-	return temp;
-}
-
-/**
-* Set the requested temperature range for high and low alert signals
-*
-* @param    hwmgr The address of the hardware manager.
-* @param    range Temperature range to be programmed for high and low alert signals
-* @exception PP_Result_BadInput if the input data is not valid.
-*/
-static int fiji_thermal_set_temperature_range(struct pp_hwmgr *hwmgr,
-		uint32_t low_temp, uint32_t high_temp)
-{
-	uint32_t low = FIJI_THERMAL_MINIMUM_ALERT_TEMP *
-			PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-	uint32_t high = FIJI_THERMAL_MAXIMUM_ALERT_TEMP *
-			PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-
-	if (low < low_temp)
-		low = low_temp;
-	if (high > high_temp)
-		high = high_temp;
-
-	if (low > high)
-		return -EINVAL;
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, DIG_THERM_INTH,
-			(high / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, DIG_THERM_INTL,
-			(low / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_CTRL, DIG_THERM_DPM,
-			(high / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-
-	return 0;
-}
-
-/**
-* Programs thermal controller one-time setting registers
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-static int fiji_thermal_initialize(struct pp_hwmgr *hwmgr)
-{
-	if (hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution)
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_TACH_CTRL, EDGE_PER_REV,
-				hwmgr->thermal_controller.fanInfo.
-				ucTachometerPulsesPerRevolution - 1);
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL2, TACH_PWM_RESP_RATE, 0x28);
-
-	return 0;
-}
-
-/**
-* Enable thermal alerts on the RV770 thermal controller.
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-static int fiji_thermal_enable_alert(struct pp_hwmgr *hwmgr)
-{
-	uint32_t alert;
-
-	alert = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, THERM_INT_MASK);
-	alert &= ~(FIJI_THERMAL_HIGH_ALERT_MASK | FIJI_THERMAL_LOW_ALERT_MASK);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, THERM_INT_MASK, alert);
-
-	/* send message to SMU to enable internal thermal interrupts */
-	return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_Thermal_Cntl_Enable);
-}
-
-/**
-* Disable thermal alerts on the RV770 thermal controller.
-* @param    hwmgr The address of the hardware manager.
-*/
-static int fiji_thermal_disable_alert(struct pp_hwmgr *hwmgr)
-{
-	uint32_t alert;
-
-	alert = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, THERM_INT_MASK);
-	alert |= (FIJI_THERMAL_HIGH_ALERT_MASK | FIJI_THERMAL_LOW_ALERT_MASK);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, THERM_INT_MASK, alert);
-
-	/* send message to SMU to disable internal thermal interrupts */
-	return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_Thermal_Cntl_Disable);
-}
-
-/**
-* Uninitialize the thermal controller.
-* Currently just disables alerts.
-* @param    hwmgr The address of the hardware manager.
-*/
-int fiji_thermal_stop_thermal_controller(struct pp_hwmgr *hwmgr)
-{
-	int result = fiji_thermal_disable_alert(hwmgr);
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		fiji_fan_ctrl_set_default_mode(hwmgr);
-
-	return result;
-}
-
-/**
-* Set up the fan table to control the fan using the SMC.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-static int tf_fiji_thermal_setup_fan_table(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	struct fiji_hwmgr *data = (struct fiji_hwmgr *)(hwmgr->backend);
-	SMU73_Discrete_FanTable fan_table = { FDO_MODE_HARDWARE };
-	uint32_t duty100;
-	uint32_t t_diff1, t_diff2, pwm_diff1, pwm_diff2;
-	uint16_t fdo_min, slope1, slope2;
-	uint32_t reference_clock;
-	int res;
-	uint64_t tmp64;
-
-	if (data->fan_table_start == 0) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl);
-		return 0;
-	}
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL1, FMAX_DUTY100);
-
-	if (duty100 == 0) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl);
-		return 0;
-	}
-
-	tmp64 = hwmgr->thermal_controller.advanceFanControlParameters.
-			usPWMMin * duty100;
-	do_div(tmp64, 10000);
-	fdo_min = (uint16_t)tmp64;
-
-	t_diff1 = hwmgr->thermal_controller.advanceFanControlParameters.usTMed -
-			hwmgr->thermal_controller.advanceFanControlParameters.usTMin;
-	t_diff2 = hwmgr->thermal_controller.advanceFanControlParameters.usTHigh -
-			hwmgr->thermal_controller.advanceFanControlParameters.usTMed;
-
-	pwm_diff1 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMMed -
-			hwmgr->thermal_controller.advanceFanControlParameters.usPWMMin;
-	pwm_diff2 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMHigh -
-			hwmgr->thermal_controller.advanceFanControlParameters.usPWMMed;
-
-	slope1 = (uint16_t)((50 + ((16 * duty100 * pwm_diff1) / t_diff1)) / 100);
-	slope2 = (uint16_t)((50 + ((16 * duty100 * pwm_diff2) / t_diff2)) / 100);
-
-	fan_table.TempMin = cpu_to_be16((50 + hwmgr->
-			thermal_controller.advanceFanControlParameters.usTMin) / 100);
-	fan_table.TempMed = cpu_to_be16((50 + hwmgr->
-			thermal_controller.advanceFanControlParameters.usTMed) / 100);
-	fan_table.TempMax = cpu_to_be16((50 + hwmgr->
-			thermal_controller.advanceFanControlParameters.usTMax) / 100);
-
-	fan_table.Slope1 = cpu_to_be16(slope1);
-	fan_table.Slope2 = cpu_to_be16(slope2);
-
-	fan_table.FdoMin = cpu_to_be16(fdo_min);
-
-	fan_table.HystDown = cpu_to_be16(hwmgr->
-			thermal_controller.advanceFanControlParameters.ucTHyst);
-
-	fan_table.HystUp = cpu_to_be16(1);
-
-	fan_table.HystSlope = cpu_to_be16(1);
-
-	fan_table.TempRespLim = cpu_to_be16(5);
-
-	reference_clock = tonga_get_xclk(hwmgr);
-
-	fan_table.RefreshPeriod = cpu_to_be32((hwmgr->
-			thermal_controller.advanceFanControlParameters.ulCycleDelay *
-			reference_clock) / 1600);
-
-	fan_table.FdoMax = cpu_to_be16((uint16_t)duty100);
-
-	fan_table.TempSrc = (uint8_t)PHM_READ_VFPF_INDIRECT_FIELD(
-			hwmgr->device, CGS_IND_REG__SMC,
-			CG_MULT_THERMAL_CTRL, TEMP_SEL);
-
-	res = fiji_copy_bytes_to_smc(hwmgr->smumgr, data->fan_table_start,
-			(uint8_t *)&fan_table, (uint32_t)sizeof(fan_table),
-			data->sram_end);
-
-	if (!res && hwmgr->thermal_controller.
-			advanceFanControlParameters.ucMinimumPWMLimit)
-		res = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_SetFanMinPwm,
-				hwmgr->thermal_controller.
-				advanceFanControlParameters.ucMinimumPWMLimit);
-
-	if (!res && hwmgr->thermal_controller.
-			advanceFanControlParameters.ulMinFanSCLKAcousticLimit)
-		res = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_SetFanSclkTarget,
-				hwmgr->thermal_controller.
-				advanceFanControlParameters.ulMinFanSCLKAcousticLimit);
-
-	if (res)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl);
-
-	return 0;
-}
-
-/**
-* Start the fan control on the SMC.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-static int tf_fiji_thermal_start_smc_fan_control(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-/* If the fantable setup has failed we could have disabled
- * PHM_PlatformCaps_MicrocodeFanControl even after
- * this function was included in the table.
- * Make sure that we still think controlling the fan is OK.
-*/
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MicrocodeFanControl)) {
-		fiji_fan_ctrl_start_smc_fan_control(hwmgr);
-		fiji_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-	}
-
-	return 0;
-}
-
-/**
-* Set temperature range for high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-int tf_fiji_thermal_set_temperature_range(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	struct PP_TemperatureRange *range = (struct PP_TemperatureRange *)input;
-
-	if (range == NULL)
-		return -EINVAL;
-
-	return fiji_thermal_set_temperature_range(hwmgr, range->min, range->max);
-}
-
-/**
-* Programs one-time setting registers
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from initialize thermal controller routine
-*/
-int tf_fiji_thermal_initialize(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-    return fiji_thermal_initialize(hwmgr);
-}
-
-/**
-* Enable high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from enable alert routine
-*/
-int tf_fiji_thermal_enable_alert(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	return fiji_thermal_enable_alert(hwmgr);
-}
-
-/**
-* Disable high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from disable alert routine
-*/
-static int tf_fiji_thermal_disable_alert(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	return fiji_thermal_disable_alert(hwmgr);
-}
-
-static const struct phm_master_table_item
-fiji_thermal_start_thermal_controller_master_list[] = {
-	{NULL, tf_fiji_thermal_initialize},
-	{NULL, tf_fiji_thermal_set_temperature_range},
-	{NULL, tf_fiji_thermal_enable_alert},
-/* We should restrict performance levels to low before we halt the SMC.
- * On the other hand we are still in boot state when we do this
- * so it would be pointless.
- * If this assumption changes we have to revisit this table.
- */
-	{NULL, tf_fiji_thermal_setup_fan_table},
-	{NULL, tf_fiji_thermal_start_smc_fan_control},
-	{NULL, NULL}
-};
-
-static const struct phm_master_table_header
-fiji_thermal_start_thermal_controller_master = {
-	0,
-	PHM_MasterTableFlag_None,
-	fiji_thermal_start_thermal_controller_master_list
-};
-
-static const struct phm_master_table_item
-fiji_thermal_set_temperature_range_master_list[] = {
-	{NULL, tf_fiji_thermal_disable_alert},
-	{NULL, tf_fiji_thermal_set_temperature_range},
-	{NULL, tf_fiji_thermal_enable_alert},
-	{NULL, NULL}
-};
-
-static const struct phm_master_table_header
-fiji_thermal_set_temperature_range_master = {
-	0,
-	PHM_MasterTableFlag_None,
-	fiji_thermal_set_temperature_range_master_list
-};
-
-int fiji_thermal_ctrl_uninitialize_thermal_controller(struct pp_hwmgr *hwmgr)
-{
-	if (!hwmgr->thermal_controller.fanInfo.bNoFan)
-		fiji_fan_ctrl_set_default_mode(hwmgr);
-	return 0;
-}
-
-/**
-* Initializes the thermal controller related functions in the Hardware Manager structure.
-* @param    hwmgr The address of the hardware manager.
-* @exception Any error code from the low-level communication.
-*/
-int pp_fiji_thermal_initialize(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	result = phm_construct_table(hwmgr,
-			&fiji_thermal_set_temperature_range_master,
-			&(hwmgr->set_temperature_range));
-
-	if (!result) {
-		result = phm_construct_table(hwmgr,
-				&fiji_thermal_start_thermal_controller_master,
-				&(hwmgr->start_thermal_controller));
-		if (result)
-			phm_destroy_table(hwmgr, &(hwmgr->set_temperature_range));
-	}
-
-	if (!result)
-		hwmgr->fan_ctrl_is_in_default_mode = true;
-	return result;
-}
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.h b/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.h
deleted file mode 100644
index 8621493..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/fiji_thermal.h
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef FIJI_THERMAL_H
-#define FIJI_THERMAL_H
-
-#include "hwmgr.h"
-
-#define FIJI_THERMAL_HIGH_ALERT_MASK         0x1
-#define FIJI_THERMAL_LOW_ALERT_MASK          0x2
-
-#define FIJI_THERMAL_MINIMUM_TEMP_READING    -256
-#define FIJI_THERMAL_MAXIMUM_TEMP_READING    255
-
-#define FIJI_THERMAL_MINIMUM_ALERT_TEMP      0
-#define FIJI_THERMAL_MAXIMUM_ALERT_TEMP      255
-
-#define FDO_PWM_MODE_STATIC  1
-#define FDO_PWM_MODE_STATIC_RPM 5
-
-
-extern int tf_fiji_thermal_initialize(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result);
-extern int tf_fiji_thermal_set_temperature_range(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result);
-extern int tf_fiji_thermal_enable_alert(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result);
-
-extern int fiji_thermal_get_temperature(struct pp_hwmgr *hwmgr);
-extern int fiji_thermal_stop_thermal_controller(struct pp_hwmgr *hwmgr);
-extern int fiji_fan_ctrl_get_fan_speed_info(struct pp_hwmgr *hwmgr, struct phm_fan_speed_info *fan_speed_info);
-extern int fiji_fan_ctrl_get_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t *speed);
-extern int fiji_fan_ctrl_set_default_mode(struct pp_hwmgr *hwmgr);
-extern int fiji_fan_ctrl_set_static_mode(struct pp_hwmgr *hwmgr, uint32_t mode);
-extern int fiji_fan_ctrl_set_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t speed);
-extern int fiji_fan_ctrl_reset_fan_speed_to_default(struct pp_hwmgr *hwmgr);
-extern int pp_fiji_thermal_initialize(struct pp_hwmgr *hwmgr);
-extern int fiji_thermal_ctrl_uninitialize_thermal_controller(struct pp_hwmgr *hwmgr);
-extern int fiji_fan_ctrl_set_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t speed);
-extern int fiji_fan_ctrl_get_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t *speed);
-extern int fiji_fan_ctrl_stop_smc_fan_control(struct pp_hwmgr *hwmgr);
-extern uint32_t tonga_get_xclk(struct pp_hwmgr *hwmgr);
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c b/drivers/gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c
index b196f1d..0eb8e886 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/hardwaremanager.c
@@ -146,37 +146,12 @@ int phm_disable_dynamic_state_management(struct pp_hwmgr *hwmgr)
 
 int phm_force_dpm_levels(struct pp_hwmgr *hwmgr, enum amd_dpm_forced_level level)
 {
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	int ret = 0;
-
-	PHM_FUNC_CHECK(hwmgr);
-
-	if (hwmgr->hwmgr_func->force_dpm_level != NULL) {
-		ret = hwmgr->hwmgr_func->force_dpm_level(hwmgr, level);
-		if (ret)
-			return ret;
-
-		if (hwmgr->hwmgr_func->set_power_profile_state) {
-			if (hwmgr->current_power_profile == PP_GFX_PROFILE)
-				ret = hwmgr->hwmgr_func->set_power_profile_state(
-						hwmgr,
-						&hwmgr->gfx_power_profile);
-			else
-				ret = hwmgr->hwmgr_func->set_power_profile_state(
-						hwmgr,
-						&hwmgr->compute_power_profile);
-		}
-	}
-
-	return ret;
-#else
 	PHM_FUNC_CHECK(hwmgr);
 
 	if (hwmgr->hwmgr_func->force_dpm_level != NULL)
 		return hwmgr->hwmgr_func->force_dpm_level(hwmgr, level);
 
 	return 0;
-#endif
 }
 
 int phm_apply_state_adjust_rules(struct pp_hwmgr *hwmgr,
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/hwmgr.c
index 086874b..bd4ca68 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/hwmgr.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/hwmgr.c
@@ -713,8 +713,10 @@ int phm_get_voltage_evv_on_sclk(struct pp_hwmgr *hwmgr, uint8_t voltage_type,
 	uint32_t vol;
 	int ret = 0;
 
-	if (hwmgr->chip_id < CHIP_POLARIS10) {
-		atomctrl_get_voltage_evv_on_sclk(hwmgr, voltage_type, sclk, id, voltage);
+	if (hwmgr->chip_id < CHIP_TONGA) {
+		ret = atomctrl_get_voltage_evv(hwmgr, id, voltage);
+	} else if (hwmgr->chip_id < CHIP_POLARIS10) {
+		ret = atomctrl_get_voltage_evv_on_sclk(hwmgr, voltage_type, sclk, id, voltage);
 		if (*voltage >= 2000 || *voltage == 0)
 			*voltage = 1150;
 	} else {
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_clockpowergating.c b/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_clockpowergating.c
deleted file mode 100644
index 47949f5..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_clockpowergating.c
+++ /dev/null
@@ -1,119 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Author: Huang Rui <ray.huang@amd.com>
- *
- */
-
-#include "hwmgr.h"
-#include "iceland_clockpowergating.h"
-#include "ppsmc.h"
-#include "iceland_hwmgr.h"
-
-int iceland_phm_powerdown_uvd(struct pp_hwmgr *hwmgr)
-{
-	/* iceland does not have MM hardware block */
-	return 0;
-}
-
-static int iceland_phm_powerup_uvd(struct pp_hwmgr *hwmgr)
-{
-	/* iceland does not have MM hardware block */
-	return 0;
-}
-
-static int iceland_phm_powerdown_vce(struct pp_hwmgr *hwmgr)
-{
-	/* iceland does not have MM hardware block */
-	return 0;
-}
-
-static int iceland_phm_powerup_vce(struct pp_hwmgr *hwmgr)
-{
-	/* iceland does not have MM hardware block */
-	return 0;
-}
-
-int iceland_phm_set_asic_block_gating(struct pp_hwmgr *hwmgr, enum
-		PHM_AsicBlock block, enum PHM_ClockGateSetting gating)
-{
-	int ret = 0;
-
-	switch (block) {
-	case PHM_AsicBlock_UVD_MVC:
-	case PHM_AsicBlock_UVD:
-	case PHM_AsicBlock_UVD_HD:
-	case PHM_AsicBlock_UVD_SD:
-		if (gating == PHM_ClockGateSetting_StaticOff)
-			ret = iceland_phm_powerdown_uvd(hwmgr);
-		else
-			ret = iceland_phm_powerup_uvd(hwmgr);
-		break;
-	case PHM_AsicBlock_GFX:
-	default:
-		break;
-	}
-
-	return ret;
-}
-
-int iceland_phm_disable_clock_power_gating(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	data->uvd_power_gated = false;
-	data->vce_power_gated = false;
-
-	iceland_phm_powerup_uvd(hwmgr);
-	iceland_phm_powerup_vce(hwmgr);
-
-	return 0;
-}
-
-int iceland_phm_powergate_uvd(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	if (bgate) {
-		iceland_update_uvd_dpm(hwmgr, true);
-		iceland_phm_powerdown_uvd(hwmgr);
-	} else {
-		iceland_phm_powerup_uvd(hwmgr);
-		iceland_update_uvd_dpm(hwmgr, false);
-	}
-
-	return 0;
-}
-
-int iceland_phm_powergate_vce(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	if (bgate)
-		return iceland_phm_powerdown_vce(hwmgr);
-	else
-		return iceland_phm_powerup_vce(hwmgr);
-
-	return 0;
-}
-
-int iceland_phm_update_clock_gatings(struct pp_hwmgr *hwmgr,
-					const uint32_t *msg_id)
-{
-	/* iceland does not have MM hardware block */
-	return 0;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_clockpowergating.h b/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_clockpowergating.h
deleted file mode 100644
index ff5ef00..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_clockpowergating.h
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Author: Huang Rui <ray.huang@amd.com>
- *
- */
-
-#ifndef _ICELAND_CLOCK_POWER_GATING_H_
-#define _ICELAND_CLOCK_POWER_GATING_H_
-
-#include "iceland_hwmgr.h"
-#include "pp_asicblocks.h"
-
-extern int iceland_phm_set_asic_block_gating(struct pp_hwmgr *hwmgr, enum PHM_AsicBlock block, enum PHM_ClockGateSetting gating);
-extern int iceland_phm_powergate_vce(struct pp_hwmgr *hwmgr, bool bgate);
-extern int iceland_phm_powergate_uvd(struct pp_hwmgr *hwmgr, bool bgate);
-extern int iceland_phm_powerdown_uvd(struct pp_hwmgr *hwmgr);
-extern int iceland_phm_disable_clock_power_gating(struct pp_hwmgr *hwmgr);
-extern int iceland_phm_update_clock_gatings(struct pp_hwmgr *hwmgr, const uint32_t *msg_id);
-#endif /* _ICELAND_CLOCK_POWER_GATING_H_ */
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_dyn_defaults.h b/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_dyn_defaults.h
deleted file mode 100644
index a7b4bc6..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_dyn_defaults.h
+++ /dev/null
@@ -1,41 +0,0 @@
-#ifndef ICELAND_DYN_DEFAULTS_H
-#define ICELAND_DYN_DEFAULTS_H
-
-enum ICELANDdpm_TrendDetection
-{
-	ICELANDdpm_TrendDetection_AUTO,
-	ICELANDdpm_TrendDetection_UP,
-	ICELANDdpm_TrendDetection_DOWN
-};
-typedef enum ICELANDdpm_TrendDetection ICELANDdpm_TrendDetection;
-
-
-#define PPICELAND_VOTINGRIGHTSCLIENTS_DFLT0         0x3FFFC102
-#define PPICELAND_VOTINGRIGHTSCLIENTS_DFLT1         0x000400
-#define PPICELAND_VOTINGRIGHTSCLIENTS_DFLT2         0xC00080
-#define PPICELAND_VOTINGRIGHTSCLIENTS_DFLT3         0xC00200
-#define PPICELAND_VOTINGRIGHTSCLIENTS_DFLT4         0xC01680
-#define PPICELAND_VOTINGRIGHTSCLIENTS_DFLT5         0xC00033
-#define PPICELAND_VOTINGRIGHTSCLIENTS_DFLT6         0xC00033
-#define PPICELAND_VOTINGRIGHTSCLIENTS_DFLT7         0x3FFFC000
-
-
-#define PPICELAND_THERMALPROTECTCOUNTER_DFLT        0x200
-
-#define PPICELAND_STATICSCREENTHRESHOLDUNIT_DFLT    0
-
-#define PPICELAND_STATICSCREENTHRESHOLD_DFLT        0x00C8
-
-#define PPICELAND_GFXIDLECLOCKSTOPTHRESHOLD_DFLT    0x200
-
-#define PPICELAND_REFERENCEDIVIDER_DFLT             4
-
-#define PPICELAND_ULVVOLTAGECHANGEDELAY_DFLT        1687
-
-#define PPICELAND_CGULVPARAMETER_DFLT               0x00040035
-#define PPICELAND_CGULVCONTROL_DFLT                 0x00007450
-#define PPICELAND_TARGETACTIVITY_DFLT               30
-#define PPICELAND_MCLK_TARGETACTIVITY_DFLT          10
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.c
deleted file mode 100644
index 50aa23f..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.c
+++ /dev/null
@@ -1,5666 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Author: Huang Rui <ray.huang@amd.com>
- *
- */
-#include <linux/module.h>
-#include <linux/slab.h>
-#include <linux/fb.h>
-#include "linux/delay.h"
-#include "pp_acpi.h"
-#include "hwmgr.h"
-#include <atombios.h>
-#include "iceland_hwmgr.h"
-#include "pptable.h"
-#include "processpptables.h"
-#include "pp_debug.h"
-#include "ppsmc.h"
-#include "cgs_common.h"
-#include "pppcielanes.h"
-#include "iceland_dyn_defaults.h"
-#include "smumgr.h"
-#include "iceland_smumgr.h"
-#include "iceland_clockpowergating.h"
-#include "iceland_thermal.h"
-#include "iceland_powertune.h"
-
-#include "gmc/gmc_8_1_d.h"
-#include "gmc/gmc_8_1_sh_mask.h"
-
-#include "bif/bif_5_0_d.h"
-#include "bif/bif_5_0_sh_mask.h"
-
-#include "smu/smu_7_1_1_d.h"
-#include "smu/smu_7_1_1_sh_mask.h"
-
-#include "cgs_linux.h"
-#include "eventmgr.h"
-#include "amd_pcie_helpers.h"
-
-#define MC_CG_ARB_FREQ_F0           0x0a
-#define MC_CG_ARB_FREQ_F1           0x0b
-#define MC_CG_ARB_FREQ_F2           0x0c
-#define MC_CG_ARB_FREQ_F3           0x0d
-
-#define MC_CG_SEQ_DRAMCONF_S0       0x05
-#define MC_CG_SEQ_DRAMCONF_S1       0x06
-#define MC_CG_SEQ_YCLK_SUSPEND      0x04
-#define MC_CG_SEQ_YCLK_RESUME       0x0a
-
-#define PCIE_BUS_CLK                10000
-#define TCLK                        (PCIE_BUS_CLK / 10)
-
-#define SMC_RAM_END		    0x40000
-#define SMC_CG_IND_START            0xc0030000
-#define SMC_CG_IND_END              0xc0040000  /* First byte after SMC_CG_IND*/
-
-#define VOLTAGE_SCALE               4
-#define VOLTAGE_VID_OFFSET_SCALE1   625
-#define VOLTAGE_VID_OFFSET_SCALE2   100
-
-const uint32_t iceland_magic = (uint32_t)(PHM_VIslands_Magic);
-
-#define MC_SEQ_MISC0_GDDR5_SHIFT 28
-#define MC_SEQ_MISC0_GDDR5_MASK  0xf0000000
-#define MC_SEQ_MISC0_GDDR5_VALUE 5
-
-/** Values for the CG_THERMAL_CTRL::DPM_EVENT_SRC field. */
-enum DPM_EVENT_SRC {
-    DPM_EVENT_SRC_ANALOG = 0,               /* Internal analog trip point */
-    DPM_EVENT_SRC_EXTERNAL = 1,             /* External (GPIO 17) signal */
-    DPM_EVENT_SRC_DIGITAL = 2,              /* Internal digital trip point (DIG_THERM_DPM) */
-    DPM_EVENT_SRC_ANALOG_OR_EXTERNAL = 3,   /* Internal analog or external */
-    DPM_EVENT_SRC_DIGITAL_OR_EXTERNAL = 4   /* Internal digital or external */
-};
-
-static int iceland_read_clock_registers(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	data->clock_registers.vCG_SPLL_FUNC_CNTL         =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL);
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_2       =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_2);
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_3       =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_3);
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_4       =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_4);
-	data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM   =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_SPREAD_SPECTRUM);
-	data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2 =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_SPREAD_SPECTRUM_2);
-	data->clock_registers.vDLL_CNTL                  =
-		cgs_read_register(hwmgr->device, mmDLL_CNTL);
-	data->clock_registers.vMCLK_PWRMGT_CNTL          =
-		cgs_read_register(hwmgr->device, mmMCLK_PWRMGT_CNTL);
-	data->clock_registers.vMPLL_AD_FUNC_CNTL         =
-		cgs_read_register(hwmgr->device, mmMPLL_AD_FUNC_CNTL);
-	data->clock_registers.vMPLL_DQ_FUNC_CNTL         =
-		cgs_read_register(hwmgr->device, mmMPLL_DQ_FUNC_CNTL);
-	data->clock_registers.vMPLL_FUNC_CNTL            =
-		cgs_read_register(hwmgr->device, mmMPLL_FUNC_CNTL);
-	data->clock_registers.vMPLL_FUNC_CNTL_1          =
-		cgs_read_register(hwmgr->device, mmMPLL_FUNC_CNTL_1);
-	data->clock_registers.vMPLL_FUNC_CNTL_2          =
-		cgs_read_register(hwmgr->device, mmMPLL_FUNC_CNTL_2);
-	data->clock_registers.vMPLL_SS1                  =
-		cgs_read_register(hwmgr->device, mmMPLL_SS1);
-	data->clock_registers.vMPLL_SS2                  =
-		cgs_read_register(hwmgr->device, mmMPLL_SS2);
-
-	return 0;
-}
-
-/**
- * Find out if memory is GDDR5.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int iceland_get_memory_type(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	uint32_t temp;
-
-	temp = cgs_read_register(hwmgr->device, mmMC_SEQ_MISC0);
-
-	data->is_memory_GDDR5 = (MC_SEQ_MISC0_GDDR5_VALUE ==
-			((temp & MC_SEQ_MISC0_GDDR5_MASK) >>
-			 MC_SEQ_MISC0_GDDR5_SHIFT));
-
-	return 0;
-}
-
-int iceland_update_uvd_dpm(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	/* iceland does not have MM hardware blocks */
-	return 0;
-}
-
-/**
- * Enables Dynamic Power Management by SMC
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int iceland_enable_acpi_power_management(struct pp_hwmgr *hwmgr)
-{
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT, STATIC_PM_EN, 1);
-
-	return 0;
-}
-
-/**
- * Find the MC microcode version and store it in the HwMgr struct
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int iceland_get_mc_microcode_version(struct pp_hwmgr *hwmgr)
-{
-	cgs_write_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_INDEX, 0x9F);
-
-	hwmgr->microcode_version_info.MC = cgs_read_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_DATA);
-
-	return 0;
-}
-
-static int iceland_init_sclk_threshold(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	data->low_sclk_interrupt_threshold = 0;
-
-	return 0;
-}
-
-
-static int iceland_setup_asic_task(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = iceland_read_clock_registers(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to read clock registers!", result = tmp_result);
-
-	tmp_result = iceland_get_memory_type(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to get memory type!", result = tmp_result);
-
-	tmp_result = iceland_enable_acpi_power_management(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to enable ACPI power management!", result = tmp_result);
-
-	tmp_result = iceland_get_mc_microcode_version(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to get MC microcode version!", result = tmp_result);
-
-	tmp_result = iceland_init_sclk_threshold(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to init sclk threshold!", result = tmp_result);
-
-	return result;
-}
-
-static bool cf_iceland_voltage_control(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	return ICELAND_VOLTAGE_CONTROL_NONE != data->voltage_control;
-}
-
-/*
- * -------------- Voltage Tables ----------------------
- * If the voltage table would be bigger than what will fit into the
- * state table on the SMC keep only the higher entries.
- */
-
-static void iceland_trim_voltage_table_to_fit_state_table(
-		struct pp_hwmgr *hwmgr,
-		uint32_t max_voltage_steps,
-		pp_atomctrl_voltage_table *voltage_table)
-{
-	unsigned int i, diff;
-
-	if (voltage_table->count <= max_voltage_steps) {
-		return;
-	}
-
-	diff = voltage_table->count - max_voltage_steps;
-
-	for (i = 0; i < max_voltage_steps; i++) {
-		voltage_table->entries[i] = voltage_table->entries[i + diff];
-	}
-
-	voltage_table->count = max_voltage_steps;
-
-	return;
-}
-
-/**
- * Enable voltage control
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int iceland_enable_voltage_control(struct pp_hwmgr *hwmgr)
-{
-	/* enable voltage control */
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT, VOLT_PWRMGT_EN, 1);
-
-	return 0;
-}
-
-static int iceland_get_svi2_voltage_table(struct pp_hwmgr *hwmgr,
-		struct phm_clock_voltage_dependency_table *voltage_dependency_table,
-		pp_atomctrl_voltage_table *voltage_table)
-{
-	uint32_t i;
-
-	PP_ASSERT_WITH_CODE((NULL != voltage_table),
-			"Voltage Dependency Table empty.", return -EINVAL;);
-
-	voltage_table->mask_low = 0;
-	voltage_table->phase_delay = 0;
-	voltage_table->count = voltage_dependency_table->count;
-
-	for (i = 0; i < voltage_dependency_table->count; i++) {
-		voltage_table->entries[i].value =
-			voltage_dependency_table->entries[i].v;
-		voltage_table->entries[i].smio_low = 0;
-	}
-
-	return 0;
-}
-
-/**
- * Create Voltage Tables.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int iceland_construct_voltage_tables(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	int result;
-
-	/* GPIO voltage */
-	if (ICELAND_VOLTAGE_CONTROL_BY_GPIO == data->voltage_control) {
-		result = atomctrl_get_voltage_table_v3(hwmgr,
-					VOLTAGE_TYPE_VDDC, VOLTAGE_OBJ_GPIO_LUT,
-					&data->vddc_voltage_table);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve VDDC table.", return result;);
-	} else if (ICELAND_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {
-		/* SVI2 VDDC voltage */
-		result = iceland_get_svi2_voltage_table(hwmgr,
-					hwmgr->dyn_state.vddc_dependency_on_mclk,
-					&data->vddc_voltage_table);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve SVI2 VDDC table from dependancy table.", return result;);
-	}
-
-	PP_ASSERT_WITH_CODE(
-			(data->vddc_voltage_table.count <= (SMU71_MAX_LEVELS_VDDC)),
-			"Too many voltage values for VDDC. Trimming to fit state table.",
-			iceland_trim_voltage_table_to_fit_state_table(hwmgr,
-			SMU71_MAX_LEVELS_VDDC, &(data->vddc_voltage_table));
-			);
-
-	/* GPIO */
-	if (ICELAND_VOLTAGE_CONTROL_BY_GPIO == data->vdd_ci_control) {
-		result = atomctrl_get_voltage_table_v3(hwmgr,
-					VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_GPIO_LUT, &(data->vddci_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve VDDCI table.", return result;);
-	}
-
-	/* SVI2 VDDCI voltage */
-	if (ICELAND_VOLTAGE_CONTROL_BY_SVID2 == data->vdd_ci_control) {
-		result = iceland_get_svi2_voltage_table(hwmgr,
-					hwmgr->dyn_state.vddci_dependency_on_mclk,
-					&data->vddci_voltage_table);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve SVI2 VDDCI table from dependancy table.", return result;);
-	}
-
-	PP_ASSERT_WITH_CODE(
-			(data->vddci_voltage_table.count <= (SMU71_MAX_LEVELS_VDDCI)),
-			"Too many voltage values for VDDCI. Trimming to fit state table.",
-			iceland_trim_voltage_table_to_fit_state_table(hwmgr,
-			SMU71_MAX_LEVELS_VDDCI, &(data->vddci_voltage_table));
-			);
-
-
-	/* GPIO */
-	if (ICELAND_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control) {
-		result = atomctrl_get_voltage_table_v3(hwmgr,
-					VOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_GPIO_LUT, &(data->mvdd_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve table.", return result;);
-	}
-
-	/* SVI2 voltage control */
-	if (ICELAND_VOLTAGE_CONTROL_BY_SVID2 == data->mvdd_control) {
-		result = iceland_get_svi2_voltage_table(hwmgr,
-					hwmgr->dyn_state.mvdd_dependency_on_mclk,
-					&data->mvdd_voltage_table);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve SVI2 MVDD table from dependancy table.", return result;);
-	}
-
-	PP_ASSERT_WITH_CODE(
-			(data->mvdd_voltage_table.count <= (SMU71_MAX_LEVELS_MVDD)),
-			"Too many voltage values for MVDD. Trimming to fit state table.",
-			iceland_trim_voltage_table_to_fit_state_table(hwmgr,
-			SMU71_MAX_LEVELS_MVDD, &(data->mvdd_voltage_table));
-			);
-
-	return 0;
-}
-
-/*---------------------------MC----------------------------*/
-
-uint8_t iceland_get_memory_module_index(struct pp_hwmgr *hwmgr)
-{
-	return (uint8_t) (0xFF & (cgs_read_register(hwmgr->device, mmBIOS_SCRATCH_4) >> 16));
-}
-
-bool iceland_check_s0_mc_reg_index(uint16_t inReg, uint16_t *outReg)
-{
-	bool result = true;
-
-	switch (inReg) {
-	case  mmMC_SEQ_RAS_TIMING:
-		*outReg = mmMC_SEQ_RAS_TIMING_LP;
-		break;
-
-	case  mmMC_SEQ_DLL_STBY:
-		*outReg = mmMC_SEQ_DLL_STBY_LP;
-		break;
-
-	case  mmMC_SEQ_G5PDX_CMD0:
-		*outReg = mmMC_SEQ_G5PDX_CMD0_LP;
-		break;
-
-	case  mmMC_SEQ_G5PDX_CMD1:
-		*outReg = mmMC_SEQ_G5PDX_CMD1_LP;
-		break;
-
-	case  mmMC_SEQ_G5PDX_CTRL:
-		*outReg = mmMC_SEQ_G5PDX_CTRL_LP;
-		break;
-
-	case mmMC_SEQ_CAS_TIMING:
-		*outReg = mmMC_SEQ_CAS_TIMING_LP;
-		break;
-
-	case mmMC_SEQ_MISC_TIMING:
-		*outReg = mmMC_SEQ_MISC_TIMING_LP;
-		break;
-
-	case mmMC_SEQ_MISC_TIMING2:
-		*outReg = mmMC_SEQ_MISC_TIMING2_LP;
-		break;
-
-	case mmMC_SEQ_PMG_DVS_CMD:
-		*outReg = mmMC_SEQ_PMG_DVS_CMD_LP;
-		break;
-
-	case mmMC_SEQ_PMG_DVS_CTL:
-		*outReg = mmMC_SEQ_PMG_DVS_CTL_LP;
-		break;
-
-	case mmMC_SEQ_RD_CTL_D0:
-		*outReg = mmMC_SEQ_RD_CTL_D0_LP;
-		break;
-
-	case mmMC_SEQ_RD_CTL_D1:
-		*outReg = mmMC_SEQ_RD_CTL_D1_LP;
-		break;
-
-	case mmMC_SEQ_WR_CTL_D0:
-		*outReg = mmMC_SEQ_WR_CTL_D0_LP;
-		break;
-
-	case mmMC_SEQ_WR_CTL_D1:
-		*outReg = mmMC_SEQ_WR_CTL_D1_LP;
-		break;
-
-	case mmMC_PMG_CMD_EMRS:
-		*outReg = mmMC_SEQ_PMG_CMD_EMRS_LP;
-		break;
-
-	case mmMC_PMG_CMD_MRS:
-		*outReg = mmMC_SEQ_PMG_CMD_MRS_LP;
-		break;
-
-	case mmMC_PMG_CMD_MRS1:
-		*outReg = mmMC_SEQ_PMG_CMD_MRS1_LP;
-		break;
-
-	case mmMC_SEQ_PMG_TIMING:
-		*outReg = mmMC_SEQ_PMG_TIMING_LP;
-		break;
-
-	case mmMC_PMG_CMD_MRS2:
-		*outReg = mmMC_SEQ_PMG_CMD_MRS2_LP;
-		break;
-
-	case mmMC_SEQ_WR_CTL_2:
-		*outReg = mmMC_SEQ_WR_CTL_2_LP;
-		break;
-
-	default:
-		result = false;
-		break;
-	}
-
-	return result;
-}
-
-int iceland_set_s0_mc_reg_index(phw_iceland_mc_reg_table *table)
-{
-	uint32_t i;
-	uint16_t address;
-
-	for (i = 0; i < table->last; i++) {
-		table->mc_reg_address[i].s0 =
-			iceland_check_s0_mc_reg_index(table->mc_reg_address[i].s1, &address)
-			? address : table->mc_reg_address[i].s1;
-	}
-	return 0;
-}
-
-int iceland_copy_vbios_smc_reg_table(const pp_atomctrl_mc_reg_table *table, phw_iceland_mc_reg_table *ni_table)
-{
-	uint8_t i, j;
-
-	PP_ASSERT_WITH_CODE((table->last <= SMU71_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-		"Invalid VramInfo table.", return -1);
-	PP_ASSERT_WITH_CODE((table->num_entries <= MAX_AC_TIMING_ENTRIES),
-		"Invalid VramInfo table.", return -1);
-
-	for (i = 0; i < table->last; i++) {
-		ni_table->mc_reg_address[i].s1 = table->mc_reg_address[i].s1;
-	}
-	ni_table->last = table->last;
-
-	for (i = 0; i < table->num_entries; i++) {
-		ni_table->mc_reg_table_entry[i].mclk_max =
-			table->mc_reg_table_entry[i].mclk_max;
-		for (j = 0; j < table->last; j++) {
-			ni_table->mc_reg_table_entry[i].mc_data[j] =
-				table->mc_reg_table_entry[i].mc_data[j];
-		}
-	}
-
-	ni_table->num_entries = table->num_entries;
-
-	return 0;
-}
-
-/**
- * VBIOS omits some information to reduce size, we need to recover them here.
- * 1.   when we see mmMC_SEQ_MISC1, bit[31:16] EMRS1, need to be write to  mmMC_PMG_CMD_EMRS /_LP[15:0].
- *      Bit[15:0] MRS, need to be update mmMC_PMG_CMD_MRS/_LP[15:0]
- * 2.   when we see mmMC_SEQ_RESERVE_M, bit[15:0] EMRS2, need to be write to mmMC_PMG_CMD_MRS1/_LP[15:0].
- * 3.   need to set these data for each clock range
- *
- * @param    hwmgr the address of the powerplay hardware manager.
- * @param    table the address of MCRegTable
- * @return   always 0
- */
-static int iceland_set_mc_special_registers(struct pp_hwmgr *hwmgr, phw_iceland_mc_reg_table *table)
-{
-	uint8_t i, j, k;
-	uint32_t temp_reg;
-	const iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	for (i = 0, j = table->last; i < table->last; i++) {
-		PP_ASSERT_WITH_CODE((j < SMU71_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-			"Invalid VramInfo table.", return -1);
-		switch (table->mc_reg_address[i].s1) {
-		/*
-		 * mmMC_SEQ_MISC1, bit[31:16] EMRS1, need to be write
-		 * to mmMC_PMG_CMD_EMRS/_LP[15:0]. Bit[15:0] MRS, need
-		 * to be update mmMC_PMG_CMD_MRS/_LP[15:0]
-		 */
-		case mmMC_SEQ_MISC1:
-			temp_reg = cgs_read_register(hwmgr->device, mmMC_PMG_CMD_EMRS);
-			table->mc_reg_address[j].s1 = mmMC_PMG_CMD_EMRS;
-			table->mc_reg_address[j].s0 = mmMC_SEQ_PMG_CMD_EMRS_LP;
-			for (k = 0; k < table->num_entries; k++) {
-				table->mc_reg_table_entry[k].mc_data[j] =
-					((temp_reg & 0xffff0000)) |
-					((table->mc_reg_table_entry[k].mc_data[i] & 0xffff0000) >> 16);
-			}
-			j++;
-			PP_ASSERT_WITH_CODE((j < SMU71_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-				"Invalid VramInfo table.", return -1);
-
-			temp_reg = cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS);
-			table->mc_reg_address[j].s1 = mmMC_PMG_CMD_MRS;
-			table->mc_reg_address[j].s0 = mmMC_SEQ_PMG_CMD_MRS_LP;
-			for (k = 0; k < table->num_entries; k++) {
-				table->mc_reg_table_entry[k].mc_data[j] =
-					(temp_reg & 0xffff0000) |
-					(table->mc_reg_table_entry[k].mc_data[i] & 0x0000ffff);
-
-				if (!data->is_memory_GDDR5) {
-					table->mc_reg_table_entry[k].mc_data[j] |= 0x100;
-				}
-			}
-			j++;
-			PP_ASSERT_WITH_CODE((j <= SMU71_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-				"Invalid VramInfo table.", return -1);
-
-			if (!data->is_memory_GDDR5) {
-				table->mc_reg_address[j].s1 = mmMC_PMG_AUTO_CMD;
-				table->mc_reg_address[j].s0 = mmMC_PMG_AUTO_CMD;
-				for (k = 0; k < table->num_entries; k++) {
-					table->mc_reg_table_entry[k].mc_data[j] =
-						(table->mc_reg_table_entry[k].mc_data[i] & 0xffff0000) >> 16;
-				}
-				j++;
-				PP_ASSERT_WITH_CODE((j <= SMU71_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-					"Invalid VramInfo table.", return -1);
-			}
-
-			break;
-
-		case mmMC_SEQ_RESERVE_M:
-			temp_reg = cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS1);
-			table->mc_reg_address[j].s1 = mmMC_PMG_CMD_MRS1;
-			table->mc_reg_address[j].s0 = mmMC_SEQ_PMG_CMD_MRS1_LP;
-			for (k = 0; k < table->num_entries; k++) {
-				table->mc_reg_table_entry[k].mc_data[j] =
-					(temp_reg & 0xffff0000) |
-					(table->mc_reg_table_entry[k].mc_data[i] & 0x0000ffff);
-			}
-			j++;
-			PP_ASSERT_WITH_CODE((j <= SMU71_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-				"Invalid VramInfo table.", return -1);
-			break;
-
-		default:
-			break;
-		}
-
-	}
-
-	table->last = j;
-
-	return 0;
-}
-
-
-static int iceland_set_valid_flag(phw_iceland_mc_reg_table *table)
-{
-	uint8_t i, j;
-	for (i = 0; i < table->last; i++) {
-		for (j = 1; j < table->num_entries; j++) {
-			if (table->mc_reg_table_entry[j-1].mc_data[i] !=
-				table->mc_reg_table_entry[j].mc_data[i]) {
-				table->validflag |= (1<<i);
-				break;
-			}
-		}
-	}
-
-	return 0;
-}
-
-static int iceland_initialize_mc_reg_table(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	pp_atomctrl_mc_reg_table *table;
-	phw_iceland_mc_reg_table *ni_table = &data->iceland_mc_reg_table;
-	uint8_t module_index = iceland_get_memory_module_index(hwmgr);
-
-	table = kzalloc(sizeof(pp_atomctrl_mc_reg_table), GFP_KERNEL);
-
-	if (NULL == table)
-		return -ENOMEM;
-
-	/* Program additional LP registers that are no longer programmed by VBIOS */
-	cgs_write_register(hwmgr->device, mmMC_SEQ_RAS_TIMING_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_RAS_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_CAS_TIMING_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_CAS_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_DLL_STBY_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_DLL_STBY));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_G5PDX_CMD0_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_G5PDX_CMD0));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_G5PDX_CMD1_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_G5PDX_CMD1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_G5PDX_CTRL_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_G5PDX_CTRL));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_DVS_CMD_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_PMG_DVS_CMD));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_DVS_CTL_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_PMG_DVS_CTL));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_MISC_TIMING_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_MISC_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_MISC_TIMING2_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_MISC_TIMING2));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_CMD_EMRS_LP, cgs_read_register(hwmgr->device, mmMC_PMG_CMD_EMRS));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_CMD_MRS_LP, cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_CMD_MRS1_LP, cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_WR_CTL_D0_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_WR_CTL_D0));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_WR_CTL_D1_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_WR_CTL_D1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_RD_CTL_D0_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_RD_CTL_D0));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_RD_CTL_D1_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_RD_CTL_D1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_TIMING_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_PMG_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_CMD_MRS2_LP, cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS2));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_WR_CTL_2_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_WR_CTL_2));
-
-	memset(table, 0x00, sizeof(pp_atomctrl_mc_reg_table));
-
-	result = atomctrl_initialize_mc_reg_table(hwmgr, module_index, table);
-
-	if (0 == result)
-		result = iceland_copy_vbios_smc_reg_table(table, ni_table);
-
-	if (0 == result) {
-		iceland_set_s0_mc_reg_index(ni_table);
-		result = iceland_set_mc_special_registers(hwmgr, ni_table);
-	}
-
-	if (0 == result)
-		iceland_set_valid_flag(ni_table);
-
-	kfree(table);
-	return result;
-}
-
-/**
- * Programs static screed detection parameters
- *
- * @param   hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int iceland_program_static_screen_threshold_parameters(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	/* Set static screen threshold unit*/
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device,
-		CGS_IND_REG__SMC, CG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD_UNIT,
-		data->static_screen_threshold_unit);
-	/* Set static screen threshold*/
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device,
-		CGS_IND_REG__SMC, CG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD,
-		data->static_screen_threshold);
-
-	return 0;
-}
-
-/**
- * Setup display gap for glitch free memory clock switching.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int iceland_enable_display_gap(struct pp_hwmgr *hwmgr)
-{
-	uint32_t display_gap = cgs_read_ind_register(hwmgr->device,
-							CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL);
-
-	display_gap = PHM_SET_FIELD(display_gap,
-					CG_DISPLAY_GAP_CNTL, DISP_GAP, DISPLAY_GAP_IGNORE);
-
-	display_gap = PHM_SET_FIELD(display_gap,
-					CG_DISPLAY_GAP_CNTL, DISP_GAP_MCHG, DISPLAY_GAP_VBLANK);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_DISPLAY_GAP_CNTL, display_gap);
-
-	return 0;
-}
-
-/**
- * Programs activity state transition voting clients
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int iceland_program_voting_clients(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	/* Clear reset for voting clients before enabling DPM */
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-		SCLK_PWRMGT_CNTL, RESET_SCLK_CNT, 0);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-		SCLK_PWRMGT_CNTL, RESET_BUSY_CNT, 0);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_0, data->voting_rights_clients0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_1, data->voting_rights_clients1);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_2, data->voting_rights_clients2);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_3, data->voting_rights_clients3);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_4, data->voting_rights_clients4);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_5, data->voting_rights_clients5);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_6, data->voting_rights_clients6);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_7, data->voting_rights_clients7);
-
-	return 0;
-}
-
-static int iceland_upload_firmware(struct pp_hwmgr *hwmgr)
-{
-	return 0;
-}
-
-/**
- * Get the location of various tables inside the FW image.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int iceland_process_firmware_header(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	uint32_t tmp;
-	int result;
-	bool error = 0;
-
-	result = smu7_read_smc_sram_dword(hwmgr->smumgr,
-				SMU71_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU71_Firmware_Header, DpmTable),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->dpm_table_start = tmp;
-	}
-
-	error |= (0 != result);
-
-	result = smu7_read_smc_sram_dword(hwmgr->smumgr,
-				SMU71_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU71_Firmware_Header, SoftRegisters),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->soft_regs_start = tmp;
-	}
-
-	error |= (0 != result);
-
-
-	result = smu7_read_smc_sram_dword(hwmgr->smumgr,
-				SMU71_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU71_Firmware_Header, mcRegisterTable),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->mc_reg_table_start = tmp;
-	}
-
-	result = smu7_read_smc_sram_dword(hwmgr->smumgr,
-				SMU71_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU71_Firmware_Header, FanTable),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->fan_table_start = tmp;
-	}
-
-	error |= (0 != result);
-
-	result = smu7_read_smc_sram_dword(hwmgr->smumgr,
-				SMU71_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU71_Firmware_Header, mcArbDramTimingTable),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->arb_table_start = tmp;
-	}
-
-	error |= (0 != result);
-
-
-	result = smu7_read_smc_sram_dword(hwmgr->smumgr,
-				SMU71_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU71_Firmware_Header, Version),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		hwmgr->microcode_version_info.SMC = tmp;
-	}
-
-	error |= (0 != result);
-
-	result = smu7_read_smc_sram_dword(hwmgr->smumgr,
-				SMU71_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU71_Firmware_Header, UlvSettings),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->ulv_settings_start = tmp;
-	}
-
-	error |= (0 != result);
-
-	return error ? 1 : 0;
-}
-
-/*
-* Copy one arb setting to another and then switch the active set.
-* arbFreqSrc and arbFreqDest is one of the MC_CG_ARB_FREQ_Fx constants.
-*/
-int iceland_copy_and_switch_arb_sets(struct pp_hwmgr *hwmgr,
-		uint32_t arbFreqSrc, uint32_t arbFreqDest)
-{
-	uint32_t mc_arb_dram_timing;
-	uint32_t mc_arb_dram_timing2;
-	uint32_t burst_time;
-	uint32_t mc_cg_config;
-
-	switch (arbFreqSrc) {
-	case MC_CG_ARB_FREQ_F0:
-		mc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING);
-		mc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2);
-		burst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0);
-		break;
-
-	case MC_CG_ARB_FREQ_F1:
-		mc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1);
-		mc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1);
-		burst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1);
-		break;
-
-	default:
-		return -1;
-	}
-
-	switch (arbFreqDest) {
-	case MC_CG_ARB_FREQ_F0:
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING, mc_arb_dram_timing);
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2, mc_arb_dram_timing2);
-		PHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0, burst_time);
-		break;
-
-	case MC_CG_ARB_FREQ_F1:
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1, mc_arb_dram_timing);
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1, mc_arb_dram_timing2);
-		PHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1, burst_time);
-		break;
-
-	default:
-		return -1;
-	}
-
-	mc_cg_config = cgs_read_register(hwmgr->device, mmMC_CG_CONFIG);
-	mc_cg_config |= 0x0000000F;
-	cgs_write_register(hwmgr->device, mmMC_CG_CONFIG, mc_cg_config);
-	PHM_WRITE_FIELD(hwmgr->device, MC_ARB_CG, CG_ARB_REQ, arbFreqDest);
-
-	return 0;
-}
-
-/**
- * Initial switch from ARB F0->F1
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- * This function is to be called from the SetPowerState table.
- */
-int iceland_initial_switch_from_arb_f0_to_f1(struct pp_hwmgr *hwmgr)
-{
-	return iceland_copy_and_switch_arb_sets(hwmgr, MC_CG_ARB_FREQ_F0, MC_CG_ARB_FREQ_F1);
-}
-
-/* ---------------------------------------- ULV related functions ----------------------------------------------------*/
-
-
-static int iceland_reset_single_dpm_table(
-	struct pp_hwmgr *hwmgr,
-	struct iceland_single_dpm_table *dpm_table,
-	uint32_t count)
-{
-	uint32_t i;
-	if (!(count <= MAX_REGULAR_DPM_NUMBER))
-		printk(KERN_ERR "[ powerplay ] Fatal error, can not set up single DPM \
-			table entries to exceed max number! \n");
-
-	dpm_table->count = count;
-	for (i = 0; i < MAX_REGULAR_DPM_NUMBER; i++) {
-		dpm_table->dpm_levels[i].enabled = 0;
-	}
-
-	return 0;
-}
-
-static void iceland_setup_pcie_table_entry(
-	struct iceland_single_dpm_table *dpm_table,
-	uint32_t index, uint32_t pcie_gen,
-	uint32_t pcie_lanes)
-{
-	dpm_table->dpm_levels[index].value = pcie_gen;
-	dpm_table->dpm_levels[index].param1 = pcie_lanes;
-	dpm_table->dpm_levels[index].enabled = 1;
-}
-
-/*
- * Set up the PCIe DPM table as follows:
- *
- * A  = Performance State, Max, Gen Speed
- * C  = Performance State, Min, Gen Speed
- * 1  = Performance State, Max, Lane #
- * 3  = Performance State, Min, Lane #
- *
- * B  = Power Saving State, Max, Gen Speed
- * D  = Power Saving State, Min, Gen Speed
- * 2  = Power Saving State, Max, Lane #
- * 4  = Power Saving State, Min, Lane #
- *
- *
- * DPM Index   Gen Speed   Lane #
- * 5           A           1
- * 4           B           2
- * 3           C           1
- * 2           D           2
- * 1           C           3
- * 0           D           4
- *
- */
-static int iceland_setup_default_pcie_tables(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	PP_ASSERT_WITH_CODE((data->use_pcie_performance_levels ||
-				data->use_pcie_power_saving_levels),
-			"No pcie performance levels!", return -EINVAL);
-
-	if (data->use_pcie_performance_levels && !data->use_pcie_power_saving_levels) {
-		data->pcie_gen_power_saving = data->pcie_gen_performance;
-		data->pcie_lane_power_saving = data->pcie_lane_performance;
-	} else if (!data->use_pcie_performance_levels && data->use_pcie_power_saving_levels) {
-		data->pcie_gen_performance = data->pcie_gen_power_saving;
-		data->pcie_lane_performance = data->pcie_lane_power_saving;
-	}
-
-	iceland_reset_single_dpm_table(hwmgr, &data->dpm_table.pcie_speed_table, SMU71_MAX_LEVELS_LINK);
-
-	/* Hardcode Pcie Table */
-	iceland_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 0,
-		get_pcie_gen_support(data->pcie_gen_cap, PP_Min_PCIEGen),
-		get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-	iceland_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 1,
-		get_pcie_gen_support(data->pcie_gen_cap, PP_Min_PCIEGen),
-		get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-	iceland_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 2,
-		get_pcie_gen_support(data->pcie_gen_cap, PP_Max_PCIEGen),
-		get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-	iceland_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 3,
-		get_pcie_gen_support(data->pcie_gen_cap, PP_Max_PCIEGen),
-		get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-	iceland_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 4,
-		get_pcie_gen_support(data->pcie_gen_cap, PP_Max_PCIEGen),
-		get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-	iceland_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 5,
-		get_pcie_gen_support(data->pcie_gen_cap, PP_Max_PCIEGen),
-		get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-	data->dpm_table.pcie_speed_table.count = 6;
-
-	return 0;
-
-}
-
-
-/*
- * This function is to initalize all DPM state tables for SMU7 based on the dependency table.
- * Dynamic state patching function will then trim these state tables to the allowed range based
- * on the power policy or external client requests, such as UVD request, etc.
- */
-static int iceland_setup_default_dpm_tables(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	uint32_t i;
-
-	struct phm_clock_voltage_dependency_table *allowed_vdd_sclk_table =
-		hwmgr->dyn_state.vddc_dependency_on_sclk;
-	struct phm_clock_voltage_dependency_table *allowed_vdd_mclk_table =
-		hwmgr->dyn_state.vddc_dependency_on_mclk;
-	struct phm_cac_leakage_table *std_voltage_table =
-		hwmgr->dyn_state.cac_leakage_table;
-
-	PP_ASSERT_WITH_CODE(allowed_vdd_sclk_table != NULL,
-		"SCLK dependency table is missing. This table is mandatory", return -1);
-	PP_ASSERT_WITH_CODE(allowed_vdd_sclk_table->count >= 1,
-		"SCLK dependency table has to have is missing. This table is mandatory", return -1);
-
-	PP_ASSERT_WITH_CODE(allowed_vdd_mclk_table != NULL,
-		"MCLK dependency table is missing. This table is mandatory", return -1);
-	PP_ASSERT_WITH_CODE(allowed_vdd_mclk_table->count >= 1,
-		"VMCLK dependency table has to have is missing. This table is mandatory", return -1);
-
-	/* clear the state table to reset everything to default */
-	memset(&(data->dpm_table), 0x00, sizeof(data->dpm_table));
-	iceland_reset_single_dpm_table(hwmgr, &data->dpm_table.sclk_table, SMU71_MAX_LEVELS_GRAPHICS);
-	iceland_reset_single_dpm_table(hwmgr, &data->dpm_table.mclk_table, SMU71_MAX_LEVELS_MEMORY);
-	iceland_reset_single_dpm_table(hwmgr, &data->dpm_table.vddc_table, SMU71_MAX_LEVELS_VDDC);
-	iceland_reset_single_dpm_table(hwmgr, &data->dpm_table.vdd_ci_table, SMU71_MAX_LEVELS_VDDCI);
-	iceland_reset_single_dpm_table(hwmgr, &data->dpm_table.mvdd_table, SMU71_MAX_LEVELS_MVDD);
-
-	PP_ASSERT_WITH_CODE(allowed_vdd_sclk_table != NULL,
-		"SCLK dependency table is missing. This table is mandatory", return -1);
-	/* Initialize Sclk DPM table based on allow Sclk values*/
-	data->dpm_table.sclk_table.count = 0;
-
-	for (i = 0; i < allowed_vdd_sclk_table->count; i++) {
-		if (i == 0 || data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count-1].value !=
-				allowed_vdd_sclk_table->entries[i].clk) {
-			data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].value =
-				allowed_vdd_sclk_table->entries[i].clk;
-			data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].enabled = 1; /*(i==0) ? 1 : 0; to do */
-			data->dpm_table.sclk_table.count++;
-		}
-	}
-
-	PP_ASSERT_WITH_CODE(allowed_vdd_mclk_table != NULL,
-		"MCLK dependency table is missing. This table is mandatory", return -1);
-	/* Initialize Mclk DPM table based on allow Mclk values */
-	data->dpm_table.mclk_table.count = 0;
-	for (i = 0; i < allowed_vdd_mclk_table->count; i++) {
-		if (i == 0 || data->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count-1].value !=
-			allowed_vdd_mclk_table->entries[i].clk) {
-			data->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].value =
-				allowed_vdd_mclk_table->entries[i].clk;
-			data->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].enabled = 1; /*(i==0) ? 1 : 0; */
-			data->dpm_table.mclk_table.count++;
-		}
-	}
-
-	/* Initialize Vddc DPM table based on allow Vddc values.  And populate corresponding std values. */
-	for (i = 0; i < allowed_vdd_sclk_table->count; i++) {
-		data->dpm_table.vddc_table.dpm_levels[i].value = allowed_vdd_mclk_table->entries[i].v;
-		data->dpm_table.vddc_table.dpm_levels[i].param1 = std_voltage_table->entries[i].Leakage;
-		/* param1 is for corresponding std voltage */
-		data->dpm_table.vddc_table.dpm_levels[i].enabled = 1;
-	}
-
-	data->dpm_table.vddc_table.count = allowed_vdd_sclk_table->count;
-	allowed_vdd_mclk_table = hwmgr->dyn_state.vddci_dependency_on_mclk;
-
-	if (NULL != allowed_vdd_mclk_table) {
-		/* Initialize Vddci DPM table based on allow Mclk values */
-		for (i = 0; i < allowed_vdd_mclk_table->count; i++) {
-			data->dpm_table.vdd_ci_table.dpm_levels[i].value = allowed_vdd_mclk_table->entries[i].v;
-			data->dpm_table.vdd_ci_table.dpm_levels[i].enabled = 1;
-		}
-		data->dpm_table.vdd_ci_table.count = allowed_vdd_mclk_table->count;
-	}
-
-	allowed_vdd_mclk_table = hwmgr->dyn_state.mvdd_dependency_on_mclk;
-
-	if (NULL != allowed_vdd_mclk_table) {
-		/*
-		 * Initialize MVDD DPM table based on allow Mclk
-		 * values
-		 */
-		for (i = 0; i < allowed_vdd_mclk_table->count; i++) {
-			data->dpm_table.mvdd_table.dpm_levels[i].value = allowed_vdd_mclk_table->entries[i].v;
-			data->dpm_table.mvdd_table.dpm_levels[i].enabled = 1;
-		}
-		data->dpm_table.mvdd_table.count = allowed_vdd_mclk_table->count;
-	}
-
-	/* setup PCIE gen speed levels*/
-	iceland_setup_default_pcie_tables(hwmgr);
-
-	/* save a copy of the default DPM table*/
-	memcpy(&(data->golden_dpm_table), &(data->dpm_table), sizeof(struct iceland_dpm_table));
-
-	return 0;
-}
-
-/**
- * @brief PhwIceland_GetVoltageOrder
- *  Returns index of requested voltage record in lookup(table)
- * @param hwmgr - pointer to hardware manager
- * @param lookutab - lookup list to search in
- * @param voltage - voltage to look for
- * @return 0 on success
- */
-uint8_t iceland_get_voltage_index(phm_ppt_v1_voltage_lookup_table *look_up_table,
-		uint16_t voltage)
-{
-	uint8_t count = (uint8_t) (look_up_table->count);
-	uint8_t i;
-
-	PP_ASSERT_WITH_CODE((NULL != look_up_table), "Lookup Table empty.", return 0;);
-	PP_ASSERT_WITH_CODE((0 != count), "Lookup Table empty.", return 0;);
-
-	for (i = 0; i < count; i++) {
-		/* find first voltage equal or bigger than requested */
-		if (look_up_table->entries[i].us_vdd >= voltage)
-			return i;
-	}
-
-	/* voltage is bigger than max voltage in the table */
-	return i-1;
-}
-
-
-static int iceland_get_std_voltage_value_sidd(struct pp_hwmgr *hwmgr,
-		pp_atomctrl_voltage_table_entry *tab, uint16_t *hi,
-		uint16_t *lo)
-{
-	uint16_t v_index;
-	bool vol_found = false;
-	*hi = tab->value * VOLTAGE_SCALE;
-	*lo = tab->value * VOLTAGE_SCALE;
-
-	/* SCLK/VDDC Dependency Table has to exist. */
-	PP_ASSERT_WITH_CODE(NULL != hwmgr->dyn_state.vddc_dependency_on_sclk,
-	                    "The SCLK/VDDC Dependency Table does not exist.\n",
-	                    return -EINVAL);
-
-	if (NULL == hwmgr->dyn_state.cac_leakage_table) {
-		pr_warning("CAC Leakage Table does not exist, using vddc.\n");
-		return 0;
-	}
-
-	/*
-	 * Since voltage in the sclk/vddc dependency table is not
-	 * necessarily in ascending order because of ELB voltage
-	 * patching, loop through entire list to find exact voltage.
-	 */
-	for (v_index = 0; (uint32_t)v_index < hwmgr->dyn_state.vddc_dependency_on_sclk->count; v_index++) {
-		if (tab->value == hwmgr->dyn_state.vddc_dependency_on_sclk->entries[v_index].v) {
-			vol_found = true;
-			if ((uint32_t)v_index < hwmgr->dyn_state.cac_leakage_table->count) {
-				*lo = hwmgr->dyn_state.cac_leakage_table->entries[v_index].Vddc * VOLTAGE_SCALE;
-				*hi = (uint16_t)(hwmgr->dyn_state.cac_leakage_table->entries[v_index].Leakage * VOLTAGE_SCALE);
-			} else {
-				pr_warning("Index from SCLK/VDDC Dependency Table exceeds the CAC Leakage Table index, using maximum index from CAC table.\n");
-				*lo = hwmgr->dyn_state.cac_leakage_table->entries[hwmgr->dyn_state.cac_leakage_table->count - 1].Vddc * VOLTAGE_SCALE;
-				*hi = (uint16_t)(hwmgr->dyn_state.cac_leakage_table->entries[hwmgr->dyn_state.cac_leakage_table->count - 1].Leakage * VOLTAGE_SCALE);
-			}
-			break;
-		}
-	}
-
-	/*
-	 * If voltage is not found in the first pass, loop again to
-	 * find the best match, equal or higher value.
-	 */
-	if (!vol_found) {
-		for (v_index = 0; (uint32_t)v_index < hwmgr->dyn_state.vddc_dependency_on_sclk->count; v_index++) {
-			if (tab->value <= hwmgr->dyn_state.vddc_dependency_on_sclk->entries[v_index].v) {
-				vol_found = true;
-				if ((uint32_t)v_index < hwmgr->dyn_state.cac_leakage_table->count) {
-					*lo = hwmgr->dyn_state.cac_leakage_table->entries[v_index].Vddc * VOLTAGE_SCALE;
-					*hi = (uint16_t)(hwmgr->dyn_state.cac_leakage_table->entries[v_index].Leakage) * VOLTAGE_SCALE;
-				} else {
-					pr_warning("Index from SCLK/VDDC Dependency Table exceeds the CAC Leakage Table index in second look up, using maximum index from CAC table.");
-					*lo = hwmgr->dyn_state.cac_leakage_table->entries[hwmgr->dyn_state.cac_leakage_table->count - 1].Vddc * VOLTAGE_SCALE;
-					*hi = (uint16_t)(hwmgr->dyn_state.cac_leakage_table->entries[hwmgr->dyn_state.cac_leakage_table->count - 1].Leakage * VOLTAGE_SCALE);
-				}
-				break;
-			}
-		}
-
-		if (!vol_found)
-			pr_warning("Unable to get std_vddc from SCLK/VDDC Dependency Table, using vddc.\n");
-	}
-
-	return 0;
-}
-
-static int iceland_populate_smc_voltage_table(struct pp_hwmgr *hwmgr,
-		pp_atomctrl_voltage_table_entry *tab,
-		SMU71_Discrete_VoltageLevel *smc_voltage_tab) {
-	int result;
-
-
-	result = iceland_get_std_voltage_value_sidd(hwmgr, tab,
-			&smc_voltage_tab->StdVoltageHiSidd,
-			&smc_voltage_tab->StdVoltageLoSidd);
-	if (0 != result) {
-		smc_voltage_tab->StdVoltageHiSidd = tab->value * VOLTAGE_SCALE;
-		smc_voltage_tab->StdVoltageLoSidd = tab->value * VOLTAGE_SCALE;
-	}
-
-	smc_voltage_tab->Voltage = PP_HOST_TO_SMC_US(tab->value * VOLTAGE_SCALE);
-	CONVERT_FROM_HOST_TO_SMC_US(smc_voltage_tab->StdVoltageHiSidd);
-	CONVERT_FROM_HOST_TO_SMC_US(smc_voltage_tab->StdVoltageHiSidd);
-
-	return 0;
-}
-
-/**
- * Vddc table preparation for SMC.
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    table     the SMC DPM table structure to be populated
- * @return   always 0
- */
-static int iceland_populate_smc_vddc_table(struct pp_hwmgr *hwmgr,
-			SMU71_Discrete_DpmTable *table)
-{
-	unsigned int count;
-	int result;
-
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	table->VddcLevelCount = data->vddc_voltage_table.count;
-	for (count = 0; count < table->VddcLevelCount; count++) {
-		result = iceland_populate_smc_voltage_table(hwmgr,
-				&data->vddc_voltage_table.entries[count],
-				&table->VddcLevel[count]);
-		PP_ASSERT_WITH_CODE(0 == result, "do not populate SMC VDDC voltage table", return -EINVAL);
-
-		/* GPIO voltage control */
-		if (ICELAND_VOLTAGE_CONTROL_BY_GPIO == data->voltage_control)
-			table->VddcLevel[count].Smio |= data->vddc_voltage_table.entries[count].smio_low;
-		else if (ICELAND_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control)
-			table->VddcLevel[count].Smio = 0;
-	}
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->VddcLevelCount);
-
-	return 0;
-}
-
-/**
- * Vddci table preparation for SMC.
- *
- * @param    *hwmgr The address of the hardware manager.
- * @param    *table The SMC DPM table structure to be populated.
- * @return   0
- */
-static int iceland_populate_smc_vdd_ci_table(struct pp_hwmgr *hwmgr,
-			SMU71_Discrete_DpmTable *table)
-{
-	int result;
-	uint32_t count;
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	table->VddciLevelCount = data->vddci_voltage_table.count;
-	for (count = 0; count < table->VddciLevelCount; count++) {
-		result = iceland_populate_smc_voltage_table(hwmgr,
-				&data->vddci_voltage_table.entries[count],
-				&table->VddciLevel[count]);
-		PP_ASSERT_WITH_CODE(0 == result, "do not populate SMC VDDCI voltage table", return -EINVAL);
-
-		/* GPIO voltage control */
-		if (ICELAND_VOLTAGE_CONTROL_BY_GPIO == data->vdd_ci_control)
-			table->VddciLevel[count].Smio |= data->vddci_voltage_table.entries[count].smio_low;
-		else
-			table->VddciLevel[count].Smio = 0;
-	}
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->VddcLevelCount);
-
-	return 0;
-}
-
-/**
- * Mvdd table preparation for SMC.
- *
- * @param    *hwmgr The address of the hardware manager.
- * @param    *table The SMC DPM table structure to be populated.
- * @return   0
- */
-static int iceland_populate_smc_mvdd_table(struct pp_hwmgr *hwmgr,
-			SMU71_Discrete_DpmTable *table)
-{
-	int result;
-	uint32_t count;
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	table->MvddLevelCount = data->mvdd_voltage_table.count;
-	for (count = 0; count < table->MvddLevelCount; count++) {
-		result = iceland_populate_smc_voltage_table(hwmgr,
-				&data->mvdd_voltage_table.entries[count],
-				&table->MvddLevel[count]);
-		PP_ASSERT_WITH_CODE(0 == result, "do not populate SMC VDDCI voltage table", return -EINVAL);
-
-		/* GPIO voltage control */
-		if (ICELAND_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control)
-			table->MvddLevel[count].Smio |= data->mvdd_voltage_table.entries[count].smio_low;
-		else
-			table->MvddLevel[count].Smio = 0;
-	}
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->MvddLevelCount);
-
-	return 0;
-}
-
-int iceland_populate_bapm_vddc_vid_sidd(struct pp_hwmgr *hwmgr)
-{
-	int i;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint8_t * hi_vid = data->power_tune_table.BapmVddCVidHiSidd;
-	uint8_t * lo_vid = data->power_tune_table.BapmVddCVidLoSidd;
-
-	PP_ASSERT_WITH_CODE(NULL != hwmgr->dyn_state.cac_leakage_table,
-			    "The CAC Leakage table does not exist!", return -EINVAL);
-	PP_ASSERT_WITH_CODE(hwmgr->dyn_state.cac_leakage_table->count <= 8,
-			    "There should never be more than 8 entries for BapmVddcVid!!!", return -EINVAL);
-	PP_ASSERT_WITH_CODE(hwmgr->dyn_state.cac_leakage_table->count == hwmgr->dyn_state.vddc_dependency_on_sclk->count,
-			    "CACLeakageTable->count and VddcDependencyOnSCLk->count not equal", return -EINVAL);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_EVV)) {
-		for (i = 0; (uint32_t) i < hwmgr->dyn_state.cac_leakage_table->count; i++) {
-			lo_vid[i] = convert_to_vid(hwmgr->dyn_state.cac_leakage_table->entries[i].Vddc1);
-			hi_vid[i] = convert_to_vid(hwmgr->dyn_state.cac_leakage_table->entries[i].Vddc2);
-		}
-	} else {
-		PP_ASSERT_WITH_CODE(false, "Iceland should always support EVV", return -EINVAL);
-	}
-
-	return 0;
-}
-
-int iceland_populate_vddc_vid(struct pp_hwmgr *hwmgr)
-{
-	int i;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint8_t *vid = data->power_tune_table.VddCVid;
-
-	PP_ASSERT_WITH_CODE(data->vddc_voltage_table.count <= 8,
-		"There should never be more than 8 entries for VddcVid!!!",
-		return -EINVAL);
-
-	for (i = 0; i < (int)data->vddc_voltage_table.count; i++) {
-		vid[i] = convert_to_vid(data->vddc_voltage_table.entries[i].value);
-	}
-
-	return 0;
-}
-
-/**
- * Preparation of voltage tables for SMC.
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    table     the SMC DPM table structure to be populated
- * @return   always 0
- */
-
-int iceland_populate_smc_voltage_tables(struct pp_hwmgr *hwmgr,
-	SMU71_Discrete_DpmTable *table)
-{
-	int result;
-
-	result = iceland_populate_smc_vddc_table(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"can not populate VDDC voltage table to SMC", return -1);
-
-	result = iceland_populate_smc_vdd_ci_table(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"can not populate VDDCI voltage table to SMC", return -1);
-
-	result = iceland_populate_smc_mvdd_table(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"can not populate MVDD voltage table to SMC", return -1);
-
-	return 0;
-}
-
-
-/**
- * Re-generate the DPM level mask value
- * @param    hwmgr      the address of the hardware manager
- */
-static uint32_t iceland_get_dpm_level_enable_mask_value(
-			struct iceland_single_dpm_table * dpm_table)
-{
-	uint32_t i;
-	uint32_t mask_value = 0;
-
-	for (i = dpm_table->count; i > 0; i--) {
-		mask_value = mask_value << 1;
-
-		if (dpm_table->dpm_levels[i-1].enabled)
-			mask_value |= 0x1;
-		else
-			mask_value &= 0xFFFFFFFE;
-	}
-	return mask_value;
-}
-
-int iceland_populate_memory_timing_parameters(
-		struct pp_hwmgr *hwmgr,
-		uint32_t engine_clock,
-		uint32_t memory_clock,
-		struct SMU71_Discrete_MCArbDramTimingTableEntry *arb_regs
-		)
-{
-	uint32_t dramTiming;
-	uint32_t dramTiming2;
-	uint32_t burstTime;
-	int result;
-
-	result = atomctrl_set_engine_dram_timings_rv770(hwmgr,
-				engine_clock, memory_clock);
-
-	PP_ASSERT_WITH_CODE(result == 0,
-		"Error calling VBIOS to set DRAM_TIMING.", return result);
-
-	dramTiming  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING);
-	dramTiming2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2);
-	burstTime = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0);
-
-	arb_regs->McArbDramTiming  = PP_HOST_TO_SMC_UL(dramTiming);
-	arb_regs->McArbDramTiming2 = PP_HOST_TO_SMC_UL(dramTiming2);
-	arb_regs->McArbBurstTime = (uint8_t)burstTime;
-
-	return 0;
-}
-
-/**
- * Setup parameters for the MC ARB.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- * This function is to be called from the SetPowerState table.
- */
-int iceland_program_memory_timing_parameters(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	int result = 0;
-	SMU71_Discrete_MCArbDramTimingTable  arb_regs;
-	uint32_t i, j;
-
-	memset(&arb_regs, 0x00, sizeof(SMU71_Discrete_MCArbDramTimingTable));
-
-	for (i = 0; i < data->dpm_table.sclk_table.count; i++) {
-		for (j = 0; j < data->dpm_table.mclk_table.count; j++) {
-			result = iceland_populate_memory_timing_parameters
-				(hwmgr, data->dpm_table.sclk_table.dpm_levels[i].value,
-				 data->dpm_table.mclk_table.dpm_levels[j].value,
-				 &arb_regs.entries[i][j]);
-
-			if (0 != result) {
-				break;
-			}
-		}
-	}
-
-	if (0 == result) {
-		result = smu7_copy_bytes_to_smc(
-				hwmgr->smumgr,
-				data->arb_table_start,
-				(uint8_t *)&arb_regs,
-				sizeof(SMU71_Discrete_MCArbDramTimingTable),
-				data->sram_end
-				);
-	}
-
-	return result;
-}
-
-static int iceland_populate_smc_link_level(struct pp_hwmgr *hwmgr, SMU71_Discrete_DpmTable *table)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_dpm_table *dpm_table = &data->dpm_table;
-	uint32_t i;
-
-	/* Index (dpm_table->pcie_speed_table.count) is reserved for PCIE boot level. */
-	for (i = 0; i <= dpm_table->pcie_speed_table.count; i++) {
-		table->LinkLevel[i].PcieGenSpeed  =
-			(uint8_t)dpm_table->pcie_speed_table.dpm_levels[i].value;
-		table->LinkLevel[i].PcieLaneCount =
-			(uint8_t)encode_pcie_lane_width(dpm_table->pcie_speed_table.dpm_levels[i].param1);
-		table->LinkLevel[i].EnabledForActivity =
-			1;
-		table->LinkLevel[i].SPC =
-			(uint8_t)(data->pcie_spc_cap & 0xff);
-		table->LinkLevel[i].DownThreshold =
-			PP_HOST_TO_SMC_UL(5);
-		table->LinkLevel[i].UpThreshold =
-			PP_HOST_TO_SMC_UL(30);
-	}
-
-	data->smc_state_table.LinkLevelCount =
-		(uint8_t)dpm_table->pcie_speed_table.count;
-	data->dpm_level_enable_mask.pcie_dpm_enable_mask =
-		iceland_get_dpm_level_enable_mask_value(&dpm_table->pcie_speed_table);
-
-	return 0;
-}
-
-static int iceland_populate_smc_uvd_level(struct pp_hwmgr *hwmgr,
-					SMU71_Discrete_DpmTable *table)
-{
-	return 0;
-}
-
-uint8_t iceland_get_voltage_id(pp_atomctrl_voltage_table *voltage_table,
-		uint32_t voltage)
-{
-	uint8_t count = (uint8_t) (voltage_table->count);
-	uint8_t i = 0;
-
-	PP_ASSERT_WITH_CODE((NULL != voltage_table),
-		"Voltage Table empty.", return 0;);
-	PP_ASSERT_WITH_CODE((0 != count),
-		"Voltage Table empty.", return 0;);
-
-	for (i = 0; i < count; i++) {
-		/* find first voltage bigger than requested */
-		if (voltage_table->entries[i].value >= voltage)
-			return i;
-	}
-
-	/* voltage is bigger than max voltage in the table */
-	return i - 1;
-}
-
-static int iceland_populate_smc_vce_level(struct pp_hwmgr *hwmgr,
-					  SMU71_Discrete_DpmTable *table)
-{
-	return 0;
-}
-
-static int iceland_populate_smc_acp_level(struct pp_hwmgr *hwmgr,
-					  SMU71_Discrete_DpmTable *table)
-{
-	return 0;
-}
-
-static int iceland_populate_smc_samu_level(struct pp_hwmgr *hwmgr,
-					   SMU71_Discrete_DpmTable *table)
-{
-	return 0;
-}
-
-
-static int iceland_populate_smc_svi2_config(struct pp_hwmgr *hwmgr,
-					    SMU71_Discrete_DpmTable *tab)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	if(ICELAND_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control)
-		tab->SVI2Enable |= VDDC_ON_SVI2;
-
-	if(ICELAND_VOLTAGE_CONTROL_BY_SVID2 == data->vdd_ci_control)
-		tab->SVI2Enable |= VDDCI_ON_SVI2;
-	else
-		tab->MergedVddci = 1;
-
-	if(ICELAND_VOLTAGE_CONTROL_BY_SVID2 == data->mvdd_control)
-		tab->SVI2Enable |= MVDD_ON_SVI2;
-
-	PP_ASSERT_WITH_CODE( tab->SVI2Enable != (VDDC_ON_SVI2 | VDDCI_ON_SVI2 | MVDD_ON_SVI2) &&
-	        (tab->SVI2Enable & VDDC_ON_SVI2), "SVI2 domain configuration is incorrect!", return -EINVAL);
-
-	return 0;
-}
-
-static int iceland_get_dependecy_volt_by_clk(struct pp_hwmgr *hwmgr,
-	struct phm_clock_voltage_dependency_table *allowed_clock_voltage_table,
-	uint32_t clock, uint32_t *vol)
-{
-	uint32_t i = 0;
-
-	/* clock - voltage dependency table is empty table */
-	if (allowed_clock_voltage_table->count == 0)
-		return -EINVAL;
-
-	for (i = 0; i < allowed_clock_voltage_table->count; i++) {
-		/* find first sclk bigger than request */
-		if (allowed_clock_voltage_table->entries[i].clk >= clock) {
-			*vol = allowed_clock_voltage_table->entries[i].v;
-			return 0;
-		}
-	}
-
-	/* sclk is bigger than max sclk in the dependence table */
-	*vol = allowed_clock_voltage_table->entries[i - 1].v;
-
-	return 0;
-}
-
-static uint8_t iceland_get_mclk_frequency_ratio(uint32_t memory_clock,
-		bool strobe_mode)
-{
-	uint8_t mc_para_index;
-
-	if (strobe_mode) {
-		if (memory_clock < 12500) {
-			mc_para_index = 0x00;
-		} else if (memory_clock > 47500) {
-			mc_para_index = 0x0f;
-		} else {
-			mc_para_index = (uint8_t)((memory_clock - 10000) / 2500);
-		}
-	} else {
-		if (memory_clock < 65000) {
-			mc_para_index = 0x00;
-		} else if (memory_clock > 135000) {
-			mc_para_index = 0x0f;
-		} else {
-			mc_para_index = (uint8_t)((memory_clock - 60000) / 5000);
-		}
-	}
-
-	return mc_para_index;
-}
-
-static uint8_t iceland_get_ddr3_mclk_frequency_ratio(uint32_t memory_clock)
-{
-	uint8_t mc_para_index;
-
-	if (memory_clock < 10000) {
-		mc_para_index = 0;
-	} else if (memory_clock >= 80000) {
-		mc_para_index = 0x0f;
-	} else {
-		mc_para_index = (uint8_t)((memory_clock - 10000) / 5000 + 1);
-	}
-
-	return mc_para_index;
-}
-
-static int iceland_populate_phase_value_based_on_sclk(struct pp_hwmgr *hwmgr, const struct phm_phase_shedding_limits_table *pl,
-					uint32_t sclk, uint32_t *p_shed)
-{
-	unsigned int i;
-
-	/* use the minimum phase shedding */
-	*p_shed = 1;
-
-	/*
-	 * PPGen ensures the phase shedding limits table is sorted
-	 * from lowest voltage/sclk/mclk to highest voltage/sclk/mclk.
-	 * VBIOS ensures the phase shedding masks table is sorted from
-	 * least phases enabled (phase shedding on) to most phases
-	 * enabled (phase shedding off).
-	 */
-	for (i = 0; i < pl->count; i++) {
-	    if (sclk < pl->entries[i].Sclk) {
-	        /* Enable phase shedding */
-	        *p_shed = i;
-	        break;
-	    }
-	}
-
-	return 0;
-}
-
-static int iceland_populate_phase_value_based_on_mclk(struct pp_hwmgr *hwmgr, const struct phm_phase_shedding_limits_table *pl,
-					uint32_t memory_clock, uint32_t *p_shed)
-{
-	unsigned int i;
-
-	/* use the minimum phase shedding */
-	*p_shed = 1;
-
-	/*
-	 * PPGen ensures the phase shedding limits table is sorted
-	 * from lowest voltage/sclk/mclk to highest voltage/sclk/mclk.
-	 * VBIOS ensures the phase shedding masks table is sorted from
-	 * least phases enabled (phase shedding on) to most phases
-	 * enabled (phase shedding off).
-	 */
-	for (i = 0; i < pl->count; i++) {
-	    if (memory_clock < pl->entries[i].Mclk) {
-	        /* Enable phase shedding */
-	        *p_shed = i;
-	        break;
-	    }
-	}
-
-	return 0;
-}
-
-/**
- * Populates the SMC MCLK structure using the provided memory clock
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    memory_clock the memory clock to use to populate the structure
- * @param    sclk        the SMC SCLK structure to be populated
- */
-static int iceland_calculate_mclk_params(
-		struct pp_hwmgr *hwmgr,
-		uint32_t memory_clock,
-		SMU71_Discrete_MemoryLevel *mclk,
-		bool strobe_mode,
-		bool dllStateOn
-		)
-{
-	const iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	uint32_t  dll_cntl = data->clock_registers.vDLL_CNTL;
-	uint32_t  mclk_pwrmgt_cntl = data->clock_registers.vMCLK_PWRMGT_CNTL;
-	uint32_t  mpll_ad_func_cntl = data->clock_registers.vMPLL_AD_FUNC_CNTL;
-	uint32_t  mpll_dq_func_cntl = data->clock_registers.vMPLL_DQ_FUNC_CNTL;
-	uint32_t  mpll_func_cntl = data->clock_registers.vMPLL_FUNC_CNTL;
-	uint32_t  mpll_func_cntl_1 = data->clock_registers.vMPLL_FUNC_CNTL_1;
-	uint32_t  mpll_func_cntl_2 = data->clock_registers.vMPLL_FUNC_CNTL_2;
-	uint32_t  mpll_ss1 = data->clock_registers.vMPLL_SS1;
-	uint32_t  mpll_ss2 = data->clock_registers.vMPLL_SS2;
-
-	pp_atomctrl_memory_clock_param mpll_param;
-	int result;
-
-	result = atomctrl_get_memory_pll_dividers_si(hwmgr,
-				memory_clock, &mpll_param, strobe_mode);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Error retrieving Memory Clock Parameters from VBIOS.", return result);
-
-	/* MPLL_FUNC_CNTL setup*/
-	mpll_func_cntl = PHM_SET_FIELD(mpll_func_cntl, MPLL_FUNC_CNTL, BWCTRL, mpll_param.bw_ctrl);
-
-	/* MPLL_FUNC_CNTL_1 setup*/
-	mpll_func_cntl_1  = PHM_SET_FIELD(mpll_func_cntl_1,
-							MPLL_FUNC_CNTL_1, CLKF, mpll_param.mpll_fb_divider.cl_kf);
-	mpll_func_cntl_1  = PHM_SET_FIELD(mpll_func_cntl_1,
-							MPLL_FUNC_CNTL_1, CLKFRAC, mpll_param.mpll_fb_divider.clk_frac);
-	mpll_func_cntl_1  = PHM_SET_FIELD(mpll_func_cntl_1,
-							MPLL_FUNC_CNTL_1, VCO_MODE, mpll_param.vco_mode);
-
-	/* MPLL_AD_FUNC_CNTL setup*/
-	mpll_ad_func_cntl = PHM_SET_FIELD(mpll_ad_func_cntl,
-							MPLL_AD_FUNC_CNTL, YCLK_POST_DIV, mpll_param.mpll_post_divider);
-
-	if (data->is_memory_GDDR5) {
-		/* MPLL_DQ_FUNC_CNTL setup*/
-		mpll_dq_func_cntl  = PHM_SET_FIELD(mpll_dq_func_cntl,
-								MPLL_DQ_FUNC_CNTL, YCLK_SEL, mpll_param.yclk_sel);
-		mpll_dq_func_cntl  = PHM_SET_FIELD(mpll_dq_func_cntl,
-								MPLL_DQ_FUNC_CNTL, YCLK_POST_DIV, mpll_param.mpll_post_divider);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MemorySpreadSpectrumSupport)) {
-		/*
-		 ************************************
-		 Fref = Reference Frequency
-		 NF = Feedback divider ratio
-		 NR = Reference divider ratio
-		 Fnom = Nominal VCO output frequency = Fref * NF / NR
-		 Fs = Spreading Rate
-		 D = Percentage down-spread / 2
-		 Fint = Reference input frequency to PFD = Fref / NR
-		 NS = Spreading rate divider ratio = int(Fint / (2 * Fs))
-		 CLKS = NS - 1 = ISS_STEP_NUM[11:0]
-		 NV = D * Fs / Fnom * 4 * ((Fnom/Fref * NR) ^ 2)
-		 CLKV = 65536 * NV = ISS_STEP_SIZE[25:0]
-		 *************************************
-		 */
-		pp_atomctrl_internal_ss_info ss_info;
-		uint32_t freq_nom;
-		uint32_t tmp;
-		uint32_t reference_clock = atomctrl_get_mpll_reference_clock(hwmgr);
-
-		/* for GDDR5 for all modes and DDR3 */
-		if (1 == mpll_param.qdr)
-			freq_nom = memory_clock * 4 * (1 << mpll_param.mpll_post_divider);
-		else
-			freq_nom = memory_clock * 2 * (1 << mpll_param.mpll_post_divider);
-
-		/* tmp = (freq_nom / reference_clock * reference_divider) ^ 2  Note: S.I. reference_divider = 1*/
-		tmp = (freq_nom / reference_clock);
-		tmp = tmp * tmp;
-
-		if (0 == atomctrl_get_memory_clock_spread_spectrum(hwmgr, freq_nom, &ss_info)) {
-			/* ss_info.speed_spectrum_percentage -- in unit of 0.01% */
-			/* ss.Info.speed_spectrum_rate -- in unit of khz */
-			/* CLKS = reference_clock / (2 * speed_spectrum_rate * reference_divider) * 10 */
-			/*     = reference_clock * 5 / speed_spectrum_rate */
-			uint32_t clks = reference_clock * 5 / ss_info.speed_spectrum_rate;
-
-			/* CLKV = 65536 * speed_spectrum_percentage / 2 * spreadSpecrumRate / freq_nom * 4 / 100000 * ((freq_nom / reference_clock) ^ 2) */
-			/*     = 131 * speed_spectrum_percentage * speed_spectrum_rate / 100 * ((freq_nom / reference_clock) ^ 2) / freq_nom */
-			uint32_t clkv =
-				(uint32_t)((((131 * ss_info.speed_spectrum_percentage *
-							ss_info.speed_spectrum_rate) / 100) * tmp) / freq_nom);
-
-			mpll_ss1 = PHM_SET_FIELD(mpll_ss1, MPLL_SS1, CLKV, clkv);
-			mpll_ss2 = PHM_SET_FIELD(mpll_ss2, MPLL_SS2, CLKS, clks);
-		}
-	}
-
-	/* MCLK_PWRMGT_CNTL setup */
-	mclk_pwrmgt_cntl = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, DLL_SPEED, mpll_param.dll_speed);
-	mclk_pwrmgt_cntl = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK0_PDNB, dllStateOn);
-	mclk_pwrmgt_cntl = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK1_PDNB, dllStateOn);
-
-
-	/* Save the result data to outpupt memory level structure */
-	mclk->MclkFrequency   = memory_clock;
-	mclk->MpllFuncCntl    = mpll_func_cntl;
-	mclk->MpllFuncCntl_1  = mpll_func_cntl_1;
-	mclk->MpllFuncCntl_2  = mpll_func_cntl_2;
-	mclk->MpllAdFuncCntl  = mpll_ad_func_cntl;
-	mclk->MpllDqFuncCntl  = mpll_dq_func_cntl;
-	mclk->MclkPwrmgtCntl  = mclk_pwrmgt_cntl;
-	mclk->DllCntl         = dll_cntl;
-	mclk->MpllSs1         = mpll_ss1;
-	mclk->MpllSs2         = mpll_ss2;
-
-	return 0;
-}
-
-static int iceland_populate_single_memory_level(
-		struct pp_hwmgr *hwmgr,
-		uint32_t memory_clock,
-		SMU71_Discrete_MemoryLevel *memory_level
-		)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	int result = 0;
-	bool dllStateOn;
-	struct cgs_display_info info = {0};
-
-
-	if (NULL != hwmgr->dyn_state.vddc_dependency_on_mclk) {
-		result = iceland_get_dependecy_volt_by_clk(hwmgr,
-			hwmgr->dyn_state.vddc_dependency_on_mclk, memory_clock, &memory_level->MinVddc);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"can not find MinVddc voltage value from memory VDDC voltage dependency table", return result);
-	}
-
-	if (data->vdd_ci_control == ICELAND_VOLTAGE_CONTROL_NONE) {
-		memory_level->MinVddci = memory_level->MinVddc;
-	} else if (NULL != hwmgr->dyn_state.vddci_dependency_on_mclk) {
-		result = iceland_get_dependecy_volt_by_clk(hwmgr,
-				hwmgr->dyn_state.vddci_dependency_on_mclk,
-				memory_clock,
-				&memory_level->MinVddci);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"can not find MinVddci voltage value from memory VDDCI voltage dependency table", return result);
-	}
-
-	if (NULL != hwmgr->dyn_state.mvdd_dependency_on_mclk) {
-		result = iceland_get_dependecy_volt_by_clk(hwmgr,
-			hwmgr->dyn_state.mvdd_dependency_on_mclk, memory_clock, &memory_level->MinMvdd);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"can not find MinMVDD voltage value from memory MVDD voltage dependency table", return result);
-	}
-
-	memory_level->MinVddcPhases = 1;
-
-	if (data->vddc_phase_shed_control) {
-		iceland_populate_phase_value_based_on_mclk(hwmgr, hwmgr->dyn_state.vddc_phase_shed_limits_table,
-				memory_clock, &memory_level->MinVddcPhases);
-	}
-
-	memory_level->EnabledForThrottle = 1;
-	memory_level->EnabledForActivity = 1;
-	memory_level->UpHyst = 0;
-	memory_level->DownHyst = 100;
-	memory_level->VoltageDownHyst = 0;
-
-	/* Indicates maximum activity level for this performance level.*/
-	memory_level->ActivityLevel = (uint16_t)data->mclk_activity_target;
-	memory_level->StutterEnable = 0;
-	memory_level->StrobeEnable = 0;
-	memory_level->EdcReadEnable = 0;
-	memory_level->EdcWriteEnable = 0;
-	memory_level->RttEnable = 0;
-
-	/* default set to low watermark. Highest level will be set to high later.*/
-	memory_level->DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-	data->display_timing.num_existing_displays = info.display_count;
-
-	//if ((data->mclk_stutter_mode_threshold != 0) &&
-	//    (memory_clock <= data->mclk_stutter_mode_threshold) &&
-	//    (data->is_uvd_enabled == 0)
-	//    && (PHM_READ_FIELD(hwmgr->device, DPG_PIPE_STUTTER_CONTROL, STUTTER_ENABLE) & 0x1)
-	//    && (data->display_timing.num_existing_displays <= 2)
-	//    && (data->display_timing.num_existing_displays != 0))
-	//	memory_level->StutterEnable = 1;
-
-	/* decide strobe mode*/
-	memory_level->StrobeEnable = (data->mclk_strobe_mode_threshold != 0) &&
-		(memory_clock <= data->mclk_strobe_mode_threshold);
-
-	/* decide EDC mode and memory clock ratio*/
-	if (data->is_memory_GDDR5) {
-		memory_level->StrobeRatio = iceland_get_mclk_frequency_ratio(memory_clock,
-					memory_level->StrobeEnable);
-
-		if ((data->mclk_edc_enable_threshold != 0) &&
-				(memory_clock > data->mclk_edc_enable_threshold)) {
-			memory_level->EdcReadEnable = 1;
-		}
-
-		if ((data->mclk_edc_wr_enable_threshold != 0) &&
-				(memory_clock > data->mclk_edc_wr_enable_threshold)) {
-			memory_level->EdcWriteEnable = 1;
-		}
-
-		if (memory_level->StrobeEnable) {
-			if (iceland_get_mclk_frequency_ratio(memory_clock, 1) >=
-					((cgs_read_register(hwmgr->device, mmMC_SEQ_MISC7) >> 16) & 0xf)) {
-				dllStateOn = ((cgs_read_register(hwmgr->device, mmMC_SEQ_MISC5) >> 1) & 0x1) ? 1 : 0;
-			} else {
-				dllStateOn = ((cgs_read_register(hwmgr->device, mmMC_SEQ_MISC6) >> 1) & 0x1) ? 1 : 0;
-			}
-
-		} else {
-			dllStateOn = data->dll_defaule_on;
-		}
-	} else {
-		memory_level->StrobeRatio =
-			iceland_get_ddr3_mclk_frequency_ratio(memory_clock);
-		dllStateOn = ((cgs_read_register(hwmgr->device, mmMC_SEQ_MISC5) >> 1) & 0x1) ? 1 : 0;
-	}
-
-	result = iceland_calculate_mclk_params(hwmgr,
-		memory_clock, memory_level, memory_level->StrobeEnable, dllStateOn);
-
-	if (0 == result) {
-		memory_level->MinVddc = PP_HOST_TO_SMC_UL(memory_level->MinVddc * VOLTAGE_SCALE);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MinVddcPhases);
-		memory_level->MinVddci = PP_HOST_TO_SMC_UL(memory_level->MinVddci * VOLTAGE_SCALE);
-		memory_level->MinMvdd = PP_HOST_TO_SMC_UL(memory_level->MinMvdd * VOLTAGE_SCALE);
-		/* MCLK frequency in units of 10KHz*/
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MclkFrequency);
-		/* Indicates maximum activity level for this performance level.*/
-		CONVERT_FROM_HOST_TO_SMC_US(memory_level->ActivityLevel);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllFuncCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllFuncCntl_1);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllFuncCntl_2);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllAdFuncCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllDqFuncCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MclkPwrmgtCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->DllCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllSs1);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllSs2);
-	}
-
-	return result;
-}
-
-/**
- * Populates the SMC MVDD structure using the provided memory clock.
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    mclk        the MCLK value to be used in the decision if MVDD should be high or low.
- * @param    voltage     the SMC VOLTAGE structure to be populated
- */
-int iceland_populate_mvdd_value(struct pp_hwmgr *hwmgr, uint32_t mclk, SMU71_Discrete_VoltageLevel *voltage)
-{
-	const iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	uint32_t i = 0;
-
-	if (ICELAND_VOLTAGE_CONTROL_NONE != data->mvdd_control) {
-		/* find mvdd value which clock is more than request */
-		for (i = 0; i < hwmgr->dyn_state.mvdd_dependency_on_mclk->count; i++) {
-			if (mclk <= hwmgr->dyn_state.mvdd_dependency_on_mclk->entries[i].clk) {
-				/* Always round to higher voltage. */
-				voltage->Voltage = data->mvdd_voltage_table.entries[i].value;
-				break;
-			}
-		}
-
-		PP_ASSERT_WITH_CODE(i < hwmgr->dyn_state.mvdd_dependency_on_mclk->count,
-			"MVDD Voltage is outside the supported range.", return -1);
-
-	} else {
-		return -1;
-	}
-
-	return 0;
-}
-
-
-static int iceland_populate_smc_acpi_level(struct pp_hwmgr *hwmgr,
-	SMU71_Discrete_DpmTable *table)
-{
-	int result = 0;
-	const iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	pp_atomctrl_clock_dividers_vi dividers;
-	SMU71_Discrete_VoltageLevel voltage_level;
-	uint32_t spll_func_cntl    = data->clock_registers.vCG_SPLL_FUNC_CNTL;
-	uint32_t spll_func_cntl_2  = data->clock_registers.vCG_SPLL_FUNC_CNTL_2;
-	uint32_t dll_cntl          = data->clock_registers.vDLL_CNTL;
-	uint32_t mclk_pwrmgt_cntl  = data->clock_registers.vMCLK_PWRMGT_CNTL;
-
-	/* The ACPI state should not do DPM on DC (or ever).*/
-	table->ACPILevel.Flags &= ~PPSMC_SWSTATE_FLAG_DC;
-
-	if (data->acpi_vddc)
-		table->ACPILevel.MinVddc = PP_HOST_TO_SMC_UL(data->acpi_vddc * VOLTAGE_SCALE);
-	else
-		table->ACPILevel.MinVddc = PP_HOST_TO_SMC_UL(data->min_vddc_in_pp_table * VOLTAGE_SCALE);
-
-	table->ACPILevel.MinVddcPhases = (data->vddc_phase_shed_control) ? 0 : 1;
-
-	/* assign zero for now*/
-	table->ACPILevel.SclkFrequency = atomctrl_get_reference_clock(hwmgr);
-
-	/* get the engine clock dividers for this clock value*/
-	result = atomctrl_get_engine_pll_dividers_vi(hwmgr,
-		table->ACPILevel.SclkFrequency,  &dividers);
-
-	PP_ASSERT_WITH_CODE(result == 0,
-		"Error retrieving Engine Clock dividers from VBIOS.", return result);
-
-	/* divider ID for required SCLK*/
-	table->ACPILevel.SclkDid = (uint8_t)dividers.pll_post_divider;
-	table->ACPILevel.DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-	table->ACPILevel.DeepSleepDivId = 0;
-
-	spll_func_cntl      = PHM_SET_FIELD(spll_func_cntl,
-							CG_SPLL_FUNC_CNTL,   SPLL_PWRON,     0);
-	spll_func_cntl      = PHM_SET_FIELD(spll_func_cntl,
-							CG_SPLL_FUNC_CNTL,   SPLL_RESET,     1);
-	spll_func_cntl_2    = PHM_SET_FIELD(spll_func_cntl_2,
-							CG_SPLL_FUNC_CNTL_2, SCLK_MUX_SEL,   4);
-
-	table->ACPILevel.CgSpllFuncCntl = spll_func_cntl;
-	table->ACPILevel.CgSpllFuncCntl2 = spll_func_cntl_2;
-	table->ACPILevel.CgSpllFuncCntl3 = data->clock_registers.vCG_SPLL_FUNC_CNTL_3;
-	table->ACPILevel.CgSpllFuncCntl4 = data->clock_registers.vCG_SPLL_FUNC_CNTL_4;
-	table->ACPILevel.SpllSpreadSpectrum = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM;
-	table->ACPILevel.SpllSpreadSpectrum2 = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2;
-	table->ACPILevel.CcPwrDynRm = 0;
-	table->ACPILevel.CcPwrDynRm1 = 0;
-
-
-	/* For various features to be enabled/disabled while this level is active.*/
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.Flags);
-	/* SCLK frequency in units of 10KHz*/
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SclkFrequency);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl2);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl3);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl4);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SpllSpreadSpectrum);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SpllSpreadSpectrum2);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CcPwrDynRm);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CcPwrDynRm1);
-
-	table->MemoryACPILevel.MinVddc = table->ACPILevel.MinVddc;
-	table->MemoryACPILevel.MinVddcPhases = table->ACPILevel.MinVddcPhases;
-
-	/*  CONVERT_FROM_HOST_TO_SMC_UL(table->MemoryACPILevel.MinVoltage);*/
-
-	if (0 == iceland_populate_mvdd_value(hwmgr, 0, &voltage_level))
-		table->MemoryACPILevel.MinMvdd =
-			PP_HOST_TO_SMC_UL(voltage_level.Voltage * VOLTAGE_SCALE);
-	else
-		table->MemoryACPILevel.MinMvdd = 0;
-
-	/* Force reset on DLL*/
-	mclk_pwrmgt_cntl    = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK0_RESET, 0x1);
-	mclk_pwrmgt_cntl    = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK1_RESET, 0x1);
-
-	/* Disable DLL in ACPIState*/
-	mclk_pwrmgt_cntl    = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK0_PDNB, 0);
-	mclk_pwrmgt_cntl    = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK1_PDNB, 0);
-
-	/* Enable DLL bypass signal*/
-	dll_cntl            = PHM_SET_FIELD(dll_cntl,
-		DLL_CNTL, MRDCK0_BYPASS, 0);
-	dll_cntl            = PHM_SET_FIELD(dll_cntl,
-		DLL_CNTL, MRDCK1_BYPASS, 0);
-
-	table->MemoryACPILevel.DllCntl            =
-		PP_HOST_TO_SMC_UL(dll_cntl);
-	table->MemoryACPILevel.MclkPwrmgtCntl     =
-		PP_HOST_TO_SMC_UL(mclk_pwrmgt_cntl);
-	table->MemoryACPILevel.MpllAdFuncCntl     =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_AD_FUNC_CNTL);
-	table->MemoryACPILevel.MpllDqFuncCntl     =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_DQ_FUNC_CNTL);
-	table->MemoryACPILevel.MpllFuncCntl       =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_FUNC_CNTL);
-	table->MemoryACPILevel.MpllFuncCntl_1     =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_FUNC_CNTL_1);
-	table->MemoryACPILevel.MpllFuncCntl_2     =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_FUNC_CNTL_2);
-	table->MemoryACPILevel.MpllSs1            =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_SS1);
-	table->MemoryACPILevel.MpllSs2            =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_SS2);
-
-	table->MemoryACPILevel.EnabledForThrottle = 0;
-	table->MemoryACPILevel.EnabledForActivity = 0;
-	table->MemoryACPILevel.UpHyst = 0;
-	table->MemoryACPILevel.DownHyst = 100;
-	table->MemoryACPILevel.VoltageDownHyst = 0;
-	/* Indicates maximum activity level for this performance level.*/
-	table->MemoryACPILevel.ActivityLevel = PP_HOST_TO_SMC_US((uint16_t)data->mclk_activity_target);
-
-	table->MemoryACPILevel.StutterEnable = 0;
-	table->MemoryACPILevel.StrobeEnable = 0;
-	table->MemoryACPILevel.EdcReadEnable = 0;
-	table->MemoryACPILevel.EdcWriteEnable = 0;
-	table->MemoryACPILevel.RttEnable = 0;
-
-	return result;
-}
-
-static int iceland_find_boot_level(struct iceland_single_dpm_table *table, uint32_t value, uint32_t *boot_level)
-{
-	int result = 0;
-	uint32_t i;
-
-	for (i = 0; i < table->count; i++) {
-		if (value == table->dpm_levels[i].value) {
-			*boot_level = i;
-			result = 0;
-		}
-	}
-	return result;
-}
-
-/**
- * Calculates the SCLK dividers using the provided engine clock
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    engine_clock the engine clock to use to populate the structure
- * @param    sclk        the SMC SCLK structure to be populated
- */
-int iceland_calculate_sclk_params(struct pp_hwmgr *hwmgr,
-		uint32_t engine_clock, SMU71_Discrete_GraphicsLevel *sclk)
-{
-	const iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	pp_atomctrl_clock_dividers_vi dividers;
-	uint32_t spll_func_cntl            = data->clock_registers.vCG_SPLL_FUNC_CNTL;
-	uint32_t spll_func_cntl_3          = data->clock_registers.vCG_SPLL_FUNC_CNTL_3;
-	uint32_t spll_func_cntl_4          = data->clock_registers.vCG_SPLL_FUNC_CNTL_4;
-	uint32_t cg_spll_spread_spectrum   = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM;
-	uint32_t cg_spll_spread_spectrum_2 = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2;
-	uint32_t    reference_clock;
-	uint32_t reference_divider;
-	uint32_t fbdiv;
-	int result;
-
-	/* get the engine clock dividers for this clock value*/
-	result = atomctrl_get_engine_pll_dividers_vi(hwmgr, engine_clock,  &dividers);
-
-	PP_ASSERT_WITH_CODE(result == 0,
-		"Error retrieving Engine Clock dividers from VBIOS.", return result);
-
-	/* To get FBDIV we need to multiply this by 16384 and divide it by Fref.*/
-	reference_clock = atomctrl_get_reference_clock(hwmgr);
-
-	reference_divider = 1 + dividers.uc_pll_ref_div;
-
-	/* low 14 bits is fraction and high 12 bits is divider*/
-	fbdiv = dividers.ul_fb_div.ul_fb_divider & 0x3FFFFFF;
-
-	/* SPLL_FUNC_CNTL setup*/
-	spll_func_cntl = PHM_SET_FIELD(spll_func_cntl,
-		CG_SPLL_FUNC_CNTL, SPLL_REF_DIV, dividers.uc_pll_ref_div);
-	spll_func_cntl = PHM_SET_FIELD(spll_func_cntl,
-		CG_SPLL_FUNC_CNTL, SPLL_PDIV_A,  dividers.uc_pll_post_div);
-
-	/* SPLL_FUNC_CNTL_3 setup*/
-	spll_func_cntl_3 = PHM_SET_FIELD(spll_func_cntl_3,
-		CG_SPLL_FUNC_CNTL_3, SPLL_FB_DIV, fbdiv);
-
-	/* set to use fractional accumulation*/
-	spll_func_cntl_3 = PHM_SET_FIELD(spll_func_cntl_3,
-		CG_SPLL_FUNC_CNTL_3, SPLL_DITHEN, 1);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EngineSpreadSpectrumSupport)) {
-		pp_atomctrl_internal_ss_info ss_info;
-
-		uint32_t vcoFreq = engine_clock * dividers.uc_pll_post_div;
-		if (0 == atomctrl_get_engine_clock_spread_spectrum(hwmgr, vcoFreq, &ss_info)) {
-			/*
-			* ss_info.speed_spectrum_percentage -- in unit of 0.01%
-			* ss_info.speed_spectrum_rate -- in unit of khz
-			*/
-			/* clks = reference_clock * 10 / (REFDIV + 1) / speed_spectrum_rate / 2 */
-			uint32_t clkS = reference_clock * 5 / (reference_divider * ss_info.speed_spectrum_rate);
-
-			/* clkv = 2 * D * fbdiv / NS */
-			uint32_t clkV = 4 * ss_info.speed_spectrum_percentage * fbdiv / (clkS * 10000);
-
-			cg_spll_spread_spectrum =
-				PHM_SET_FIELD(cg_spll_spread_spectrum, CG_SPLL_SPREAD_SPECTRUM, CLKS, clkS);
-			cg_spll_spread_spectrum =
-				PHM_SET_FIELD(cg_spll_spread_spectrum, CG_SPLL_SPREAD_SPECTRUM, SSEN, 1);
-			cg_spll_spread_spectrum_2 =
-				PHM_SET_FIELD(cg_spll_spread_spectrum_2, CG_SPLL_SPREAD_SPECTRUM_2, CLKV, clkV);
-		}
-	}
-
-	sclk->SclkFrequency        = engine_clock;
-	sclk->CgSpllFuncCntl3      = spll_func_cntl_3;
-	sclk->CgSpllFuncCntl4      = spll_func_cntl_4;
-	sclk->SpllSpreadSpectrum   = cg_spll_spread_spectrum;
-	sclk->SpllSpreadSpectrum2  = cg_spll_spread_spectrum_2;
-	sclk->SclkDid              = (uint8_t)dividers.pll_post_divider;
-
-	return 0;
-}
-
-static uint8_t iceland_get_sleep_divider_id_from_clock(struct pp_hwmgr *hwmgr,
-		uint32_t engine_clock, uint32_t min_engine_clock_in_sr)
-{
-	uint32_t i, temp;
-	uint32_t min = (min_engine_clock_in_sr > ICELAND_MINIMUM_ENGINE_CLOCK) ?
-			min_engine_clock_in_sr : ICELAND_MINIMUM_ENGINE_CLOCK;
-
-	PP_ASSERT_WITH_CODE((engine_clock >= min),
-			"Engine clock can't satisfy stutter requirement!", return 0);
-
-	for (i = ICELAND_MAX_DEEPSLEEP_DIVIDER_ID;; i--) {
-		temp = engine_clock / (1 << i);
-
-		if(temp >= min || i == 0)
-			break;
-	}
-	return (uint8_t)i;
-}
-
-/**
- * Populates single SMC SCLK structure using the provided engine clock
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    engine_clock the engine clock to use to populate the structure
- * @param    sclk        the SMC SCLK structure to be populated
- */
-static int iceland_populate_single_graphic_level(struct pp_hwmgr *hwmgr,
-		uint32_t engine_clock, uint16_t	sclk_activity_level_threshold,
-		SMU71_Discrete_GraphicsLevel *graphic_level)
-{
-	int result;
-	uint32_t threshold;
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	result = iceland_calculate_sclk_params(hwmgr, engine_clock, graphic_level);
-
-
-	/* populate graphics levels*/
-	result = iceland_get_dependecy_volt_by_clk(hwmgr,
-			hwmgr->dyn_state.vddc_dependency_on_sclk, engine_clock, &graphic_level->MinVddc);
-	PP_ASSERT_WITH_CODE((0 == result),
-		"can not find VDDC voltage value for VDDC engine clock dependency table", return result);
-
-	/* SCLK frequency in units of 10KHz*/
-	graphic_level->SclkFrequency = engine_clock;
-
-	/*
-	 * Minimum VDDC phases required to support this level, it
-	 * should get from dependence table.
-	 */
-	graphic_level->MinVddcPhases = 1;
-
-	if (data->vddc_phase_shed_control) {
-		iceland_populate_phase_value_based_on_sclk(hwmgr,
-				hwmgr->dyn_state.vddc_phase_shed_limits_table,
-				engine_clock,
-				&graphic_level->MinVddcPhases);
-	}
-
-	/* Indicates maximum activity level for this performance level. 50% for now*/
-	graphic_level->ActivityLevel = sclk_activity_level_threshold;
-
-	graphic_level->CcPwrDynRm = 0;
-	graphic_level->CcPwrDynRm1 = 0;
-	/* this level can be used if activity is high enough.*/
-	graphic_level->EnabledForActivity = 1;
-	/* this level can be used for throttling.*/
-	graphic_level->EnabledForThrottle = 1;
-	graphic_level->UpHyst = 0;
-	graphic_level->DownHyst = 100;
-	graphic_level->VoltageDownHyst = 0;
-	graphic_level->PowerThrottle = 0;
-
-	threshold = engine_clock * data->fast_watermark_threshold / 100;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkDeepSleep)) {
-		graphic_level->DeepSleepDivId =
-				iceland_get_sleep_divider_id_from_clock(hwmgr, engine_clock,
-						data->display_timing.min_clock_insr);
-	}
-
-	/* Default to slow, highest DPM level will be set to PPSMC_DISPLAY_WATERMARK_LOW later.*/
-	graphic_level->DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-
-	if (0 == result) {
-		graphic_level->MinVddc = PP_HOST_TO_SMC_UL(graphic_level->MinVddc * VOLTAGE_SCALE);
-		/* CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->MinVoltage);*/
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->MinVddcPhases);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->SclkFrequency);
-		CONVERT_FROM_HOST_TO_SMC_US(graphic_level->ActivityLevel);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->CgSpllFuncCntl3);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->CgSpllFuncCntl4);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->SpllSpreadSpectrum);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->SpllSpreadSpectrum2);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->CcPwrDynRm);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->CcPwrDynRm1);
-	}
-
-	return result;
-}
-
-/**
- * Populates all SMC SCLK levels' structure based on the trimmed allowed dpm engine clock states
- *
- * @param    hwmgr      the address of the hardware manager
- */
-static int iceland_populate_all_graphic_levels(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_dpm_table *dpm_table = &data->dpm_table;
-	int result = 0;
-	uint32_t level_array_adress = data->dpm_table_start +
-		offsetof(SMU71_Discrete_DpmTable, GraphicsLevel);
-
-	uint32_t level_array_size = sizeof(SMU71_Discrete_GraphicsLevel) * SMU71_MAX_LEVELS_GRAPHICS;
-	SMU71_Discrete_GraphicsLevel *levels = data->smc_state_table.GraphicsLevel;
-	uint32_t i;
-	uint8_t highest_pcie_level_enabled = 0, lowest_pcie_level_enabled = 0, mid_pcie_level_enabled = 0, count = 0;
-	memset(levels, 0x00, level_array_size);
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		result = iceland_populate_single_graphic_level(hwmgr,
-					dpm_table->sclk_table.dpm_levels[i].value,
-					(uint16_t)data->activity_target[i],
-					&(data->smc_state_table.GraphicsLevel[i]));
-		if (0 != result)
-			return result;
-
-		/* Making sure only DPM level 0-1 have Deep Sleep Div ID populated. */
-		if (i > 1)
-			data->smc_state_table.GraphicsLevel[i].DeepSleepDivId = 0;
-	}
-
-	/* set highest level watermark to high */
-	if (dpm_table->sclk_table.count > 1)
-		data->smc_state_table.GraphicsLevel[dpm_table->sclk_table.count-1].DisplayWatermark =
-			PPSMC_DISPLAY_WATERMARK_HIGH;
-
-	data->smc_state_table.GraphicsDpmLevelCount =
-		(uint8_t)dpm_table->sclk_table.count;
-	data->dpm_level_enable_mask.sclk_dpm_enable_mask =
-		iceland_get_dpm_level_enable_mask_value(&dpm_table->sclk_table);
-
-	while ((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-				(1 << (highest_pcie_level_enabled + 1))) != 0) {
-		highest_pcie_level_enabled++;
-	}
-
-	while ((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-	       (1 << lowest_pcie_level_enabled)) == 0) {
-		lowest_pcie_level_enabled++;
-	}
-
-	while ((count < highest_pcie_level_enabled) &&
-			((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-				(1 << (lowest_pcie_level_enabled + 1 + count))) == 0)) {
-		count++;
-	}
-
-	mid_pcie_level_enabled = (lowest_pcie_level_enabled+1+count) < highest_pcie_level_enabled ?
-		(lowest_pcie_level_enabled + 1 + count) : highest_pcie_level_enabled;
-
-	/* set pcieDpmLevel to highest_pcie_level_enabled*/
-	for (i = 2; i < dpm_table->sclk_table.count; i++) {
-		data->smc_state_table.GraphicsLevel[i].pcieDpmLevel = highest_pcie_level_enabled;
-	}
-
-	/* set pcieDpmLevel to lowest_pcie_level_enabled*/
-	data->smc_state_table.GraphicsLevel[0].pcieDpmLevel = lowest_pcie_level_enabled;
-
-	/* set pcieDpmLevel to mid_pcie_level_enabled*/
-	data->smc_state_table.GraphicsLevel[1].pcieDpmLevel = mid_pcie_level_enabled;
-
-	/* level count will send to smc once at init smc table and never change*/
-	result = smu7_copy_bytes_to_smc(hwmgr->smumgr, level_array_adress, (uint8_t *)levels, (uint32_t)level_array_size, data->sram_end);
-
-	if (0 != result)
-		return result;
-
-	return 0;
-}
-
-/**
- * Populates all SMC MCLK levels' structure based on the trimmed allowed dpm memory clock states
- *
- * @param    hwmgr      the address of the hardware manager
- */
-
-static int iceland_populate_all_memory_levels(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_dpm_table *dpm_table = &data->dpm_table;
-	int result;
-	/* populate MCLK dpm table to SMU7 */
-	uint32_t level_array_adress = data->dpm_table_start + offsetof(SMU71_Discrete_DpmTable, MemoryLevel);
-	uint32_t level_array_size = sizeof(SMU71_Discrete_MemoryLevel) * SMU71_MAX_LEVELS_MEMORY;
-	SMU71_Discrete_MemoryLevel *levels = data->smc_state_table.MemoryLevel;
-	uint32_t i;
-
-	memset(levels, 0x00, level_array_size);
-
-	for (i = 0; i < dpm_table->mclk_table.count; i++) {
-		PP_ASSERT_WITH_CODE((0 != dpm_table->mclk_table.dpm_levels[i].value),
-			"can not populate memory level as memory clock is zero", return -1);
-		result = iceland_populate_single_memory_level(hwmgr, dpm_table->mclk_table.dpm_levels[i].value,
-			&(data->smc_state_table.MemoryLevel[i]));
-		if (0 != result) {
-			return result;
-		}
-	}
-
-	/* Only enable level 0 for now.*/
-	data->smc_state_table.MemoryLevel[0].EnabledForActivity = 1;
-
-	/*
-	* in order to prevent MC activity from stutter mode to push DPM up.
-	* the UVD change complements this by putting the MCLK in a higher state
-	* by default such that we are not effected by up threshold or and MCLK DPM latency.
-	*/
-	data->smc_state_table.MemoryLevel[0].ActivityLevel = 0x1F;
-	CONVERT_FROM_HOST_TO_SMC_US(data->smc_state_table.MemoryLevel[0].ActivityLevel);
-
-	data->smc_state_table.MemoryDpmLevelCount = (uint8_t)dpm_table->mclk_table.count;
-	data->dpm_level_enable_mask.mclk_dpm_enable_mask = iceland_get_dpm_level_enable_mask_value(&dpm_table->mclk_table);
-	/* set highest level watermark to high*/
-	data->smc_state_table.MemoryLevel[dpm_table->mclk_table.count-1].DisplayWatermark = PPSMC_DISPLAY_WATERMARK_HIGH;
-
-	/* level count will send to smc once at init smc table and never change*/
-	result = smu7_copy_bytes_to_smc(hwmgr->smumgr,
-		level_array_adress, (uint8_t *)levels, (uint32_t)level_array_size, data->sram_end);
-
-	if (0 != result) {
-		return result;
-	}
-
-	return 0;
-}
-
-struct ICELAND_DLL_SPEED_SETTING
-{
-	uint16_t        Min;           /* Minimum Data Rate*/
-	uint16_t        Max;           /* Maximum Data Rate*/
-	uint32_t	dll_speed;     /* The desired DLL_SPEED setting*/
-};
-
-static int iceland_populate_ulv_level(struct pp_hwmgr *hwmgr, SMU71_Discrete_Ulv *pstate)
-{
-	int result = 0;
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	uint32_t voltage_response_time, ulv_voltage;
-
-	pstate->CcPwrDynRm = 0;
-	pstate->CcPwrDynRm1 = 0;
-
-	//backbiasResponseTime is use for ULV state voltage value.
-	result = pp_tables_get_response_times(hwmgr, &voltage_response_time, &ulv_voltage);
-	PP_ASSERT_WITH_CODE((0 == result), "can not get ULV voltage value", return result;);
-
-	if(!ulv_voltage) {
-		data->ulv.ulv_supported = false;
-		return 0;
-	}
-
-	if (ICELAND_VOLTAGE_CONTROL_BY_SVID2 != data->voltage_control) {
-		/* use minimum voltage if ulv voltage in pptable is bigger than minimum voltage */
-		if (ulv_voltage > hwmgr->dyn_state.vddc_dependency_on_sclk->entries[0].v) {
-			pstate->VddcOffset = 0;
-		}
-		else {
-			/* used in SMIO Mode. not implemented for now. this is backup only for CI. */
-			pstate->VddcOffset = (uint16_t)(hwmgr->dyn_state.vddc_dependency_on_sclk->entries[0].v - ulv_voltage);
-		}
-	} else {
-		/* use minimum voltage if ulv voltage in pptable is bigger than minimum voltage */
-		if(ulv_voltage > hwmgr->dyn_state.vddc_dependency_on_sclk->entries[0].v) {
-			pstate->VddcOffsetVid = 0;
-		} else {
-			/* used in SVI2 Mode */
-			pstate->VddcOffsetVid = (uint8_t)((hwmgr->dyn_state.vddc_dependency_on_sclk->entries[0].v - ulv_voltage) * VOLTAGE_VID_OFFSET_SCALE2 / VOLTAGE_VID_OFFSET_SCALE1);
-		}
-	}
-
-	/* used in SVI2 Mode to shed phase */
-	pstate->VddcPhase = (data->vddc_phase_shed_control) ? 0 : 1;
-
-	if (0 == result) {
-		CONVERT_FROM_HOST_TO_SMC_UL(pstate->CcPwrDynRm);
-		CONVERT_FROM_HOST_TO_SMC_UL(pstate->CcPwrDynRm1);
-		CONVERT_FROM_HOST_TO_SMC_US(pstate->VddcOffset);
-	}
-
-	return result;
-}
-
-static int iceland_populate_ulv_state(struct pp_hwmgr *hwmgr, SMU71_Discrete_Ulv *ulv)
-{
-	return iceland_populate_ulv_level(hwmgr, ulv);
-}
-
-static int iceland_populate_smc_initial_state(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	uint8_t count, level;
-
-	count = (uint8_t)(hwmgr->dyn_state.vddc_dependency_on_sclk->count);
-
-	for (level = 0; level < count; level++) {
-		if (hwmgr->dyn_state.vddc_dependency_on_sclk->entries[level].clk
-			 >= data->vbios_boot_state.sclk_bootup_value) {
-			data->smc_state_table.GraphicsBootLevel = level;
-			break;
-		}
-	}
-
-	count = (uint8_t)(hwmgr->dyn_state.vddc_dependency_on_mclk->count);
-
-	for (level = 0; level < count; level++) {
-		if (hwmgr->dyn_state.vddc_dependency_on_mclk->entries[level].clk
-			>= data->vbios_boot_state.mclk_bootup_value) {
-			data->smc_state_table.MemoryBootLevel = level;
-			break;
-		}
-	}
-
-	return 0;
-}
-
-/**
- * Initializes the SMC table and uploads it
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @param    pInput  the pointer to input data (PowerState)
- * @return   always 0
- */
-static int iceland_init_smc_table(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	SMU71_Discrete_DpmTable  *table = &(data->smc_state_table);
-	const struct phw_iceland_ulv_parm *ulv = &(data->ulv);
-
-	result = iceland_setup_default_dpm_tables(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to setup default DPM tables!", return result;);
-	memset(&(data->smc_state_table), 0x00, sizeof(data->smc_state_table));
-
-	if (ICELAND_VOLTAGE_CONTROL_NONE != data->voltage_control) {
-		iceland_populate_smc_voltage_tables(hwmgr, table);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_AutomaticDCTransition)) {
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_GPIO_DC;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_StepVddc)) {
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_STEPVDDC;
-	}
-
-	if (data->is_memory_GDDR5) {
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_GDDR5;
-	}
-
-	if (ulv->ulv_supported) {
-		result = iceland_populate_ulv_state(hwmgr, &data->ulv_setting);
-		PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize ULV state!", return result;);
-
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_ULV_PARAMETER, ulv->ch_ulv_parameter);
-	}
-
-	result = iceland_populate_smc_link_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize Link Level!", return result;);
-
-	result = iceland_populate_all_graphic_levels(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize Graphics Level!", return result;);
-
-	result = iceland_populate_all_memory_levels(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize Memory Level!", return result;);
-
-	result = iceland_populate_smc_acpi_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize ACPI Level!", return result;);
-
-	result = iceland_populate_smc_vce_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize VCE Level!", return result;);
-
-	result = iceland_populate_smc_acp_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize ACP Level!", return result;);
-
-	result = iceland_populate_smc_samu_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize SAMU Level!", return result;);
-
-	/*
-	 * Since only the initial state is completely set up at this
-	 * point (the other states are just copies of the boot state)
-	 * we only need to populate the  ARB settings for the initial
-	 * state.
-	 */
-	result = iceland_program_memory_timing_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to Write ARB settings for the initial state.", return result;);
-
-	result = iceland_populate_smc_uvd_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize UVD Level!", return result;);
-
-	table->GraphicsBootLevel = 0;
-	table->MemoryBootLevel = 0;
-
-	/* find boot level from dpm table */
-	result = iceland_find_boot_level(&(data->dpm_table.sclk_table),
-			data->vbios_boot_state.sclk_bootup_value,
-			(uint32_t *)&(data->smc_state_table.GraphicsBootLevel));
-
-	if (result)
-		pr_warning("VBIOS did not find boot engine clock value in dependency table.\n");
-
-	result = iceland_find_boot_level(&(data->dpm_table.mclk_table),
-				data->vbios_boot_state.mclk_bootup_value,
-				(uint32_t *)&(data->smc_state_table.MemoryBootLevel));
-
-	if (result)
-		pr_warning("VBIOS did not find boot memory clock value in dependency table.\n");
-
-	table->BootVddc = data->vbios_boot_state.vddc_bootup_value;
-	if (ICELAND_VOLTAGE_CONTROL_NONE == data->vdd_ci_control) {
-		table->BootVddci = table->BootVddc;
-	}
-	else {
-		table->BootVddci = data->vbios_boot_state.vddci_bootup_value;
-	}
-	table->BootMVdd = data->vbios_boot_state.mvdd_bootup_value;
-
-	result = iceland_populate_smc_initial_state(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result, "Failed to initialize Boot State!", return result);
-
-	result = iceland_populate_bapm_parameters_in_dpm_table(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result, "Failed to populate BAPM Parameters!", return result);
-
-	table->GraphicsVoltageChangeEnable  = 1;
-	table->GraphicsThermThrottleEnable  = 1;
-	table->GraphicsInterval = 1;
-	table->VoltageInterval  = 1;
-	table->ThermalInterval  = 1;
-	table->TemperatureLimitHigh =
-		(data->thermal_temp_setting.temperature_high *
-		 ICELAND_Q88_FORMAT_CONVERSION_UNIT) / PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-	table->TemperatureLimitLow =
-		(data->thermal_temp_setting.temperature_low *
-		ICELAND_Q88_FORMAT_CONVERSION_UNIT) / PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-	table->MemoryVoltageChangeEnable  = 1;
-	table->MemoryInterval  = 1;
-	table->VoltageResponseTime  = 0;
-	table->PhaseResponseTime  = 0;
-	table->MemoryThermThrottleEnable  = 1;
-	table->PCIeBootLinkLevel = 0;
-	table->PCIeGenInterval = 1;
-
-	result = iceland_populate_smc_svi2_config(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to populate SVI2 setting!", return result);
-
-	table->ThermGpio  = 17;
-	table->SclkStepSize = 0x4000;
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SystemFlags);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMaskVddcVid);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMaskVddcPhase);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMaskVddciVid);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMaskMvddVid);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SclkStepSize);
-	CONVERT_FROM_HOST_TO_SMC_US(table->TemperatureLimitHigh);
-	CONVERT_FROM_HOST_TO_SMC_US(table->TemperatureLimitLow);
-	CONVERT_FROM_HOST_TO_SMC_US(table->VoltageResponseTime);
-	CONVERT_FROM_HOST_TO_SMC_US(table->PhaseResponseTime);
-
-	table->BootVddc = PP_HOST_TO_SMC_US(table->BootVddc * VOLTAGE_SCALE);
-	table->BootVddci = PP_HOST_TO_SMC_US(table->BootVddci * VOLTAGE_SCALE);
-	table->BootMVdd = PP_HOST_TO_SMC_US(table->BootMVdd * VOLTAGE_SCALE);
-
-	/* Upload all dpm data to SMC memory.(dpm level, dpm level count etc) */
-	result = smu7_copy_bytes_to_smc(hwmgr->smumgr, data->dpm_table_start +
-				offsetof(SMU71_Discrete_DpmTable, SystemFlags),
-				(uint8_t *)&(table->SystemFlags),
-				sizeof(SMU71_Discrete_DpmTable) - 3 * sizeof(SMU71_PIDController),
-				data->sram_end);
-
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to upload dpm data to SMC memory!", return result);
-
-	/* Upload all ulv setting to SMC memory.(dpm level, dpm level count etc) */
-	result = smu7_copy_bytes_to_smc(hwmgr->smumgr,
-			data->ulv_settings_start,
-			(uint8_t *)&(data->ulv_setting),
-			sizeof(SMU71_Discrete_Ulv),
-			data->sram_end);
-
-#if 0
-	/* Notify SMC to follow new GPIO scheme */
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_AutomaticDCTransition)) {
-		if (0 == iceland_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_UseNewGPIOScheme))
-			phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_SMCtoPPLIBAcdcGpioScheme);
-	}
-#endif
-
-	return result;
-}
-
-int iceland_populate_mc_reg_address(struct pp_hwmgr *hwmgr, SMU71_Discrete_MCRegisters *mc_reg_table)
-{
-	const struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	uint32_t i, j;
-
-	for (i = 0, j = 0; j < data->iceland_mc_reg_table.last; j++) {
-		if (data->iceland_mc_reg_table.validflag & 1<<j) {
-			PP_ASSERT_WITH_CODE(i < SMU71_DISCRETE_MC_REGISTER_ARRAY_SIZE,
-				"Index of mc_reg_table->address[] array out of boundary", return -1);
-			mc_reg_table->address[i].s0 =
-				PP_HOST_TO_SMC_US(data->iceland_mc_reg_table.mc_reg_address[j].s0);
-			mc_reg_table->address[i].s1 =
-				PP_HOST_TO_SMC_US(data->iceland_mc_reg_table.mc_reg_address[j].s1);
-			i++;
-		}
-	}
-
-	mc_reg_table->last = (uint8_t)i;
-
-	return 0;
-}
-
-/* convert register values from driver to SMC format */
-void iceland_convert_mc_registers(
-	const phw_iceland_mc_reg_entry * pEntry,
-	SMU71_Discrete_MCRegisterSet *pData,
-	uint32_t numEntries, uint32_t validflag)
-{
-	uint32_t i, j;
-
-	for (i = 0, j = 0; j < numEntries; j++) {
-		if (validflag & 1<<j) {
-			pData->value[i] = PP_HOST_TO_SMC_UL(pEntry->mc_data[j]);
-			i++;
-		}
-	}
-}
-
-/* find the entry in the memory range table, then populate the value to SMC's iceland_mc_reg_table */
-int iceland_convert_mc_reg_table_entry_to_smc(
-		struct pp_hwmgr *hwmgr,
-		const uint32_t memory_clock,
-		SMU71_Discrete_MCRegisterSet *mc_reg_table_data
-		)
-{
-	const iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint32_t i = 0;
-
-	for (i = 0; i < data->iceland_mc_reg_table.num_entries; i++) {
-		if (memory_clock <=
-			data->iceland_mc_reg_table.mc_reg_table_entry[i].mclk_max) {
-			break;
-		}
-	}
-
-	if ((i == data->iceland_mc_reg_table.num_entries) && (i > 0))
-		--i;
-
-	iceland_convert_mc_registers(&data->iceland_mc_reg_table.mc_reg_table_entry[i],
-		mc_reg_table_data, data->iceland_mc_reg_table.last, data->iceland_mc_reg_table.validflag);
-
-	return 0;
-}
-
-int iceland_convert_mc_reg_table_to_smc(struct pp_hwmgr *hwmgr,
-		SMU71_Discrete_MCRegisters *mc_reg_table)
-{
-	int result = 0;
-	iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	int res;
-	uint32_t i;
-
-	for (i = 0; i < data->dpm_table.mclk_table.count; i++) {
-		res = iceland_convert_mc_reg_table_entry_to_smc(
-				hwmgr,
-				data->dpm_table.mclk_table.dpm_levels[i].value,
-				&mc_reg_table->data[i]
-				);
-
-		if (0 != res)
-			result = res;
-	}
-
-	return result;
-}
-
-int iceland_populate_initial_mc_reg_table(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	memset(&data->mc_reg_table, 0x00, sizeof(SMU71_Discrete_MCRegisters));
-	result = iceland_populate_mc_reg_address(hwmgr, &(data->mc_reg_table));
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize MCRegTable for the MC register addresses!", return result;);
-
-	result = iceland_convert_mc_reg_table_to_smc(hwmgr, &data->mc_reg_table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize MCRegTable for driver state!", return result;);
-
-	return smu7_copy_bytes_to_smc(hwmgr->smumgr, data->mc_reg_table_start,
-			(uint8_t *)&data->mc_reg_table, sizeof(SMU71_Discrete_MCRegisters), data->sram_end);
-}
-
-int iceland_notify_smc_display_change(struct pp_hwmgr *hwmgr, bool has_display)
-{
-	PPSMC_Msg msg = has_display? (PPSMC_Msg)PPSMC_HasDisplay : (PPSMC_Msg)PPSMC_NoDisplay;
-
-	return (smum_send_msg_to_smc(hwmgr->smumgr, msg) == 0) ?  0 : -1;
-}
-
-int iceland_enable_sclk_control(struct pp_hwmgr *hwmgr)
-{
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL, SCLK_PWRMGT_OFF, 0);
-
-	return 0;
-}
-
-int iceland_enable_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	/* enable SCLK dpm */
-	if (0 == data->sclk_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						   PPSMC_MSG_DPM_Enable)),
-				"Failed to enable SCLK DPM during DPM Start Function!",
-				return -1);
-	}
-
-	/* enable MCLK dpm */
-	if (0 == data->mclk_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					     PPSMC_MSG_MCLKDPM_Enable)),
-				"Failed to enable MCLK DPM during DPM Start Function!",
-				return -1);
-
-		PHM_WRITE_FIELD(hwmgr->device, MC_SEQ_CNTL_3, CAC_EN, 0x1);
-
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_MC0_CNTL, 0x05);/* CH0,1 read */
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_MC1_CNTL, 0x05);/* CH2,3 read */
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_CPL_CNTL, 0x100005);/*Read */
-
-		udelay(10);
-
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_MC0_CNTL, 0x400005);/* CH0,1 write */
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_MC1_CNTL, 0x400005);/* CH2,3 write */
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_CPL_CNTL, 0x500005);/* write */
-
-	}
-
-	return 0;
-}
-
-int iceland_start_dpm(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	/* enable general power management */
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT, GLOBAL_PWRMGT_EN, 1);
-	/* enable sclk deep sleep */
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL, DYNAMIC_PM_EN, 1);
-
-	/* prepare for PCIE DPM */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SOFT_REGISTERS_TABLE_12, VoltageChangeTimeout, 0x1000);
-
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__PCIE, SWRST_COMMAND_1, RESETLC, 0x0);
-
-	PP_ASSERT_WITH_CODE(
-			(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					PPSMC_MSG_Voltage_Cntl_Enable)),
-			"Failed to enable voltage DPM during DPM Start Function!",
-			return -1);
-
-	if (0 != iceland_enable_sclk_mclk_dpm(hwmgr)) {
-		PP_ASSERT_WITH_CODE(0, "Failed to enable Sclk DPM and Mclk DPM!", return -1);
-	}
-
-	/* enable PCIE dpm */
-	if (0 == data->pcie_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_PCIeDPM_Enable)),
-				"Failed to enable pcie DPM during DPM Start Function!",
-				return -1
-				);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			    PHM_PlatformCaps_Falcon_QuickTransition)) {
-		smum_send_msg_to_smc(hwmgr->smumgr,
-				     PPSMC_MSG_EnableACDCGPIOInterrupt);
-	}
-
-	return 0;
-}
-
-static void iceland_set_dpm_event_sources(struct pp_hwmgr *hwmgr,
-		uint32_t sources)
-{
-	bool protection;
-	enum DPM_EVENT_SRC src;
-
-	switch (sources) {
-	default:
-		printk(KERN_ERR "Unknown throttling event sources.");
-		/* fall through */
-	case 0:
-		protection = false;
-		/* src is unused */
-		break;
-	case (1 << PHM_AutoThrottleSource_Thermal):
-		protection = true;
-		src = DPM_EVENT_SRC_DIGITAL;
-		break;
-	case (1 << PHM_AutoThrottleSource_External):
-		protection = true;
-		src = DPM_EVENT_SRC_EXTERNAL;
-		break;
-	case (1 << PHM_AutoThrottleSource_External) |
-			(1 << PHM_AutoThrottleSource_Thermal):
-		protection = true;
-		src = DPM_EVENT_SRC_DIGITAL_OR_EXTERNAL;
-		break;
-	}
-	/* Order matters - don't enable thermal protection for the wrong source. */
-	if (protection) {
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_CTRL,
-				DPM_EVENT_SRC, src);
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-				THERMAL_PROTECTION_DIS,
-				!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_ThermalController));
-	} else
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-				THERMAL_PROTECTION_DIS, 1);
-}
-
-static int iceland_enable_auto_throttle_source(struct pp_hwmgr *hwmgr,
-		PHM_AutoThrottleSource source)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	if (!(data->active_auto_throttle_sources & (1 << source))) {
-		data->active_auto_throttle_sources |= 1 << source;
-		iceland_set_dpm_event_sources(hwmgr, data->active_auto_throttle_sources);
-	}
-	return 0;
-}
-
-static int iceland_enable_thermal_auto_throttle(struct pp_hwmgr *hwmgr)
-{
-	return iceland_enable_auto_throttle_source(hwmgr, PHM_AutoThrottleSource_Thermal);
-}
-
-
-/**
-* Programs the Deep Sleep registers
-*
-* @param    pHwMgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data (PhwEvergreen_DisplayConfiguration)
-* @param    pOutput the pointer to output data (unused)
-* @param    pStorage the pointer to temporary storage (unused)
-* @param    Result the last failure code (unused)
-* @return   always 0
-*/
-static int iceland_enable_deep_sleep_master_switch(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			    PHM_PlatformCaps_SclkDeepSleep)) {
-		if (smum_send_msg_to_smc(hwmgr->smumgr,
-					 PPSMC_MSG_MASTER_DeepSleep_ON) != 0)
-			PP_ASSERT_WITH_CODE(false,
-					    "Attempt to enable Master Deep Sleep switch failed!",
-					    return -EINVAL);
-	} else {
-		if (smum_send_msg_to_smc(hwmgr->smumgr,
-					 PPSMC_MSG_MASTER_DeepSleep_OFF) != 0)
-			PP_ASSERT_WITH_CODE(false,
-					    "Attempt to disable Master Deep Sleep switch failed!",
-					    return -EINVAL);
-	}
-
-	return 0;
-}
-
-static int iceland_enable_dpm_tasks(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	if (cf_iceland_voltage_control(hwmgr)) {
-		tmp_result = iceland_enable_voltage_control(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable voltage control!", return tmp_result);
-
-		tmp_result = iceland_construct_voltage_tables(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to contruct voltage tables!", return tmp_result);
-	}
-
-	tmp_result = iceland_initialize_mc_reg_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to initialize MC reg table!", return tmp_result);
-
-	tmp_result = iceland_program_static_screen_threshold_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to program static screen threshold parameters!", return tmp_result);
-
-	tmp_result = iceland_enable_display_gap(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to enable display gap!", return tmp_result);
-
-	tmp_result = iceland_program_voting_clients(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to program voting clients!", return tmp_result);
-
-	tmp_result = iceland_upload_firmware(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to upload firmware header!", return tmp_result);
-
-	tmp_result = iceland_process_firmware_header(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to process firmware header!", return tmp_result);
-
-	tmp_result = iceland_initial_switch_from_arb_f0_to_f1(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to initialize switch from ArbF0 to F1!", return tmp_result);
-
-	tmp_result = iceland_init_smc_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to initialize SMC table!", return tmp_result);
-
-	tmp_result = iceland_populate_initial_mc_reg_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to populate initialize MC Reg table!", return tmp_result);
-
-	tmp_result = iceland_populate_pm_fuses(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to populate PM fuses!", return tmp_result);
-
-
-	/* enable SCLK control */
-	tmp_result = iceland_enable_sclk_control(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to enable SCLK control!", return tmp_result);
-
-	tmp_result = iceland_enable_deep_sleep_master_switch(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-		"Failed to enable deep sleep!", return tmp_result);
-
-	/* enable DPM */
-	tmp_result = iceland_start_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to start DPM!", return tmp_result);
-
-	tmp_result = iceland_enable_smc_cac(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to enable SMC CAC!", return tmp_result);
-
-	tmp_result = iceland_enable_power_containment(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to enable power containment!", return tmp_result);
-
-	tmp_result = iceland_power_control_set_level(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to power control set level!", result = tmp_result);
-
-	tmp_result = iceland_enable_thermal_auto_throttle(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable thermal auto throttle!", result = tmp_result);
-
-	return result;
-}
-
-static int iceland_hwmgr_backend_fini(struct pp_hwmgr *hwmgr)
-{
-	return phm_hwmgr_backend_fini(hwmgr);
-}
-
-static void iceland_initialize_dpm_defaults(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct phw_iceland_ulv_parm *ulv;
-
-	ulv = &data->ulv;
-	ulv->ch_ulv_parameter = PPICELAND_CGULVPARAMETER_DFLT;
-	data->voting_rights_clients0 = PPICELAND_VOTINGRIGHTSCLIENTS_DFLT0;
-	data->voting_rights_clients1 = PPICELAND_VOTINGRIGHTSCLIENTS_DFLT1;
-	data->voting_rights_clients2 = PPICELAND_VOTINGRIGHTSCLIENTS_DFLT2;
-	data->voting_rights_clients3 = PPICELAND_VOTINGRIGHTSCLIENTS_DFLT3;
-	data->voting_rights_clients4 = PPICELAND_VOTINGRIGHTSCLIENTS_DFLT4;
-	data->voting_rights_clients5 = PPICELAND_VOTINGRIGHTSCLIENTS_DFLT5;
-	data->voting_rights_clients6 = PPICELAND_VOTINGRIGHTSCLIENTS_DFLT6;
-	data->voting_rights_clients7 = PPICELAND_VOTINGRIGHTSCLIENTS_DFLT7;
-
-	data->static_screen_threshold_unit = PPICELAND_STATICSCREENTHRESHOLDUNIT_DFLT;
-	data->static_screen_threshold = PPICELAND_STATICSCREENTHRESHOLD_DFLT;
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		      PHM_PlatformCaps_ABM);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		    PHM_PlatformCaps_NonABMSupportInPPLib);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		    PHM_PlatformCaps_DynamicACTiming);
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		      PHM_PlatformCaps_DisableMemoryTransition);
-
-	iceland_initialize_power_tune_defaults(hwmgr);
-
-	data->mclk_strobe_mode_threshold = 40000;
-	data->mclk_stutter_mode_threshold = 30000;
-	data->mclk_edc_enable_threshold = 40000;
-	data->mclk_edc_wr_enable_threshold = 40000;
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		      PHM_PlatformCaps_DisableMCLS);
-
-	data->pcie_gen_performance.max = PP_PCIEGen1;
-	data->pcie_gen_performance.min = PP_PCIEGen3;
-	data->pcie_gen_power_saving.max = PP_PCIEGen1;
-	data->pcie_gen_power_saving.min = PP_PCIEGen3;
-
-	data->pcie_lane_performance.max = 0;
-	data->pcie_lane_performance.min = 16;
-	data->pcie_lane_power_saving.max = 0;
-	data->pcie_lane_power_saving.min = 16;
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		      PHM_PlatformCaps_SclkThrottleLowNotification);
-}
-
-static int iceland_get_evv_voltage(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-	uint16_t    virtual_voltage_id;
-	uint16_t    vddc = 0;
-	uint16_t    i;
-
-	/* the count indicates actual number of entries */
-	data->vddc_leakage.count = 0;
-	data->vddci_leakage.count = 0;
-
-	if (!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_EVV)) {
-		pr_err("Iceland should always support EVV\n");
-		return -EINVAL;
-	}
-
-	/* retrieve voltage for leakage ID (0xff01 + i) */
-	for (i = 0; i < ICELAND_MAX_LEAKAGE_COUNT; i++) {
-		virtual_voltage_id = ATOM_VIRTUAL_VOLTAGE_ID0 + i;
-
-		PP_ASSERT_WITH_CODE((0 == atomctrl_get_voltage_evv(hwmgr, virtual_voltage_id, &vddc)),
-				    "Error retrieving EVV voltage value!\n", continue);
-
-		if (vddc >= 2000)
-			pr_warning("Invalid VDDC value!\n");
-
-		if (vddc != 0 && vddc != virtual_voltage_id) {
-			data->vddc_leakage.actual_voltage[data->vddc_leakage.count] = vddc;
-			data->vddc_leakage.leakage_id[data->vddc_leakage.count] = virtual_voltage_id;
-			data->vddc_leakage.count++;
-		}
-	}
-
-	return 0;
-}
-
-static void iceland_patch_with_vddc_leakage(struct pp_hwmgr *hwmgr,
-					    uint32_t *vddc)
-{
-	iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint32_t leakage_index;
-	struct phw_iceland_leakage_voltage *leakage_table = &data->vddc_leakage;
-
-	/* search for leakage voltage ID 0xff01 ~ 0xff08 */
-	for (leakage_index = 0; leakage_index < leakage_table->count; leakage_index++) {
-		/*
-		 * If this voltage matches a leakage voltage ID, patch
-		 * with actual leakage voltage.
-		 */
-		if (leakage_table->leakage_id[leakage_index] == *vddc) {
-			/*
-			 * Need to make sure vddc is less than 2v or
-			 * else, it could burn the ASIC.
-			 */
-			if (leakage_table->actual_voltage[leakage_index] >= 2000)
-				pr_warning("Invalid VDDC value!\n");
-			*vddc = leakage_table->actual_voltage[leakage_index];
-			/* we found leakage voltage */
-			break;
-		}
-	}
-
-	if (*vddc >= ATOM_VIRTUAL_VOLTAGE_ID0)
-		pr_warning("Voltage value looks like a Leakage ID but it's not patched\n");
-}
-
-static void iceland_patch_with_vddci_leakage(struct pp_hwmgr *hwmgr,
-					     uint32_t *vddci)
-{
-	iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint32_t leakage_index;
-	struct phw_iceland_leakage_voltage *leakage_table = &data->vddci_leakage;
-
-	/* search for leakage voltage ID 0xff01 ~ 0xff08 */
-	for (leakage_index = 0; leakage_index < leakage_table->count; leakage_index++) {
-		/*
-		 * If this voltage matches a leakage voltage ID, patch
-		 * with actual leakage voltage.
-		 */
-		if (leakage_table->leakage_id[leakage_index] == *vddci) {
-			*vddci = leakage_table->actual_voltage[leakage_index];
-			/* we found leakage voltage */
-			break;
-		}
-	}
-
-	if (*vddci >= ATOM_VIRTUAL_VOLTAGE_ID0)
-		pr_warning("Voltage value looks like a Leakage ID but it's not patched\n");
-}
-
-static int iceland_patch_vddc(struct pp_hwmgr *hwmgr,
-			      struct phm_clock_voltage_dependency_table *tab)
-{
-	uint16_t i;
-
-	if (tab)
-		for (i = 0; i < tab->count; i++)
-			iceland_patch_with_vddc_leakage(hwmgr, &tab->entries[i].v);
-
-	return 0;
-}
-
-static int iceland_patch_vddci(struct pp_hwmgr *hwmgr,
-			       struct phm_clock_voltage_dependency_table *tab)
-{
-	uint16_t i;
-
-	if (tab)
-		for (i = 0; i < tab->count; i++)
-			iceland_patch_with_vddci_leakage(hwmgr, &tab->entries[i].v);
-
-	return 0;
-}
-
-static int iceland_patch_vce_vddc(struct pp_hwmgr *hwmgr,
-				  struct phm_vce_clock_voltage_dependency_table *tab)
-{
-	uint16_t i;
-
-	if (tab)
-		for (i = 0; i < tab->count; i++)
-			iceland_patch_with_vddc_leakage(hwmgr, &tab->entries[i].v);
-
-	return 0;
-}
-
-
-static int iceland_patch_uvd_vddc(struct pp_hwmgr *hwmgr,
-				  struct phm_uvd_clock_voltage_dependency_table *tab)
-{
-	uint16_t i;
-
-	if (tab)
-		for (i = 0; i < tab->count; i++)
-			iceland_patch_with_vddc_leakage(hwmgr, &tab->entries[i].v);
-
-	return 0;
-}
-
-static int iceland_patch_vddc_shed_limit(struct pp_hwmgr *hwmgr,
-					 struct phm_phase_shedding_limits_table *tab)
-{
-	uint16_t i;
-
-	if (tab)
-		for (i = 0; i < tab->count; i++)
-			iceland_patch_with_vddc_leakage(hwmgr, &tab->entries[i].Voltage);
-
-	return 0;
-}
-
-static int iceland_patch_samu_vddc(struct pp_hwmgr *hwmgr,
-				   struct phm_samu_clock_voltage_dependency_table *tab)
-{
-	uint16_t i;
-
-	if (tab)
-		for (i = 0; i < tab->count; i++)
-			iceland_patch_with_vddc_leakage(hwmgr, &tab->entries[i].v);
-
-	return 0;
-}
-
-static int iceland_patch_acp_vddc(struct pp_hwmgr *hwmgr,
-				  struct phm_acp_clock_voltage_dependency_table *tab)
-{
-	uint16_t i;
-
-	if (tab)
-		for (i = 0; i < tab->count; i++)
-			iceland_patch_with_vddc_leakage(hwmgr, &tab->entries[i].v);
-
-	return 0;
-}
-
-static int iceland_patch_limits_vddc(struct pp_hwmgr *hwmgr,
-				     struct phm_clock_and_voltage_limits *tab)
-{
-	if (tab) {
-		iceland_patch_with_vddc_leakage(hwmgr, (uint32_t *)&tab->vddc);
-		iceland_patch_with_vddci_leakage(hwmgr, (uint32_t *)&tab->vddci);
-	}
-
-	return 0;
-}
-
-static int iceland_patch_cac_vddc(struct pp_hwmgr *hwmgr, struct phm_cac_leakage_table *tab)
-{
-	uint32_t i;
-	uint32_t vddc;
-
-	if (tab) {
-		for (i = 0; i < tab->count; i++) {
-			vddc = (uint32_t)(tab->entries[i].Vddc);
-			iceland_patch_with_vddc_leakage(hwmgr, &vddc);
-			tab->entries[i].Vddc = (uint16_t)vddc;
-		}
-	}
-
-	return 0;
-}
-
-static int iceland_patch_dependency_tables_with_leakage(struct pp_hwmgr *hwmgr)
-{
-	int tmp;
-
-	tmp = iceland_patch_vddc(hwmgr, hwmgr->dyn_state.vddc_dependency_on_sclk);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_vddc(hwmgr, hwmgr->dyn_state.vddc_dependency_on_mclk);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_vddc(hwmgr, hwmgr->dyn_state.vddc_dep_on_dal_pwrl);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_vddci(hwmgr, hwmgr->dyn_state.vddci_dependency_on_mclk);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_vce_vddc(hwmgr, hwmgr->dyn_state.vce_clock_voltage_dependency_table);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_uvd_vddc(hwmgr, hwmgr->dyn_state.uvd_clock_voltage_dependency_table);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_samu_vddc(hwmgr, hwmgr->dyn_state.samu_clock_voltage_dependency_table);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_acp_vddc(hwmgr, hwmgr->dyn_state.acp_clock_voltage_dependency_table);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_vddc_shed_limit(hwmgr, hwmgr->dyn_state.vddc_phase_shed_limits_table);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_limits_vddc(hwmgr, &hwmgr->dyn_state.max_clock_voltage_on_ac);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_limits_vddc(hwmgr, &hwmgr->dyn_state.max_clock_voltage_on_dc);
-	if(tmp)
-		return -EINVAL;
-
-	tmp = iceland_patch_cac_vddc(hwmgr, hwmgr->dyn_state.cac_leakage_table);
-	if(tmp)
-		return -EINVAL;
-
-	return 0;
-}
-
-static int iceland_set_private_var_based_on_pptale(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	struct phm_clock_voltage_dependency_table *allowed_sclk_vddc_table = hwmgr->dyn_state.vddc_dependency_on_sclk;
-	struct phm_clock_voltage_dependency_table *allowed_mclk_vddc_table = hwmgr->dyn_state.vddc_dependency_on_mclk;
-	struct phm_clock_voltage_dependency_table *allowed_mclk_vddci_table = hwmgr->dyn_state.vddci_dependency_on_mclk;
-
-	PP_ASSERT_WITH_CODE(allowed_sclk_vddc_table != NULL,
-		"VDDC dependency on SCLK table is missing. This table is mandatory\n", return -EINVAL);
-	PP_ASSERT_WITH_CODE(allowed_sclk_vddc_table->count >= 1,
-		"VDDC dependency on SCLK table has to have is missing. This table is mandatory\n", return -EINVAL);
-
-	PP_ASSERT_WITH_CODE(allowed_mclk_vddc_table != NULL,
-		"VDDC dependency on MCLK table is missing. This table is mandatory\n", return -EINVAL);
-	PP_ASSERT_WITH_CODE(allowed_mclk_vddc_table->count >= 1,
-		"VDD dependency on MCLK table has to have is missing. This table is mandatory\n", return -EINVAL);
-
-	data->min_vddc_in_pp_table = (uint16_t)allowed_sclk_vddc_table->entries[0].v;
-	data->max_vddc_in_pp_table = (uint16_t)allowed_sclk_vddc_table->entries[allowed_sclk_vddc_table->count - 1].v;
-
-	hwmgr->dyn_state.max_clock_voltage_on_ac.sclk =
-		allowed_sclk_vddc_table->entries[allowed_sclk_vddc_table->count - 1].clk;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.mclk =
-		allowed_mclk_vddc_table->entries[allowed_mclk_vddc_table->count - 1].clk;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.vddc =
-		allowed_sclk_vddc_table->entries[allowed_sclk_vddc_table->count - 1].v;
-
-	if (allowed_mclk_vddci_table != NULL && allowed_mclk_vddci_table->count >= 1) {
-		data->min_vddci_in_pp_table = (uint16_t)allowed_mclk_vddci_table->entries[0].v;
-		data->max_vddci_in_pp_table = (uint16_t)allowed_mclk_vddci_table->entries[allowed_mclk_vddci_table->count - 1].v;
-	}
-
-	if (hwmgr->dyn_state.vddci_dependency_on_mclk != NULL && hwmgr->dyn_state.vddci_dependency_on_mclk->count > 1)
-		hwmgr->dyn_state.max_clock_voltage_on_ac.vddci = hwmgr->dyn_state.vddci_dependency_on_mclk->entries[hwmgr->dyn_state.vddci_dependency_on_mclk->count - 1].v;
-
-	return 0;
-}
-
-static int iceland_initializa_dynamic_state_adjustment_rule_settings(struct pp_hwmgr *hwmgr)
-{
-	uint32_t table_size;
-	struct phm_clock_voltage_dependency_table *table_clk_vlt;
-
-	hwmgr->dyn_state.mclk_sclk_ratio = 4;
-	hwmgr->dyn_state.sclk_mclk_delta = 15000;      /* 150 MHz */
-	hwmgr->dyn_state.vddc_vddci_delta = 200;       /* 200mV */
-
-	/* initialize vddc_dep_on_dal_pwrl table */
-	table_size = sizeof(uint32_t) + 4 * sizeof(struct phm_clock_voltage_dependency_record);
-	table_clk_vlt = (struct phm_clock_voltage_dependency_table *)kzalloc(table_size, GFP_KERNEL);
-
-	if (NULL == table_clk_vlt) {
-		pr_err("[ powerplay ] Can not allocate space for vddc_dep_on_dal_pwrl! \n");
-		return -ENOMEM;
-	} else {
-		table_clk_vlt->count = 4;
-		table_clk_vlt->entries[0].clk = PP_DAL_POWERLEVEL_ULTRALOW;
-		table_clk_vlt->entries[0].v = 0;
-		table_clk_vlt->entries[1].clk = PP_DAL_POWERLEVEL_LOW;
-		table_clk_vlt->entries[1].v = 720;
-		table_clk_vlt->entries[2].clk = PP_DAL_POWERLEVEL_NOMINAL;
-		table_clk_vlt->entries[2].v = 810;
-		table_clk_vlt->entries[3].clk = PP_DAL_POWERLEVEL_PERFORMANCE;
-		table_clk_vlt->entries[3].v = 900;
-		hwmgr->dyn_state.vddc_dep_on_dal_pwrl = table_clk_vlt;
-	}
-
-	return 0;
-}
-
-/**
- * Initializes the Volcanic Islands Hardware Manager
- *
- * @param   hwmgr the address of the powerplay hardware manager.
- * @return   1 if success; otherwise appropriate error code.
- */
-static int iceland_hwmgr_backend_init(struct pp_hwmgr *hwmgr)
-{
-	int result = 0;
-	SMU71_Discrete_DpmTable *table = NULL;
-	iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	pp_atomctrl_gpio_pin_assignment gpio_pin_assignment;
-	bool stay_in_boot;
-	struct phw_iceland_ulv_parm *ulv;
-	struct cgs_system_info sys_info = {0};
-
-	PP_ASSERT_WITH_CODE((NULL != hwmgr),
-		"Invalid Parameter!", return -EINVAL;);
-
-	data->dll_defaule_on = 0;
-	data->sram_end = SMC_RAM_END;
-
-	data->activity_target[0] = PPICELAND_TARGETACTIVITY_DFLT;
-	data->activity_target[1] = PPICELAND_TARGETACTIVITY_DFLT;
-	data->activity_target[2] = PPICELAND_TARGETACTIVITY_DFLT;
-	data->activity_target[3] = PPICELAND_TARGETACTIVITY_DFLT;
-	data->activity_target[4] = PPICELAND_TARGETACTIVITY_DFLT;
-	data->activity_target[5] = PPICELAND_TARGETACTIVITY_DFLT;
-	data->activity_target[6] = PPICELAND_TARGETACTIVITY_DFLT;
-	data->activity_target[7] = PPICELAND_TARGETACTIVITY_DFLT;
-
-	data->mclk_activity_target = PPICELAND_MCLK_TARGETACTIVITY_DFLT;
-
-	data->sclk_dpm_key_disabled = 0;
-	data->mclk_dpm_key_disabled = 0;
-	data->pcie_dpm_key_disabled = 0;
-	data->pcc_monitor_enabled = 0;
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		    PHM_PlatformCaps_UnTabledHardwareInterface);
-
-	data->gpio_debug = 0;
-	data->engine_clock_data = 0;
-	data->memory_clock_data = 0;
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		      PHM_PlatformCaps_SclkDeepSleepAboveLow);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		    PHM_PlatformCaps_DynamicPatchPowerState);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		    PHM_PlatformCaps_TablelessHardwareInterface);
-
-	/* Initializes DPM default values. */
-	iceland_initialize_dpm_defaults(hwmgr);
-
-	/* Enable Platform EVV support. */
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		    PHM_PlatformCaps_EVV);
-
-	/* Get leakage voltage based on leakage ID. */
-	result = iceland_get_evv_voltage(hwmgr);
-	if (result)
-		goto failed;
-
-	/**
-	 * Patch our voltage dependency table with actual leakage
-	 * voltage. We need to perform leakage translation before it's
-	 * used by other functions such as
-	 * iceland_set_hwmgr_variables_based_on_pptable.
-	 */
-	result = iceland_patch_dependency_tables_with_leakage(hwmgr);
-	if (result)
-		goto failed;
-
-	/* Parse pptable data read from VBIOS. */
-	result = iceland_set_private_var_based_on_pptale(hwmgr);
-	if (result)
-		goto failed;
-
-	/* ULV support */
-	ulv = &(data->ulv);
-	ulv->ulv_supported = 1;
-
-	/* Initalize Dynamic State Adjustment Rule Settings*/
-	result = iceland_initializa_dynamic_state_adjustment_rule_settings(hwmgr);
-	if (result) {
-		pr_err("[ powerplay ] iceland_initializa_dynamic_state_adjustment_rule_settings failed!\n");
-		goto failed;
-	}
-
-	data->voltage_control = ICELAND_VOLTAGE_CONTROL_NONE;
-	data->vdd_ci_control = ICELAND_VOLTAGE_CONTROL_NONE;
-	data->mvdd_control = ICELAND_VOLTAGE_CONTROL_NONE;
-
-	/*
-	 * Hardcode thermal temperature settings for now, these will
-	 * be overwritten if a custom policy exists.
-	 */
-	data->thermal_temp_setting.temperature_low = 99500;
-	data->thermal_temp_setting.temperature_high = 100000;
-	data->thermal_temp_setting.temperature_shutdown = 104000;
-	data->uvd_enabled = false;
-
-	table = &data->smc_state_table;
-
-	if (atomctrl_get_pp_assign_pin(hwmgr, VDDC_VRHOT_GPIO_PINID,
-				       &gpio_pin_assignment)) {
-		table->VRHotGpio = gpio_pin_assignment.uc_gpio_pin_bit_shift;
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			    PHM_PlatformCaps_RegulatorHot);
-	} else {
-		table->VRHotGpio = ICELAND_UNUSED_GPIO_PIN;
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			      PHM_PlatformCaps_RegulatorHot);
-	}
-
-	if (atomctrl_get_pp_assign_pin(hwmgr, PP_AC_DC_SWITCH_GPIO_PINID,
-				       &gpio_pin_assignment)) {
-		table->AcDcGpio = gpio_pin_assignment.uc_gpio_pin_bit_shift;
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			    PHM_PlatformCaps_AutomaticDCTransition);
-	} else {
-		table->AcDcGpio = ICELAND_UNUSED_GPIO_PIN;
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			      PHM_PlatformCaps_AutomaticDCTransition);
-	}
-
-	/*
-	 * If ucGPIO_ID=VDDC_PCC_GPIO_PINID in GPIO_LUTable, Peak.
-	 * Current Control feature is enabled and we should program
-	 * PCC HW register
-	 */
-	if (atomctrl_get_pp_assign_pin(hwmgr, VDDC_PCC_GPIO_PINID,
-				       &gpio_pin_assignment)) {
-		uint32_t temp_reg = cgs_read_ind_register(hwmgr->device,
-							  CGS_IND_REG__SMC,
-							  ixCNB_PWRMGT_CNTL);
-
-		switch (gpio_pin_assignment.uc_gpio_pin_bit_shift) {
-		case 0:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, GNB_SLOW_MODE, 0x1);
-			break;
-		case 1:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, GNB_SLOW_MODE, 0x2);
-			break;
-		case 2:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, GNB_SLOW, 0x1);
-			break;
-		case 3:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, FORCE_NB_PS1, 0x1);
-			break;
-		case 4:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, DPM_ENABLED, 0x1);
-			break;
-		default:
-			pr_warning("[ powerplay ] Failed to setup PCC HW register! Wrong GPIO assigned for VDDC_PCC_GPIO_PINID!\n");
-			break;
-		}
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				       ixCNB_PWRMGT_CNTL, temp_reg);
-	}
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		    PHM_PlatformCaps_EnableSMU7ThermalManagement);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		    PHM_PlatformCaps_SMU7);
-
-	if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-						     VOLTAGE_TYPE_VDDC,
-						     VOLTAGE_OBJ_GPIO_LUT))
-		data->voltage_control = ICELAND_VOLTAGE_CONTROL_BY_GPIO;
-	else if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-							  VOLTAGE_TYPE_VDDC,
-							  VOLTAGE_OBJ_SVID2))
-		data->voltage_control = ICELAND_VOLTAGE_CONTROL_BY_SVID2;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			    PHM_PlatformCaps_ControlVDDCI)) {
-		if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-							     VOLTAGE_TYPE_VDDCI,
-							     VOLTAGE_OBJ_GPIO_LUT))
-			data->vdd_ci_control = ICELAND_VOLTAGE_CONTROL_BY_GPIO;
-		else if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-								  VOLTAGE_TYPE_VDDCI,
-								  VOLTAGE_OBJ_SVID2))
-			data->vdd_ci_control = ICELAND_VOLTAGE_CONTROL_BY_SVID2;
-	}
-
-	if (data->vdd_ci_control == ICELAND_VOLTAGE_CONTROL_NONE)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			      PHM_PlatformCaps_ControlVDDCI);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			    PHM_PlatformCaps_EnableMVDDControl)) {
-		if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-							     VOLTAGE_TYPE_MVDDC,
-							     VOLTAGE_OBJ_GPIO_LUT))
-			data->mvdd_control = ICELAND_VOLTAGE_CONTROL_BY_GPIO;
-		else if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-								  VOLTAGE_TYPE_MVDDC,
-								  VOLTAGE_OBJ_SVID2))
-			data->mvdd_control = ICELAND_VOLTAGE_CONTROL_BY_SVID2;
-	}
-
-	if (data->mvdd_control == ICELAND_VOLTAGE_CONTROL_NONE)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			      PHM_PlatformCaps_EnableMVDDControl);
-
-	data->vddc_phase_shed_control = false;
-
-	stay_in_boot = phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				       PHM_PlatformCaps_StayInBootState);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DynamicPowerManagement);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ActivityReporting);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_GFXClockGatingSupport);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MemorySpreadSpectrumSupport);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EngineSpreadSpectrumSupport);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DynamicPCIEGen2Support);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SMC);
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DisablePowerGating);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_BACO);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ThermalAutoThrottling);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DisableLSClockGating);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SamuDPM);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_AcpDPM);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_OD6inACSupport);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EnablePlatformPowerManagement);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PauseMMSessions);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_OD6PlusinACSupport);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PauseMMSessions);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_GFXClockGatingManagedInCAIL);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_IcelandULPSSWWorkAround);
-
-
-	/* iceland doesn't support UVD and VCE */
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		      PHM_PlatformCaps_UVDPowerGating);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		      PHM_PlatformCaps_VCEPowerGating);
-
-	sys_info.size = sizeof(struct cgs_system_info);
-	sys_info.info_id = CGS_SYSTEM_INFO_PG_FLAGS;
-	result = cgs_query_system_info(hwmgr->device, &sys_info);
-	if (!result) {
-		if (sys_info.value & AMD_PG_SUPPORT_UVD)
-			phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				      PHM_PlatformCaps_UVDPowerGating);
-		if (sys_info.value & AMD_PG_SUPPORT_VCE)
-			phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				      PHM_PlatformCaps_VCEPowerGating);
-
-		data->is_tlu_enabled = false;
-		hwmgr->platform_descriptor.hardwareActivityPerformanceLevels =
-			ICELAND_MAX_HARDWARE_POWERLEVELS;
-		hwmgr->platform_descriptor.hardwarePerformanceLevels = 2;
-		hwmgr->platform_descriptor.minimumClocksReductionPercentage  = 50;
-
-		sys_info.size = sizeof(struct cgs_system_info);
-		sys_info.info_id = CGS_SYSTEM_INFO_PCIE_GEN_INFO;
-		result = cgs_query_system_info(hwmgr->device, &sys_info);
-		if (result)
-			data->pcie_gen_cap = AMDGPU_DEFAULT_PCIE_GEN_MASK;
-		else
-			data->pcie_gen_cap = (uint32_t)sys_info.value;
-		if (data->pcie_gen_cap & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)
-			data->pcie_spc_cap = 20;
-		sys_info.size = sizeof(struct cgs_system_info);
-		sys_info.info_id = CGS_SYSTEM_INFO_PCIE_MLW;
-		result = cgs_query_system_info(hwmgr->device, &sys_info);
-		if (result)
-			data->pcie_lane_cap = AMDGPU_DEFAULT_PCIE_MLW_MASK;
-		else
-			data->pcie_lane_cap = (uint32_t)sys_info.value;
-	} else {
-		/* Ignore return value in here, we are cleaning up a mess. */
-		iceland_hwmgr_backend_fini(hwmgr);
-	}
-
-	return 0;
-failed:
-	return result;
-}
-
-static int iceland_get_num_of_entries(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	unsigned long ret = 0;
-
-	result = pp_tables_get_num_of_entries(hwmgr, &ret);
-
-	return result ? 0 : ret;
-}
-
-static const unsigned long PhwIceland_Magic = (unsigned long)(PHM_VIslands_Magic);
-
-struct iceland_power_state *cast_phw_iceland_power_state(
-				  struct pp_hw_power_state *hw_ps)
-{
-	if (hw_ps == NULL)
-		return NULL;
-
-	PP_ASSERT_WITH_CODE((PhwIceland_Magic == hw_ps->magic),
-				"Invalid Powerstate Type!",
-				 return NULL);
-
-	return (struct iceland_power_state *)hw_ps;
-}
-
-static int iceland_apply_state_adjust_rules(struct pp_hwmgr *hwmgr,
-				struct pp_power_state  *prequest_ps,
-			const struct pp_power_state *pcurrent_ps)
-{
-	struct iceland_power_state *iceland_ps =
-				cast_phw_iceland_power_state(&prequest_ps->hardware);
-
-	uint32_t sclk;
-	uint32_t mclk;
-	struct PP_Clocks minimum_clocks = {0};
-	bool disable_mclk_switching;
-	bool disable_mclk_switching_for_frame_lock;
-	struct cgs_display_info info = {0};
-	const struct phm_clock_and_voltage_limits *max_limits;
-	uint32_t i;
-	iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	int32_t count;
-	int32_t stable_pstate_sclk = 0, stable_pstate_mclk = 0;
-
-	data->battery_state = (PP_StateUILabel_Battery == prequest_ps->classification.ui_label);
-
-	PP_ASSERT_WITH_CODE(iceland_ps->performance_level_count == 2,
-				 "VI should always have 2 performance levels",
-				 );
-
-	max_limits = (PP_PowerSource_AC == hwmgr->power_source) ?
-			&(hwmgr->dyn_state.max_clock_voltage_on_ac) :
-			&(hwmgr->dyn_state.max_clock_voltage_on_dc);
-
-	if (PP_PowerSource_DC == hwmgr->power_source) {
-		for (i = 0; i < iceland_ps->performance_level_count; i++) {
-			if (iceland_ps->performance_levels[i].memory_clock > max_limits->mclk)
-				iceland_ps->performance_levels[i].memory_clock = max_limits->mclk;
-			if (iceland_ps->performance_levels[i].engine_clock > max_limits->sclk)
-				iceland_ps->performance_levels[i].engine_clock = max_limits->sclk;
-		}
-	}
-
-	iceland_ps->vce_clocks.EVCLK = hwmgr->vce_arbiter.evclk;
-	iceland_ps->vce_clocks.ECCLK = hwmgr->vce_arbiter.ecclk;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) {
-
-		max_limits = &(hwmgr->dyn_state.max_clock_voltage_on_ac);
-		stable_pstate_sclk = (max_limits->sclk * 75) / 100;
-
-		for (count = hwmgr->dyn_state.vddc_dependency_on_sclk->count-1; count >= 0; count--) {
-			if (stable_pstate_sclk >= hwmgr->dyn_state.vddc_dependency_on_sclk->entries[count].clk) {
-				stable_pstate_sclk = hwmgr->dyn_state.vddc_dependency_on_sclk->entries[count].clk;
-				break;
-			}
-		}
-
-		if (count < 0)
-			stable_pstate_sclk = hwmgr->dyn_state.vddc_dependency_on_sclk->entries[0].clk;
-
-		stable_pstate_mclk = max_limits->mclk;
-
-		minimum_clocks.engineClock = stable_pstate_sclk;
-		minimum_clocks.memoryClock = stable_pstate_mclk;
-	}
-
-	if (minimum_clocks.engineClock < hwmgr->gfx_arbiter.sclk)
-		minimum_clocks.engineClock = hwmgr->gfx_arbiter.sclk;
-
-	if (minimum_clocks.memoryClock < hwmgr->gfx_arbiter.mclk)
-		minimum_clocks.memoryClock = hwmgr->gfx_arbiter.mclk;
-
-	iceland_ps->sclk_threshold = hwmgr->gfx_arbiter.sclk_threshold;
-
-	if (0 != hwmgr->gfx_arbiter.sclk_over_drive) {
-		PP_ASSERT_WITH_CODE((hwmgr->gfx_arbiter.sclk_over_drive <= hwmgr->platform_descriptor.overdriveLimit.engineClock),
-					"Overdrive sclk exceeds limit",
-					hwmgr->gfx_arbiter.sclk_over_drive = hwmgr->platform_descriptor.overdriveLimit.engineClock);
-
-		if (hwmgr->gfx_arbiter.sclk_over_drive >= hwmgr->gfx_arbiter.sclk)
-			iceland_ps->performance_levels[1].engine_clock = hwmgr->gfx_arbiter.sclk_over_drive;
-	}
-
-	if (0 != hwmgr->gfx_arbiter.mclk_over_drive) {
-		PP_ASSERT_WITH_CODE((hwmgr->gfx_arbiter.mclk_over_drive <= hwmgr->platform_descriptor.overdriveLimit.memoryClock),
-			"Overdrive mclk exceeds limit",
-			hwmgr->gfx_arbiter.mclk_over_drive = hwmgr->platform_descriptor.overdriveLimit.memoryClock);
-
-		if (hwmgr->gfx_arbiter.mclk_over_drive >= hwmgr->gfx_arbiter.mclk)
-			iceland_ps->performance_levels[1].memory_clock = hwmgr->gfx_arbiter.mclk_over_drive;
-	}
-
-	disable_mclk_switching_for_frame_lock = phm_cap_enabled(
-				    hwmgr->platform_descriptor.platformCaps,
-				    PHM_PlatformCaps_DisableMclkSwitchingForFrameLock);
-
-	disable_mclk_switching = (1 < info.display_count) ||
-				    disable_mclk_switching_for_frame_lock;
-
-	sclk  = iceland_ps->performance_levels[0].engine_clock;
-	mclk  = iceland_ps->performance_levels[0].memory_clock;
-
-	if (disable_mclk_switching)
-		mclk  = iceland_ps->performance_levels[iceland_ps->performance_level_count - 1].memory_clock;
-
-	if (sclk < minimum_clocks.engineClock)
-		sclk = (minimum_clocks.engineClock > max_limits->sclk) ? max_limits->sclk : minimum_clocks.engineClock;
-
-	if (mclk < minimum_clocks.memoryClock)
-		mclk = (minimum_clocks.memoryClock > max_limits->mclk) ? max_limits->mclk : minimum_clocks.memoryClock;
-
-	iceland_ps->performance_levels[0].engine_clock = sclk;
-	iceland_ps->performance_levels[0].memory_clock = mclk;
-
-	iceland_ps->performance_levels[1].engine_clock =
-		(iceland_ps->performance_levels[1].engine_clock >= iceland_ps->performance_levels[0].engine_clock) ?
-			      iceland_ps->performance_levels[1].engine_clock :
-			      iceland_ps->performance_levels[0].engine_clock;
-
-	if (disable_mclk_switching) {
-		if (mclk < iceland_ps->performance_levels[1].memory_clock)
-			mclk = iceland_ps->performance_levels[1].memory_clock;
-
-		iceland_ps->performance_levels[0].memory_clock = mclk;
-		iceland_ps->performance_levels[1].memory_clock = mclk;
-	} else {
-		if (iceland_ps->performance_levels[1].memory_clock < iceland_ps->performance_levels[0].memory_clock)
-			iceland_ps->performance_levels[1].memory_clock = iceland_ps->performance_levels[0].memory_clock;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) {
-		for (i=0; i < iceland_ps->performance_level_count; i++) {
-			iceland_ps->performance_levels[i].engine_clock = stable_pstate_sclk;
-			iceland_ps->performance_levels[i].memory_clock = stable_pstate_mclk;
-			iceland_ps->performance_levels[i].pcie_gen = data->pcie_gen_performance.max;
-			iceland_ps->performance_levels[i].pcie_lane = data->pcie_gen_performance.max;
-		}
-	}
-
-	return 0;
-}
-
-static bool iceland_is_dpm_running(struct pp_hwmgr *hwmgr)
-{
-	/*
-	 * We return the status of Voltage Control instead of checking SCLK/MCLK DPM
-	 * because we may have test scenarios that need us intentionly disable SCLK/MCLK DPM,
-	 * whereas voltage control is a fundemental change that will not be disabled
-	 */
-	return (0 == PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-					FEATURE_STATUS, VOLTAGE_CONTROLLER_ON) ? 1 : 0);
-}
-
-/**
- * force DPM power State
- *
- * @param    hwmgr:  the address of the powerplay hardware manager.
- * @param    n     :  DPM level
- * @return   The response that came from the SMC.
- */
-int iceland_dpm_force_state(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	/* Checking if DPM is running.  If we discover hang because of this, we should skip this message. */
-	PP_ASSERT_WITH_CODE(0 == iceland_is_dpm_running(hwmgr),
-			"Trying to force SCLK when DPM is disabled", return -1;);
-	if (0 == data->sclk_dpm_key_disabled)
-		return (0 == smum_send_msg_to_smc_with_parameter(
-							     hwmgr->smumgr,
-							     PPSMC_MSG_DPM_ForceState,
-							     n) ? 0 : 1);
-
-	return 0;
-}
-
-/**
- * force DPM power State
- *
- * @param    hwmgr:  the address of the powerplay hardware manager.
- * @param    n     :  DPM level
- * @return   The response that came from the SMC.
- */
-int iceland_dpm_force_state_mclk(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	/* Checking if DPM is running.  If we discover hang because of this, we should skip this message. */
-	PP_ASSERT_WITH_CODE(0 == iceland_is_dpm_running(hwmgr),
-			"Trying to Force MCLK when DPM is disabled", return -1;);
-	if (0 == data->mclk_dpm_key_disabled)
-		return (0 == smum_send_msg_to_smc_with_parameter(
-								hwmgr->smumgr,
-								PPSMC_MSG_MCLKDPM_ForceState,
-								n) ? 0 : 1);
-
-	return 0;
-}
-
-/**
- * force DPM power State
- *
- * @param    hwmgr:  the address of the powerplay hardware manager.
- * @param    n     :  DPM level
- * @return   The response that came from the SMC.
- */
-int iceland_dpm_force_state_pcie(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	/* Checking if DPM is running.  If we discover hang because of this, we should skip this message.*/
-	PP_ASSERT_WITH_CODE(0 == iceland_is_dpm_running(hwmgr),
-			"Trying to Force PCIE level when DPM is disabled", return -1;);
-	if (0 == data->pcie_dpm_key_disabled)
-		return (0 == smum_send_msg_to_smc_with_parameter(
-							     hwmgr->smumgr,
-							     PPSMC_MSG_PCIeDPM_ForceLevel,
-							     n) ? 0 : 1);
-
-	return 0;
-}
-
-static int iceland_force_dpm_highest(struct pp_hwmgr *hwmgr)
-{
-	uint32_t level, tmp;
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->sclk_dpm_key_disabled) {
-		/* SCLK */
-		if (data->dpm_level_enable_mask.sclk_dpm_enable_mask != 0) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.sclk_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++ ;
-
-			if (0 != level) {
-				PP_ASSERT_WITH_CODE((0 == iceland_dpm_force_state(hwmgr, level)),
-					"force highest sclk dpm state failed!", return -1);
-				PHM_WAIT_INDIRECT_FIELD(hwmgr->device,
-					SMC_IND, TARGET_AND_CURRENT_PROFILE_INDEX, CURR_SCLK_INDEX, level);
-			}
-		}
-	}
-
-	if (0 == data->mclk_dpm_key_disabled) {
-		/* MCLK */
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask != 0) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.mclk_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++ ;
-
-			if (0 != level) {
-				PP_ASSERT_WITH_CODE((0 == iceland_dpm_force_state_mclk(hwmgr, level)),
-					"force highest mclk dpm state failed!", return -1);
-				PHM_WAIT_INDIRECT_FIELD(hwmgr->device, SMC_IND,
-					TARGET_AND_CURRENT_PROFILE_INDEX, CURR_MCLK_INDEX, level);
-			}
-		}
-	}
-
-	if (0 == data->pcie_dpm_key_disabled) {
-		/* PCIE */
-		if (data->dpm_level_enable_mask.pcie_dpm_enable_mask != 0) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.pcie_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++ ;
-
-			if (0 != level) {
-				PP_ASSERT_WITH_CODE((0 == iceland_dpm_force_state_pcie(hwmgr, level)),
-					"force highest pcie dpm state failed!", return -1);
-			}
-		}
-	}
-
-	return 0;
-}
-
-static uint32_t iceland_get_lowest_enable_level(struct pp_hwmgr *hwmgr,
-						uint32_t level_mask)
-{
-	uint32_t level = 0;
-
-	while (0 == (level_mask & (1 << level)))
-		level++;
-
-	return level;
-}
-
-static int iceland_force_dpm_lowest(struct pp_hwmgr *hwmgr)
-{
-	uint32_t level;
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	/* for now force only sclk */
-	if (0 != data->dpm_level_enable_mask.sclk_dpm_enable_mask) {
-		level = iceland_get_lowest_enable_level(hwmgr,
-						      data->dpm_level_enable_mask.sclk_dpm_enable_mask);
-
-		PP_ASSERT_WITH_CODE((0 == iceland_dpm_force_state(hwmgr, level)),
-				    "force sclk dpm state failed!", return -1);
-
-		PHM_WAIT_INDIRECT_FIELD(hwmgr->device, SMC_IND,
-					TARGET_AND_CURRENT_PROFILE_INDEX,
-					CURR_SCLK_INDEX,
-					level);
-	}
-
-	return 0;
-}
-
-int iceland_unforce_dpm_levels(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	PP_ASSERT_WITH_CODE (0 == iceland_is_dpm_running(hwmgr),
-		"Trying to Unforce DPM when DPM is disabled. Returning without sending SMC message.",
-		return -1);
-
-	if (0 == data->sclk_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE((0 == smum_send_msg_to_smc(
-							     hwmgr->smumgr,
-					PPSMC_MSG_NoForcedLevel)),
-					   "unforce sclk dpm state failed!",
-								return -1);
-	}
-
-	if (0 == data->mclk_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE((0 == smum_send_msg_to_smc(
-							     hwmgr->smumgr,
-					PPSMC_MSG_MCLKDPM_NoForcedLevel)),
-					   "unforce mclk dpm state failed!",
-								return -1);
-	}
-
-	if (0 == data->pcie_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE((0 == smum_send_msg_to_smc(
-							     hwmgr->smumgr,
-					PPSMC_MSG_PCIeDPM_UnForceLevel)),
-					   "unforce pcie level failed!",
-								return -1);
-	}
-
-	return 0;
-}
-
-static int iceland_force_dpm_level(struct pp_hwmgr *hwmgr,
-		enum amd_dpm_forced_level level)
-{
-	int ret = 0;
-
-	switch (level) {
-	case AMD_DPM_FORCED_LEVEL_HIGH:
-		ret = iceland_force_dpm_highest(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	case AMD_DPM_FORCED_LEVEL_LOW:
-		ret = iceland_force_dpm_lowest(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	case AMD_DPM_FORCED_LEVEL_AUTO:
-		ret = iceland_unforce_dpm_levels(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	default:
-		break;
-	}
-
-	hwmgr->dpm_level = level;
-	return ret;
-}
-
-const struct iceland_power_state *cast_const_phw_iceland_power_state(
-				 const struct pp_hw_power_state *hw_ps)
-{
-	if (hw_ps == NULL)
-		return NULL;
-
-	PP_ASSERT_WITH_CODE((PhwIceland_Magic == hw_ps->magic),
-			    "Invalid Powerstate Type!",
-			    return NULL);
-
-	return (const struct iceland_power_state *)hw_ps;
-}
-
-static int iceland_find_dpm_states_clocks_in_dpm_table(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	const struct iceland_power_state *iceland_ps = cast_const_phw_iceland_power_state(states->pnew_state);
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_single_dpm_table *psclk_table = &(data->dpm_table.sclk_table);
-	uint32_t sclk = iceland_ps->performance_levels[iceland_ps->performance_level_count-1].engine_clock;
-	struct iceland_single_dpm_table *pmclk_table = &(data->dpm_table.mclk_table);
-	uint32_t mclk = iceland_ps->performance_levels[iceland_ps->performance_level_count-1].memory_clock;
-	struct PP_Clocks min_clocks = {0};
-	uint32_t i;
-	struct cgs_display_info info = {0};
-
-	data->need_update_smu7_dpm_table = 0;
-
-	for (i = 0; i < psclk_table->count; i++) {
-		if (sclk == psclk_table->dpm_levels[i].value)
-			break;
-	}
-
-	if (i >= psclk_table->count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_SCLK;
-	else {
-		/*
-		 * TODO: Check SCLK in DAL's minimum clocks in case DeepSleep
-		 * divider update is required.
-		 */
-		if(data->display_timing.min_clock_insr != min_clocks.engineClockInSR)
-			data->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_SCLK;
-	}
-
-	for (i = 0; i < pmclk_table->count; i++) {
-		if (mclk == pmclk_table->dpm_levels[i].value)
-			break;
-	}
-
-	if (i >= pmclk_table->count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_MCLK;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (data->display_timing.num_existing_displays != info.display_count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_MCLK;
-
-	return 0;
-}
-
-static uint16_t iceland_get_maximum_link_speed(struct pp_hwmgr *hwmgr, const struct iceland_power_state *hw_ps)
-{
-	uint32_t i;
-	uint32_t pcie_speed, max_speed = 0;
-
-	for (i = 0; i < hw_ps->performance_level_count; i++) {
-		pcie_speed = hw_ps->performance_levels[i].pcie_gen;
-		if (max_speed < pcie_speed)
-			max_speed = pcie_speed;
-	}
-
-	return max_speed;
-}
-
-static uint16_t iceland_get_current_pcie_speed(struct pp_hwmgr *hwmgr)
-{
-	uint32_t speed_cntl = 0;
-
-	speed_cntl = cgs_read_ind_register(hwmgr->device,
-					   CGS_IND_REG__PCIE,
-					   ixPCIE_LC_SPEED_CNTL);
-	return((uint16_t)PHM_GET_FIELD(speed_cntl,
-			PCIE_LC_SPEED_CNTL, LC_CURRENT_DATA_RATE));
-}
-
-
-static int iceland_request_link_speed_change_before_state_change(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	const struct iceland_power_state *iceland_nps = cast_const_phw_iceland_power_state(states->pnew_state);
-	const struct iceland_power_state *iceland_cps = cast_const_phw_iceland_power_state(states->pcurrent_state);
-
-	uint16_t target_link_speed = iceland_get_maximum_link_speed(hwmgr, iceland_nps);
-	uint16_t current_link_speed;
-
-	if (data->force_pcie_gen == PP_PCIEGenInvalid)
-		current_link_speed = iceland_get_maximum_link_speed(hwmgr, iceland_cps);
-	else
-		current_link_speed = data->force_pcie_gen;
-
-	data->force_pcie_gen = PP_PCIEGenInvalid;
-	data->pspp_notify_required = false;
-	if (target_link_speed > current_link_speed) {
-		switch(target_link_speed) {
-		case PP_PCIEGen3:
-			if (0 == acpi_pcie_perf_request(hwmgr->device, PCIE_PERF_REQ_GEN3, false))
-				break;
-			data->force_pcie_gen = PP_PCIEGen2;
-			if (current_link_speed == PP_PCIEGen2)
-				break;
-		case PP_PCIEGen2:
-			if (0 == acpi_pcie_perf_request(hwmgr->device, PCIE_PERF_REQ_GEN2, false))
-				break;
-		default:
-			data->force_pcie_gen = iceland_get_current_pcie_speed(hwmgr);
-			break;
-		}
-	} else {
-		if (target_link_speed < current_link_speed)
-			data->pspp_notify_required = true;
-	}
-
-	return 0;
-}
-
-static int iceland_freeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if ((0 == data->sclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK))) {
-		PP_ASSERT_WITH_CODE(
-			0 == iceland_is_dpm_running(hwmgr),
-			"Trying to freeze SCLK DPM when DPM is disabled",
-			);
-		PP_ASSERT_WITH_CODE(
-			0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					  PPSMC_MSG_SCLKDPM_FreezeLevel),
-			"Failed to freeze SCLK DPM during FreezeSclkMclkDPM Function!",
-			return -1);
-	}
-
-	if ((0 == data->mclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		 DPMTABLE_OD_UPDATE_MCLK)) {
-		PP_ASSERT_WITH_CODE(0 == iceland_is_dpm_running(hwmgr),
-			"Trying to freeze MCLK DPM when DPM is disabled",
-			);
-		PP_ASSERT_WITH_CODE(
-			0 == smum_send_msg_to_smc(hwmgr->smumgr,
-							PPSMC_MSG_MCLKDPM_FreezeLevel),
-			"Failed to freeze MCLK DPM during FreezeSclkMclkDPM Function!",
-			return -1);
-	}
-
-	return 0;
-}
-
-static int iceland_populate_and_upload_sclk_mclk_dpm_levels(struct pp_hwmgr *hwmgr, const void *input)
-{
-	int result = 0;
-
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	const struct iceland_power_state *iceland_ps = cast_const_phw_iceland_power_state(states->pnew_state);
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint32_t sclk = iceland_ps->performance_levels[iceland_ps->performance_level_count-1].engine_clock;
-	uint32_t mclk = iceland_ps->performance_levels[iceland_ps->performance_level_count-1].memory_clock;
-	struct iceland_dpm_table *pdpm_table = &data->dpm_table;
-
-	struct iceland_dpm_table *pgolden_dpm_table = &data->golden_dpm_table;
-	uint32_t dpm_count, clock_percent;
-	uint32_t i;
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_SCLK) {
-		pdpm_table->sclk_table.dpm_levels[pdpm_table->sclk_table.count-1].value = sclk;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinACSupport) ||
-		    phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinDCSupport)) {
-			/*
-			 * Need to do calculation based on the golden DPM table
-			 * as the Heatmap GPU Clock axis is also based on the default values
-			 */
-			PP_ASSERT_WITH_CODE(
-				(pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value != 0),
-				"Divide by 0!",
-				return -1);
-			dpm_count = pdpm_table->sclk_table.count < 2 ? 0 : pdpm_table->sclk_table.count-2;
-			for (i = dpm_count; i > 1; i--) {
-				if (sclk > pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value) {
-					clock_percent = ((sclk - pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value)*100) /
-							pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value;
-
-					pdpm_table->sclk_table.dpm_levels[i].value =
-							pgolden_dpm_table->sclk_table.dpm_levels[i].value +
-							(pgolden_dpm_table->sclk_table.dpm_levels[i].value * clock_percent)/100;
-
-				} else if (pgolden_dpm_table->sclk_table.dpm_levels[pdpm_table->sclk_table.count-1].value > sclk) {
-					clock_percent = ((pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value - sclk)*100) /
-								pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value;
-
-					pdpm_table->sclk_table.dpm_levels[i].value =
-							pgolden_dpm_table->sclk_table.dpm_levels[i].value -
-							(pgolden_dpm_table->sclk_table.dpm_levels[i].value * clock_percent)/100;
-				} else
-					pdpm_table->sclk_table.dpm_levels[i].value =
-							pgolden_dpm_table->sclk_table.dpm_levels[i].value;
-			}
-		}
-	}
-
-	if (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK) {
-		pdpm_table->mclk_table.dpm_levels[pdpm_table->mclk_table.count-1].value = mclk;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinACSupport) ||
-			phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinDCSupport)) {
-
-			PP_ASSERT_WITH_CODE(
-					(pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value != 0),
-					"Divide by 0!",
-					return -1);
-			dpm_count = pdpm_table->mclk_table.count < 2? 0 : pdpm_table->mclk_table.count-2;
-			for (i = dpm_count; i > 1; i--) {
-				if (mclk > pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value) {
-						clock_percent = ((mclk - pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value)*100) /
-								    pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value;
-
-						pdpm_table->mclk_table.dpm_levels[i].value =
-										pgolden_dpm_table->mclk_table.dpm_levels[i].value +
-										(pgolden_dpm_table->mclk_table.dpm_levels[i].value * clock_percent)/100;
-
-				} else if (pgolden_dpm_table->mclk_table.dpm_levels[pdpm_table->mclk_table.count-1].value > mclk) {
-						clock_percent = ((pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value - mclk)*100) /
-								    pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value;
-
-						pdpm_table->mclk_table.dpm_levels[i].value =
-									pgolden_dpm_table->mclk_table.dpm_levels[i].value -
-									(pgolden_dpm_table->mclk_table.dpm_levels[i].value * clock_percent)/100;
-				} else
-					pdpm_table->mclk_table.dpm_levels[i].value = pgolden_dpm_table->mclk_table.dpm_levels[i].value;
-			}
-		}
-	}
-
-
-	if (data->need_update_smu7_dpm_table & (DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK)) {
-		result = iceland_populate_all_graphic_levels(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to populate SCLK during PopulateNewDPMClocksStates Function!",
-			return result);
-	}
-
-	if (data->need_update_smu7_dpm_table & (DPMTABLE_OD_UPDATE_MCLK + DPMTABLE_UPDATE_MCLK)) {
-		/*populate MCLK dpm table to SMU7 */
-		result = iceland_populate_all_memory_levels(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to populate MCLK during PopulateNewDPMClocksStates Function!",
-				return result);
-	}
-
-	return result;
-}
-
-static int iceland_trim_single_dpm_states(struct pp_hwmgr *hwmgr,
-			  struct iceland_single_dpm_table *pdpm_table,
-			     uint32_t low_limit, uint32_t high_limit)
-{
-	uint32_t i;
-
-	for (i = 0; i < pdpm_table->count; i++) {
-		if ((pdpm_table->dpm_levels[i].value < low_limit) ||
-		    (pdpm_table->dpm_levels[i].value > high_limit))
-			pdpm_table->dpm_levels[i].enabled = false;
-		else
-			pdpm_table->dpm_levels[i].enabled = true;
-	}
-	return 0;
-}
-
-static int iceland_trim_dpm_states(struct pp_hwmgr *hwmgr, const struct iceland_power_state *hw_state)
-{
-	int result = 0;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint32_t high_limit_count;
-
-	PP_ASSERT_WITH_CODE((hw_state->performance_level_count >= 1),
-				"power state did not have any performance level",
-				 return -1);
-
-	high_limit_count = (1 == hw_state->performance_level_count) ? 0: 1;
-
-	iceland_trim_single_dpm_states(hwmgr, &(data->dpm_table.sclk_table),
-					hw_state->performance_levels[0].engine_clock,
-					hw_state->performance_levels[high_limit_count].engine_clock);
-
-	iceland_trim_single_dpm_states(hwmgr, &(data->dpm_table.mclk_table),
-					hw_state->performance_levels[0].memory_clock,
-					hw_state->performance_levels[high_limit_count].memory_clock);
-
-	return result;
-}
-
-static int iceland_generate_dpm_level_enable_mask(struct pp_hwmgr *hwmgr, const void *input)
-{
-	int result;
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	const struct iceland_power_state *iceland_ps = cast_const_phw_iceland_power_state(states->pnew_state);
-
-	result = iceland_trim_dpm_states(hwmgr, iceland_ps);
-	if (0 != result)
-		return result;
-
-	data->dpm_level_enable_mask.sclk_dpm_enable_mask = iceland_get_dpm_level_enable_mask_value(&data->dpm_table.sclk_table);
-	data->dpm_level_enable_mask.mclk_dpm_enable_mask = iceland_get_dpm_level_enable_mask_value(&data->dpm_table.mclk_table);
-	data->last_mclk_dpm_enable_mask = data->dpm_level_enable_mask.mclk_dpm_enable_mask;
-	if (data->uvd_enabled && (data->dpm_level_enable_mask.mclk_dpm_enable_mask & 1))
-		data->dpm_level_enable_mask.mclk_dpm_enable_mask &= 0xFFFFFFFE;
-
-	data->dpm_level_enable_mask.pcie_dpm_enable_mask = iceland_get_dpm_level_enable_mask_value(&data->dpm_table.pcie_speed_table);
-
-	return 0;
-}
-
-static int iceland_update_vce_dpm(struct pp_hwmgr *hwmgr, const void *input)
-{
-	return 0;
-}
-
-static int iceland_update_sclk_threshold(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	int result = 0;
-	uint32_t low_sclk_interrupt_threshold = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkThrottleLowNotification)
-		&& (hwmgr->gfx_arbiter.sclk_threshold != data->low_sclk_interrupt_threshold)) {
-		data->low_sclk_interrupt_threshold = hwmgr->gfx_arbiter.sclk_threshold;
-		low_sclk_interrupt_threshold = data->low_sclk_interrupt_threshold;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(low_sclk_interrupt_threshold);
-
-		result = smu7_copy_bytes_to_smc(
-				hwmgr->smumgr,
-				data->dpm_table_start + offsetof(SMU71_Discrete_DpmTable,
-				LowSclkInterruptThreshold),
-				(uint8_t *)&low_sclk_interrupt_threshold,
-				sizeof(uint32_t),
-				data->sram_end
-				);
-	}
-
-	return result;
-}
-
-static int iceland_update_and_upload_mc_reg_table(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	uint32_t address;
-	int32_t result;
-
-	if (0 == (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK))
-		return 0;
-
-
-	memset(&data->mc_reg_table, 0, sizeof(SMU71_Discrete_MCRegisters));
-
-	result = iceland_convert_mc_reg_table_to_smc(hwmgr, &(data->mc_reg_table));
-
-	if(result != 0)
-		return result;
-
-
-	address = data->mc_reg_table_start + (uint32_t)offsetof(SMU71_Discrete_MCRegisters, data[0]);
-
-	return  smu7_copy_bytes_to_smc(hwmgr->smumgr, address,
-				 (uint8_t *)&data->mc_reg_table.data[0],
-				sizeof(SMU71_Discrete_MCRegisterSet) * data->dpm_table.mclk_table.count,
-				data->sram_end);
-}
-
-static int iceland_program_memory_timing_parameters_conditionally(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	if (data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_OD_UPDATE_MCLK))
-		return iceland_program_memory_timing_parameters(hwmgr);
-
-	return 0;
-}
-
-static int iceland_unfreeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if ((0 == data->sclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK))) {
-
-		PP_ASSERT_WITH_CODE(0 == iceland_is_dpm_running(hwmgr),
-			"Trying to Unfreeze SCLK DPM when DPM is disabled",
-			);
-		PP_ASSERT_WITH_CODE(
-			 0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					 PPSMC_MSG_SCLKDPM_UnfreezeLevel),
-			"Failed to unfreeze SCLK DPM during UnFreezeSclkMclkDPM Function!",
-			return -1);
-	}
-
-	if ((0 == data->mclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK)) {
-
-		PP_ASSERT_WITH_CODE(
-				0 == iceland_is_dpm_running(hwmgr),
-				"Trying to Unfreeze MCLK DPM when DPM is disabled",
-				);
-		PP_ASSERT_WITH_CODE(
-			 0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					 PPSMC_MSG_MCLKDPM_UnfreezeLevel),
-		    "Failed to unfreeze MCLK DPM during UnFreezeSclkMclkDPM Function!",
-		    return -1);
-	}
-
-	data->need_update_smu7_dpm_table = 0;
-
-	return 0;
-}
-
-static int iceland_notify_link_speed_change_after_state_change(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	const struct iceland_power_state *iceland_ps = cast_const_phw_iceland_power_state(states->pnew_state);
-	uint16_t target_link_speed = iceland_get_maximum_link_speed(hwmgr, iceland_ps);
-	uint8_t  request;
-
-	if (data->pspp_notify_required  ||
-	    data->pcie_performance_request) {
-		if (target_link_speed == PP_PCIEGen3)
-			request = PCIE_PERF_REQ_GEN3;
-		else if (target_link_speed == PP_PCIEGen2)
-			request = PCIE_PERF_REQ_GEN2;
-		else
-			request = PCIE_PERF_REQ_GEN1;
-
-		if(request == PCIE_PERF_REQ_GEN1 && iceland_get_current_pcie_speed(hwmgr) > 0) {
-			data->pcie_performance_request = false;
-			return 0;
-		}
-
-		if (0 != acpi_pcie_perf_request(hwmgr->device, request, false)) {
-			if (PP_PCIEGen2 == target_link_speed)
-				printk("PSPP request to switch to Gen2 from Gen3 Failed!");
-			else
-				printk("PSPP request to switch to Gen1 from Gen2 Failed!");
-		}
-	}
-
-	data->pcie_performance_request = false;
-	return 0;
-}
-
-int iceland_upload_dpm_level_enable_mask(struct pp_hwmgr *hwmgr)
-{
-	PPSMC_Result result;
-	iceland_hwmgr *data = (iceland_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->sclk_dpm_key_disabled) {
-		/* Checking if DPM is running.  If we discover hang because of this, we should skip this message.*/
-		if (0 != iceland_is_dpm_running(hwmgr))
-			printk(KERN_ERR "[ powerplay ] Trying to set Enable Sclk Mask when DPM is disabled \n");
-
-		if (0 != data->dpm_level_enable_mask.sclk_dpm_enable_mask) {
-			result = smum_send_msg_to_smc_with_parameter(
-								hwmgr->smumgr,
-				(PPSMC_Msg)PPSMC_MSG_SCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.sclk_dpm_enable_mask);
-			PP_ASSERT_WITH_CODE((0 == result),
-				"Set Sclk Dpm enable Mask failed", return -1);
-		}
-	}
-
-	if (0 == data->mclk_dpm_key_disabled) {
-		/* Checking if DPM is running.  If we discover hang because of this, we should skip this message.*/
-		if (0 != iceland_is_dpm_running(hwmgr))
-			printk(KERN_ERR "[ powerplay ] Trying to set Enable Mclk Mask when DPM is disabled \n");
-
-		if (0 != data->dpm_level_enable_mask.mclk_dpm_enable_mask) {
-			result = smum_send_msg_to_smc_with_parameter(
-								hwmgr->smumgr,
-				(PPSMC_Msg)PPSMC_MSG_MCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.mclk_dpm_enable_mask);
-			PP_ASSERT_WITH_CODE((0 == result),
-				"Set Mclk Dpm enable Mask failed", return -1);
-		}
-	}
-
-	return 0;
-}
-
-static int iceland_set_power_state_tasks(struct pp_hwmgr *hwmgr, const void *input)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = iceland_find_dpm_states_clocks_in_dpm_table(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to find DPM states clocks in DPM table!", result = tmp_result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_PCIEPerformanceRequest)) {
-		tmp_result = iceland_request_link_speed_change_before_state_change(hwmgr, input);
-		PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to request link speed change before state change!", result = tmp_result);
-	}
-
-	tmp_result = iceland_freeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to freeze SCLK MCLK DPM!", result = tmp_result);
-
-	tmp_result = iceland_populate_and_upload_sclk_mclk_dpm_levels(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to populate and upload SCLK MCLK DPM levels!", result = tmp_result);
-
-	tmp_result = iceland_generate_dpm_level_enable_mask(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to generate DPM level enabled mask!", result = tmp_result);
-
-	tmp_result = iceland_update_vce_dpm(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to update VCE DPM!", result = tmp_result);
-
-	tmp_result = iceland_update_sclk_threshold(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to update SCLK threshold!", result = tmp_result);
-
-	tmp_result = iceland_update_and_upload_mc_reg_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to upload MC reg table!", result = tmp_result);
-
-	tmp_result = iceland_program_memory_timing_parameters_conditionally(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to program memory timing parameters!", result = tmp_result);
-
-	tmp_result = iceland_unfreeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to unfreeze SCLK MCLK DPM!", result = tmp_result);
-
-	tmp_result = iceland_upload_dpm_level_enable_mask(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to upload DPM level enabled mask!", result = tmp_result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_PCIEPerformanceRequest)) {
-		tmp_result = iceland_notify_link_speed_change_after_state_change(hwmgr, input);
-		PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to notify link speed change after state change!", result = tmp_result);
-	}
-
-	return result;
-}
-
-static int iceland_get_power_state_size(struct pp_hwmgr *hwmgr)
-{
-	return sizeof(struct iceland_power_state);
-}
-
-static int iceland_dpm_get_mclk(struct pp_hwmgr *hwmgr, bool low)
-{
-	struct pp_power_state  *ps;
-	struct iceland_power_state  *iceland_ps;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	iceland_ps = cast_phw_iceland_power_state(&ps->hardware);
-
-	if (low)
-		return iceland_ps->performance_levels[0].memory_clock;
-	else
-		return iceland_ps->performance_levels[iceland_ps->performance_level_count-1].memory_clock;
-}
-
-static int iceland_dpm_get_sclk(struct pp_hwmgr *hwmgr, bool low)
-{
-	struct pp_power_state  *ps;
-	struct iceland_power_state  *iceland_ps;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	iceland_ps = cast_phw_iceland_power_state(&ps->hardware);
-
-	if (low)
-		return iceland_ps->performance_levels[0].engine_clock;
-	else
-		return iceland_ps->performance_levels[iceland_ps->performance_level_count-1].engine_clock;
-}
-
-static int iceland_get_current_pcie_lane_number(
-						   struct pp_hwmgr *hwmgr)
-{
-	uint32_t link_width;
-
-	link_width = PHM_READ_INDIRECT_FIELD(hwmgr->device,
-							CGS_IND_REG__PCIE,
-						  PCIE_LC_LINK_WIDTH_CNTL,
-							LC_LINK_WIDTH_RD);
-
-	PP_ASSERT_WITH_CODE((7 >= link_width),
-			"Invalid PCIe lane width!", return 0);
-
-	return decode_pcie_lane_width(link_width);
-}
-
-static int iceland_dpm_patch_boot_state(struct pp_hwmgr *hwmgr,
-					struct pp_hw_power_state *hw_ps)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_power_state *ps = (struct iceland_power_state *)hw_ps;
-	ATOM_FIRMWARE_INFO_V2_2 *fw_info;
-	uint16_t size;
-	uint8_t frev, crev;
-	int index = GetIndexIntoMasterTable(DATA, FirmwareInfo);
-
-	/* First retrieve the Boot clocks and VDDC from the firmware info table.
-	 * We assume here that fw_info is unchanged if this call fails.
-	 */
-	fw_info = (ATOM_FIRMWARE_INFO_V2_2 *)cgs_atom_get_data_table(
-			hwmgr->device, index,
-			&size, &frev, &crev);
-	if (!fw_info)
-		/* During a test, there is no firmware info table. */
-		return 0;
-
-	/* Patch the state. */
-	data->vbios_boot_state.sclk_bootup_value  = le32_to_cpu(fw_info->ulDefaultEngineClock);
-	data->vbios_boot_state.mclk_bootup_value  = le32_to_cpu(fw_info->ulDefaultMemoryClock);
-	data->vbios_boot_state.mvdd_bootup_value  = le16_to_cpu(fw_info->usBootUpMVDDCVoltage);
-	data->vbios_boot_state.vddc_bootup_value  = le16_to_cpu(fw_info->usBootUpVDDCVoltage);
-	data->vbios_boot_state.vddci_bootup_value = le16_to_cpu(fw_info->usBootUpVDDCIVoltage);
-	data->vbios_boot_state.pcie_gen_bootup_value = iceland_get_current_pcie_speed(hwmgr);
-	data->vbios_boot_state.pcie_lane_bootup_value =
-			(uint16_t)iceland_get_current_pcie_lane_number(hwmgr);
-
-	/* set boot power state */
-	ps->performance_levels[0].memory_clock = data->vbios_boot_state.mclk_bootup_value;
-	ps->performance_levels[0].engine_clock = data->vbios_boot_state.sclk_bootup_value;
-	ps->performance_levels[0].pcie_gen = data->vbios_boot_state.pcie_gen_bootup_value;
-	ps->performance_levels[0].pcie_lane = data->vbios_boot_state.pcie_lane_bootup_value;
-
-	return 0;
-}
-
-static int iceland_get_pp_table_entry_callback_func(struct pp_hwmgr *hwmgr,
-					struct pp_hw_power_state *power_state,
-					unsigned int index, const void *clock_info)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_power_state  *iceland_power_state = cast_phw_iceland_power_state(power_state);
-	const ATOM_PPLIB_CI_CLOCK_INFO *visland_clk_info = clock_info;
-	struct iceland_performance_level *performance_level;
-	uint32_t engine_clock, memory_clock;
-	uint16_t pcie_gen_from_bios;
-
-	engine_clock = visland_clk_info->ucEngineClockHigh << 16 | visland_clk_info->usEngineClockLow;
-	memory_clock = visland_clk_info->ucMemoryClockHigh << 16 | visland_clk_info->usMemoryClockLow;
-
-	if (!(data->mc_micro_code_feature & DISABLE_MC_LOADMICROCODE) && memory_clock > data->highest_mclk)
-		data->highest_mclk = memory_clock;
-
-	performance_level = &(iceland_power_state->performance_levels
-			[iceland_power_state->performance_level_count++]);
-
-	PP_ASSERT_WITH_CODE(
-			(iceland_power_state->performance_level_count < SMU71_MAX_LEVELS_GRAPHICS),
-			"Performance levels exceeds SMC limit!",
-			return -1);
-
-	PP_ASSERT_WITH_CODE(
-			(iceland_power_state->performance_level_count <=
-					hwmgr->platform_descriptor.hardwareActivityPerformanceLevels),
-			"Performance levels exceeds Driver limit!",
-			return -1);
-
-	/* Performance levels are arranged from low to high. */
-	performance_level->memory_clock = memory_clock;
-	performance_level->engine_clock = engine_clock;
-
-	pcie_gen_from_bios = visland_clk_info->ucPCIEGen;
-
-	performance_level->pcie_gen = get_pcie_gen_support(data->pcie_gen_cap, pcie_gen_from_bios);
-	performance_level->pcie_lane = get_pcie_lane_support(data->pcie_lane_cap, visland_clk_info->usPCIELane);
-
-	return 0;
-}
-
-static int iceland_get_pp_table_entry(struct pp_hwmgr *hwmgr,
-		unsigned long entry_index, struct pp_power_state *state)
-{
-	int result;
-	struct iceland_power_state *ps;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct phm_clock_voltage_dependency_table *dep_mclk_table =
-			hwmgr->dyn_state.vddci_dependency_on_mclk;
-
-	memset(&state->hardware, 0x00, sizeof(struct pp_hw_power_state));
-
-	state->hardware.magic = PHM_VIslands_Magic;
-
-	ps = (struct iceland_power_state *)(&state->hardware);
-
-	result = pp_tables_get_entry(hwmgr, entry_index, state,
-			iceland_get_pp_table_entry_callback_func);
-
-	/*
-	 * This is the earliest time we have all the dependency table
-	 * and the VBIOS boot state as
-	 * PP_Tables_GetPowerPlayTableEntry retrieves the VBIOS boot
-	 * state if there is only one VDDCI/MCLK level, check if it's
-	 * the same as VBIOS boot state
-	 */
-	if (dep_mclk_table != NULL && dep_mclk_table->count == 1) {
-		if (dep_mclk_table->entries[0].clk !=
-				data->vbios_boot_state.mclk_bootup_value)
-			printk(KERN_ERR "Single MCLK entry VDDCI/MCLK dependency table "
-					"does not match VBIOS boot MCLK level");
-		if (dep_mclk_table->entries[0].v !=
-				data->vbios_boot_state.vddci_bootup_value)
-			printk(KERN_ERR "Single VDDCI entry VDDCI/MCLK dependency table "
-					"does not match VBIOS boot VDDCI level");
-	}
-
-	/* set DC compatible flag if this state supports DC */
-	if (!state->validation.disallowOnDC)
-		ps->dc_compatible = true;
-
-	if (state->classification.flags & PP_StateClassificationFlag_ACPI)
-		data->acpi_pcie_gen = ps->performance_levels[0].pcie_gen;
-	else if (0 != (state->classification.flags & PP_StateClassificationFlag_Boot)) {
-		if (data->bacos.best_match == 0xffff) {
-			/* For C.I. use boot state as base BACO state */
-			data->bacos.best_match = PP_StateClassificationFlag_Boot;
-			data->bacos.performance_level = ps->performance_levels[0];
-		}
-	}
-
-
-	ps->uvd_clocks.VCLK = state->uvd_clocks.VCLK;
-	ps->uvd_clocks.DCLK = state->uvd_clocks.DCLK;
-
-	if (!result) {
-		uint32_t i;
-
-		switch (state->classification.ui_label) {
-		case PP_StateUILabel_Performance:
-			data->use_pcie_performance_levels = true;
-
-			for (i = 0; i < ps->performance_level_count; i++) {
-				if (data->pcie_gen_performance.max <
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_performance.max =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_gen_performance.min >
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_performance.min =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_lane_performance.max <
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_performance.max =
-							ps->performance_levels[i].pcie_lane;
-
-				if (data->pcie_lane_performance.min >
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_performance.min =
-							ps->performance_levels[i].pcie_lane;
-			}
-			break;
-		case PP_StateUILabel_Battery:
-			data->use_pcie_power_saving_levels = true;
-
-			for (i = 0; i < ps->performance_level_count; i++) {
-				if (data->pcie_gen_power_saving.max <
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_power_saving.max =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_gen_power_saving.min >
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_power_saving.min =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_lane_power_saving.max <
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_power_saving.max =
-							ps->performance_levels[i].pcie_lane;
-
-				if (data->pcie_lane_power_saving.min >
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_power_saving.min =
-							ps->performance_levels[i].pcie_lane;
-			}
-			break;
-		default:
-			break;
-		}
-	}
-	return 0;
-}
-
-static void
-iceland_print_current_perforce_level(struct pp_hwmgr *hwmgr, struct seq_file *m)
-{
-	uint32_t sclk, mclk, activity_percent;
-	uint32_t offset;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	smum_send_msg_to_smc(hwmgr->smumgr, (PPSMC_Msg)(PPSMC_MSG_API_GetSclkFrequency));
-
-	sclk = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-	smum_send_msg_to_smc(hwmgr->smumgr, (PPSMC_Msg)(PPSMC_MSG_API_GetMclkFrequency));
-
-	mclk = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-	seq_printf(m, "\n [  mclk  ]: %u MHz\n\n [  sclk  ]: %u MHz\n", mclk/100, sclk/100);
-
-	offset = data->soft_regs_start + offsetof(SMU71_SoftRegisters, AverageGraphicsActivity);
-	activity_percent = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, offset);
-	activity_percent += 0x80;
-	activity_percent >>= 8;
-
-	seq_printf(m, "\n [GPU load]: %u%%\n\n", activity_percent > 100 ? 100 : activity_percent);
-
-	seq_printf(m, "uvd    %sabled\n", data->uvd_power_gated ? "dis" : "en");
-
-	seq_printf(m, "vce    %sabled\n", data->vce_power_gated ? "dis" : "en");
-}
-
-int iceland_notify_smc_display_config_after_ps_adjustment(struct pp_hwmgr *hwmgr)
-{
-	uint32_t num_active_displays = 0;
-	struct cgs_display_info info = {0};
-	info.mode_info = NULL;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	num_active_displays = info.display_count;
-
-	if (num_active_displays > 1)  /* to do && (pHwMgr->pPECI->displayConfiguration.bMultiMonitorInSync != TRUE)) */
-		iceland_notify_smc_display_change(hwmgr, false);
-	else
-		iceland_notify_smc_display_change(hwmgr, true);
-
-	return 0;
-}
-
-/**
-* Programs the display gap
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always OK
-*/
-int iceland_program_display_gap(struct pp_hwmgr *hwmgr)
-{
-	uint32_t num_active_displays = 0;
-	uint32_t display_gap = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL);
-	uint32_t display_gap2;
-	uint32_t pre_vbi_time_in_us;
-	uint32_t frame_time_in_us;
-	uint32_t ref_clock;
-	uint32_t refresh_rate = 0;
-	struct cgs_display_info info = {0};
-	struct cgs_mode_info mode_info;
-
-	info.mode_info = &mode_info;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-	num_active_displays = info.display_count;
-
-	display_gap = PHM_SET_FIELD(display_gap, CG_DISPLAY_GAP_CNTL, DISP_GAP, (num_active_displays > 0)? DISPLAY_GAP_VBLANK_OR_WM : DISPLAY_GAP_IGNORE);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL, display_gap);
-
-	ref_clock = mode_info.ref_clock;
-	refresh_rate = mode_info.refresh_rate;
-
-	if(0 == refresh_rate)
-		refresh_rate = 60;
-
-	frame_time_in_us = 1000000 / refresh_rate;
-
-	pre_vbi_time_in_us = frame_time_in_us - 200 - mode_info.vblank_time_us;
-	display_gap2 = pre_vbi_time_in_us * (ref_clock / 100);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL2, display_gap2);
-
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SOFT_REGISTERS_TABLE_4, PreVBlankGap, 0x64);
-
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SOFT_REGISTERS_TABLE_5, VBlankTimeout, (frame_time_in_us - pre_vbi_time_in_us));
-
-	if (num_active_displays == 1)
-		iceland_notify_smc_display_change(hwmgr, true);
-
-	return 0;
-}
-
-int iceland_display_configuration_changed_task(struct pp_hwmgr *hwmgr)
-{
-	iceland_program_display_gap(hwmgr);
-
-	return 0;
-}
-
-/**
-*  Set maximum target operating fan output PWM
-*
-* @param    pHwMgr:  the address of the powerplay hardware manager.
-* @param    usMaxFanPwm:  max operating fan PWM in percents
-* @return   The response that came from the SMC.
-*/
-static int iceland_set_max_fan_pwm_output(struct pp_hwmgr *hwmgr, uint16_t us_max_fan_pwm)
-{
-	hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanPWM = us_max_fan_pwm;
-
-	if (phm_is_hw_access_blocked(hwmgr))
-		return 0;
-
-	return (0 == smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_SetFanPwmMax, us_max_fan_pwm) ? 0 : -1);
-}
-
-/**
-*  Set maximum target operating fan output RPM
-*
-* @param    pHwMgr:  the address of the powerplay hardware manager.
-* @param    usMaxFanRpm:  max operating fan RPM value.
-* @return   The response that came from the SMC.
-*/
-static int iceland_set_max_fan_rpm_output(struct pp_hwmgr *hwmgr, uint16_t us_max_fan_pwm)
-{
-	hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanRPM = us_max_fan_pwm;
-
-	if (phm_is_hw_access_blocked(hwmgr))
-		return 0;
-
-	return (0 == smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_SetFanRpmMax, us_max_fan_pwm) ? 0 : -1);
-}
-
-static int iceland_dpm_set_interrupt_state(void *private_data,
-					 unsigned src_id, unsigned type,
-					 int enabled)
-{
-	uint32_t cg_thermal_int;
-	struct pp_hwmgr *hwmgr = ((struct pp_eventmgr *)private_data)->hwmgr;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	switch (type) {
-	case AMD_THERMAL_IRQ_LOW_TO_HIGH:
-		if (enabled) {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int |= CG_THERMAL_INT_CTRL__THERM_INTH_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		} else {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int &= ~CG_THERMAL_INT_CTRL__THERM_INTH_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		}
-		break;
-
-	case AMD_THERMAL_IRQ_HIGH_TO_LOW:
-		if (enabled) {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int |= CG_THERMAL_INT_CTRL__THERM_INTL_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		} else {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int &= ~CG_THERMAL_INT_CTRL__THERM_INTL_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		}
-		break;
-	default:
-		break;
-	}
-	return 0;
-}
-
-static int iceland_register_internal_thermal_interrupt(struct pp_hwmgr *hwmgr,
-					const void *thermal_interrupt_info)
-{
-	int result;
-	const struct pp_interrupt_registration_info *info =
-			(const struct pp_interrupt_registration_info *)thermal_interrupt_info;
-
-	if (info == NULL)
-		return -EINVAL;
-
-	result = cgs_add_irq_source(hwmgr->device, 230, AMD_THERMAL_IRQ_LAST,
-				iceland_dpm_set_interrupt_state,
-				info->call_back, info->context);
-
-	if (result)
-		return -EINVAL;
-
-	result = cgs_add_irq_source(hwmgr->device, 231, AMD_THERMAL_IRQ_LAST,
-				iceland_dpm_set_interrupt_state,
-				info->call_back, info->context);
-
-	if (result)
-		return -EINVAL;
-
-	return 0;
-}
-
-
-static bool iceland_check_smc_update_required_for_display_configuration(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	bool is_update_required = false;
-	struct cgs_display_info info = {0,0,NULL};
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (data->display_timing.num_existing_displays != info.display_count)
-		is_update_required = true;
-/* TO DO NEED TO GET DEEP SLEEP CLOCK FROM DAL
-	if (phm_cap_enabled(hwmgr->hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_SclkDeepSleep)) {
-		cgs_get_min_clock_settings(hwmgr->device, &min_clocks);
-		if(min_clocks.engineClockInSR != data->display_timing.minClockInSR)
-			is_update_required = true;
-*/
-	return is_update_required;
-}
-
-
-static inline bool iceland_are_power_levels_equal(const struct iceland_performance_level *pl1,
-							   const struct iceland_performance_level *pl2)
-{
-	return ((pl1->memory_clock == pl2->memory_clock) &&
-		  (pl1->engine_clock == pl2->engine_clock) &&
-		  (pl1->pcie_gen == pl2->pcie_gen) &&
-		  (pl1->pcie_lane == pl2->pcie_lane));
-}
-
-int iceland_check_states_equal(struct pp_hwmgr *hwmgr, const struct pp_hw_power_state *pstate1,
-		const struct pp_hw_power_state *pstate2, bool *equal)
-{
-	const struct iceland_power_state *psa = cast_const_phw_iceland_power_state(pstate1);
-	const struct iceland_power_state *psb = cast_const_phw_iceland_power_state(pstate2);
-	int i;
-
-	if (equal == NULL || psa == NULL || psb == NULL)
-		return -EINVAL;
-
-	/* If the two states don't even have the same number of performance levels they cannot be the same state. */
-	if (psa->performance_level_count != psb->performance_level_count) {
-		*equal = false;
-		return 0;
-	}
-
-	for (i = 0; i < psa->performance_level_count; i++) {
-		if (!iceland_are_power_levels_equal(&(psa->performance_levels[i]), &(psb->performance_levels[i]))) {
-			/* If we have found even one performance level pair that is different the states are different. */
-			*equal = false;
-			return 0;
-		}
-	}
-
-	/* If all performance levels are the same try to use the UVD clocks to break the tie.*/
-	*equal = ((psa->uvd_clocks.VCLK == psb->uvd_clocks.VCLK) && (psa->uvd_clocks.DCLK == psb->uvd_clocks.DCLK));
-	*equal &= ((psa->vce_clocks.EVCLK == psb->vce_clocks.EVCLK) && (psa->vce_clocks.ECCLK == psb->vce_clocks.ECCLK));
-	*equal &= (psa->sclk_threshold == psb->sclk_threshold);
-	*equal &= (psa->acp_clk == psb->acp_clk);
-
-	return 0;
-}
-
-static int iceland_set_fan_control_mode(struct pp_hwmgr *hwmgr, uint32_t mode)
-{
-	if (mode) {
-		/* stop auto-manage */
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl))
-			iceland_fan_ctrl_stop_smc_fan_control(hwmgr);
-		iceland_fan_ctrl_set_static_mode(hwmgr, mode);
-	} else
-		/* restart auto-manage */
-		iceland_fan_ctrl_reset_fan_speed_to_default(hwmgr);
-
-	return 0;
-}
-
-static int iceland_get_fan_control_mode(struct pp_hwmgr *hwmgr)
-{
-	if (hwmgr->fan_ctrl_is_in_default_mode)
-		return hwmgr->fan_ctrl_default_mode;
-	else
-		return PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_FDO_CTRL2, FDO_PWM_MODE);
-}
-
-static int iceland_force_clock_level(struct pp_hwmgr *hwmgr,
-		enum pp_clock_type type, uint32_t mask)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	if (hwmgr->dpm_level != AMD_DPM_FORCED_LEVEL_MANUAL)
-		return -EINVAL;
-
-	switch (type) {
-	case PP_SCLK:
-		if (!data->sclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_SCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.sclk_dpm_enable_mask & mask);
-		break;
-	case PP_MCLK:
-		if (!data->mclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_MCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.mclk_dpm_enable_mask & mask);
-		break;
-	case PP_PCIE:
-	{
-		uint32_t tmp = mask & data->dpm_level_enable_mask.pcie_dpm_enable_mask;
-		uint32_t level = 0;
-
-		while (tmp >>= 1)
-			level++;
-
-		if (!data->pcie_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_PCIeDPM_ForceLevel,
-					level);
-		break;
-	}
-	default:
-		break;
-	}
-
-	return 0;
-}
-
-static int iceland_print_clock_levels(struct pp_hwmgr *hwmgr,
-		enum pp_clock_type type, char *buf)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	struct iceland_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	struct iceland_single_dpm_table *pcie_table = &(data->dpm_table.pcie_speed_table);
-	int i, now, size = 0;
-	uint32_t clock, pcie_speed;
-
-	switch (type) {
-	case PP_SCLK:
-		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetSclkFrequency);
-		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-		for (i = 0; i < sclk_table->count; i++) {
-			if (clock > sclk_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < sclk_table->count; i++)
-			size += sprintf(buf + size, "%d: %uMhz %s\n",
-					i, sclk_table->dpm_levels[i].value / 100,
-					(i == now) ? "*" : "");
-		break;
-	case PP_MCLK:
-		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetMclkFrequency);
-		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-		for (i = 0; i < mclk_table->count; i++) {
-			if (clock > mclk_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < mclk_table->count; i++)
-			size += sprintf(buf + size, "%d: %uMhz %s\n",
-					i, mclk_table->dpm_levels[i].value / 100,
-					(i == now) ? "*" : "");
-		break;
-	case PP_PCIE:
-		pcie_speed = iceland_get_current_pcie_speed(hwmgr);
-		for (i = 0; i < pcie_table->count; i++) {
-			if (pcie_speed != pcie_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < pcie_table->count; i++)
-			size += sprintf(buf + size, "%d: %s %s\n", i,
-					(pcie_table->dpm_levels[i].value == 0) ? "2.5GB, x8" :
-					(pcie_table->dpm_levels[i].value == 1) ? "5.0GB, x16" :
-					(pcie_table->dpm_levels[i].value == 2) ? "8.0GB, x16" : "",
-					(i == now) ? "*" : "");
-		break;
-	default:
-		break;
-	}
-	return size;
-}
-
-static int iceland_get_sclk_od(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	struct iceland_single_dpm_table *golden_sclk_table =
-			&(data->golden_dpm_table.sclk_table);
-	int value;
-
-	value = (sclk_table->dpm_levels[sclk_table->count - 1].value -
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value) *
-			100 /
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
-
-	return value;
-}
-
-static int iceland_set_sclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_single_dpm_table *golden_sclk_table =
-			&(data->golden_dpm_table.sclk_table);
-	struct pp_power_state  *ps;
-	struct iceland_power_state  *iceland_ps;
-
-	if (value > 20)
-		value = 20;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	iceland_ps = cast_phw_iceland_power_state(&ps->hardware);
-
-	iceland_ps->performance_levels[iceland_ps->performance_level_count - 1].engine_clock =
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value *
-			value / 100 +
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
-
-	return 0;
-}
-
-static int iceland_get_mclk_od(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	struct iceland_single_dpm_table *golden_mclk_table =
-			&(data->golden_dpm_table.mclk_table);
-	int value;
-
-	value = (mclk_table->dpm_levels[mclk_table->count - 1].value -
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value) *
-			100 /
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
-
-	return value;
-}
-
-uint32_t iceland_get_xclk(struct pp_hwmgr *hwmgr)
-{
-	uint32_t reference_clock;
-	uint32_t tc;
-	uint32_t divide;
-
-	ATOM_FIRMWARE_INFO *fw_info;
-	uint16_t size;
-	uint8_t frev, crev;
-	int index = GetIndexIntoMasterTable(DATA, FirmwareInfo);
-
-	tc = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_CLKPIN_CNTL_2, MUX_TCLK_TO_XCLK);
-
-	if (tc)
-		return TCLK;
-
-	fw_info = (ATOM_FIRMWARE_INFO *)cgs_atom_get_data_table(hwmgr->device, index,
-						  &size, &frev, &crev);
-
-	if (!fw_info)
-		return 0;
-
-	reference_clock = le16_to_cpu(fw_info->usReferenceClock);
-
-	divide = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_CLKPIN_CNTL, XTALIN_DIVIDE);
-
-	if (0 != divide)
-		return reference_clock / 4;
-
-	return reference_clock;
-}
-
-static int iceland_set_mclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_single_dpm_table *golden_mclk_table =
-			&(data->golden_dpm_table.mclk_table);
-	struct pp_power_state  *ps;
-	struct iceland_power_state  *iceland_ps;
-
-	if (value > 20)
-		value = 20;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	iceland_ps = cast_phw_iceland_power_state(&ps->hardware);
-
-	iceland_ps->performance_levels[iceland_ps->performance_level_count - 1].memory_clock =
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value *
-			value / 100 +
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
-
-	return 0;
-}
-
-static const struct pp_hwmgr_func iceland_hwmgr_funcs = {
-	.backend_init = &iceland_hwmgr_backend_init,
-	.backend_fini = &iceland_hwmgr_backend_fini,
-	.asic_setup = &iceland_setup_asic_task,
-	.dynamic_state_management_enable = &iceland_enable_dpm_tasks,
-	.apply_state_adjust_rules = iceland_apply_state_adjust_rules,
-	.force_dpm_level = &iceland_force_dpm_level,
-	.power_state_set = iceland_set_power_state_tasks,
-	.get_power_state_size = iceland_get_power_state_size,
-	.get_mclk = iceland_dpm_get_mclk,
-	.get_sclk = iceland_dpm_get_sclk,
-	.patch_boot_state = iceland_dpm_patch_boot_state,
-	.get_pp_table_entry = iceland_get_pp_table_entry,
-	.get_num_of_pp_table_entries = iceland_get_num_of_entries,
-	.print_current_perforce_level = iceland_print_current_perforce_level,
-	.powerdown_uvd = iceland_phm_powerdown_uvd,
-	.powergate_uvd = iceland_phm_powergate_uvd,
-	.powergate_vce = iceland_phm_powergate_vce,
-	.disable_clock_power_gating = iceland_phm_disable_clock_power_gating,
-	.update_clock_gatings = iceland_phm_update_clock_gatings,
-	.notify_smc_display_config_after_ps_adjustment = iceland_notify_smc_display_config_after_ps_adjustment,
-	.display_config_changed = iceland_display_configuration_changed_task,
-	.set_max_fan_pwm_output = iceland_set_max_fan_pwm_output,
-	.set_max_fan_rpm_output = iceland_set_max_fan_rpm_output,
-	.get_temperature = iceland_thermal_get_temperature,
-	.stop_thermal_controller = iceland_thermal_stop_thermal_controller,
-	.get_fan_speed_info = iceland_fan_ctrl_get_fan_speed_info,
-	.get_fan_speed_percent = iceland_fan_ctrl_get_fan_speed_percent,
-	.set_fan_speed_percent = iceland_fan_ctrl_set_fan_speed_percent,
-	.reset_fan_speed_to_default = iceland_fan_ctrl_reset_fan_speed_to_default,
-	.get_fan_speed_rpm = iceland_fan_ctrl_get_fan_speed_rpm,
-	.set_fan_speed_rpm = iceland_fan_ctrl_set_fan_speed_rpm,
-	.uninitialize_thermal_controller = iceland_thermal_ctrl_uninitialize_thermal_controller,
-	.register_internal_thermal_interrupt = iceland_register_internal_thermal_interrupt,
-	.check_smc_update_required_for_display_configuration = iceland_check_smc_update_required_for_display_configuration,
-	.check_states_equal = iceland_check_states_equal,
-	.set_fan_control_mode = iceland_set_fan_control_mode,
-	.get_fan_control_mode = iceland_get_fan_control_mode,
-	.force_clock_level = iceland_force_clock_level,
-	.print_clock_levels = iceland_print_clock_levels,
-	.get_sclk_od = iceland_get_sclk_od,
-	.set_sclk_od = iceland_set_sclk_od,
-	.get_mclk_od = iceland_get_mclk_od,
-	.set_mclk_od = iceland_set_mclk_od,
-};
-
-int iceland_hwmgr_init(struct pp_hwmgr *hwmgr)
-{
-	iceland_hwmgr  *data;
-
-	data = kzalloc (sizeof(iceland_hwmgr), GFP_KERNEL);
-	if (data == NULL)
-		return -ENOMEM;
-	memset(data, 0x00, sizeof(iceland_hwmgr));
-
-	hwmgr->backend = data;
-	hwmgr->hwmgr_func = &iceland_hwmgr_funcs;
-	hwmgr->pptable_func = &pptable_funcs;
-
-	/* thermal */
-	pp_iceland_thermal_initialize(hwmgr);
-	return 0;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.h b/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.h
deleted file mode 100644
index f253988..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_hwmgr.h
+++ /dev/null
@@ -1,424 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Author: Huang Rui <ray.huang@amd.com>
- *
- */
-#ifndef ICELAND_HWMGR_H
-#define ICELAND_HWMGR_H
-
-#include "hwmgr.h"
-#include "ppatomctrl.h"
-#include "ppinterrupt.h"
-#include "ppsmc.h"
-#include "iceland_powertune.h"
-#include "pp_endian.h"
-#include "smu71_discrete.h"
-
-#define ICELAND_MAX_HARDWARE_POWERLEVELS 2
-#define ICELAND_DYNCLK_NUMBER_OF_TREND_COEFFICIENTS 15
-
-struct iceland_performance_level {
-	uint32_t	memory_clock;
-	uint32_t	engine_clock;
-	uint16_t	pcie_gen;
-	uint16_t	pcie_lane;
-};
-
-struct _phw_iceland_bacos {
-	uint32_t                          best_match;
-	uint32_t                          baco_flags;
-	struct iceland_performance_level		  performance_level;
-};
-typedef struct _phw_iceland_bacos phw_iceland_bacos;
-
-struct _phw_iceland_uvd_clocks {
-	uint32_t   VCLK;
-	uint32_t   DCLK;
-};
-
-typedef struct _phw_iceland_uvd_clocks phw_iceland_uvd_clocks;
-
-struct _phw_iceland_vce_clocks {
-	uint32_t   EVCLK;
-	uint32_t   ECCLK;
-};
-
-typedef struct _phw_iceland_vce_clocks phw_iceland_vce_clocks;
-
-struct iceland_power_state {
-	uint32_t                    magic;
-	phw_iceland_uvd_clocks        uvd_clocks;
-	phw_iceland_vce_clocks        vce_clocks;
-	uint32_t                    sam_clk;
-	uint32_t                    acp_clk;
-	uint16_t                    performance_level_count;
-	bool                        dc_compatible;
-	uint32_t                    sclk_threshold;
-	struct iceland_performance_level performance_levels[ICELAND_MAX_HARDWARE_POWERLEVELS];
-};
-
-struct _phw_iceland_dpm_level {
-	bool		enabled;
-	uint32_t    value;
-	uint32_t    param1;
-};
-typedef struct _phw_iceland_dpm_level phw_iceland_dpm_level;
-
-#define ICELAND_MAX_DEEPSLEEP_DIVIDER_ID 5
-#define MAX_REGULAR_DPM_NUMBER 8
-#define ICELAND_MINIMUM_ENGINE_CLOCK 5000
-
-struct iceland_single_dpm_table {
-	uint32_t count;
-	phw_iceland_dpm_level dpm_levels[MAX_REGULAR_DPM_NUMBER];
-};
-
-struct iceland_dpm_table {
-	struct iceland_single_dpm_table  sclk_table;
-	struct iceland_single_dpm_table  mclk_table;
-	struct iceland_single_dpm_table  pcie_speed_table;
-	struct iceland_single_dpm_table  vddc_table;
-	struct iceland_single_dpm_table  vdd_gfx_table;
-	struct iceland_single_dpm_table  vdd_ci_table;
-	struct iceland_single_dpm_table  mvdd_table;
-};
-typedef struct _phw_iceland_dpm_table phw_iceland_dpm_table;
-
-
-struct _phw_iceland_clock_regisiters {
-	uint32_t  vCG_SPLL_FUNC_CNTL;
-	uint32_t  vCG_SPLL_FUNC_CNTL_2;
-	uint32_t  vCG_SPLL_FUNC_CNTL_3;
-	uint32_t  vCG_SPLL_FUNC_CNTL_4;
-	uint32_t  vCG_SPLL_SPREAD_SPECTRUM;
-	uint32_t  vCG_SPLL_SPREAD_SPECTRUM_2;
-	uint32_t  vDLL_CNTL;
-	uint32_t  vMCLK_PWRMGT_CNTL;
-	uint32_t  vMPLL_AD_FUNC_CNTL;
-	uint32_t  vMPLL_DQ_FUNC_CNTL;
-	uint32_t  vMPLL_FUNC_CNTL;
-	uint32_t  vMPLL_FUNC_CNTL_1;
-	uint32_t  vMPLL_FUNC_CNTL_2;
-	uint32_t  vMPLL_SS1;
-	uint32_t  vMPLL_SS2;
-};
-typedef struct _phw_iceland_clock_regisiters phw_iceland_clock_registers;
-
-struct _phw_iceland_voltage_smio_registers {
-	uint32_t vs0_vid_lower_smio_cntl;
-};
-typedef struct _phw_iceland_voltage_smio_registers phw_iceland_voltage_smio_registers;
-
-
-struct _phw_iceland_mc_reg_entry {
-	uint32_t mclk_max;
-	uint32_t mc_data[SMU71_DISCRETE_MC_REGISTER_ARRAY_SIZE];
-};
-typedef struct _phw_iceland_mc_reg_entry phw_iceland_mc_reg_entry;
-
-struct _phw_iceland_mc_reg_table {
-	uint8_t   last;               /* number of registers*/
-	uint8_t   num_entries;        /* number of entries in mc_reg_table_entry used*/
-	uint16_t  validflag;          /* indicate the corresponding register is valid or not. 1: valid, 0: invalid. bit0->address[0], bit1->address[1], etc.*/
-	phw_iceland_mc_reg_entry    mc_reg_table_entry[MAX_AC_TIMING_ENTRIES];
-	SMU71_Discrete_MCRegisterAddress mc_reg_address[SMU71_DISCRETE_MC_REGISTER_ARRAY_SIZE];
-};
-typedef struct _phw_iceland_mc_reg_table phw_iceland_mc_reg_table;
-
-#define DISABLE_MC_LOADMICROCODE   1
-#define DISABLE_MC_CFGPROGRAMMING  2
-
-
-/*Ultra Low Voltage parameter structure */
-struct phw_iceland_ulv_parm{
-	bool					ulv_supported;
-	uint32_t   				ch_ulv_parameter;
-	uint32_t				ulv_volt_change_delay;
-	struct iceland_performance_level	ulv_power_level;
-};
-
-#define ICELAND_MAX_LEAKAGE_COUNT  8
-
-struct phw_iceland_leakage_voltage {
-	uint16_t  count;
-	uint16_t  leakage_id[ICELAND_MAX_LEAKAGE_COUNT];
-	uint16_t  actual_voltage[ICELAND_MAX_LEAKAGE_COUNT];
-};
-
-struct _phw_iceland_display_timing {
-	uint32_t min_clock_insr;
-	uint32_t num_existing_displays;
-};
-typedef struct _phw_iceland_display_timing phw_iceland_display_timing;
-
-
-struct phw_iceland_thermal_temperature_setting
-{
-	long temperature_low;
-	long temperature_high;
-	long temperature_shutdown;
-};
-
-struct _phw_iceland_dpmlevel_enable_mask {
-	uint32_t uvd_dpm_enable_mask;
-	uint32_t vce_dpm_enable_mask;
-	uint32_t acp_dpm_enable_mask;
-	uint32_t samu_dpm_enable_mask;
-	uint32_t sclk_dpm_enable_mask;
-	uint32_t mclk_dpm_enable_mask;
-	uint32_t pcie_dpm_enable_mask;
-};
-typedef struct _phw_iceland_dpmlevel_enable_mask phw_iceland_dpmlevel_enable_mask;
-
-struct _phw_iceland_pcie_perf_range {
-	uint16_t max;
-	uint16_t min;
-};
-typedef struct _phw_iceland_pcie_perf_range phw_iceland_pcie_perf_range;
-
-struct _phw_iceland_vbios_boot_state {
-	uint16_t					mvdd_bootup_value;
-	uint16_t					vddc_bootup_value;
-	uint16_t					vddci_bootup_value;
-	uint16_t					vddgfx_bootup_value;
-	uint32_t					sclk_bootup_value;
-	uint32_t					mclk_bootup_value;
-	uint16_t					pcie_gen_bootup_value;
-	uint16_t					pcie_lane_bootup_value;
-};
-typedef struct _phw_iceland_vbios_boot_state phw_iceland_vbios_boot_state;
-
-#define DPMTABLE_OD_UPDATE_SCLK     0x00000001
-#define DPMTABLE_OD_UPDATE_MCLK     0x00000002
-#define DPMTABLE_UPDATE_SCLK        0x00000004
-#define DPMTABLE_UPDATE_MCLK        0x00000008
-
-/* We need to review which fields are needed. */
-/* This is mostly a copy of the RV7xx/Evergreen structure which is close, but not identical to the N.Islands one. */
-struct iceland_hwmgr {
-	struct iceland_dpm_table               dpm_table;
-	struct iceland_dpm_table               golden_dpm_table;
-
-	uint32_t                           voting_rights_clients0;
-	uint32_t                           voting_rights_clients1;
-	uint32_t                           voting_rights_clients2;
-	uint32_t                           voting_rights_clients3;
-	uint32_t                           voting_rights_clients4;
-	uint32_t                           voting_rights_clients5;
-	uint32_t                           voting_rights_clients6;
-	uint32_t                           voting_rights_clients7;
-	uint32_t                           static_screen_threshold_unit;
-	uint32_t                           static_screen_threshold;
-	uint32_t                           voltage_control;
-	uint32_t                           vdd_gfx_control;
-
-	uint32_t                           vddc_vddci_delta;
-	uint32_t                           vddc_vddgfx_delta;
-
-	struct pp_interrupt_registration_info    internal_high_thermal_interrupt_info;
-	struct pp_interrupt_registration_info    internal_low_thermal_interrupt_info;
-	struct pp_interrupt_registration_info    smc_to_host_interrupt_info;
-	uint32_t                          active_auto_throttle_sources;
-
-	struct pp_interrupt_registration_info    external_throttle_interrupt;
-	irq_handler_func_t             external_throttle_callback;
-	void                             *external_throttle_context;
-
-	struct pp_interrupt_registration_info    ctf_interrupt_info;
-	irq_handler_func_t             ctf_callback;
-	void                             *ctf_context;
-
-	phw_iceland_clock_registers	  clock_registers;
-	phw_iceland_voltage_smio_registers  voltage_smio_registers;
-
-	bool	is_memory_GDDR5;
-	uint16_t                          acpi_vddc;
-	bool	pspp_notify_required;        /* Flag to indicate if PSPP notification to SBIOS is required */
-	uint16_t                          force_pcie_gen;            /* The forced PCI-E speed if not 0xffff */
-	uint16_t                          acpi_pcie_gen;             /* The PCI-E speed at ACPI time */
-	uint32_t                           pcie_gen_cap;             /* The PCI-E speed capabilities bitmap from CAIL */
-	uint32_t                           pcie_lane_cap;            /* The PCI-E lane capabilities bitmap from CAIL */
-	uint32_t                           pcie_spc_cap;             /* Symbol Per Clock Capabilities from registry */
-	struct phw_iceland_leakage_voltage	vddc_leakage;        /* The Leakage VDDC supported (based on leakage ID).*/
-	struct phw_iceland_leakage_voltage	vddcgfx_leakage;     /* The Leakage VDDC supported (based on leakage ID). */
-	struct phw_iceland_leakage_voltage	vddci_leakage;       /* The Leakage VDDCI supported (based on leakage ID). */
-
-	uint32_t                           mvdd_control;
-	uint32_t                           vddc_mask_low;
-	uint32_t                           mvdd_mask_low;
-	uint16_t                          max_vddc_in_pp_table;        /* the maximum VDDC value in the powerplay table*/
-	uint16_t                          min_vddc_in_pp_table;
-	uint16_t                          max_vddci_in_pp_table;       /* the maximum VDDCI value in the powerplay table */
-	uint16_t                          min_vddci_in_pp_table;
-	uint32_t                           mclk_strobe_mode_threshold;
-	uint32_t                           mclk_stutter_mode_threshold;
-	uint32_t                           mclk_edc_enable_threshold;
-	uint32_t                           mclk_edc_wr_enable_threshold;
-	bool	is_uvd_enabled;
-	bool	is_xdma_enabled;
-	phw_iceland_vbios_boot_state      vbios_boot_state;
-
-	bool                         battery_state;
-	bool                         is_tlu_enabled;
-	bool                         pcie_performance_request;
-
-	/* -------------- SMC SRAM Address of firmware header tables ----------------*/
-	uint32_t			sram_end;           /* The first address after the SMC SRAM. */
-	uint32_t			dpm_table_start;    /* The start of the dpm table in the SMC SRAM. */
-	uint32_t			soft_regs_start;    /* The start of the soft registers in the SMC SRAM. */
-	uint32_t			mc_reg_table_start; /* The start of the mc register table in the SMC SRAM. */
-	uint32_t			fan_table_start;    /* The start of the fan table in the SMC SRAM. */
-	uint32_t			arb_table_start;    /* The start of the ARB setting table in the SMC SRAM. */
-	uint32_t			ulv_settings_start;
-	SMU71_Discrete_DpmTable		smc_state_table;    /* The carbon copy of the SMC state table. */
-	SMU71_Discrete_MCRegisters      mc_reg_table;
-	SMU71_Discrete_Ulv              ulv_setting;         /* The carbon copy of ULV setting. */
-
-	/* -------------- Stuff originally coming from Evergreen --------------------*/
-	phw_iceland_mc_reg_table	 iceland_mc_reg_table;
-	uint32_t                         vdd_ci_control;
-	pp_atomctrl_voltage_table        vddc_voltage_table;
-	pp_atomctrl_voltage_table        vddci_voltage_table;
-	pp_atomctrl_voltage_table        vddgfx_voltage_table;
-	pp_atomctrl_voltage_table        mvdd_voltage_table;
-
-	uint32_t                           mgcg_cgtt_local2;
-	uint32_t                           mgcg_cgtt_local3;
-	uint32_t                           gpio_debug;
-	uint32_t			mc_micro_code_feature;
-	uint32_t			highest_mclk;
-	uint16_t                          acpi_vdd_ci;
-	uint8_t                           mvdd_high_index;
-	uint8_t                           mvdd_low_index;
-	bool                         dll_defaule_on;
-	bool                         performance_request_registered;
-
-	/* ----------------- Low Power Features ---------------------*/
-	phw_iceland_bacos					bacos;
-	struct phw_iceland_ulv_parm              	ulv;
-
-	/* ----------------- CAC Stuff ---------------------*/
-	uint32_t					cac_table_start;
-	bool                         cac_configuration_required;    /* TRUE if PP_CACConfigurationRequired == 1 */
-	bool                         driver_calculate_cac_leakage;  /* TRUE if PP_DriverCalculateCACLeakage == 1 */
-	bool                         cac_enabled;
-
-	/* ----------------- DPM2 Parameters ---------------------*/
-	uint32_t		power_containment_features;
-	bool                         enable_bapm_feature;
-	bool			     enable_dte_feature;
-	bool                         enable_tdc_limit_feature;
-	bool                         enable_pkg_pwr_tracking_feature;
-	bool                         disable_uvd_power_tune_feature;
-	struct iceland_pt_defaults           *power_tune_defaults;
-	SMU71_Discrete_PmFuses           power_tune_table;
-	uint32_t                           ul_dte_tj_offset;     /* Fudge factor in DPM table to correct HW DTE errors */
-	uint32_t                           fast_watermark_threshold;  /* use fast watermark if clock is equal or above this. In percentage of the target high sclk. */
-
-	/* ----------------- Phase Shedding ---------------------*/
-	bool                         vddc_phase_shed_control;
-
-	/* --------------------- DI/DT --------------------------*/
-	phw_iceland_display_timing       display_timing;
-
-	/* --------- ReadRegistry data for memory and engine clock margins ---- */
-	uint32_t                           engine_clock_data;
-	uint32_t                           memory_clock_data;
-
-	/* -------- Thermal Temperature Setting --------------*/
-	struct phw_iceland_thermal_temperature_setting 	thermal_temp_setting;
-	phw_iceland_dpmlevel_enable_mask     dpm_level_enable_mask;
-
-	uint32_t				need_update_smu7_dpm_table;
-	uint32_t				sclk_dpm_key_disabled;
-	uint32_t				mclk_dpm_key_disabled;
-	uint32_t				pcie_dpm_key_disabled;
-	/* used to store the previous dal min sclock */
-	uint32_t                           min_engine_clocks;
-	phw_iceland_pcie_perf_range       pcie_gen_performance;
-	phw_iceland_pcie_perf_range       pcie_lane_performance;
-	phw_iceland_pcie_perf_range       pcie_gen_power_saving;
-	phw_iceland_pcie_perf_range       pcie_lane_power_saving;
-	bool                            use_pcie_performance_levels;
-	bool                            use_pcie_power_saving_levels;
-	/* percentage value from 0-100, default 50 */
-	uint32_t                           activity_target[SMU71_MAX_LEVELS_GRAPHICS];
-	uint32_t                           mclk_activity_target;
-	uint32_t                           low_sclk_interrupt_threshold;
-	uint32_t                           last_mclk_dpm_enable_mask;
-	bool								uvd_enabled;
-	uint32_t                           pcc_monitor_enabled;
-
-	/* --------- Power Gating States ------------*/
-	bool                           uvd_power_gated;  /* 1: gated, 0:not gated */
-	bool                           vce_power_gated;  /* 1: gated, 0:not gated */
-	bool                           samu_power_gated; /* 1: gated, 0:not gated */
-	bool                           acp_power_gated;  /* 1: gated, 0:not gated */
-	bool                           pg_acp_init;
-
-	/* soft pptable for re-uploading into smu */
-	void *soft_pp_table;
-};
-
-typedef struct iceland_hwmgr iceland_hwmgr;
-
-int iceland_hwmgr_init(struct pp_hwmgr *hwmgr);
-int iceland_update_uvd_dpm(struct pp_hwmgr *hwmgr, bool bgate);
-uint32_t iceland_get_xclk(struct pp_hwmgr *hwmgr);
-int iceland_populate_bapm_vddc_vid_sidd(struct pp_hwmgr *hwmgr);
-int iceland_populate_vddc_vid(struct pp_hwmgr *hwmgr);
-
-#define ICELAND_DPM2_NEAR_TDP_DEC          10
-#define ICELAND_DPM2_ABOVE_SAFE_INC        5
-#define ICELAND_DPM2_BELOW_SAFE_INC        20
-
-/*
- * Log2 of the LTA window size (l2numWin_TDP). Eg. If LTA windows size
- * is 128, then this value should be Log2(128) = 7.
- */
-#define ICELAND_DPM2_LTA_WINDOW_SIZE       7
-
-#define ICELAND_DPM2_LTS_TRUNCATE          0
-
-#define ICELAND_DPM2_TDP_SAFE_LIMIT_PERCENT            80  // Maximum 100
-
-#define ICELAND_DPM2_MAXPS_PERCENT_H                   90  // Maximum 0xFF
-#define ICELAND_DPM2_MAXPS_PERCENT_M                   90  // Maximum 0xFF
-
-#define ICELAND_DPM2_PWREFFICIENCYRATIO_MARGIN         50
-
-#define ICELAND_DPM2_SQ_RAMP_MAX_POWER                 0x3FFF
-#define ICELAND_DPM2_SQ_RAMP_MIN_POWER                 0x12
-#define ICELAND_DPM2_SQ_RAMP_MAX_POWER_DELTA           0x15
-#define ICELAND_DPM2_SQ_RAMP_SHORT_TERM_INTERVAL_SIZE  0x1E
-#define ICELAND_DPM2_SQ_RAMP_LONG_TERM_INTERVAL_RATIO  0xF
-
-#define ICELAND_VOLTAGE_CONTROL_NONE                   0x0
-#define ICELAND_VOLTAGE_CONTROL_BY_GPIO                0x1
-#define ICELAND_VOLTAGE_CONTROL_BY_SVID2               0x2
-
-/* convert to Q8.8 format for firmware */
-#define ICELAND_Q88_FORMAT_CONVERSION_UNIT             256
-
-#define ICELAND_UNUSED_GPIO_PIN 0x7F
-
-#endif
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_powertune.c b/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_powertune.c
deleted file mode 100644
index 7662806..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_powertune.c
+++ /dev/null
@@ -1,490 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Author: Huang Rui <ray.huang@amd.com>
- *
- */
-
-#include "amdgpu.h"
-#include "hwmgr.h"
-#include "smumgr.h"
-#include "iceland_hwmgr.h"
-#include "iceland_powertune.h"
-#include "iceland_smumgr.h"
-#include "smu71_discrete.h"
-#include "smu71.h"
-#include "pp_debug.h"
-#include "cgs_common.h"
-#include "pp_endian.h"
-
-#include "bif/bif_5_0_d.h"
-#include "bif/bif_5_0_sh_mask.h"
-
-#define VOLTAGE_SCALE  4
-#define POWERTUNE_DEFAULT_SET_MAX    1
-
-#define DEVICE_ID_VI_ICELAND_M_6900	0x6900
-#define DEVICE_ID_VI_ICELAND_M_6901	0x6901
-#define DEVICE_ID_VI_ICELAND_M_6902	0x6902
-#define DEVICE_ID_VI_ICELAND_M_6903	0x6903
-
-
-struct iceland_pt_defaults defaults_iceland =
-{
-	/*
-	 * sviLoadLIneEn, SviLoadLineVddC, TDC_VDDC_ThrottleReleaseLimitPerc,
-	 * TDC_MAWt, TdcWaterfallCtl, DTEAmbientTempBase, DisplayCac, BAPM_TEMP_GRADIENT
-	 */
-	1, 0xF, 0xFD, 0x19, 5, 45, 0, 0xB0000,
-	{ 0x79,  0x253, 0x25D, 0xAE,  0x72,  0x80,  0x83,  0x86,  0x6F,  0xC8,  0xC9,  0xC9,  0x2F,  0x4D,  0x61  },
-	{ 0x17C, 0x172, 0x180, 0x1BC, 0x1B3, 0x1BD, 0x206, 0x200, 0x203, 0x25D, 0x25A, 0x255, 0x2C3, 0x2C5, 0x2B4 }
-};
-
-/* 35W - XT, XTL */
-struct iceland_pt_defaults defaults_icelandxt =
-{
-	/*
-	 * sviLoadLIneEn, SviLoadLineVddC,
-	 * TDC_VDDC_ThrottleReleaseLimitPerc, TDC_MAWt,
-	 * TdcWaterfallCtl, DTEAmbientTempBase, DisplayCac,
-	 * BAPM_TEMP_GRADIENT
-	 */
-	1, 0xF, 0xFD, 0x19, 5, 45, 0, 0x0,
-	{ 0xA7,  0x0, 0x0, 0xB5,  0x0, 0x0, 0x9F,  0x0, 0x0, 0xD6,  0x0, 0x0, 0xD7,  0x0, 0x0},
-	{ 0x1EA, 0x0, 0x0, 0x224, 0x0, 0x0, 0x25E, 0x0, 0x0, 0x28E, 0x0, 0x0, 0x2AB, 0x0, 0x0}
-};
-
-/* 25W - PRO, LE */
-struct iceland_pt_defaults defaults_icelandpro =
-{
-	/*
-	 * sviLoadLIneEn, SviLoadLineVddC,
-	 * TDC_VDDC_ThrottleReleaseLimitPerc, TDC_MAWt,
-	 * TdcWaterfallCtl, DTEAmbientTempBase, DisplayCac,
-	 * BAPM_TEMP_GRADIENT
-	 */
-	1, 0xF, 0xFD, 0x19, 5, 45, 0, 0x0,
-	{ 0xB7,  0x0, 0x0, 0xC3,  0x0, 0x0, 0xB5,  0x0, 0x0, 0xEA,  0x0, 0x0, 0xE6,  0x0, 0x0},
-	{ 0x1EA, 0x0, 0x0, 0x224, 0x0, 0x0, 0x25E, 0x0, 0x0, 0x28E, 0x0, 0x0, 0x2AB, 0x0, 0x0}
-};
-
-void iceland_initialize_power_tune_defaults(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint32_t tmp = 0;
-	struct cgs_system_info sys_info = {0};
-	uint32_t pdev_id;
-
-	sys_info.size = sizeof(struct cgs_system_info);
-	sys_info.info_id = CGS_SYSTEM_INFO_PCIE_DEV;
-	cgs_query_system_info(hwmgr->device, &sys_info);
-	pdev_id = (uint32_t)sys_info.value;
-
-	switch (pdev_id) {
-	case DEVICE_ID_VI_ICELAND_M_6900:
-	case DEVICE_ID_VI_ICELAND_M_6903:
-		data->power_tune_defaults = &defaults_icelandxt;
-		break;
-
-	case DEVICE_ID_VI_ICELAND_M_6901:
-	case DEVICE_ID_VI_ICELAND_M_6902:
-		data->power_tune_defaults = &defaults_icelandpro;
-		break;
-	default:
-	    /* TODO: need to assign valid defaults */
-	    data->power_tune_defaults = &defaults_iceland;
-	    pr_warning("Unknown V.I. Device ID.\n");
-	    break;
-	}
-
-	/* Assume disabled */
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SQRamping);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DBRamping);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_TDRamping);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_TCPRamping);
-
-	data->ul_dte_tj_offset = tmp;
-
-	if (!tmp) {
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_CAC);
-
-		data->fast_watermark_threshold = 100;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_PowerContainment)) {
-			tmp = 1;
-			data->enable_dte_feature = tmp ? false : true;
-			data->enable_tdc_limit_feature = tmp ? true : false;
-			data->enable_pkg_pwr_tracking_feature = tmp ? true : false;
-		}
-	}
-}
-
-int iceland_populate_bapm_parameters_in_dpm_table(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	struct iceland_pt_defaults *defaults = data->power_tune_defaults;
-	SMU71_Discrete_DpmTable  *dpm_table = &(data->smc_state_table);
-	struct phm_cac_tdp_table *cac_dtp_table = hwmgr->dyn_state.cac_dtp_table;
-	struct phm_ppm_table *ppm = hwmgr->dyn_state.ppm_parameter_table;
-	uint16_t *def1, *def2;
-	int i, j, k;
-
-	/*
-	 * TDP number of fraction bits are changed from 8 to 7 for Iceland
-	 * as requested by SMC team
-	 */
-	dpm_table->DefaultTdp = PP_HOST_TO_SMC_US((uint16_t)(cac_dtp_table->usTDP * 256));
-	dpm_table->TargetTdp = PP_HOST_TO_SMC_US((uint16_t)(cac_dtp_table->usConfigurableTDP * 256));
-
-	dpm_table->DTETjOffset = (uint8_t)data->ul_dte_tj_offset;
-
-	dpm_table->GpuTjMax = (uint8_t)(data->thermal_temp_setting.temperature_high / PP_TEMPERATURE_UNITS_PER_CENTIGRADES);
-	dpm_table->GpuTjHyst = 8;
-
-	dpm_table->DTEAmbientTempBase = defaults->dte_ambient_temp_base;
-
-	/* The following are for new Iceland Multi-input fan/thermal control */
-	if(NULL != ppm) {
-		dpm_table->PPM_PkgPwrLimit = (uint16_t)ppm->dgpu_tdp * 256 / 1000;
-		dpm_table->PPM_TemperatureLimit = (uint16_t)ppm->tj_max * 256;
-	} else {
-		dpm_table->PPM_PkgPwrLimit = 0;
-		dpm_table->PPM_TemperatureLimit = 0;
-	}
-
-	CONVERT_FROM_HOST_TO_SMC_US(dpm_table->PPM_PkgPwrLimit);
-	CONVERT_FROM_HOST_TO_SMC_US(dpm_table->PPM_TemperatureLimit);
-
-	dpm_table->BAPM_TEMP_GRADIENT = PP_HOST_TO_SMC_UL(defaults->bamp_temp_gradient);
-	def1 = defaults->bapmti_r;
-	def2 = defaults->bapmti_rc;
-
-	for (i = 0; i < SMU71_DTE_ITERATIONS; i++) {
-		for (j = 0; j < SMU71_DTE_SOURCES; j++) {
-			for (k = 0; k < SMU71_DTE_SINKS; k++) {
-				dpm_table->BAPMTI_R[i][j][k] = PP_HOST_TO_SMC_US(*def1);
-				dpm_table->BAPMTI_RC[i][j][k] = PP_HOST_TO_SMC_US(*def2);
-				def1++;
-				def2++;
-			}
-		}
-	}
-
-	return 0;
-}
-
-static int iceland_populate_svi_load_line(struct pp_hwmgr *hwmgr)
-{
-    struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-    const struct iceland_pt_defaults *defaults = data->power_tune_defaults;
-
-    data->power_tune_table.SviLoadLineEn = defaults->svi_load_line_en;
-    data->power_tune_table.SviLoadLineVddC = defaults->svi_load_line_vddc;
-    data->power_tune_table.SviLoadLineTrimVddC = 3;
-    data->power_tune_table.SviLoadLineOffsetVddC = 0;
-
-    return 0;
-}
-
-static int iceland_populate_tdc_limit(struct pp_hwmgr *hwmgr)
-{
-	uint16_t tdc_limit;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	const struct iceland_pt_defaults *defaults = data->power_tune_defaults;
-
-	/* TDC number of fraction bits are changed from 8 to 7
-	 * for Iceland as requested by SMC team
-	 */
-	tdc_limit = (uint16_t)(hwmgr->dyn_state.cac_dtp_table->usTDC * 256);
-	data->power_tune_table.TDC_VDDC_PkgLimit =
-			CONVERT_FROM_HOST_TO_SMC_US(tdc_limit);
-	data->power_tune_table.TDC_VDDC_ThrottleReleaseLimitPerc =
-			defaults->tdc_vddc_throttle_release_limit_perc;
-	data->power_tune_table.TDC_MAWt = defaults->tdc_mawt;
-
-	return 0;
-}
-
-static int iceland_populate_dw8(struct pp_hwmgr *hwmgr, uint32_t fuse_table_offset)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	const struct iceland_pt_defaults *defaults = data->power_tune_defaults;
-	uint32_t temp;
-
-	if (smu7_read_smc_sram_dword(hwmgr->smumgr,
-			fuse_table_offset +
-			offsetof(SMU71_Discrete_PmFuses, TdcWaterfallCtl),
-			(uint32_t *)&temp, data->sram_end))
-		PP_ASSERT_WITH_CODE(false,
-				"Attempt to read PmFuses.DW6 (SviLoadLineEn) from SMC Failed!",
-				return -EINVAL);
-	else
-		data->power_tune_table.TdcWaterfallCtl = defaults->tdc_waterfall_ctl;
-
-	return 0;
-}
-
-static int iceland_populate_temperature_scaler(struct pp_hwmgr *hwmgr)
-{
-	return 0;
-}
-
-static int iceland_populate_gnb_lpml(struct pp_hwmgr *hwmgr)
-{
-	int i;
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	/* Currently not used. Set all to zero. */
-	for (i = 0; i < 8; i++)
-		data->power_tune_table.GnbLPML[i] = 0;
-
-	return 0;
-}
-
-static int iceland_min_max_vgnb_lpml_id_from_bapm_vddc(struct pp_hwmgr *hwmgr)
-{
-    return 0;
-}
-
-static int iceland_populate_bapm_vddc_base_leakage_sidd(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint16_t HiSidd = data->power_tune_table.BapmVddCBaseLeakageHiSidd;
-	uint16_t LoSidd = data->power_tune_table.BapmVddCBaseLeakageLoSidd;
-	struct phm_cac_tdp_table *cac_table = hwmgr->dyn_state.cac_dtp_table;
-
-	HiSidd = (uint16_t)(cac_table->usHighCACLeakage / 100 * 256);
-	LoSidd = (uint16_t)(cac_table->usLowCACLeakage / 100 * 256);
-
-	data->power_tune_table.BapmVddCBaseLeakageHiSidd =
-			CONVERT_FROM_HOST_TO_SMC_US(HiSidd);
-	data->power_tune_table.BapmVddCBaseLeakageLoSidd =
-			CONVERT_FROM_HOST_TO_SMC_US(LoSidd);
-
-	return 0;
-}
-
-int iceland_populate_pm_fuses(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	uint32_t pm_fuse_table_offset;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		if (smu7_read_smc_sram_dword(hwmgr->smumgr,
-				SMU71_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU71_Firmware_Header, PmFuseTable),
-				&pm_fuse_table_offset, data->sram_end))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to get pm_fuse_table_offset Failed!",
-					return -EINVAL);
-
-		/* DW0 - DW3 */
-		if (iceland_populate_bapm_vddc_vid_sidd(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate bapm vddc vid Failed!",
-					return -EINVAL);
-
-		/* DW4 - DW5 */
-		if (iceland_populate_vddc_vid(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate vddc vid Failed!",
-					return -EINVAL);
-
-		/* DW6 */
-		if (iceland_populate_svi_load_line(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate SviLoadLine Failed!",
-					return -EINVAL);
-		/* DW7 */
-		if (iceland_populate_tdc_limit(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate TDCLimit Failed!", return -EINVAL);
-		/* DW8 */
-		if (iceland_populate_dw8(hwmgr, pm_fuse_table_offset))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate TdcWaterfallCtl, "
-					"LPMLTemperature Min and Max Failed!",
-					return -EINVAL);
-
-		/* DW9-DW12 */
-		if (0 != iceland_populate_temperature_scaler(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate LPMLTemperatureScaler Failed!",
-					return -EINVAL);
-
-		/* DW13-DW16 */
-		if (iceland_populate_gnb_lpml(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate GnbLPML Failed!",
-					return -EINVAL);
-
-		/* DW17 */
-		if (iceland_min_max_vgnb_lpml_id_from_bapm_vddc(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate GnbLPML Min and Max Vid Failed!",
-					return -EINVAL);
-
-		/* DW18 */
-		if (iceland_populate_bapm_vddc_base_leakage_sidd(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate BapmVddCBaseLeakage Hi and Lo Sidd Failed!",
-					return -EINVAL);
-
-		if (smu7_copy_bytes_to_smc(hwmgr->smumgr, pm_fuse_table_offset,
-				(uint8_t *)&data->power_tune_table,
-				sizeof(struct SMU71_Discrete_PmFuses), data->sram_end))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to download PmFuseTable Failed!",
-					return -EINVAL);
-	}
-	return 0;
-}
-
-int iceland_enable_smc_cac(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC)) {
-		int smc_result;
-		smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-				(uint16_t)(PPSMC_MSG_EnableCac));
-		PP_ASSERT_WITH_CODE((0 == smc_result),
-				"Failed to enable CAC in SMC.", result = -1);
-
-		data->cac_enabled = (0 == smc_result) ? true : false;
-	}
-	return result;
-}
-
-static int iceland_set_power_limit(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-
-	if(data->power_containment_features &
-			POWERCONTAINMENT_FEATURE_PkgPwrLimit)
-		return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_PkgPwrSetLimit, n);
-	return 0;
-}
-
-static int iceland_set_overdriver_target_tdp(struct pp_hwmgr *pHwMgr, uint32_t target_tdp)
-{
-	return smum_send_msg_to_smc_with_parameter(pHwMgr->smumgr,
-			PPSMC_MSG_OverDriveSetTargetTdp, target_tdp);
-}
-
-int iceland_enable_power_containment(struct pp_hwmgr *hwmgr)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	SMU71_Discrete_DpmTable *dpm_table = &data->smc_state_table;
-	int smc_result;
-	int result = 0;
-	uint32_t is_asic_kicker;
-
-	data->power_containment_features = 0;
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		is_asic_kicker = cgs_read_register(hwmgr->device, mmCC_BIF_BX_STRAP2);
-		is_asic_kicker = (is_asic_kicker >> 12) & 0x01;
-
-		if (data->enable_bapm_feature &&
-			(!is_asic_kicker ||
-			 phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				 PHM_PlatformCaps_DisableUsingActualTemperatureForPowerCalc))) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_EnableDTE));
-			PP_ASSERT_WITH_CODE((0 == smc_result),
-					"Failed to enable BAPM in SMC.", result = -1;);
-			if (0 == smc_result)
-				data->power_containment_features |= POWERCONTAINMENT_FEATURE_BAPM;
-		}
-
-		if (is_asic_kicker && !phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_DisableUsingActualTemperatureForPowerCalc))
-			dpm_table->DTEMode = 2;
-
-		if (data->enable_tdc_limit_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_TDCLimitEnable));
-			PP_ASSERT_WITH_CODE((0 == smc_result),
-					"Failed to enable TDCLimit in SMC.", result = -1;);
-			if (0 == smc_result)
-				data->power_containment_features |=
-						POWERCONTAINMENT_FEATURE_TDCLimit;
-		}
-
-		if (data->enable_pkg_pwr_tracking_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_PkgPwrLimitEnable));
-			PP_ASSERT_WITH_CODE((0 == smc_result),
-					"Failed to enable PkgPwrTracking in SMC.", result = -1;);
-			if (0 == smc_result) {
-				struct phm_cac_tdp_table *cac_table =
-						hwmgr->dyn_state.cac_dtp_table;
-				uint32_t default_limit =
-					(uint32_t)(cac_table->usMaximumPowerDeliveryLimit * 256);
-
-				data->power_containment_features |=
-						POWERCONTAINMENT_FEATURE_PkgPwrLimit;
-
-				if (iceland_set_power_limit(hwmgr, default_limit))
-					printk(KERN_ERR "Failed to set Default Power Limit in SMC!");
-			}
-		}
-	}
-	return result;
-}
-
-int iceland_power_control_set_level(struct pp_hwmgr *hwmgr)
-{
-	struct phm_cac_tdp_table *cac_table = hwmgr->dyn_state.cac_dtp_table;
-	int adjust_percent, target_tdp;
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		/* adjustment percentage has already been validated */
-		adjust_percent = hwmgr->platform_descriptor.TDPAdjustmentPolarity ?
-				hwmgr->platform_descriptor.TDPAdjustment :
-				(-1 * hwmgr->platform_descriptor.TDPAdjustment);
-		/*
-		 * SMC requested that target_tdp to be 7 bit fraction in DPM table
-		 * but message to be 8 bit fraction for messages
-		 */
-		target_tdp = ((100 + adjust_percent) * (int)(cac_table->usTDP * 256)) / 100;
-		result = iceland_set_overdriver_target_tdp(hwmgr, (uint32_t)target_tdp);
-	}
-
-	return result;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_powertune.h b/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_powertune.h
deleted file mode 100644
index 4008d49..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_powertune.h
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Author: Huang Rui <ray.huang@amd.com>
- *
- */
-#ifndef ICELAND_POWERTUNE_H
-#define ICELAND_POWERTUNE_H
-
-#include "smu71.h"
-
-enum iceland_pt_config_reg_type {
-	ICELAND_CONFIGREG_MMR = 0,
-	ICELAND_CONFIGREG_SMC_IND,
-	ICELAND_CONFIGREG_DIDT_IND,
-	ICELAND_CONFIGREG_CACHE,
-	ICELAND_CONFIGREG_MAX
-};
-
-/* PowerContainment Features */
-#define POWERCONTAINMENT_FEATURE_DTE             0x00000001
-#define POWERCONTAINMENT_FEATURE_TDCLimit        0x00000002
-#define POWERCONTAINMENT_FEATURE_PkgPwrLimit     0x00000004
-#define POWERCONTAINMENT_FEATURE_BAPM		 0x00000001
-
-struct iceland_pt_config_reg {
-	uint32_t                           offset;
-	uint32_t                           mask;
-	uint32_t                           shift;
-	uint32_t                           value;
-	enum iceland_pt_config_reg_type       type;
-};
-
-void iceland_initialize_power_tune_defaults(struct pp_hwmgr *hwmgr);
-int iceland_populate_bapm_parameters_in_dpm_table(struct pp_hwmgr *hwmgr);
-int iceland_populate_pm_fuses(struct pp_hwmgr *hwmgr);
-int iceland_enable_smc_cac(struct pp_hwmgr *hwmgr);
-int iceland_enable_power_containment(struct pp_hwmgr *hwmgr);
-int iceland_power_control_set_level(struct pp_hwmgr *hwmgr);
-
-#endif  /* ICELAND_POWERTUNE_H */
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_thermal.c b/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_thermal.c
deleted file mode 100644
index 45d17d7..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_thermal.c
+++ /dev/null
@@ -1,595 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Author: Huang Rui <ray.huang@amd.com>
- *
- */
-#include <asm/div64.h>
-#include "iceland_thermal.h"
-#include "iceland_hwmgr.h"
-#include "iceland_smumgr.h"
-#include "atombios.h"
-#include "ppsmc.h"
-
-#include "gmc/gmc_8_1_d.h"
-#include "gmc/gmc_8_1_sh_mask.h"
-
-#include "bif/bif_5_0_d.h"
-#include "bif/bif_5_0_sh_mask.h"
-
-#include "smu/smu_7_1_1_d.h"
-#include "smu/smu_7_1_1_sh_mask.h"
-
-
-/**
-* Get Fan Speed Control Parameters.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pSpeed is the address of the structure where the result is to be placed.
-* @exception Always succeeds except if we cannot zero out the output structure.
-*/
-int iceland_fan_ctrl_get_fan_speed_info(struct pp_hwmgr *hwmgr,
-					struct phm_fan_speed_info *fan_speed_info)
-{
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	fan_speed_info->supports_percent_read = true;
-	fan_speed_info->supports_percent_write = true;
-	fan_speed_info->min_percent = 0;
-	fan_speed_info->max_percent = 100;
-
-	if (0 != hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution) {
-		fan_speed_info->supports_rpm_read = true;
-		fan_speed_info->supports_rpm_write = true;
-		fan_speed_info->min_rpm = hwmgr->thermal_controller.fanInfo.ulMinRPM;
-		fan_speed_info->max_rpm = hwmgr->thermal_controller.fanInfo.ulMaxRPM;
-	} else {
-		fan_speed_info->min_rpm = 0;
-		fan_speed_info->max_rpm = 0;
-	}
-
-	return 0;
-}
-
-/**
-* Get Fan Speed in percent.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pSpeed is the address of the structure where the result is to be placed.
-* @exception Fails is the 100% setting appears to be 0.
-*/
-int iceland_fan_ctrl_get_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t *speed)
-{
-	uint32_t duty100;
-	uint32_t duty;
-	uint64_t tmp64;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL1, FMAX_DUTY100);
-	duty = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_STATUS, FDO_PWM_DUTY);
-
-	if (0 == duty100)
-		return -EINVAL;
-
-
-	tmp64 = (uint64_t)duty * 100;
-	do_div(tmp64, duty100);
-	*speed = (uint32_t)tmp64;
-
-	if (*speed > 100)
-		*speed = 100;
-
-	return 0;
-}
-
-/**
-* Get Fan Speed in RPM.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the address of the structure where the result is to be placed.
-* @exception Returns not supported if no fan is found or if pulses per revolution are not set
-*/
-int iceland_fan_ctrl_get_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t *speed)
-{
-	return 0;
-}
-
-/**
-* Set Fan Speed Control to static mode, so that the user can decide what speed to use.
-* @param    hwmgr  the address of the powerplay hardware manager.
-*           mode    the fan control mode, 0 default, 1 by percent, 5, by RPM
-* @exception Should always succeed.
-*/
-int iceland_fan_ctrl_set_static_mode(struct pp_hwmgr *hwmgr, uint32_t mode)
-{
-
-	if (hwmgr->fan_ctrl_is_in_default_mode) {
-		hwmgr->fan_ctrl_default_mode = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, FDO_PWM_MODE);
-		hwmgr->tmin = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, TMIN);
-		hwmgr->fan_ctrl_is_in_default_mode = false;
-	}
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, TMIN, 0);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, FDO_PWM_MODE, mode);
-
-	return 0;
-}
-
-/**
-* Reset Fan Speed Control to default mode.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @exception Should always succeed.
-*/
-static int iceland_fan_ctrl_set_default_mode(struct pp_hwmgr *hwmgr)
-{
-	if (!hwmgr->fan_ctrl_is_in_default_mode) {
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, FDO_PWM_MODE, hwmgr->fan_ctrl_default_mode);
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, TMIN, hwmgr->tmin);
-		hwmgr->fan_ctrl_is_in_default_mode = true;
-	}
-
-	return 0;
-}
-
-int iceland_fan_ctrl_start_smc_fan_control(struct pp_hwmgr *hwmgr)
-{
-	return (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StartFanControl) == 0) ?  0 : -EINVAL;
-}
-
-
-int iceland_fan_ctrl_stop_smc_fan_control(struct pp_hwmgr *hwmgr)
-{
-	return (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StopFanControl) == 0) ?  0 : -EINVAL;
-}
-
-/**
-* Set Fan Speed in percent.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the percentage value (0% - 100%) to be set.
-* @exception Fails is the 100% setting appears to be 0.
-*/
-int iceland_fan_ctrl_set_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t speed)
-{
-	uint32_t duty100;
-	uint32_t duty;
-	uint64_t tmp64;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return -EINVAL;
-
-	if (speed > 100) {
-		pr_warning("Cannot set more than 100%% duty cycle. Set it to 100.\n");
-		speed = 100;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl))
-		iceland_fan_ctrl_stop_smc_fan_control(hwmgr);
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL1, FMAX_DUTY100);
-
-	if (0 == duty100)
-		return -EINVAL;
-
-	tmp64 = (uint64_t)speed * duty100;
-	do_div(tmp64, 100);
-	duty = (uint32_t)tmp64;
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL0, FDO_STATIC_DUTY, duty);
-
-	return iceland_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-}
-
-/**
-* Reset Fan Speed to default.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @exception Always succeeds.
-*/
-int iceland_fan_ctrl_reset_fan_speed_to_default(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl)) {
-		result = iceland_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-		if (0 == result)
-			result = iceland_fan_ctrl_start_smc_fan_control(hwmgr);
-	} else
-		result = iceland_fan_ctrl_set_default_mode(hwmgr);
-
-	return result;
-}
-
-/**
-* Set Fan Speed in RPM.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the percentage value (min - max) to be set.
-* @exception Fails is the speed not lie between min and max.
-*/
-int iceland_fan_ctrl_set_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t speed)
-{
-	return 0;
-}
-
-/**
-* Reads the remote temperature from the SIslands thermal controller.
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-int iceland_thermal_get_temperature(struct pp_hwmgr *hwmgr)
-{
-	int temp;
-
-	temp = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_MULT_THERMAL_STATUS, CTF_TEMP);
-
-	/*
-	 * Bit 9 means the reading is lower than the lowest usable
-	 * value.
-	 */
-	if (0 != (0x200 & temp))
-		temp = ICELAND_THERMAL_MAXIMUM_TEMP_READING;
-	else
-		temp = (temp & 0x1ff);
-
-	temp = temp * PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-
-	return temp;
-}
-
-/**
-* Set the requested temperature range for high and low alert signals
-*
-* @param    hwmgr The address of the hardware manager.
-* @param    range Temperature range to be programmed for high and low alert signals
-* @exception PP_Result_BadInput if the input data is not valid.
-*/
-static int iceland_thermal_set_temperature_range(struct pp_hwmgr *hwmgr, uint32_t low_temp, uint32_t high_temp)
-{
-	uint32_t low = ICELAND_THERMAL_MINIMUM_ALERT_TEMP * PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-	uint32_t high = ICELAND_THERMAL_MAXIMUM_ALERT_TEMP * PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-
-	if (low < low_temp)
-		low = low_temp;
-	if (high > high_temp)
-		high = high_temp;
-
-	if (low > high)
-		return -EINVAL;
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, DIG_THERM_INTH, (high / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, DIG_THERM_INTL, (low / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_CTRL, DIG_THERM_DPM, (high / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-
-	return 0;
-}
-
-/**
-* Programs thermal controller one-time setting registers
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-static int iceland_thermal_initialize(struct pp_hwmgr *hwmgr)
-{
-	if (0 != hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution)
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-						CG_TACH_CTRL, EDGE_PER_REV,
-						hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution - 1);
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, TACH_PWM_RESP_RATE, 0x28);
-
-	return 0;
-}
-
-/**
-* Enable thermal alerts on the RV770 thermal controller.
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-static int iceland_thermal_enable_alert(struct pp_hwmgr *hwmgr)
-{
-	uint32_t alert;
-
-	alert = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, THERM_INT_MASK);
-	alert &= ~(ICELAND_THERMAL_HIGH_ALERT_MASK | ICELAND_THERMAL_LOW_ALERT_MASK);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, THERM_INT_MASK, alert);
-
-	/* send message to SMU to enable internal thermal interrupts */
-	return (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_Thermal_Cntl_Enable) == 0) ? 0 : -1;
-}
-
-/**
-* Disable thermal alerts on the RV770 thermal controller.
-* @param    hwmgr The address of the hardware manager.
-*/
-static int iceland_thermal_disable_alert(struct pp_hwmgr *hwmgr)
-{
-	uint32_t alert;
-
-	alert = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, THERM_INT_MASK);
-	alert |= (ICELAND_THERMAL_HIGH_ALERT_MASK | ICELAND_THERMAL_LOW_ALERT_MASK);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, THERM_INT_MASK, alert);
-
-	/* send message to SMU to disable internal thermal interrupts */
-	return (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_Thermal_Cntl_Disable) == 0) ? 0 : -1;
-}
-
-/**
-* Uninitialize the thermal controller.
-* Currently just disables alerts.
-* @param    hwmgr The address of the hardware manager.
-*/
-int iceland_thermal_stop_thermal_controller(struct pp_hwmgr *hwmgr)
-{
-	int result = iceland_thermal_disable_alert(hwmgr);
-
-	if (result)
-		pr_warning("Failed to disable thermal alerts!\n");
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		iceland_fan_ctrl_set_default_mode(hwmgr);
-
-	return result;
-}
-
-/**
-* Set up the fan table to control the fan using the SMC.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-int tf_iceland_thermal_setup_fan_table(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result)
-{
-	struct iceland_hwmgr *data = (struct iceland_hwmgr *)(hwmgr->backend);
-	SMU71_Discrete_FanTable fan_table = { FDO_MODE_HARDWARE };
-	uint32_t duty100;
-	uint32_t t_diff1, t_diff2, pwm_diff1, pwm_diff2;
-	uint16_t fdo_min, slope1, slope2;
-	uint32_t reference_clock;
-	int res;
-	uint64_t tmp64;
-
-	if (!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl))
-		return 0;
-
-	if (0 == data->fan_table_start) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl);
-		return 0;
-	}
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL1, FMAX_DUTY100);
-
-	if (0 == duty100) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl);
-		return 0;
-	}
-
-	tmp64 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMMin * duty100;
-	do_div(tmp64, 10000);
-	fdo_min = (uint16_t)tmp64;
-
-	t_diff1 = hwmgr->thermal_controller.advanceFanControlParameters.usTMed - hwmgr->thermal_controller.advanceFanControlParameters.usTMin;
-	t_diff2 = hwmgr->thermal_controller.advanceFanControlParameters.usTHigh - hwmgr->thermal_controller.advanceFanControlParameters.usTMed;
-
-	pwm_diff1 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMMed - hwmgr->thermal_controller.advanceFanControlParameters.usPWMMin;
-	pwm_diff2 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMHigh - hwmgr->thermal_controller.advanceFanControlParameters.usPWMMed;
-
-	slope1 = (uint16_t)((50 + ((16 * duty100 * pwm_diff1) / t_diff1)) / 100);
-	slope2 = (uint16_t)((50 + ((16 * duty100 * pwm_diff2) / t_diff2)) / 100);
-
-	fan_table.TempMin = cpu_to_be16((50 + hwmgr->thermal_controller.advanceFanControlParameters.usTMin) / 100);
-	fan_table.TempMed = cpu_to_be16((50 + hwmgr->thermal_controller.advanceFanControlParameters.usTMed) / 100);
-	fan_table.TempMax = cpu_to_be16((50 + hwmgr->thermal_controller.advanceFanControlParameters.usTMax) / 100);
-
-	fan_table.Slope1 = cpu_to_be16(slope1);
-	fan_table.Slope2 = cpu_to_be16(slope2);
-
-	fan_table.FdoMin = cpu_to_be16(fdo_min);
-
-	fan_table.HystDown = cpu_to_be16(hwmgr->thermal_controller.advanceFanControlParameters.ucTHyst);
-
-	fan_table.HystUp = cpu_to_be16(1);
-
-	fan_table.HystSlope = cpu_to_be16(1);
-
-	fan_table.TempRespLim = cpu_to_be16(5);
-
-	reference_clock = iceland_get_xclk(hwmgr);
-
-	fan_table.RefreshPeriod = cpu_to_be32((hwmgr->thermal_controller.advanceFanControlParameters.ulCycleDelay * reference_clock) / 1600);
-
-	fan_table.FdoMax = cpu_to_be16((uint16_t)duty100);
-
-	fan_table.TempSrc = (uint8_t)PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_MULT_THERMAL_CTRL, TEMP_SEL);
-
-	//fan_table.FanControl_GL_Flag = 1;
-
-	res = smu7_copy_bytes_to_smc(hwmgr->smumgr, data->fan_table_start, (uint8_t *)&fan_table, (uint32_t)sizeof(fan_table), data->sram_end);
-/* TO DO FOR SOME DEVICE ID 0X692b, send this msg return invalid command.
-	if (res == 0 && hwmgr->thermal_controller.advanceFanControlParameters.ucMinimumPWMLimit != 0)
-		res = (0 == smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_SetFanMinPwm, \
-						hwmgr->thermal_controller.advanceFanControlParameters.ucMinimumPWMLimit) ? 0 : -1);
-
-	if (res == 0 && hwmgr->thermal_controller.advanceFanControlParameters.ulMinFanSCLKAcousticLimit != 0)
-		res = (0 == smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_SetFanSclkTarget, \
-					hwmgr->thermal_controller.advanceFanControlParameters.ulMinFanSCLKAcousticLimit) ? 0 : -1);
-
-	if (0 != res)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl);
-*/
-	return 0;
-}
-
-/**
-* Start the fan control on the SMC.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-int tf_iceland_thermal_start_smc_fan_control(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result)
-{
-/* If the fantable setup has failed we could have disabled PHM_PlatformCaps_MicrocodeFanControl even after this function was included in the table.
- * Make sure that we still think controlling the fan is OK.
-*/
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl)) {
-		iceland_fan_ctrl_start_smc_fan_control(hwmgr);
-		iceland_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-	}
-
-	return 0;
-}
-
-/**
-* Set temperature range for high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-static int tf_iceland_thermal_set_temperature_range(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	struct PP_TemperatureRange *range = (struct PP_TemperatureRange *)input;
-
-	if (range == NULL)
-		return -EINVAL;
-
-	return iceland_thermal_set_temperature_range(hwmgr, range->min, range->max);
-}
-
-/**
-* Programs one-time setting registers
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from initialize thermal controller routine
-*/
-static int tf_iceland_thermal_initialize(struct pp_hwmgr *hwmgr, void *input,
-				void *output, void *storage, int result)
-{
-    return iceland_thermal_initialize(hwmgr);
-}
-
-/**
-* Enable high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from enable alert routine
-*/
-static int tf_iceland_thermal_enable_alert(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	return iceland_thermal_enable_alert(hwmgr);
-}
-
-/**
-* Disable high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from disable alert routine
-*/
-static int tf_iceland_thermal_disable_alert(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result)
-{
-	return iceland_thermal_disable_alert(hwmgr);
-}
-
-static const struct phm_master_table_item iceland_thermal_start_thermal_controller_master_list[] = {
-	{ NULL, tf_iceland_thermal_initialize },
-	{ NULL, tf_iceland_thermal_set_temperature_range },
-	{ NULL, tf_iceland_thermal_enable_alert },
-	/*
-	 * We should restrict performance levels to low before we halt
-	 * the SMC.  On the other hand we are still in boot state when
-	 * we do this so it would be pointless.  If this assumption
-	 * changes we have to revisit this table.
-	 */
-	{ NULL, tf_iceland_thermal_setup_fan_table},
-	{ NULL, tf_iceland_thermal_start_smc_fan_control},
-	{ NULL, NULL }
-};
-
-static const struct phm_master_table_header iceland_thermal_start_thermal_controller_master = {
-	0,
-	PHM_MasterTableFlag_None,
-	iceland_thermal_start_thermal_controller_master_list
-};
-
-static const struct phm_master_table_item iceland_thermal_set_temperature_range_master_list[] = {
-	{ NULL, tf_iceland_thermal_disable_alert},
-	{ NULL, tf_iceland_thermal_set_temperature_range},
-	{ NULL, tf_iceland_thermal_enable_alert},
-	{ NULL, NULL }
-};
-
-static const struct phm_master_table_header iceland_thermal_set_temperature_range_master = {
-	0,
-	PHM_MasterTableFlag_None,
-	iceland_thermal_set_temperature_range_master_list
-};
-
-int iceland_thermal_ctrl_uninitialize_thermal_controller(struct pp_hwmgr *hwmgr)
-{
-	if (!hwmgr->thermal_controller.fanInfo.bNoFan)
-		iceland_fan_ctrl_set_default_mode(hwmgr);
-	return 0;
-}
-
-/**
-* Initializes the thermal controller related functions in the Hardware Manager structure.
-* @param    hwmgr The address of the hardware manager.
-* @exception Any error code from the low-level communication.
-*/
-int pp_iceland_thermal_initialize(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	result = phm_construct_table(hwmgr, &iceland_thermal_set_temperature_range_master, &(hwmgr->set_temperature_range));
-
-	if (0 == result) {
-		result = phm_construct_table(hwmgr,
-						&iceland_thermal_start_thermal_controller_master,
-						&(hwmgr->start_thermal_controller));
-		if (0 != result)
-			phm_destroy_table(hwmgr, &(hwmgr->set_temperature_range));
-	}
-
-	if (0 == result)
-		hwmgr->fan_ctrl_is_in_default_mode = true;
-	return result;
-}
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_thermal.h b/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_thermal.h
deleted file mode 100644
index 267945f..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/iceland_thermal.h
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Author: Huang Rui <ray.huang@amd.com>
- *
- */
-
-#ifndef ICELAND_THERMAL_H
-#define ICELAND_THERMAL_H
-
-#include "hwmgr.h"
-
-#define ICELAND_THERMAL_HIGH_ALERT_MASK         0x1
-#define ICELAND_THERMAL_LOW_ALERT_MASK          0x2
-
-#define ICELAND_THERMAL_MINIMUM_TEMP_READING    -256
-#define ICELAND_THERMAL_MAXIMUM_TEMP_READING    255
-
-#define ICELAND_THERMAL_MINIMUM_ALERT_TEMP      0
-#define ICELAND_THERMAL_MAXIMUM_ALERT_TEMP      255
-
-#define FDO_PWM_MODE_STATIC  1
-#define FDO_PWM_MODE_STATIC_RPM 5
-
-
-extern int iceland_thermal_get_temperature(struct pp_hwmgr *hwmgr);
-extern int iceland_thermal_stop_thermal_controller(struct pp_hwmgr *hwmgr);
-extern int iceland_fan_ctrl_get_fan_speed_info(struct pp_hwmgr *hwmgr, struct phm_fan_speed_info *fan_speed_info);
-extern int iceland_fan_ctrl_get_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t *speed);
-extern int iceland_fan_ctrl_set_static_mode(struct pp_hwmgr *hwmgr, uint32_t mode);
-extern int iceland_fan_ctrl_set_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t speed);
-extern int iceland_fan_ctrl_reset_fan_speed_to_default(struct pp_hwmgr *hwmgr);
-extern int pp_iceland_thermal_initialize(struct pp_hwmgr *hwmgr);
-extern int iceland_thermal_ctrl_uninitialize_thermal_controller(struct pp_hwmgr *hwmgr);
-extern int iceland_fan_ctrl_set_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t speed);
-extern int iceland_fan_ctrl_get_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t *speed);
-extern int iceland_fan_ctrl_stop_smc_fan_control(struct pp_hwmgr *hwmgr);
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_clockpowergating.c b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_clockpowergating.c
deleted file mode 100644
index 7e405b0..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_clockpowergating.c
+++ /dev/null
@@ -1,444 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#include "polaris10_clockpowergating.h"
-
-int polaris10_phm_powerdown_uvd(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cf_want_uvd_power_gating(hwmgr))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_UVDPowerOFF);
-	return 0;
-}
-
-static int polaris10_phm_powerup_uvd(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cf_want_uvd_power_gating(hwmgr)) {
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				  PHM_PlatformCaps_UVDDynamicPowerGating)) {
-			return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_UVDPowerON, 1);
-		} else {
-			return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_UVDPowerON, 0);
-		}
-	}
-
-	return 0;
-}
-
-static int polaris10_phm_powerdown_vce(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cf_want_vce_power_gating(hwmgr))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_VCEPowerOFF);
-	return 0;
-}
-
-static int polaris10_phm_powerup_vce(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cf_want_vce_power_gating(hwmgr))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_VCEPowerON);
-	return 0;
-}
-
-static int polaris10_phm_powerdown_samu(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SamuPowerGating))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_SAMPowerOFF);
-	return 0;
-}
-
-static int polaris10_phm_powerup_samu(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SamuPowerGating))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_SAMPowerON);
-	return 0;
-}
-
-int polaris10_phm_disable_clock_power_gating(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	data->uvd_power_gated = false;
-	data->vce_power_gated = false;
-	data->samu_power_gated = false;
-
-	polaris10_phm_powerup_uvd(hwmgr);
-	polaris10_phm_powerup_vce(hwmgr);
-	polaris10_phm_powerup_samu(hwmgr);
-
-	return 0;
-}
-
-int polaris10_phm_powergate_uvd(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (data->uvd_power_gated == bgate)
-		return 0;
-
-	data->uvd_power_gated = bgate;
-
-	if (bgate) {
-		cgs_set_clockgating_state(hwmgr->device,
-				AMD_IP_BLOCK_TYPE_UVD,
-				AMD_CG_STATE_GATE);
-		polaris10_update_uvd_dpm(hwmgr, true);
-		polaris10_phm_powerdown_uvd(hwmgr);
-	} else {
-		polaris10_phm_powerup_uvd(hwmgr);
-		polaris10_update_uvd_dpm(hwmgr, false);
-		cgs_set_clockgating_state(hwmgr->device,
-				AMD_IP_BLOCK_TYPE_UVD,
-				AMD_CG_STATE_UNGATE);
-	}
-
-	return 0;
-}
-
-int polaris10_phm_powergate_vce(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (data->vce_power_gated == bgate)
-		return 0;
-
-	data->vce_power_gated = bgate;
-
-	if (bgate) {
-		cgs_set_clockgating_state(hwmgr->device,
-				AMD_IP_BLOCK_TYPE_VCE,
-				AMD_CG_STATE_GATE);
-		polaris10_update_vce_dpm(hwmgr, true);
-		polaris10_phm_powerdown_vce(hwmgr);
-	} else {
-		polaris10_phm_powerup_vce(hwmgr);
-		polaris10_update_vce_dpm(hwmgr, false);
-		cgs_set_clockgating_state(hwmgr->device,
-				AMD_IP_BLOCK_TYPE_VCE,
-				AMD_CG_STATE_UNGATE);
-	}
-	return 0;
-}
-
-int polaris10_phm_powergate_samu(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (data->samu_power_gated == bgate)
-		return 0;
-
-	data->samu_power_gated = bgate;
-
-	if (bgate) {
-		polaris10_update_samu_dpm(hwmgr, true);
-		polaris10_phm_powerdown_samu(hwmgr);
-	} else {
-		polaris10_phm_powerup_samu(hwmgr);
-		polaris10_update_samu_dpm(hwmgr, false);
-	}
-
-	return 0;
-}
-
-int polaris10_phm_update_clock_gatings(struct pp_hwmgr *hwmgr,
-					const uint32_t *msg_id)
-{
-	PPSMC_Msg msg;
-	uint32_t value;
-
-	switch ((*msg_id & PP_GROUP_MASK) >> PP_GROUP_SHIFT) {
-	case PP_GROUP_GFX:
-		switch ((*msg_id & PP_BLOCK_MASK) >> PP_BLOCK_SHIFT) {
-		case PP_BLOCK_GFX_CG:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG) ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_GFX_CGCG_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS
-					? PPSMC_MSG_EnableClockGatingFeature
-					: PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_GFX_CGLS_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_GFX_3D:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG) ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_GFX_3DCG_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-
-			if  (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_GFX_3DLS_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_GFX_RLC:
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_GFX_RLC_LS_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_GFX_CP:
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_GFX_CP_LS_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_GFX_MG:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG)	?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = (CG_CPF_MGCG_MASK | CG_RLC_MGCG_MASK |
-						CG_GFX_OTHERS_MGCG_MASK);
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		default:
-			return -1;
-		}
-		break;
-
-	case PP_GROUP_SYS:
-		switch ((*msg_id & PP_BLOCK_MASK) >> PP_BLOCK_SHIFT) {
-		case PP_BLOCK_SYS_BIF:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_CG ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_BIF_MGCG_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			if  (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_BIF_MGLS_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_SYS_MC:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG)	?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_MC_MGCG_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_MC_MGLS_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_SYS_DRM:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_CG ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_DRM_MGCG_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_DRM_MGLS_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_SYS_HDP:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG) ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_HDP_MGCG_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_HDP_MGLS_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_SYS_SDMA:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG)	?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_SDMA_MGCG_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_SDMA_MGLS_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_SYS_ROM:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG) ?
-						PPSMC_MSG_EnableClockGatingFeature :
-						PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_ROM_MASK;
-
-				if (smum_send_msg_to_smc_with_parameter(
-						hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		default:
-			return -1;
-
-		}
-		break;
-
-	default:
-		return -1;
-
-	}
-
-	return 0;
-}
-
-/* This function is for Polaris11 only for now,
- * Powerplay will only control the static per CU Power Gating.
- * Dynamic per CU Power Gating will be done in gfx.
- */
-int polaris10_phm_enable_per_cu_power_gating(struct pp_hwmgr *hwmgr, bool enable)
-{
-	struct cgs_system_info sys_info = {0};
-	uint32_t active_cus;
-	int result;
-
-	sys_info.size = sizeof(struct cgs_system_info);
-	sys_info.info_id = CGS_SYSTEM_INFO_GFX_CU_INFO;
-
-	result = cgs_query_system_info(hwmgr->device, &sys_info);
-
-	if (result)
-		return -EINVAL;
-	else
-		active_cus = sys_info.value;
-
-	if (enable)
-		return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_GFX_CU_PG_ENABLE, active_cus);
-	else
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_GFX_CU_PG_DISABLE);
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_clockpowergating.h b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_clockpowergating.h
deleted file mode 100644
index 88d68cb..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_clockpowergating.h
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef _POLARIS10_CLOCK_POWER_GATING_H_
-#define _POLARIS10_CLOCK_POWER_GATING_H_
-
-#include "polaris10_hwmgr.h"
-#include "pp_asicblocks.h"
-
-int polaris10_phm_powergate_vce(struct pp_hwmgr *hwmgr, bool bgate);
-int polaris10_phm_powergate_uvd(struct pp_hwmgr *hwmgr, bool bgate);
-int polaris10_phm_powerdown_uvd(struct pp_hwmgr *hwmgr);
-int polaris10_phm_powergate_samu(struct pp_hwmgr *hwmgr, bool bgate);
-int polaris10_phm_powergate_acp(struct pp_hwmgr *hwmgr, bool bgate);
-int polaris10_phm_disable_clock_power_gating(struct pp_hwmgr *hwmgr);
-int polaris10_phm_update_clock_gatings(struct pp_hwmgr *hwmgr,
-					const uint32_t *msg_id);
-int polaris10_phm_enable_per_cu_power_gating(struct pp_hwmgr *hwmgr, bool enable);
-
-#endif /* _POLARIS10_CLOCK_POWER_GATING_H_ */
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_dyn_defaults.h b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_dyn_defaults.h
deleted file mode 100644
index f78ffd9..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_dyn_defaults.h
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef POLARIS10_DYN_DEFAULTS_H
-#define POLARIS10_DYN_DEFAULTS_H
-
-
-enum Polaris10dpm_TrendDetection {
-	Polaris10Adpm_TrendDetection_AUTO,
-	Polaris10Adpm_TrendDetection_UP,
-	Polaris10Adpm_TrendDetection_DOWN
-};
-typedef enum Polaris10dpm_TrendDetection Polaris10dpm_TrendDetection;
-
-/*  We need to fill in the default values */
-
-
-#define PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT0              0x3FFFC102
-#define PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT1              0x000400
-#define PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT2              0xC00080
-#define PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT3              0xC00200
-#define PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT4              0xC01680
-#define PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT5              0xC00033
-#define PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT6              0xC00033
-#define PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT7              0x3FFFC000
-
-
-#define PPPOLARIS10_THERMALPROTECTCOUNTER_DFLT            0x200
-#define PPPOLARIS10_STATICSCREENTHRESHOLDUNIT_DFLT        0
-#define PPPOLARIS10_STATICSCREENTHRESHOLD_DFLT            0x00C8
-#define PPPOLARIS10_GFXIDLECLOCKSTOPTHRESHOLD_DFLT        0x200
-#define PPPOLARIS10_REFERENCEDIVIDER_DFLT                  4
-
-#define PPPOLARIS10_ULVVOLTAGECHANGEDELAY_DFLT             1687
-
-#define PPPOLARIS10_CGULVPARAMETER_DFLT                    0x00040035
-#define PPPOLARIS10_CGULVCONTROL_DFLT                      0x00007450
-#define PPPOLARIS10_TARGETACTIVITY_DFLT                     50
-#define PPPOLARIS10_MCLK_TARGETACTIVITY_DFLT                10
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c
deleted file mode 100644
index 2684cc0..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.c
+++ /dev/null
@@ -1,5439 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-#include <linux/module.h>
-#include <linux/slab.h>
-#include <linux/fb.h>
-#include <asm/div64.h>
-#include "linux/delay.h"
-#include "pp_acpi.h"
-#include "hwmgr.h"
-#include "polaris10_hwmgr.h"
-#include "polaris10_powertune.h"
-#include "polaris10_dyn_defaults.h"
-#include "polaris10_smumgr.h"
-#include "pp_debug.h"
-#include "ppatomctrl.h"
-#include "atombios.h"
-#include "pptable_v1_0.h"
-#include "pppcielanes.h"
-#include "amd_pcie_helpers.h"
-#include "hardwaremanager.h"
-#include "process_pptables_v1_0.h"
-#include "cgs_common.h"
-#include "smu74.h"
-#include "smu_ucode_xfer_vi.h"
-#include "smu74_discrete.h"
-#include "smu/smu_7_1_3_d.h"
-#include "smu/smu_7_1_3_sh_mask.h"
-#include "gmc/gmc_8_1_d.h"
-#include "gmc/gmc_8_1_sh_mask.h"
-#include "oss/oss_3_0_d.h"
-#include "gca/gfx_8_0_d.h"
-#include "bif/bif_5_0_d.h"
-#include "bif/bif_5_0_sh_mask.h"
-#include "gmc/gmc_8_1_d.h"
-#include "gmc/gmc_8_1_sh_mask.h"
-#include "bif/bif_5_0_d.h"
-#include "bif/bif_5_0_sh_mask.h"
-#include "dce/dce_10_0_d.h"
-#include "dce/dce_10_0_sh_mask.h"
-
-#include "polaris10_thermal.h"
-#include "polaris10_clockpowergating.h"
-
-#define MC_CG_ARB_FREQ_F0           0x0a
-#define MC_CG_ARB_FREQ_F1           0x0b
-#define MC_CG_ARB_FREQ_F2           0x0c
-#define MC_CG_ARB_FREQ_F3           0x0d
-
-#define MC_CG_SEQ_DRAMCONF_S0       0x05
-#define MC_CG_SEQ_DRAMCONF_S1       0x06
-#define MC_CG_SEQ_YCLK_SUSPEND      0x04
-#define MC_CG_SEQ_YCLK_RESUME       0x0a
-
-
-#define SMC_RAM_END 0x40000
-
-#define SMC_CG_IND_START            0xc0030000
-#define SMC_CG_IND_END              0xc0040000
-
-#define VOLTAGE_SCALE               4
-#define VOLTAGE_VID_OFFSET_SCALE1   625
-#define VOLTAGE_VID_OFFSET_SCALE2   100
-
-#define VDDC_VDDCI_DELTA            200
-
-#define MEM_FREQ_LOW_LATENCY        25000
-#define MEM_FREQ_HIGH_LATENCY       80000
-
-#define MEM_LATENCY_HIGH            45
-#define MEM_LATENCY_LOW             35
-#define MEM_LATENCY_ERR             0xFFFF
-
-#define MC_SEQ_MISC0_GDDR5_SHIFT 28
-#define MC_SEQ_MISC0_GDDR5_MASK  0xf0000000
-#define MC_SEQ_MISC0_GDDR5_VALUE 5
-
-
-#define PCIE_BUS_CLK                10000
-#define TCLK                        (PCIE_BUS_CLK / 10)
-
-#define CEILING_UCHAR(double) ((double-(uint8_t)(double)) > 0 ? (uint8_t)(double+1) : (uint8_t)(double))
-
-/** Values for the CG_THERMAL_CTRL::DPM_EVENT_SRC field. */
-enum DPM_EVENT_SRC {
-	DPM_EVENT_SRC_ANALOG = 0,
-	DPM_EVENT_SRC_EXTERNAL = 1,
-	DPM_EVENT_SRC_DIGITAL = 2,
-	DPM_EVENT_SRC_ANALOG_OR_EXTERNAL = 3,
-	DPM_EVENT_SRC_DIGITAL_OR_EXTERNAL = 4
-};
-
-static const unsigned long PhwPolaris10_Magic = (unsigned long)(PHM_VIslands_Magic);
-
-static struct polaris10_power_state *cast_phw_polaris10_power_state(
-				  struct pp_hw_power_state *hw_ps)
-{
-	PP_ASSERT_WITH_CODE((PhwPolaris10_Magic == hw_ps->magic),
-				"Invalid Powerstate Type!",
-				 return NULL);
-
-	return (struct polaris10_power_state *)hw_ps;
-}
-
-static const struct polaris10_power_state *
-cast_const_phw_polaris10_power_state(
-				 const struct pp_hw_power_state *hw_ps)
-{
-	PP_ASSERT_WITH_CODE((PhwPolaris10_Magic == hw_ps->magic),
-				"Invalid Powerstate Type!",
-				 return NULL);
-
-	return (const struct polaris10_power_state *)hw_ps;
-}
-
-static bool polaris10_is_dpm_running(struct pp_hwmgr *hwmgr)
-{
-	return (1 == PHM_READ_INDIRECT_FIELD(hwmgr->device,
-			CGS_IND_REG__SMC, FEATURE_STATUS, VOLTAGE_CONTROLLER_ON))
-			? true : false;
-}
-
-/**
- * Find the MC microcode version and store it in the HwMgr struct
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int phm_get_mc_microcode_version(struct pp_hwmgr *hwmgr)
-{
-	cgs_write_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_INDEX, 0x9F);
-
-	hwmgr->microcode_version_info.MC = cgs_read_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_DATA);
-
-	return 0;
-}
-
-static uint16_t phm_get_current_pcie_speed(struct pp_hwmgr *hwmgr)
-{
-	uint32_t speedCntl = 0;
-
-	/* mmPCIE_PORT_INDEX rename as mmPCIE_INDEX */
-	speedCntl = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__PCIE,
-			ixPCIE_LC_SPEED_CNTL);
-	return((uint16_t)PHM_GET_FIELD(speedCntl,
-			PCIE_LC_SPEED_CNTL, LC_CURRENT_DATA_RATE));
-}
-
-static int phm_get_current_pcie_lane_number(struct pp_hwmgr *hwmgr)
-{
-	uint32_t link_width;
-
-	/* mmPCIE_PORT_INDEX rename as mmPCIE_INDEX */
-	link_width = PHM_READ_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__PCIE,
-			PCIE_LC_LINK_WIDTH_CNTL, LC_LINK_WIDTH_RD);
-
-	PP_ASSERT_WITH_CODE((7 >= link_width),
-			"Invalid PCIe lane width!", return 0);
-
-	return decode_pcie_lane_width(link_width);
-}
-
-/**
-* Enable voltage control
-*
-* @param    pHwMgr  the address of the powerplay hardware manager.
-* @return   always PP_Result_OK
-*/
-static int polaris10_enable_smc_voltage_controller(struct pp_hwmgr *hwmgr)
-{
-	PP_ASSERT_WITH_CODE(
-		(hwmgr->smumgr->smumgr_funcs->send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_Voltage_Cntl_Enable) == 0),
-		"Failed to enable voltage DPM during DPM Start Function!",
-		return 1;
-	);
-
-	return 0;
-}
-
-/**
-* Checks if we want to support voltage control
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-*/
-static bool polaris10_voltage_control(const struct pp_hwmgr *hwmgr)
-{
-	const struct polaris10_hwmgr *data =
-			(const struct polaris10_hwmgr *)(hwmgr->backend);
-
-	return (POLARIS10_VOLTAGE_CONTROL_NONE != data->voltage_control);
-}
-
-/**
-* Enable voltage control
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int polaris10_enable_voltage_control(struct pp_hwmgr *hwmgr)
-{
-	/* enable voltage control */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			GENERAL_PWRMGT, VOLT_PWRMGT_EN, 1);
-
-	return 0;
-}
-
-/**
-* Create Voltage Tables.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int polaris10_construct_voltage_tables(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)hwmgr->pptable;
-	int result;
-
-	if (POLARIS10_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control) {
-		result = atomctrl_get_voltage_table_v3(hwmgr,
-				VOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_GPIO_LUT,
-				&(data->mvdd_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve MVDD table.",
-				return result);
-	} else if (POLARIS10_VOLTAGE_CONTROL_BY_SVID2 == data->mvdd_control) {
-		result = phm_get_svi2_mvdd_voltage_table(&(data->mvdd_voltage_table),
-				table_info->vdd_dep_on_mclk);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve SVI2 MVDD table from dependancy table.",
-				return result;);
-	}
-
-	if (POLARIS10_VOLTAGE_CONTROL_BY_GPIO == data->vddci_control) {
-		result = atomctrl_get_voltage_table_v3(hwmgr,
-				VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_GPIO_LUT,
-				&(data->vddci_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve VDDCI table.",
-				return result);
-	} else if (POLARIS10_VOLTAGE_CONTROL_BY_SVID2 == data->vddci_control) {
-		result = phm_get_svi2_vddci_voltage_table(&(data->vddci_voltage_table),
-				table_info->vdd_dep_on_mclk);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve SVI2 VDDCI table from dependancy table.",
-				return result);
-	}
-
-	if (POLARIS10_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {
-		result = phm_get_svi2_vdd_voltage_table(&(data->vddc_voltage_table),
-				table_info->vddc_lookup_table);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to retrieve SVI2 VDDC table from lookup table.",
-				return result);
-	}
-
-	PP_ASSERT_WITH_CODE(
-			(data->vddc_voltage_table.count <= (SMU74_MAX_LEVELS_VDDC)),
-			"Too many voltage values for VDDC. Trimming to fit state table.",
-			phm_trim_voltage_table_to_fit_state_table(SMU74_MAX_LEVELS_VDDC,
-								&(data->vddc_voltage_table)));
-
-	PP_ASSERT_WITH_CODE(
-			(data->vddci_voltage_table.count <= (SMU74_MAX_LEVELS_VDDCI)),
-			"Too many voltage values for VDDCI. Trimming to fit state table.",
-			phm_trim_voltage_table_to_fit_state_table(SMU74_MAX_LEVELS_VDDCI,
-					&(data->vddci_voltage_table)));
-
-	PP_ASSERT_WITH_CODE(
-			(data->mvdd_voltage_table.count <= (SMU74_MAX_LEVELS_MVDD)),
-			"Too many voltage values for MVDD. Trimming to fit state table.",
-			phm_trim_voltage_table_to_fit_state_table(SMU74_MAX_LEVELS_MVDD,
-							   &(data->mvdd_voltage_table)));
-
-	return 0;
-}
-
-/**
-* Programs static screed detection parameters
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int polaris10_program_static_screen_threshold_parameters(
-							struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	/* Set static screen threshold unit */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD_UNIT,
-			data->static_screen_threshold_unit);
-	/* Set static screen threshold */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD,
-			data->static_screen_threshold);
-
-	return 0;
-}
-
-/**
-* Setup display gap for glitch free memory clock switching.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always  0
-*/
-static int polaris10_enable_display_gap(struct pp_hwmgr *hwmgr)
-{
-	uint32_t display_gap =
-			cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-					ixCG_DISPLAY_GAP_CNTL);
-
-	display_gap = PHM_SET_FIELD(display_gap, CG_DISPLAY_GAP_CNTL,
-			DISP_GAP, DISPLAY_GAP_IGNORE);
-
-	display_gap = PHM_SET_FIELD(display_gap, CG_DISPLAY_GAP_CNTL,
-			DISP_GAP_MCHG, DISPLAY_GAP_VBLANK);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_DISPLAY_GAP_CNTL, display_gap);
-
-	return 0;
-}
-
-/**
-* Programs activity state transition voting clients
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always  0
-*/
-static int polaris10_program_voting_clients(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	/* Clear reset for voting clients before enabling DPM */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			SCLK_PWRMGT_CNTL, RESET_SCLK_CNT, 0);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			SCLK_PWRMGT_CNTL, RESET_BUSY_CNT, 0);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_0, data->voting_rights_clients0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_1, data->voting_rights_clients1);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_2, data->voting_rights_clients2);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_3, data->voting_rights_clients3);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_4, data->voting_rights_clients4);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_5, data->voting_rights_clients5);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_6, data->voting_rights_clients6);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_7, data->voting_rights_clients7);
-
-	return 0;
-}
-
-static int polaris10_clear_voting_clients(struct pp_hwmgr *hwmgr)
-{
-	/* Reset voting clients before disabling DPM */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			SCLK_PWRMGT_CNTL, RESET_SCLK_CNT, 1);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			SCLK_PWRMGT_CNTL, RESET_BUSY_CNT, 1);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_0, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_1, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_2, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_3, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_4, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_5, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_6, 0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_FREQ_TRAN_VOTING_7, 0);
-
-	return 0;
-}
-
-/**
-* Get the location of various tables inside the FW image.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always  0
-*/
-static int polaris10_process_firmware_header(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_smumgr *smu_data = (struct polaris10_smumgr *)(hwmgr->smumgr->backend);
-	uint32_t tmp;
-	int result;
-	bool error = false;
-
-	result = polaris10_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU74_Firmware_Header, DpmTable),
-			&tmp, data->sram_end);
-
-	if (0 == result)
-		data->dpm_table_start = tmp;
-
-	error |= (0 != result);
-
-	result = polaris10_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU74_Firmware_Header, SoftRegisters),
-			&tmp, data->sram_end);
-
-	if (!result) {
-		data->soft_regs_start = tmp;
-		smu_data->soft_regs_start = tmp;
-	}
-
-	error |= (0 != result);
-
-	result = polaris10_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU74_Firmware_Header, mcRegisterTable),
-			&tmp, data->sram_end);
-
-	if (!result)
-		data->mc_reg_table_start = tmp;
-
-	result = polaris10_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU74_Firmware_Header, FanTable),
-			&tmp, data->sram_end);
-
-	if (!result)
-		data->fan_table_start = tmp;
-
-	error |= (0 != result);
-
-	result = polaris10_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU74_Firmware_Header, mcArbDramTimingTable),
-			&tmp, data->sram_end);
-
-	if (!result)
-		data->arb_table_start = tmp;
-
-	error |= (0 != result);
-
-	result = polaris10_read_smc_sram_dword(hwmgr->smumgr,
-			SMU7_FIRMWARE_HEADER_LOCATION +
-			offsetof(SMU74_Firmware_Header, Version),
-			&tmp, data->sram_end);
-
-	if (!result)
-		hwmgr->microcode_version_info.SMC = tmp;
-
-	error |= (0 != result);
-
-	return error ? -1 : 0;
-}
-
-/* Copy one arb setting to another and then switch the active set.
- * arb_src and arb_dest is one of the MC_CG_ARB_FREQ_Fx constants.
- */
-static int polaris10_copy_and_switch_arb_sets(struct pp_hwmgr *hwmgr,
-		uint32_t arb_src, uint32_t arb_dest)
-{
-	uint32_t mc_arb_dram_timing;
-	uint32_t mc_arb_dram_timing2;
-	uint32_t burst_time;
-	uint32_t mc_cg_config;
-
-	switch (arb_src) {
-	case MC_CG_ARB_FREQ_F0:
-		mc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING);
-		mc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2);
-		burst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0);
-		break;
-	case MC_CG_ARB_FREQ_F1:
-		mc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1);
-		mc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1);
-		burst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1);
-		break;
-	default:
-		return -EINVAL;
-	}
-
-	switch (arb_dest) {
-	case MC_CG_ARB_FREQ_F0:
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING, mc_arb_dram_timing);
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2, mc_arb_dram_timing2);
-		PHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0, burst_time);
-		break;
-	case MC_CG_ARB_FREQ_F1:
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1, mc_arb_dram_timing);
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1, mc_arb_dram_timing2);
-		PHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1, burst_time);
-		break;
-	default:
-		return -EINVAL;
-	}
-
-	mc_cg_config = cgs_read_register(hwmgr->device, mmMC_CG_CONFIG);
-	mc_cg_config |= 0x0000000F;
-	cgs_write_register(hwmgr->device, mmMC_CG_CONFIG, mc_cg_config);
-	PHM_WRITE_FIELD(hwmgr->device, MC_ARB_CG, CG_ARB_REQ, arb_dest);
-
-	return 0;
-}
-
-static int polaris10_reset_to_default(struct pp_hwmgr *hwmgr)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_ResetToDefaults);
-}
-
-/**
-* Initial switch from ARB F0->F1
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-* This function is to be called from the SetPowerState table.
-*/
-static int polaris10_initial_switch_from_arbf0_to_f1(struct pp_hwmgr *hwmgr)
-{
-	return polaris10_copy_and_switch_arb_sets(hwmgr,
-			MC_CG_ARB_FREQ_F0, MC_CG_ARB_FREQ_F1);
-}
-
-static int polaris10_force_switch_to_arbf0(struct pp_hwmgr *hwmgr)
-{
-	uint32_t tmp;
-
-	tmp = (cgs_read_ind_register(hwmgr->device,
-			CGS_IND_REG__SMC, ixSMC_SCRATCH9) &
-			0x0000ff00) >> 8;
-
-	if (tmp == MC_CG_ARB_FREQ_F0)
-		return 0;
-
-	return polaris10_copy_and_switch_arb_sets(hwmgr,
-			tmp, MC_CG_ARB_FREQ_F0);
-}
-
-static int polaris10_setup_default_pcie_table(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_pcie_table *pcie_table = table_info->pcie_table;
-	uint32_t i, max_entry;
-
-	PP_ASSERT_WITH_CODE((data->use_pcie_performance_levels ||
-			data->use_pcie_power_saving_levels), "No pcie performance levels!",
-			return -EINVAL);
-
-	if (data->use_pcie_performance_levels &&
-			!data->use_pcie_power_saving_levels) {
-		data->pcie_gen_power_saving = data->pcie_gen_performance;
-		data->pcie_lane_power_saving = data->pcie_lane_performance;
-	} else if (!data->use_pcie_performance_levels &&
-			data->use_pcie_power_saving_levels) {
-		data->pcie_gen_performance = data->pcie_gen_power_saving;
-		data->pcie_lane_performance = data->pcie_lane_power_saving;
-	}
-
-	phm_reset_single_dpm_table(&data->dpm_table.pcie_speed_table,
-					SMU74_MAX_LEVELS_LINK,
-					MAX_REGULAR_DPM_NUMBER);
-
-	if (pcie_table != NULL) {
-		/* max_entry is used to make sure we reserve one PCIE level
-		 * for boot level (fix for A+A PSPP issue).
-		 * If PCIE table from PPTable have ULV entry + 8 entries,
-		 * then ignore the last entry.*/
-		max_entry = (SMU74_MAX_LEVELS_LINK < pcie_table->count) ?
-				SMU74_MAX_LEVELS_LINK : pcie_table->count;
-		for (i = 1; i < max_entry; i++) {
-			phm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, i - 1,
-					get_pcie_gen_support(data->pcie_gen_cap,
-							pcie_table->entries[i].gen_speed),
-					get_pcie_lane_support(data->pcie_lane_cap,
-							pcie_table->entries[i].lane_width));
-		}
-		data->dpm_table.pcie_speed_table.count = max_entry - 1;
-
-		/* Setup BIF_SCLK levels */
-		for (i = 0; i < max_entry; i++)
-			data->bif_sclk_table[i] = pcie_table->entries[i].pcie_sclk;
-	} else {
-		/* Hardcode Pcie Table */
-		phm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 0,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Min_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		phm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 1,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Min_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		phm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 2,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Max_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		phm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 3,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Max_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		phm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 4,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Max_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-		phm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 5,
-				get_pcie_gen_support(data->pcie_gen_cap,
-						PP_Max_PCIEGen),
-				get_pcie_lane_support(data->pcie_lane_cap,
-						PP_Max_PCIELane));
-
-		data->dpm_table.pcie_speed_table.count = 6;
-	}
-	/* Populate last level for boot PCIE level, but do not increment count. */
-	phm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table,
-			data->dpm_table.pcie_speed_table.count,
-			get_pcie_gen_support(data->pcie_gen_cap,
-					PP_Min_PCIEGen),
-			get_pcie_lane_support(data->pcie_lane_cap,
-					PP_Max_PCIELane));
-
-	return 0;
-}
-
-/*
- * This function is to initalize all DPM state tables
- * for SMU7 based on the dependency table.
- * Dynamic state patching function will then trim these
- * state tables to the allowed range based
- * on the power policy or external client requests,
- * such as UVD request, etc.
- */
-static int polaris10_setup_default_dpm_tables(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint32_t i;
-
-	struct phm_ppt_v1_clock_voltage_dependency_table *dep_sclk_table =
-			table_info->vdd_dep_on_sclk;
-	struct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table =
-			table_info->vdd_dep_on_mclk;
-
-	PP_ASSERT_WITH_CODE(dep_sclk_table != NULL,
-			"SCLK dependency table is missing. This table is mandatory",
-			return -EINVAL);
-	PP_ASSERT_WITH_CODE(dep_sclk_table->count >= 1,
-			"SCLK dependency table has to have is missing."
-			"This table is mandatory",
-			return -EINVAL);
-
-	PP_ASSERT_WITH_CODE(dep_mclk_table != NULL,
-			"MCLK dependency table is missing. This table is mandatory",
-			return -EINVAL);
-	PP_ASSERT_WITH_CODE(dep_mclk_table->count >= 1,
-			"MCLK dependency table has to have is missing."
-			"This table is mandatory",
-			return -EINVAL);
-
-	/* clear the state table to reset everything to default */
-	phm_reset_single_dpm_table(
-			&data->dpm_table.sclk_table, SMU74_MAX_LEVELS_GRAPHICS, MAX_REGULAR_DPM_NUMBER);
-	phm_reset_single_dpm_table(
-			&data->dpm_table.mclk_table, SMU74_MAX_LEVELS_MEMORY, MAX_REGULAR_DPM_NUMBER);
-
-
-	/* Initialize Sclk DPM table based on allow Sclk values */
-	data->dpm_table.sclk_table.count = 0;
-	for (i = 0; i < dep_sclk_table->count; i++) {
-		if (i == 0 || data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count - 1].value !=
-						dep_sclk_table->entries[i].clk) {
-
-			data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].value =
-					dep_sclk_table->entries[i].clk;
-
-			data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].enabled =
-					(i == 0) ? true : false;
-			data->dpm_table.sclk_table.count++;
-		}
-	}
-
-	/* Initialize Mclk DPM table based on allow Mclk values */
-	data->dpm_table.mclk_table.count = 0;
-	for (i = 0; i < dep_mclk_table->count; i++) {
-		if (i == 0 || data->dpm_table.mclk_table.dpm_levels
-				[data->dpm_table.mclk_table.count - 1].value !=
-						dep_mclk_table->entries[i].clk) {
-			data->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].value =
-							dep_mclk_table->entries[i].clk;
-			data->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].enabled =
-							(i == 0) ? true : false;
-			data->dpm_table.mclk_table.count++;
-		}
-	}
-
-	/* setup PCIE gen speed levels */
-	polaris10_setup_default_pcie_table(hwmgr);
-
-	/* save a copy of the default DPM table */
-	memcpy(&(data->golden_dpm_table), &(data->dpm_table),
-			sizeof(struct polaris10_dpm_table));
-
-	return 0;
-}
-
-/**
- * Mvdd table preparation for SMC.
- *
- * @param    *hwmgr The address of the hardware manager.
- * @param    *table The SMC DPM table structure to be populated.
- * @return   0
- */
-static int polaris10_populate_smc_mvdd_table(struct pp_hwmgr *hwmgr,
-			SMU74_Discrete_DpmTable *table)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t count, level;
-
-	if (POLARIS10_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control) {
-		count = data->mvdd_voltage_table.count;
-		if (count > SMU_MAX_SMIO_LEVELS)
-			count = SMU_MAX_SMIO_LEVELS;
-		for (level = 0; level < count; level++) {
-			table->SmioTable2.Pattern[level].Voltage =
-				PP_HOST_TO_SMC_US(data->mvdd_voltage_table.entries[count].value * VOLTAGE_SCALE);
-			/* Index into DpmTable.Smio. Drive bits from Smio entry to get this voltage level.*/
-			table->SmioTable2.Pattern[level].Smio =
-				(uint8_t) level;
-			table->Smio[level] |=
-				data->mvdd_voltage_table.entries[level].smio_low;
-		}
-		table->SmioMask2 = data->mvdd_voltage_table.mask_low;
-
-		table->MvddLevelCount = (uint32_t) PP_HOST_TO_SMC_UL(count);
-	}
-
-	return 0;
-}
-
-static int polaris10_populate_smc_vddci_table(struct pp_hwmgr *hwmgr,
-					struct SMU74_Discrete_DpmTable *table)
-{
-	uint32_t count, level;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	count = data->vddci_voltage_table.count;
-
-	if (POLARIS10_VOLTAGE_CONTROL_BY_GPIO == data->vddci_control) {
-		if (count > SMU_MAX_SMIO_LEVELS)
-			count = SMU_MAX_SMIO_LEVELS;
-		for (level = 0; level < count; ++level) {
-			table->SmioTable1.Pattern[level].Voltage =
-				PP_HOST_TO_SMC_US(data->vddci_voltage_table.entries[level].value * VOLTAGE_SCALE);
-			table->SmioTable1.Pattern[level].Smio = (uint8_t) level;
-
-			table->Smio[level] |= data->vddci_voltage_table.entries[level].smio_low;
-		}
-	}
-
-	table->SmioMask1 = data->vddci_voltage_table.mask_low;
-
-	return 0;
-}
-
-/**
-* Preparation of vddc and vddgfx CAC tables for SMC.
-*
-* @param    hwmgr  the address of the hardware manager
-* @param    table  the SMC DPM table structure to be populated
-* @return   always 0
-*/
-static int polaris10_populate_cac_table(struct pp_hwmgr *hwmgr,
-		struct SMU74_Discrete_DpmTable *table)
-{
-	uint32_t count;
-	uint8_t index;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_voltage_lookup_table *lookup_table =
-			table_info->vddc_lookup_table;
-	/* tables is already swapped, so in order to use the value from it,
-	 * we need to swap it back.
-	 * We are populating vddc CAC data to BapmVddc table
-	 * in split and merged mode
-	 */
-	for (count = 0; count < lookup_table->count; count++) {
-		index = phm_get_voltage_index(lookup_table,
-				data->vddc_voltage_table.entries[count].value);
-		table->BapmVddcVidLoSidd[count] = convert_to_vid(lookup_table->entries[index].us_cac_low);
-		table->BapmVddcVidHiSidd[count] = convert_to_vid(lookup_table->entries[index].us_cac_mid);
-		table->BapmVddcVidHiSidd2[count] = convert_to_vid(lookup_table->entries[index].us_cac_high);
-	}
-
-	return 0;
-}
-
-/**
-* Preparation of voltage tables for SMC.
-*
-* @param    hwmgr   the address of the hardware manager
-* @param    table   the SMC DPM table structure to be populated
-* @return   always  0
-*/
-
-static int polaris10_populate_smc_voltage_tables(struct pp_hwmgr *hwmgr,
-		struct SMU74_Discrete_DpmTable *table)
-{
-	polaris10_populate_smc_vddci_table(hwmgr, table);
-	polaris10_populate_smc_mvdd_table(hwmgr, table);
-	polaris10_populate_cac_table(hwmgr, table);
-
-	return 0;
-}
-
-static int polaris10_populate_ulv_level(struct pp_hwmgr *hwmgr,
-		struct SMU74_Discrete_Ulv *state)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	state->CcPwrDynRm = 0;
-	state->CcPwrDynRm1 = 0;
-
-	state->VddcOffset = (uint16_t) table_info->us_ulv_voltage_offset;
-	state->VddcOffsetVid = (uint8_t)(table_info->us_ulv_voltage_offset *
-			VOLTAGE_VID_OFFSET_SCALE2 / VOLTAGE_VID_OFFSET_SCALE1);
-
-	state->VddcPhase = (data->vddc_phase_shed_control) ? 0 : 1;
-
-	CONVERT_FROM_HOST_TO_SMC_UL(state->CcPwrDynRm);
-	CONVERT_FROM_HOST_TO_SMC_UL(state->CcPwrDynRm1);
-	CONVERT_FROM_HOST_TO_SMC_US(state->VddcOffset);
-
-	return 0;
-}
-
-static int polaris10_populate_ulv_state(struct pp_hwmgr *hwmgr,
-		struct SMU74_Discrete_DpmTable *table)
-{
-	return polaris10_populate_ulv_level(hwmgr, &table->Ulv);
-}
-
-static int polaris10_populate_smc_link_level(struct pp_hwmgr *hwmgr,
-		struct SMU74_Discrete_DpmTable *table)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_dpm_table *dpm_table = &data->dpm_table;
-	int i;
-
-	/* Index (dpm_table->pcie_speed_table.count)
-	 * is reserved for PCIE boot level. */
-	for (i = 0; i <= dpm_table->pcie_speed_table.count; i++) {
-		table->LinkLevel[i].PcieGenSpeed  =
-				(uint8_t)dpm_table->pcie_speed_table.dpm_levels[i].value;
-		table->LinkLevel[i].PcieLaneCount = (uint8_t)encode_pcie_lane_width(
-				dpm_table->pcie_speed_table.dpm_levels[i].param1);
-		table->LinkLevel[i].EnabledForActivity = 1;
-		table->LinkLevel[i].SPC = (uint8_t)(data->pcie_spc_cap & 0xff);
-		table->LinkLevel[i].DownThreshold = PP_HOST_TO_SMC_UL(5);
-		table->LinkLevel[i].UpThreshold = PP_HOST_TO_SMC_UL(30);
-	}
-
-	data->smc_state_table.LinkLevelCount =
-			(uint8_t)dpm_table->pcie_speed_table.count;
-	data->dpm_level_enable_mask.pcie_dpm_enable_mask =
-			phm_get_dpm_level_enable_mask_value(&dpm_table->pcie_speed_table);
-
-	return 0;
-}
-
-static uint32_t polaris10_get_xclk(struct pp_hwmgr *hwmgr)
-{
-	uint32_t reference_clock, tmp;
-	struct cgs_display_info info = {0};
-	struct cgs_mode_info mode_info;
-
-	info.mode_info = &mode_info;
-
-	tmp = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_CLKPIN_CNTL_2, MUX_TCLK_TO_XCLK);
-
-	if (tmp)
-		return TCLK;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-	reference_clock = mode_info.ref_clock;
-
-	tmp = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_CLKPIN_CNTL, XTALIN_DIVIDE);
-
-	if (0 != tmp)
-		return reference_clock / 4;
-
-	return reference_clock;
-}
-
-/**
-* Calculates the SCLK dividers using the provided engine clock
-*
-* @param    hwmgr  the address of the hardware manager
-* @param    clock  the engine clock to use to populate the structure
-* @param    sclk   the SMC SCLK structure to be populated
-*/
-static int polaris10_calculate_sclk_params(struct pp_hwmgr *hwmgr,
-		uint32_t clock, SMU_SclkSetting *sclk_setting)
-{
-	const struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	const SMU74_Discrete_DpmTable *table = &(data->smc_state_table);
-	struct pp_atomctrl_clock_dividers_ai dividers;
-
-	uint32_t ref_clock;
-	uint32_t pcc_target_percent, pcc_target_freq, ss_target_percent, ss_target_freq;
-	uint8_t i;
-	int result;
-	uint64_t temp;
-
-	sclk_setting->SclkFrequency = clock;
-	/* get the engine clock dividers for this clock value */
-	result = atomctrl_get_engine_pll_dividers_ai(hwmgr, clock,  &dividers);
-	if (result == 0) {
-		sclk_setting->Fcw_int = dividers.usSclk_fcw_int;
-		sclk_setting->Fcw_frac = dividers.usSclk_fcw_frac;
-		sclk_setting->Pcc_fcw_int = dividers.usPcc_fcw_int;
-		sclk_setting->PllRange = dividers.ucSclkPllRange;
-		sclk_setting->Sclk_slew_rate = 0x400;
-		sclk_setting->Pcc_up_slew_rate = dividers.usPcc_fcw_slew_frac;
-		sclk_setting->Pcc_down_slew_rate = 0xffff;
-		sclk_setting->SSc_En = dividers.ucSscEnable;
-		sclk_setting->Fcw1_int = dividers.usSsc_fcw1_int;
-		sclk_setting->Fcw1_frac = dividers.usSsc_fcw1_frac;
-		sclk_setting->Sclk_ss_slew_rate = dividers.usSsc_fcw_slew_frac;
-		return result;
-	}
-
-	ref_clock = polaris10_get_xclk(hwmgr);
-
-	for (i = 0; i < NUM_SCLK_RANGE; i++) {
-		if (clock > data->range_table[i].trans_lower_frequency
-		&& clock <= data->range_table[i].trans_upper_frequency) {
-			sclk_setting->PllRange = i;
-			break;
-		}
-	}
-
-	sclk_setting->Fcw_int = (uint16_t)((clock << table->SclkFcwRangeTable[sclk_setting->PllRange].postdiv) / ref_clock);
-	temp = clock << table->SclkFcwRangeTable[sclk_setting->PllRange].postdiv;
-	temp <<= 0x10;
-	do_div(temp, ref_clock);
-	sclk_setting->Fcw_frac = temp & 0xffff;
-
-	pcc_target_percent = 10; /*  Hardcode 10% for now. */
-	pcc_target_freq = clock - (clock * pcc_target_percent / 100);
-	sclk_setting->Pcc_fcw_int = (uint16_t)((pcc_target_freq << table->SclkFcwRangeTable[sclk_setting->PllRange].postdiv) / ref_clock);
-
-	ss_target_percent = 2; /*  Hardcode 2% for now. */
-	sclk_setting->SSc_En = 0;
-	if (ss_target_percent) {
-		sclk_setting->SSc_En = 1;
-		ss_target_freq = clock - (clock * ss_target_percent / 100);
-		sclk_setting->Fcw1_int = (uint16_t)((ss_target_freq << table->SclkFcwRangeTable[sclk_setting->PllRange].postdiv) / ref_clock);
-		temp = ss_target_freq << table->SclkFcwRangeTable[sclk_setting->PllRange].postdiv;
-		temp <<= 0x10;
-		do_div(temp, ref_clock);
-		sclk_setting->Fcw1_frac = temp & 0xffff;
-	}
-
-	return 0;
-}
-
-static int polaris10_get_dependency_volt_by_clk(struct pp_hwmgr *hwmgr,
-		struct phm_ppt_v1_clock_voltage_dependency_table *dep_table,
-		uint32_t clock, SMU_VoltageLevel *voltage, uint32_t *mvdd)
-{
-	uint32_t i;
-	uint16_t vddci;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	*voltage = *mvdd = 0;
-
-	/* clock - voltage dependency table is empty table */
-	if (dep_table->count == 0)
-		return -EINVAL;
-
-	for (i = 0; i < dep_table->count; i++) {
-		/* find first sclk bigger than request */
-		if (dep_table->entries[i].clk >= clock) {
-			*voltage |= (dep_table->entries[i].vddc *
-					VOLTAGE_SCALE) << VDDC_SHIFT;
-			if (POLARIS10_VOLTAGE_CONTROL_NONE == data->vddci_control)
-				*voltage |= (data->vbios_boot_state.vddci_bootup_value *
-						VOLTAGE_SCALE) << VDDCI_SHIFT;
-			else if (dep_table->entries[i].vddci)
-				*voltage |= (dep_table->entries[i].vddci *
-						VOLTAGE_SCALE) << VDDCI_SHIFT;
-			else {
-				vddci = phm_find_closest_vddci(&(data->vddci_voltage_table),
-						(dep_table->entries[i].vddc -
-								(uint16_t)data->vddc_vddci_delta));
-				*voltage |= (vddci * VOLTAGE_SCALE) << VDDCI_SHIFT;
-			}
-
-			if (POLARIS10_VOLTAGE_CONTROL_NONE == data->mvdd_control)
-				*mvdd = data->vbios_boot_state.mvdd_bootup_value *
-					VOLTAGE_SCALE;
-			else if (dep_table->entries[i].mvdd)
-				*mvdd = (uint32_t) dep_table->entries[i].mvdd *
-					VOLTAGE_SCALE;
-
-			*voltage |= 1 << PHASES_SHIFT;
-			return 0;
-		}
-	}
-
-	/* sclk is bigger than max sclk in the dependence table */
-	*voltage |= (dep_table->entries[i - 1].vddc * VOLTAGE_SCALE) << VDDC_SHIFT;
-
-	if (POLARIS10_VOLTAGE_CONTROL_NONE == data->vddci_control)
-		*voltage |= (data->vbios_boot_state.vddci_bootup_value *
-				VOLTAGE_SCALE) << VDDCI_SHIFT;
-	else if (dep_table->entries[i-1].vddci) {
-		vddci = phm_find_closest_vddci(&(data->vddci_voltage_table),
-				(dep_table->entries[i].vddc -
-						(uint16_t)data->vddc_vddci_delta));
-		*voltage |= (vddci * VOLTAGE_SCALE) << VDDCI_SHIFT;
-	}
-
-	if (POLARIS10_VOLTAGE_CONTROL_NONE == data->mvdd_control)
-		*mvdd = data->vbios_boot_state.mvdd_bootup_value * VOLTAGE_SCALE;
-	else if (dep_table->entries[i].mvdd)
-		*mvdd = (uint32_t) dep_table->entries[i - 1].mvdd * VOLTAGE_SCALE;
-
-	return 0;
-}
-
-static const sclkFcwRange_t Range_Table[NUM_SCLK_RANGE] =
-{ {VCO_2_4, POSTDIV_DIV_BY_16,  75, 160, 112},
-  {VCO_3_6, POSTDIV_DIV_BY_16, 112, 224, 160},
-  {VCO_2_4, POSTDIV_DIV_BY_8,   75, 160, 112},
-  {VCO_3_6, POSTDIV_DIV_BY_8,  112, 224, 160},
-  {VCO_2_4, POSTDIV_DIV_BY_4,   75, 160, 112},
-  {VCO_3_6, POSTDIV_DIV_BY_4,  112, 216, 160},
-  {VCO_2_4, POSTDIV_DIV_BY_2,   75, 160, 108},
-  {VCO_3_6, POSTDIV_DIV_BY_2,  112, 216, 160} };
-
-static void polaris10_get_sclk_range_table(struct pp_hwmgr *hwmgr)
-{
-	uint32_t i, ref_clk;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	SMU74_Discrete_DpmTable  *table = &(data->smc_state_table);
-	struct pp_atom_ctrl_sclk_range_table range_table_from_vbios = { { {0} } };
-
-	ref_clk = polaris10_get_xclk(hwmgr);
-
-	if (0 == atomctrl_get_smc_sclk_range_table(hwmgr, &range_table_from_vbios)) {
-		for (i = 0; i < NUM_SCLK_RANGE; i++) {
-			table->SclkFcwRangeTable[i].vco_setting = range_table_from_vbios.entry[i].ucVco_setting;
-			table->SclkFcwRangeTable[i].postdiv = range_table_from_vbios.entry[i].ucPostdiv;
-			table->SclkFcwRangeTable[i].fcw_pcc = range_table_from_vbios.entry[i].usFcw_pcc;
-
-			table->SclkFcwRangeTable[i].fcw_trans_upper = range_table_from_vbios.entry[i].usFcw_trans_upper;
-			table->SclkFcwRangeTable[i].fcw_trans_lower = range_table_from_vbios.entry[i].usRcw_trans_lower;
-
-			CONVERT_FROM_HOST_TO_SMC_US(table->SclkFcwRangeTable[i].fcw_pcc);
-			CONVERT_FROM_HOST_TO_SMC_US(table->SclkFcwRangeTable[i].fcw_trans_upper);
-			CONVERT_FROM_HOST_TO_SMC_US(table->SclkFcwRangeTable[i].fcw_trans_lower);
-		}
-		return;
-	}
-
-	for (i = 0; i < NUM_SCLK_RANGE; i++) {
-
-		data->range_table[i].trans_lower_frequency = (ref_clk * Range_Table[i].fcw_trans_lower) >> Range_Table[i].postdiv;
-		data->range_table[i].trans_upper_frequency = (ref_clk * Range_Table[i].fcw_trans_upper) >> Range_Table[i].postdiv;
-
-		table->SclkFcwRangeTable[i].vco_setting = Range_Table[i].vco_setting;
-		table->SclkFcwRangeTable[i].postdiv = Range_Table[i].postdiv;
-		table->SclkFcwRangeTable[i].fcw_pcc = Range_Table[i].fcw_pcc;
-
-		table->SclkFcwRangeTable[i].fcw_trans_upper = Range_Table[i].fcw_trans_upper;
-		table->SclkFcwRangeTable[i].fcw_trans_lower = Range_Table[i].fcw_trans_lower;
-
-		CONVERT_FROM_HOST_TO_SMC_US(table->SclkFcwRangeTable[i].fcw_pcc);
-		CONVERT_FROM_HOST_TO_SMC_US(table->SclkFcwRangeTable[i].fcw_trans_upper);
-		CONVERT_FROM_HOST_TO_SMC_US(table->SclkFcwRangeTable[i].fcw_trans_lower);
-	}
-}
-
-/**
-* Populates single SMC SCLK structure using the provided engine clock
-*
-* @param    hwmgr      the address of the hardware manager
-* @param    clock the engine clock to use to populate the structure
-* @param    sclk        the SMC SCLK structure to be populated
-*/
-
-static int polaris10_populate_single_graphic_level(struct pp_hwmgr *hwmgr,
-		uint32_t clock, uint16_t sclk_al_threshold,
-		struct SMU74_Discrete_GraphicsLevel *level)
-{
-	int result, i, temp;
-	/* PP_Clocks minClocks; */
-	uint32_t mvdd;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	SMU_SclkSetting curr_sclk_setting = { 0 };
-
-	result = polaris10_calculate_sclk_params(hwmgr, clock, &curr_sclk_setting);
-
-	/* populate graphics levels */
-	result = polaris10_get_dependency_volt_by_clk(hwmgr,
-			table_info->vdd_dep_on_sclk, clock,
-			&level->MinVoltage, &mvdd);
-
-	PP_ASSERT_WITH_CODE((0 == result),
-			"can not find VDDC voltage value for "
-			"VDDC engine clock dependency table",
-			return result);
-	level->ActivityLevel = sclk_al_threshold;
-
-	level->CcPwrDynRm = 0;
-	level->CcPwrDynRm1 = 0;
-	level->EnabledForActivity = 0;
-	level->EnabledForThrottle = 1;
-	level->UpHyst = 10;
-	level->DownHyst = 0;
-	level->VoltageDownHyst = 0;
-	level->PowerThrottle = 0;
-
-	/*
-	* TODO: get minimum clocks from dal configaration
-	* PECI_GetMinClockSettings(hwmgr->pPECI, &minClocks);
-	*/
-	/* data->DisplayTiming.minClockInSR = minClocks.engineClockInSR; */
-
-	/* get level->DeepSleepDivId
-	if (phm_cap_enabled(hwmgr->platformDescriptor.platformCaps, PHM_PlatformCaps_SclkDeepSleep))
-		level->DeepSleepDivId = PhwFiji_GetSleepDividerIdFromClock(hwmgr, clock, minClocks.engineClockInSR);
-	*/
-	PP_ASSERT_WITH_CODE((clock >= POLARIS10_MINIMUM_ENGINE_CLOCK), "Engine clock can't satisfy stutter requirement!", return 0);
-	for (i = POLARIS10_MAX_DEEPSLEEP_DIVIDER_ID;  ; i--) {
-		temp = clock >> i;
-
-		if (temp >= POLARIS10_MINIMUM_ENGINE_CLOCK || i == 0)
-			break;
-	}
-
-	level->DeepSleepDivId = i;
-
-	/* Default to slow, highest DPM level will be
-	 * set to PPSMC_DISPLAY_WATERMARK_LOW later.
-	 */
-	if (data->update_up_hyst)
-		level->UpHyst = (uint8_t)data->up_hyst;
-	if (data->update_down_hyst)
-		level->DownHyst = (uint8_t)data->down_hyst;
-
-	level->SclkSetting = curr_sclk_setting;
-
-	CONVERT_FROM_HOST_TO_SMC_UL(level->MinVoltage);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->CcPwrDynRm);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->CcPwrDynRm1);
-	CONVERT_FROM_HOST_TO_SMC_US(level->ActivityLevel);
-	CONVERT_FROM_HOST_TO_SMC_UL(level->SclkSetting.SclkFrequency);
-	CONVERT_FROM_HOST_TO_SMC_US(level->SclkSetting.Fcw_int);
-	CONVERT_FROM_HOST_TO_SMC_US(level->SclkSetting.Fcw_frac);
-	CONVERT_FROM_HOST_TO_SMC_US(level->SclkSetting.Pcc_fcw_int);
-	CONVERT_FROM_HOST_TO_SMC_US(level->SclkSetting.Sclk_slew_rate);
-	CONVERT_FROM_HOST_TO_SMC_US(level->SclkSetting.Pcc_up_slew_rate);
-	CONVERT_FROM_HOST_TO_SMC_US(level->SclkSetting.Pcc_down_slew_rate);
-	CONVERT_FROM_HOST_TO_SMC_US(level->SclkSetting.Fcw1_int);
-	CONVERT_FROM_HOST_TO_SMC_US(level->SclkSetting.Fcw1_frac);
-	CONVERT_FROM_HOST_TO_SMC_US(level->SclkSetting.Sclk_ss_slew_rate);
-	return 0;
-}
-
-/**
-* Populates all SMC SCLK levels' structure based on the trimmed allowed dpm engine clock states
-*
-* @param    hwmgr      the address of the hardware manager
-*/
-static int polaris10_populate_all_graphic_levels(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_dpm_table *dpm_table = &data->dpm_table;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_pcie_table *pcie_table = table_info->pcie_table;
-	uint8_t pcie_entry_cnt = (uint8_t) data->dpm_table.pcie_speed_table.count;
-	int result = 0;
-	uint32_t array = data->dpm_table_start +
-			offsetof(SMU74_Discrete_DpmTable, GraphicsLevel);
-	uint32_t array_size = sizeof(struct SMU74_Discrete_GraphicsLevel) *
-			SMU74_MAX_LEVELS_GRAPHICS;
-	struct SMU74_Discrete_GraphicsLevel *levels =
-			data->smc_state_table.GraphicsLevel;
-	uint32_t i, max_entry;
-	uint8_t hightest_pcie_level_enabled = 0,
-		lowest_pcie_level_enabled = 0,
-		mid_pcie_level_enabled = 0,
-		count = 0;
-
-	polaris10_get_sclk_range_table(hwmgr);
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-
-		result = polaris10_populate_single_graphic_level(hwmgr,
-				dpm_table->sclk_table.dpm_levels[i].value,
-				(uint16_t)data->activity_target[i],
-				&(data->smc_state_table.GraphicsLevel[i]));
-		if (result)
-			return result;
-
-		/* Making sure only DPM level 0-1 have Deep Sleep Div ID populated. */
-		if (i > 1)
-			levels[i].DeepSleepDivId = 0;
-	}
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_SPLLShutdownSupport))
-		data->smc_state_table.GraphicsLevel[0].SclkSetting.SSc_En = 0;
-
-	data->smc_state_table.GraphicsLevel[0].EnabledForActivity = 1;
-	data->smc_state_table.GraphicsDpmLevelCount =
-			(uint8_t)dpm_table->sclk_table.count;
-	data->dpm_level_enable_mask.sclk_dpm_enable_mask =
-			phm_get_dpm_level_enable_mask_value(&dpm_table->sclk_table);
-
-
-	if (pcie_table != NULL) {
-		PP_ASSERT_WITH_CODE((1 <= pcie_entry_cnt),
-				"There must be 1 or more PCIE levels defined in PPTable.",
-				return -EINVAL);
-		max_entry = pcie_entry_cnt - 1;
-		for (i = 0; i < dpm_table->sclk_table.count; i++)
-			levels[i].pcieDpmLevel =
-					(uint8_t) ((i < max_entry) ? i : max_entry);
-	} else {
-		while (data->dpm_level_enable_mask.pcie_dpm_enable_mask &&
-				((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-						(1 << (hightest_pcie_level_enabled + 1))) != 0))
-			hightest_pcie_level_enabled++;
-
-		while (data->dpm_level_enable_mask.pcie_dpm_enable_mask &&
-				((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-						(1 << lowest_pcie_level_enabled)) == 0))
-			lowest_pcie_level_enabled++;
-
-		while ((count < hightest_pcie_level_enabled) &&
-				((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-						(1 << (lowest_pcie_level_enabled + 1 + count))) == 0))
-			count++;
-
-		mid_pcie_level_enabled = (lowest_pcie_level_enabled + 1 + count) <
-				hightest_pcie_level_enabled ?
-						(lowest_pcie_level_enabled + 1 + count) :
-						hightest_pcie_level_enabled;
-
-		/* set pcieDpmLevel to hightest_pcie_level_enabled */
-		for (i = 2; i < dpm_table->sclk_table.count; i++)
-			levels[i].pcieDpmLevel = hightest_pcie_level_enabled;
-
-		/* set pcieDpmLevel to lowest_pcie_level_enabled */
-		levels[0].pcieDpmLevel = lowest_pcie_level_enabled;
-
-		/* set pcieDpmLevel to mid_pcie_level_enabled */
-		levels[1].pcieDpmLevel = mid_pcie_level_enabled;
-	}
-	/* level count will send to smc once at init smc table and never change */
-	result = polaris10_copy_bytes_to_smc(hwmgr->smumgr, array, (uint8_t *)levels,
-			(uint32_t)array_size, data->sram_end);
-
-	return result;
-}
-
-static int polaris10_populate_single_memory_level(struct pp_hwmgr *hwmgr,
-		uint32_t clock, struct SMU74_Discrete_MemoryLevel *mem_level)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	int result = 0;
-	struct cgs_display_info info = {0, 0, NULL};
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (table_info->vdd_dep_on_mclk) {
-		result = polaris10_get_dependency_volt_by_clk(hwmgr,
-				table_info->vdd_dep_on_mclk, clock,
-				&mem_level->MinVoltage, &mem_level->MinMvdd);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find MinVddc voltage value from memory "
-				"VDDC voltage dependency table", return result);
-	}
-
-	mem_level->MclkFrequency = clock;
-	mem_level->EnabledForThrottle = 1;
-	mem_level->EnabledForActivity = 0;
-	mem_level->UpHyst = 0;
-	mem_level->DownHyst = 100;
-	mem_level->VoltageDownHyst = 0;
-	mem_level->ActivityLevel = (uint16_t)data->mclk_activity_target;
-	mem_level->StutterEnable = false;
-	mem_level->DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-
-	data->display_timing.num_existing_displays = info.display_count;
-
-	if ((data->mclk_stutter_mode_threshold) &&
-		(clock <= data->mclk_stutter_mode_threshold) &&
-		(PHM_READ_FIELD(hwmgr->device, DPG_PIPE_STUTTER_CONTROL,
-				STUTTER_ENABLE) & 0x1))
-		mem_level->StutterEnable = true;
-
-	if (!result) {
-		CONVERT_FROM_HOST_TO_SMC_UL(mem_level->MinMvdd);
-		CONVERT_FROM_HOST_TO_SMC_UL(mem_level->MclkFrequency);
-		CONVERT_FROM_HOST_TO_SMC_US(mem_level->ActivityLevel);
-		CONVERT_FROM_HOST_TO_SMC_UL(mem_level->MinVoltage);
-	}
-	return result;
-}
-
-/**
-* Populates all SMC MCLK levels' structure based on the trimmed allowed dpm memory clock states
-*
-* @param    hwmgr      the address of the hardware manager
-*/
-static int polaris10_populate_all_memory_levels(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_dpm_table *dpm_table = &data->dpm_table;
-	int result;
-	/* populate MCLK dpm table to SMU7 */
-	uint32_t array = data->dpm_table_start +
-			offsetof(SMU74_Discrete_DpmTable, MemoryLevel);
-	uint32_t array_size = sizeof(SMU74_Discrete_MemoryLevel) *
-			SMU74_MAX_LEVELS_MEMORY;
-	struct SMU74_Discrete_MemoryLevel *levels =
-			data->smc_state_table.MemoryLevel;
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->mclk_table.count; i++) {
-		PP_ASSERT_WITH_CODE((0 != dpm_table->mclk_table.dpm_levels[i].value),
-				"can not populate memory level as memory clock is zero",
-				return -EINVAL);
-		result = polaris10_populate_single_memory_level(hwmgr,
-				dpm_table->mclk_table.dpm_levels[i].value,
-				&levels[i]);
-		if (i == dpm_table->mclk_table.count - 1) {
-			levels[i].DisplayWatermark = PPSMC_DISPLAY_WATERMARK_HIGH;
-			levels[i].EnabledForActivity = 1;
-		}
-		if (result)
-			return result;
-	}
-
-	/* In order to prevent MC activity from stutter mode to push DPM up,
-	 * the UVD change complements this by putting the MCLK in
-	 * a higher state by default such that we are not affected by
-	 * up threshold or and MCLK DPM latency.
-	 */
-	levels[0].ActivityLevel = 0x1f;
-	CONVERT_FROM_HOST_TO_SMC_US(levels[0].ActivityLevel);
-
-	data->smc_state_table.MemoryDpmLevelCount =
-			(uint8_t)dpm_table->mclk_table.count;
-	data->dpm_level_enable_mask.mclk_dpm_enable_mask =
-			phm_get_dpm_level_enable_mask_value(&dpm_table->mclk_table);
-
-	/* level count will send to smc once at init smc table and never change */
-	result = polaris10_copy_bytes_to_smc(hwmgr->smumgr, array, (uint8_t *)levels,
-			(uint32_t)array_size, data->sram_end);
-
-	return result;
-}
-
-/**
-* Populates the SMC MVDD structure using the provided memory clock.
-*
-* @param    hwmgr      the address of the hardware manager
-* @param    mclk        the MCLK value to be used in the decision if MVDD should be high or low.
-* @param    voltage     the SMC VOLTAGE structure to be populated
-*/
-static int polaris10_populate_mvdd_value(struct pp_hwmgr *hwmgr,
-		uint32_t mclk, SMIO_Pattern *smio_pat)
-{
-	const struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint32_t i = 0;
-
-	if (POLARIS10_VOLTAGE_CONTROL_NONE != data->mvdd_control) {
-		/* find mvdd value which clock is more than request */
-		for (i = 0; i < table_info->vdd_dep_on_mclk->count; i++) {
-			if (mclk <= table_info->vdd_dep_on_mclk->entries[i].clk) {
-				smio_pat->Voltage = data->mvdd_voltage_table.entries[i].value;
-				break;
-			}
-		}
-		PP_ASSERT_WITH_CODE(i < table_info->vdd_dep_on_mclk->count,
-				"MVDD Voltage is outside the supported range.",
-				return -EINVAL);
-	} else
-		return -EINVAL;
-
-	return 0;
-}
-
-static int polaris10_populate_smc_acpi_level(struct pp_hwmgr *hwmgr,
-		SMU74_Discrete_DpmTable *table)
-{
-	int result = 0;
-	uint32_t sclk_frequency;
-	const struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	SMIO_Pattern vol_level;
-	uint32_t mvdd;
-	uint16_t us_mvdd;
-
-	table->ACPILevel.Flags &= ~PPSMC_SWSTATE_FLAG_DC;
-
-
-	/* Get MinVoltage and Frequency from DPM0,
-	 * already converted to SMC_UL */
-	sclk_frequency = data->vbios_boot_state.sclk_bootup_value;
-	result = polaris10_get_dependency_volt_by_clk(hwmgr,
-			table_info->vdd_dep_on_sclk,
-			sclk_frequency,
-			&table->ACPILevel.MinVoltage, &mvdd);
-	PP_ASSERT_WITH_CODE((0 == result),
-			"Cannot find ACPI VDDC voltage value "
-			"in Clock Dependency Table",
-			);
-
-
-	result = polaris10_calculate_sclk_params(hwmgr, sclk_frequency,  &(table->ACPILevel.SclkSetting));
-	PP_ASSERT_WITH_CODE(result == 0, "Error retrieving Engine Clock dividers from VBIOS.", return result);
-
-	table->ACPILevel.DeepSleepDivId = 0;
-	table->ACPILevel.CcPwrDynRm = 0;
-	table->ACPILevel.CcPwrDynRm1 = 0;
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.Flags);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.MinVoltage);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CcPwrDynRm);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CcPwrDynRm1);
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SclkSetting.SclkFrequency);
-	CONVERT_FROM_HOST_TO_SMC_US(table->ACPILevel.SclkSetting.Fcw_int);
-	CONVERT_FROM_HOST_TO_SMC_US(table->ACPILevel.SclkSetting.Fcw_frac);
-	CONVERT_FROM_HOST_TO_SMC_US(table->ACPILevel.SclkSetting.Pcc_fcw_int);
-	CONVERT_FROM_HOST_TO_SMC_US(table->ACPILevel.SclkSetting.Sclk_slew_rate);
-	CONVERT_FROM_HOST_TO_SMC_US(table->ACPILevel.SclkSetting.Pcc_up_slew_rate);
-	CONVERT_FROM_HOST_TO_SMC_US(table->ACPILevel.SclkSetting.Pcc_down_slew_rate);
-	CONVERT_FROM_HOST_TO_SMC_US(table->ACPILevel.SclkSetting.Fcw1_int);
-	CONVERT_FROM_HOST_TO_SMC_US(table->ACPILevel.SclkSetting.Fcw1_frac);
-	CONVERT_FROM_HOST_TO_SMC_US(table->ACPILevel.SclkSetting.Sclk_ss_slew_rate);
-
-
-	/* Get MinVoltage and Frequency from DPM0, already converted to SMC_UL */
-	table->MemoryACPILevel.MclkFrequency = data->vbios_boot_state.mclk_bootup_value;
-	result = polaris10_get_dependency_volt_by_clk(hwmgr,
-			table_info->vdd_dep_on_mclk,
-			table->MemoryACPILevel.MclkFrequency,
-			&table->MemoryACPILevel.MinVoltage, &mvdd);
-	PP_ASSERT_WITH_CODE((0 == result),
-			"Cannot find ACPI VDDCI voltage value "
-			"in Clock Dependency Table",
-			);
-
-	us_mvdd = 0;
-	if ((POLARIS10_VOLTAGE_CONTROL_NONE == data->mvdd_control) ||
-			(data->mclk_dpm_key_disabled))
-		us_mvdd = data->vbios_boot_state.mvdd_bootup_value;
-	else {
-		if (!polaris10_populate_mvdd_value(hwmgr,
-				data->dpm_table.mclk_table.dpm_levels[0].value,
-				&vol_level))
-			us_mvdd = vol_level.Voltage;
-	}
-
-	if (0 == polaris10_populate_mvdd_value(hwmgr, 0, &vol_level))
-		table->MemoryACPILevel.MinMvdd = PP_HOST_TO_SMC_UL(vol_level.Voltage);
-	else
-		table->MemoryACPILevel.MinMvdd = 0;
-
-	table->MemoryACPILevel.StutterEnable = false;
-
-	table->MemoryACPILevel.EnabledForThrottle = 0;
-	table->MemoryACPILevel.EnabledForActivity = 0;
-	table->MemoryACPILevel.UpHyst = 0;
-	table->MemoryACPILevel.DownHyst = 100;
-	table->MemoryACPILevel.VoltageDownHyst = 0;
-	table->MemoryACPILevel.ActivityLevel =
-			PP_HOST_TO_SMC_US((uint16_t)data->mclk_activity_target);
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->MemoryACPILevel.MclkFrequency);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->MemoryACPILevel.MinVoltage);
-
-	return result;
-}
-
-static int polaris10_populate_smc_vce_level(struct pp_hwmgr *hwmgr,
-		SMU74_Discrete_DpmTable *table)
-{
-	int result = -EINVAL;
-	uint8_t count;
-	struct pp_atomctrl_clock_dividers_vi dividers;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =
-			table_info->mm_dep_table;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t vddci;
-
-	table->VceLevelCount = (uint8_t)(mm_table->count);
-	table->VceBootLevel = 0;
-
-	for (count = 0; count < table->VceLevelCount; count++) {
-		table->VceLevel[count].Frequency = mm_table->entries[count].eclk;
-		table->VceLevel[count].MinVoltage = 0;
-		table->VceLevel[count].MinVoltage |=
-				(mm_table->entries[count].vddc * VOLTAGE_SCALE) << VDDC_SHIFT;
-
-		if (POLARIS10_VOLTAGE_CONTROL_BY_GPIO == data->vddci_control)
-			vddci = (uint32_t)phm_find_closest_vddci(&(data->vddci_voltage_table),
-						mm_table->entries[count].vddc - VDDC_VDDCI_DELTA);
-		else if (POLARIS10_VOLTAGE_CONTROL_BY_SVID2 == data->vddci_control)
-			vddci = mm_table->entries[count].vddc - VDDC_VDDCI_DELTA;
-		else
-			vddci = (data->vbios_boot_state.vddci_bootup_value * VOLTAGE_SCALE) << VDDCI_SHIFT;
-
-
-		table->VceLevel[count].MinVoltage |=
-				(vddci * VOLTAGE_SCALE) << VDDCI_SHIFT;
-		table->VceLevel[count].MinVoltage |= 1 << PHASES_SHIFT;
-
-		/*retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-				table->VceLevel[count].Frequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for VCE engine clock",
-				return result);
-
-		table->VceLevel[count].Divider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->VceLevel[count].Frequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->VceLevel[count].MinVoltage);
-	}
-	return result;
-}
-
-static int polaris10_populate_smc_samu_level(struct pp_hwmgr *hwmgr,
-		SMU74_Discrete_DpmTable *table)
-{
-	int result = -EINVAL;
-	uint8_t count;
-	struct pp_atomctrl_clock_dividers_vi dividers;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =
-			table_info->mm_dep_table;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t vddci;
-
-	table->SamuBootLevel = 0;
-	table->SamuLevelCount = (uint8_t)(mm_table->count);
-
-	for (count = 0; count < table->SamuLevelCount; count++) {
-		/* not sure whether we need evclk or not */
-		table->SamuLevel[count].MinVoltage = 0;
-		table->SamuLevel[count].Frequency = mm_table->entries[count].samclock;
-		table->SamuLevel[count].MinVoltage |= (mm_table->entries[count].vddc *
-				VOLTAGE_SCALE) << VDDC_SHIFT;
-
-		if (POLARIS10_VOLTAGE_CONTROL_BY_GPIO == data->vddci_control)
-			vddci = (uint32_t)phm_find_closest_vddci(&(data->vddci_voltage_table),
-						mm_table->entries[count].vddc - VDDC_VDDCI_DELTA);
-		else if (POLARIS10_VOLTAGE_CONTROL_BY_SVID2 == data->vddci_control)
-			vddci = mm_table->entries[count].vddc - VDDC_VDDCI_DELTA;
-		else
-			vddci = (data->vbios_boot_state.vddci_bootup_value * VOLTAGE_SCALE) << VDDCI_SHIFT;
-
-		table->SamuLevel[count].MinVoltage |= (vddci * VOLTAGE_SCALE) << VDDCI_SHIFT;
-		table->SamuLevel[count].MinVoltage |= 1 << PHASES_SHIFT;
-
-		/* retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-				table->SamuLevel[count].Frequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for samu clock", return result);
-
-		table->SamuLevel[count].Divider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->SamuLevel[count].Frequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->SamuLevel[count].MinVoltage);
-	}
-	return result;
-}
-
-static int polaris10_populate_memory_timing_parameters(struct pp_hwmgr *hwmgr,
-		int32_t eng_clock, int32_t mem_clock,
-		SMU74_Discrete_MCArbDramTimingTableEntry *arb_regs)
-{
-	uint32_t dram_timing;
-	uint32_t dram_timing2;
-	uint32_t burst_time;
-	int result;
-
-	result = atomctrl_set_engine_dram_timings_rv770(hwmgr,
-			eng_clock, mem_clock);
-	PP_ASSERT_WITH_CODE(result == 0,
-			"Error calling VBIOS to set DRAM_TIMING.", return result);
-
-	dram_timing = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING);
-	dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2);
-	burst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0);
-
-
-	arb_regs->McArbDramTiming  = PP_HOST_TO_SMC_UL(dram_timing);
-	arb_regs->McArbDramTiming2 = PP_HOST_TO_SMC_UL(dram_timing2);
-	arb_regs->McArbBurstTime   = (uint8_t)burst_time;
-
-	return 0;
-}
-
-static int polaris10_program_memory_timing_parameters(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct SMU74_Discrete_MCArbDramTimingTable arb_regs;
-	uint32_t i, j;
-	int result = 0;
-
-	for (i = 0; i < data->dpm_table.sclk_table.count; i++) {
-		for (j = 0; j < data->dpm_table.mclk_table.count; j++) {
-			result = polaris10_populate_memory_timing_parameters(hwmgr,
-					data->dpm_table.sclk_table.dpm_levels[i].value,
-					data->dpm_table.mclk_table.dpm_levels[j].value,
-					&arb_regs.entries[i][j]);
-			if (result == 0)
-				result = atomctrl_set_ac_timing_ai(hwmgr, data->dpm_table.mclk_table.dpm_levels[j].value, j);
-			if (result != 0)
-				return result;
-		}
-	}
-
-	result = polaris10_copy_bytes_to_smc(
-			hwmgr->smumgr,
-			data->arb_table_start,
-			(uint8_t *)&arb_regs,
-			sizeof(SMU74_Discrete_MCArbDramTimingTable),
-			data->sram_end);
-	return result;
-}
-
-static int polaris10_populate_smc_uvd_level(struct pp_hwmgr *hwmgr,
-		struct SMU74_Discrete_DpmTable *table)
-{
-	int result = -EINVAL;
-	uint8_t count;
-	struct pp_atomctrl_clock_dividers_vi dividers;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =
-			table_info->mm_dep_table;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t vddci;
-
-	table->UvdLevelCount = (uint8_t)(mm_table->count);
-	table->UvdBootLevel = 0;
-
-	for (count = 0; count < table->UvdLevelCount; count++) {
-		table->UvdLevel[count].MinVoltage = 0;
-		table->UvdLevel[count].VclkFrequency = mm_table->entries[count].vclk;
-		table->UvdLevel[count].DclkFrequency = mm_table->entries[count].dclk;
-		table->UvdLevel[count].MinVoltage |= (mm_table->entries[count].vddc *
-				VOLTAGE_SCALE) << VDDC_SHIFT;
-
-		if (POLARIS10_VOLTAGE_CONTROL_BY_GPIO == data->vddci_control)
-			vddci = (uint32_t)phm_find_closest_vddci(&(data->vddci_voltage_table),
-						mm_table->entries[count].vddc - VDDC_VDDCI_DELTA);
-		else if (POLARIS10_VOLTAGE_CONTROL_BY_SVID2 == data->vddci_control)
-			vddci = mm_table->entries[count].vddc - VDDC_VDDCI_DELTA;
-		else
-			vddci = (data->vbios_boot_state.vddci_bootup_value * VOLTAGE_SCALE) << VDDCI_SHIFT;
-
-		table->UvdLevel[count].MinVoltage |= (vddci * VOLTAGE_SCALE) << VDDCI_SHIFT;
-		table->UvdLevel[count].MinVoltage |= 1 << PHASES_SHIFT;
-
-		/* retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-				table->UvdLevel[count].VclkFrequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for Vclk clock", return result);
-
-		table->UvdLevel[count].VclkDivider = (uint8_t)dividers.pll_post_divider;
-
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-				table->UvdLevel[count].DclkFrequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for Dclk clock", return result);
-
-		table->UvdLevel[count].DclkDivider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->UvdLevel[count].VclkFrequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->UvdLevel[count].DclkFrequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->UvdLevel[count].MinVoltage);
-	}
-
-	return result;
-}
-
-static int polaris10_populate_smc_boot_level(struct pp_hwmgr *hwmgr,
-		struct SMU74_Discrete_DpmTable *table)
-{
-	int result = 0;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	table->GraphicsBootLevel = 0;
-	table->MemoryBootLevel = 0;
-
-	/* find boot level from dpm table */
-	result = phm_find_boot_level(&(data->dpm_table.sclk_table),
-			data->vbios_boot_state.sclk_bootup_value,
-			(uint32_t *)&(table->GraphicsBootLevel));
-
-	result = phm_find_boot_level(&(data->dpm_table.mclk_table),
-			data->vbios_boot_state.mclk_bootup_value,
-			(uint32_t *)&(table->MemoryBootLevel));
-
-	table->BootVddc  = data->vbios_boot_state.vddc_bootup_value *
-			VOLTAGE_SCALE;
-	table->BootVddci = data->vbios_boot_state.vddci_bootup_value *
-			VOLTAGE_SCALE;
-	table->BootMVdd  = data->vbios_boot_state.mvdd_bootup_value *
-			VOLTAGE_SCALE;
-
-	CONVERT_FROM_HOST_TO_SMC_US(table->BootVddc);
-	CONVERT_FROM_HOST_TO_SMC_US(table->BootVddci);
-	CONVERT_FROM_HOST_TO_SMC_US(table->BootMVdd);
-
-	return 0;
-}
-
-
-static int polaris10_populate_smc_initailial_state(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint8_t count, level;
-
-	count = (uint8_t)(table_info->vdd_dep_on_sclk->count);
-
-	for (level = 0; level < count; level++) {
-		if (table_info->vdd_dep_on_sclk->entries[level].clk >=
-				data->vbios_boot_state.sclk_bootup_value) {
-			data->smc_state_table.GraphicsBootLevel = level;
-			break;
-		}
-	}
-
-	count = (uint8_t)(table_info->vdd_dep_on_mclk->count);
-	for (level = 0; level < count; level++) {
-		if (table_info->vdd_dep_on_mclk->entries[level].clk >=
-				data->vbios_boot_state.mclk_bootup_value) {
-			data->smc_state_table.MemoryBootLevel = level;
-			break;
-		}
-	}
-
-	return 0;
-}
-
-static int polaris10_populate_clock_stretcher_data_table(struct pp_hwmgr *hwmgr)
-{
-	uint32_t ro, efuse, volt_without_cks, volt_with_cks, value, max, min;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint8_t i, stretch_amount, volt_offset = 0;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_clock_voltage_dependency_table *sclk_table =
-			table_info->vdd_dep_on_sclk;
-
-	stretch_amount = (uint8_t)table_info->cac_dtp_table->usClockStretchAmount;
-
-	/* Read SMU_Eefuse to read and calculate RO and determine
-	 * if the part is SS or FF. if RO >= 1660MHz, part is FF.
-	 */
-	efuse = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixSMU_EFUSE_0 + (67 * 4));
-	efuse &= 0xFF000000;
-	efuse = efuse >> 24;
-
-	if (hwmgr->chip_id == CHIP_POLARIS10) {
-		min = 1000;
-		max = 2300;
-	} else {
-		min = 1100;
-		max = 2100;
-	}
-
-	ro = efuse * (max -min)/255 + min;
-
-        /* Populate Sclk_CKS_masterEn0_7 and Sclk_voltageOffset
-         * there is a little difference in calculating
-         * volt_with_cks with windows */
-	for (i = 0; i < sclk_table->count; i++) {
-		data->smc_state_table.Sclk_CKS_masterEn0_7 |=
-				sclk_table->entries[i].cks_enable << i;
- 
-                if (hwmgr->chip_id == CHIP_POLARIS10) {
-                        volt_without_cks = (uint32_t)((2753594000 + (sclk_table->entries[i].clk/100) * 136418 -(ro - 70) * 1000000) / \
-                                                (2424180 - (sclk_table->entries[i].clk/100) * 1132925/1000));
-                        volt_with_cks = (uint32_t)((279720200 + sclk_table->entries[i].clk * 3232 - (ro - 65) * 100000000) / \
-                                        (252248000 - sclk_table->entries[i].clk/100 * 115764));
-                } else {
-                        volt_without_cks = (uint32_t)((2416794800 + (sclk_table->entries[i].clk/100) * 1476925/10 -(ro - 50) * 1000000) / \
-                                                (2625416 - (sclk_table->entries[i].clk/100) * 12586807/10000));
-                        volt_with_cks = (uint32_t)((2999656000 + sclk_table->entries[i].clk * 392803/100 - (ro - 44) * 1000000) / \
-                                        (3422454 - sclk_table->entries[i].clk/100 * 18886376/10000));
-                }
-
-		if (volt_without_cks >= volt_with_cks)
-                        volt_offset = (uint8_t)CEILING_UCHAR((volt_without_cks - volt_with_cks +
-                                        sclk_table->entries[i].cks_voffset) * 100 / 625);	
-
-		data->smc_state_table.Sclk_voltageOffset[i] = volt_offset;
-	}
- 
-        data->smc_state_table.LdoRefSel = (table_info->cac_dtp_table->ucCKS_LDO_REFSEL != 0) ? table_info->cac_dtp_table->ucCKS_LDO_REFSEL : 6;
-	/* Populate CKS Lookup Table */
-        if (stretch_amount != 1 && stretch_amount != 2 && stretch_amount != 3 &&
-                        stretch_amount != 4 && stretch_amount != 5) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_ClockStretcher);
-		PP_ASSERT_WITH_CODE(false,
-				"Stretch Amount in PPTable not supported\n",
-				return -EINVAL);
-	}
-
-	value = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixPWR_CKS_CNTL);
-	value &= 0xFFFFFFFE;
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixPWR_CKS_CNTL, value);
-
-	return 0;
-}
-
-/**
-* Populates the SMC VRConfig field in DPM table.
-*
-* @param    hwmgr   the address of the hardware manager
-* @param    table   the SMC DPM table structure to be populated
-* @return   always 0
-*/
-static int polaris10_populate_vr_config(struct pp_hwmgr *hwmgr,
-		struct SMU74_Discrete_DpmTable *table)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint16_t config;
-
-	config = VR_MERGED_WITH_VDDC;
-	table->VRConfig |= (config << VRCONF_VDDGFX_SHIFT);
-
-	/* Set Vddc Voltage Controller */
-	if (POLARIS10_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {
-		config = VR_SVI2_PLANE_1;
-		table->VRConfig |= config;
-	} else {
-		PP_ASSERT_WITH_CODE(false,
-				"VDDC should be on SVI2 control in merged mode!",
-				);
-	}
-	/* Set Vddci Voltage Controller */
-	if (POLARIS10_VOLTAGE_CONTROL_BY_SVID2 == data->vddci_control) {
-		config = VR_SVI2_PLANE_2;  /* only in merged mode */
-		table->VRConfig |= (config << VRCONF_VDDCI_SHIFT);
-	} else if (POLARIS10_VOLTAGE_CONTROL_BY_GPIO == data->vddci_control) {
-		config = VR_SMIO_PATTERN_1;
-		table->VRConfig |= (config << VRCONF_VDDCI_SHIFT);
-	} else {
-		config = VR_STATIC_VOLTAGE;
-		table->VRConfig |= (config << VRCONF_VDDCI_SHIFT);
-	}
-	/* Set Mvdd Voltage Controller */
-	if (POLARIS10_VOLTAGE_CONTROL_BY_SVID2 == data->mvdd_control) {
-		config = VR_SVI2_PLANE_2;
-		table->VRConfig |= (config << VRCONF_MVDD_SHIFT);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, data->soft_regs_start +
-			offsetof(SMU74_SoftRegisters, AllowMvddSwitch), 0x1);
-	} else {
-		config = VR_STATIC_VOLTAGE;
-		table->VRConfig |= (config << VRCONF_MVDD_SHIFT);
-	}
-
-	return 0;
-}
-
-
-static int polaris10_populate_avfs_parameters(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	SMU74_Discrete_DpmTable  *table = &(data->smc_state_table);
-	int result = 0;
-	struct pp_atom_ctrl__avfs_parameters avfs_params = {0};
-	AVFS_meanNsigma_t AVFS_meanNsigma = { {0} };
-	AVFS_Sclk_Offset_t AVFS_SclkOffset = { {0} };
-	uint32_t tmp, i;
-	struct pp_smumgr *smumgr = hwmgr->smumgr;
-	struct polaris10_smumgr *smu_data = (struct polaris10_smumgr *)(smumgr->backend);
-
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)hwmgr->pptable;
-	struct phm_ppt_v1_clock_voltage_dependency_table *sclk_table =
-			table_info->vdd_dep_on_sclk;
-
-
-	if (smu_data->avfs.avfs_btc_status == AVFS_BTC_NOTSUPPORTED)
-		return result;
-
-	result = atomctrl_get_avfs_information(hwmgr, &avfs_params);
-
-	if (0 == result) {
-		table->BTCGB_VDROOP_TABLE[0].a0  = PP_HOST_TO_SMC_UL(avfs_params.ulGB_VDROOP_TABLE_CKSON_a0);
-		table->BTCGB_VDROOP_TABLE[0].a1  = PP_HOST_TO_SMC_UL(avfs_params.ulGB_VDROOP_TABLE_CKSON_a1);
-		table->BTCGB_VDROOP_TABLE[0].a2  = PP_HOST_TO_SMC_UL(avfs_params.ulGB_VDROOP_TABLE_CKSON_a2);
-		table->BTCGB_VDROOP_TABLE[1].a0  = PP_HOST_TO_SMC_UL(avfs_params.ulGB_VDROOP_TABLE_CKSOFF_a0);
-		table->BTCGB_VDROOP_TABLE[1].a1  = PP_HOST_TO_SMC_UL(avfs_params.ulGB_VDROOP_TABLE_CKSOFF_a1);
-		table->BTCGB_VDROOP_TABLE[1].a2  = PP_HOST_TO_SMC_UL(avfs_params.ulGB_VDROOP_TABLE_CKSOFF_a2);
-		table->AVFSGB_VDROOP_TABLE[0].m1 = PP_HOST_TO_SMC_UL(avfs_params.ulAVFSGB_FUSE_TABLE_CKSON_m1);
-		table->AVFSGB_VDROOP_TABLE[0].m2 = PP_HOST_TO_SMC_US(avfs_params.usAVFSGB_FUSE_TABLE_CKSON_m2);
-		table->AVFSGB_VDROOP_TABLE[0].b  = PP_HOST_TO_SMC_UL(avfs_params.ulAVFSGB_FUSE_TABLE_CKSON_b);
-		table->AVFSGB_VDROOP_TABLE[0].m1_shift = 24;
-		table->AVFSGB_VDROOP_TABLE[0].m2_shift  = 12;
-		table->AVFSGB_VDROOP_TABLE[1].m1 = PP_HOST_TO_SMC_UL(avfs_params.ulAVFSGB_FUSE_TABLE_CKSOFF_m1);
-		table->AVFSGB_VDROOP_TABLE[1].m2 = PP_HOST_TO_SMC_US(avfs_params.usAVFSGB_FUSE_TABLE_CKSOFF_m2);
-		table->AVFSGB_VDROOP_TABLE[1].b  = PP_HOST_TO_SMC_UL(avfs_params.ulAVFSGB_FUSE_TABLE_CKSOFF_b);
-		table->AVFSGB_VDROOP_TABLE[1].m1_shift = 24;
-		table->AVFSGB_VDROOP_TABLE[1].m2_shift  = 12;
-		table->MaxVoltage                = PP_HOST_TO_SMC_US(avfs_params.usMaxVoltage_0_25mv);
-		AVFS_meanNsigma.Aconstant[0]      = PP_HOST_TO_SMC_UL(avfs_params.ulAVFS_meanNsigma_Acontant0);
-		AVFS_meanNsigma.Aconstant[1]      = PP_HOST_TO_SMC_UL(avfs_params.ulAVFS_meanNsigma_Acontant1);
-		AVFS_meanNsigma.Aconstant[2]      = PP_HOST_TO_SMC_UL(avfs_params.ulAVFS_meanNsigma_Acontant2);
-		AVFS_meanNsigma.DC_tol_sigma      = PP_HOST_TO_SMC_US(avfs_params.usAVFS_meanNsigma_DC_tol_sigma);
-		AVFS_meanNsigma.Platform_mean     = PP_HOST_TO_SMC_US(avfs_params.usAVFS_meanNsigma_Platform_mean);
-		AVFS_meanNsigma.PSM_Age_CompFactor = PP_HOST_TO_SMC_US(avfs_params.usPSM_Age_ComFactor);
-		AVFS_meanNsigma.Platform_sigma     = PP_HOST_TO_SMC_US(avfs_params.usAVFS_meanNsigma_Platform_sigma);
-
-		for (i = 0; i < NUM_VFT_COLUMNS; i++) {
-			AVFS_meanNsigma.Static_Voltage_Offset[i] = (uint8_t)(sclk_table->entries[i].cks_voffset * 100 / 625);
-			AVFS_SclkOffset.Sclk_Offset[i] = PP_HOST_TO_SMC_US((uint16_t)(sclk_table->entries[i].sclk_offset) / 100);
-		}
-
-		result = polaris10_read_smc_sram_dword(smumgr,
-				SMU7_FIRMWARE_HEADER_LOCATION + offsetof(SMU74_Firmware_Header, AvfsMeanNSigma),
-				&tmp, data->sram_end);
-
-		polaris10_copy_bytes_to_smc(smumgr,
-					tmp,
-					(uint8_t *)&AVFS_meanNsigma,
-					sizeof(AVFS_meanNsigma_t),
-					data->sram_end);
-
-		result = polaris10_read_smc_sram_dword(smumgr,
-				SMU7_FIRMWARE_HEADER_LOCATION + offsetof(SMU74_Firmware_Header, AvfsSclkOffsetTable),
-				&tmp, data->sram_end);
-		polaris10_copy_bytes_to_smc(smumgr,
-					tmp,
-					(uint8_t *)&AVFS_SclkOffset,
-					sizeof(AVFS_Sclk_Offset_t),
-					data->sram_end);
-
-		data->avfs_vdroop_override_setting = (avfs_params.ucEnableGB_VDROOP_TABLE_CKSON << BTCGB0_Vdroop_Enable_SHIFT) |
-						(avfs_params.ucEnableGB_VDROOP_TABLE_CKSOFF << BTCGB1_Vdroop_Enable_SHIFT) |
-						(avfs_params.ucEnableGB_FUSE_TABLE_CKSON << AVFSGB0_Vdroop_Enable_SHIFT) |
-						(avfs_params.ucEnableGB_FUSE_TABLE_CKSOFF << AVFSGB1_Vdroop_Enable_SHIFT);
-		data->apply_avfs_cks_off_voltage = (avfs_params.ucEnableApplyAVFS_CKS_OFF_Voltage == 1) ? true : false;
-	}
-	return result;
-}
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static void polaris10_save_default_power_profile(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data =
-			(struct polaris10_hwmgr *)(hwmgr->backend);
-	struct SMU74_Discrete_GraphicsLevel *levels =
-				data->smc_state_table.GraphicsLevel;
-	unsigned min_level = 1;
-
-	hwmgr->default_gfx_power_profile.activity_threshold =
-			be16_to_cpu(levels[0].ActivityLevel);
-	hwmgr->default_gfx_power_profile.up_hyst = levels[0].UpHyst;
-	hwmgr->default_gfx_power_profile.down_hyst = levels[0].DownHyst;
-	hwmgr->default_gfx_power_profile.type = PP_GFX_PROFILE;
-
-	hwmgr->default_compute_power_profile = hwmgr->default_gfx_power_profile;
-	hwmgr->default_compute_power_profile.type = PP_COMPUTE_PROFILE;
-
-	/* Workaround compute SDMA instability: disable lowest SCLK
-	 * DPM level. Optimize compute power profile: Use only highest
-	 * 2 power levels (if more than 2 are available), Hysteresis:
-	 * 0ms up, 5ms down
-	 */
-	if (data->smc_state_table.GraphicsDpmLevelCount > 2)
-		min_level = data->smc_state_table.GraphicsDpmLevelCount - 2;
-	else if (data->smc_state_table.GraphicsDpmLevelCount == 2)
-		min_level = 1;
-	else
-		min_level = 0;
-	hwmgr->default_compute_power_profile.min_sclk =
-		be32_to_cpu(levels[min_level].SclkSetting.SclkFrequency);
-	hwmgr->default_compute_power_profile.up_hyst = 0;
-	hwmgr->default_compute_power_profile.down_hyst = 5;
-
-	hwmgr->gfx_power_profile = hwmgr->default_gfx_power_profile;
-	hwmgr->compute_power_profile = hwmgr->default_compute_power_profile;
-}
-#endif
-
-/**
-* Initializes the SMC table and uploads it
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int polaris10_init_smc_table(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct SMU74_Discrete_DpmTable *table = &(data->smc_state_table);
-	const struct polaris10_ulv_parm *ulv = &(data->ulv);
-	uint8_t i;
-	struct pp_atomctrl_gpio_pin_assignment gpio_pin;
-	pp_atomctrl_clock_dividers_vi dividers;
-
-	result = polaris10_setup_default_dpm_tables(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to setup default DPM tables!", return result);
-
-	if (POLARIS10_VOLTAGE_CONTROL_NONE != data->voltage_control)
-		polaris10_populate_smc_voltage_tables(hwmgr, table);
-
-	table->SystemFlags = 0;
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_AutomaticDCTransition))
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_GPIO_DC;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_StepVddc))
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_STEPVDDC;
-
-	if (data->is_memory_gddr5)
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_GDDR5;
-
-	if (ulv->ulv_supported && table_info->us_ulv_voltage_offset) {
-		result = polaris10_populate_ulv_state(hwmgr, table);
-		PP_ASSERT_WITH_CODE(0 == result,
-				"Failed to initialize ULV state!", return result);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				ixCG_ULV_PARAMETER, PPPOLARIS10_CGULVPARAMETER_DFLT);
-	}
-
-	result = polaris10_populate_smc_link_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Link Level!", return result);
-
-	result = polaris10_populate_all_graphic_levels(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Graphics Level!", return result);
-
-	result = polaris10_populate_all_memory_levels(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Memory Level!", return result);
-
-	result = polaris10_populate_smc_acpi_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize ACPI Level!", return result);
-
-	result = polaris10_populate_smc_vce_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize VCE Level!", return result);
-
-	result = polaris10_populate_smc_samu_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize SAMU Level!", return result);
-
-	/* Since only the initial state is completely set up at this point
-	 * (the other states are just copies of the boot state) we only
-	 * need to populate the  ARB settings for the initial state.
-	 */
-	result = polaris10_program_memory_timing_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to Write ARB settings for the initial state.", return result);
-
-	result = polaris10_populate_smc_uvd_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize UVD Level!", return result);
-
-	result = polaris10_populate_smc_boot_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Boot Level!", return result);
-
-	result = polaris10_populate_smc_initailial_state(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize Boot State!", return result);
-
-	result = polaris10_populate_bapm_parameters_in_dpm_table(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to populate BAPM Parameters!", return result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ClockStretcher)) {
-		result = polaris10_populate_clock_stretcher_data_table(hwmgr);
-		PP_ASSERT_WITH_CODE(0 == result,
-				"Failed to populate Clock Stretcher Data Table!",
-				return result);
-	}
-
-	result = polaris10_populate_avfs_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result, "Failed to populate AVFS Parameters!", return result;);
-
-	table->CurrSclkPllRange = 0xff;
-	table->GraphicsVoltageChangeEnable  = 1;
-	table->GraphicsThermThrottleEnable  = 1;
-	table->GraphicsInterval = 1;
-	table->VoltageInterval  = 1;
-	table->ThermalInterval  = 1;
-	table->TemperatureLimitHigh =
-			table_info->cac_dtp_table->usTargetOperatingTemp *
-			POLARIS10_Q88_FORMAT_CONVERSION_UNIT;
-	table->TemperatureLimitLow  =
-			(table_info->cac_dtp_table->usTargetOperatingTemp - 1) *
-			POLARIS10_Q88_FORMAT_CONVERSION_UNIT;
-	table->MemoryVoltageChangeEnable = 1;
-	table->MemoryInterval = 1;
-	table->VoltageResponseTime = 0;
-	table->PhaseResponseTime = 0;
-	table->MemoryThermThrottleEnable = 1;
-	table->PCIeBootLinkLevel = 0;
-	table->PCIeGenInterval = 1;
-	table->VRConfig = 0;
-
-	result = polaris10_populate_vr_config(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to populate VRConfig setting!", return result);
-
-	table->ThermGpio = 17;
-	table->SclkStepSize = 0x4000;
-
-	if (atomctrl_get_pp_assign_pin(hwmgr, VDDC_VRHOT_GPIO_PINID, &gpio_pin)) {
-		table->VRHotGpio = gpio_pin.uc_gpio_pin_bit_shift;
-	} else {
-		table->VRHotGpio = POLARIS10_UNUSED_GPIO_PIN;
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_RegulatorHot);
-	}
-
-	if (atomctrl_get_pp_assign_pin(hwmgr, PP_AC_DC_SWITCH_GPIO_PINID,
-			&gpio_pin)) {
-		table->AcDcGpio = gpio_pin.uc_gpio_pin_bit_shift;
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_AutomaticDCTransition);
-	} else {
-		table->AcDcGpio = POLARIS10_UNUSED_GPIO_PIN;
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_AutomaticDCTransition);
-	}
-
-	/* Thermal Output GPIO */
-	if (atomctrl_get_pp_assign_pin(hwmgr, THERMAL_INT_OUTPUT_GPIO_PINID,
-			&gpio_pin)) {
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_ThermalOutGPIO);
-
-		table->ThermOutGpio = gpio_pin.uc_gpio_pin_bit_shift;
-
-		/* For porlarity read GPIOPAD_A with assigned Gpio pin
-		 * since VBIOS will program this register to set 'inactive state',
-		 * driver can then determine 'active state' from this and
-		 * program SMU with correct polarity
-		 */
-		table->ThermOutPolarity = (0 == (cgs_read_register(hwmgr->device, mmGPIOPAD_A)
-					& (1 << gpio_pin.uc_gpio_pin_bit_shift))) ? 1:0;
-		table->ThermOutMode = SMU7_THERM_OUT_MODE_THERM_ONLY;
-
-		/* if required, combine VRHot/PCC with thermal out GPIO */
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_RegulatorHot)
-		&& phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_CombinePCCWithThermalSignal))
-			table->ThermOutMode = SMU7_THERM_OUT_MODE_THERM_VRHOT;
-	} else {
-		table->ThermOutGpio = 17;
-		table->ThermOutPolarity = 1;
-		table->ThermOutMode = SMU7_THERM_OUT_MODE_DISABLE;
-	}
-
-	/* Populate BIF_SCLK levels into SMC DPM table */
-	for (i = 0; i <= data->dpm_table.pcie_speed_table.count; i++) {
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr, data->bif_sclk_table[i], &dividers);
-		PP_ASSERT_WITH_CODE((result == 0), "Can not find DFS divide id for Sclk", return result);
-
-		if (i == 0)
-			table->Ulv.BifSclkDfs = PP_HOST_TO_SMC_US((USHORT)(dividers.pll_post_divider));
-		else
-			table->LinkLevel[i-1].BifSclkDfs = PP_HOST_TO_SMC_US((USHORT)(dividers.pll_post_divider));
-	}
-
-	for (i = 0; i < SMU74_MAX_ENTRIES_SMIO; i++)
-		table->Smio[i] = PP_HOST_TO_SMC_UL(table->Smio[i]);
-
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SystemFlags);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->VRConfig);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMask1);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMask2);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SclkStepSize);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->CurrSclkPllRange);
-	CONVERT_FROM_HOST_TO_SMC_US(table->TemperatureLimitHigh);
-	CONVERT_FROM_HOST_TO_SMC_US(table->TemperatureLimitLow);
-	CONVERT_FROM_HOST_TO_SMC_US(table->VoltageResponseTime);
-	CONVERT_FROM_HOST_TO_SMC_US(table->PhaseResponseTime);
-
-	/* Upload all dpm data to SMC memory.(dpm level, dpm level count etc) */
-	result = polaris10_copy_bytes_to_smc(hwmgr->smumgr,
-			data->dpm_table_start +
-			offsetof(SMU74_Discrete_DpmTable, SystemFlags),
-			(uint8_t *)&(table->SystemFlags),
-			sizeof(SMU74_Discrete_DpmTable) - 3 * sizeof(SMU74_PIDController),
-			data->sram_end);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to upload dpm data to SMC memory!", return result);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	polaris10_save_default_power_profile(hwmgr);
-#endif
-	return 0;
-}
-
-/**
-* Initialize the ARB DRAM timing table's index field.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int polaris10_init_arb_table_index(struct pp_hwmgr *hwmgr)
-{
-	const struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t tmp;
-	int result;
-
-	/* This is a read-modify-write on the first byte of the ARB table.
-	 * The first byte in the SMU73_Discrete_MCArbDramTimingTable structure
-	 * is the field 'current'.
-	 * This solution is ugly, but we never write the whole table only
-	 * individual fields in it.
-	 * In reality this field should not be in that structure
-	 * but in a soft register.
-	 */
-	result = polaris10_read_smc_sram_dword(hwmgr->smumgr,
-			data->arb_table_start, &tmp, data->sram_end);
-
-	if (result)
-		return result;
-
-	tmp &= 0x00FFFFFF;
-	tmp |= ((uint32_t)MC_CG_ARB_FREQ_F1) << 24;
-
-	return polaris10_write_smc_sram_dword(hwmgr->smumgr,
-			data->arb_table_start, tmp, data->sram_end);
-}
-
-static int polaris10_enable_vrhot_gpio_interrupt(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_RegulatorHot))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_EnableVRHotGPIOInterrupt);
-
-	return 0;
-}
-
-static int polaris10_enable_sclk_control(struct pp_hwmgr *hwmgr)
-{
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL,
-			SCLK_PWRMGT_OFF, 0);
-	return 0;
-}
-
-static int polaris10_enable_ulv(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_ulv_parm *ulv = &(data->ulv);
-
-	if (ulv->ulv_supported)
-		return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_EnableULV);
-
-	return 0;
-}
-
-static int polaris10_disable_ulv(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_ulv_parm *ulv = &(data->ulv);
-
-	if (ulv->ulv_supported)
-		return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_DisableULV);
-
-	return 0;
-}
-
-static int polaris10_enable_deep_sleep_master_switch(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkDeepSleep)) {
-		if (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_MASTER_DeepSleep_ON))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to enable Master Deep Sleep switch failed!",
-					return -1);
-	} else {
-		if (smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_MASTER_DeepSleep_OFF)) {
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to disable Master Deep Sleep switch failed!",
-					return -1);
-		}
-	}
-
-	return 0;
-}
-
-static int polaris10_disable_deep_sleep_master_switch(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkDeepSleep)) {
-		if (smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_MASTER_DeepSleep_OFF)) {
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to disable Master Deep Sleep switch failed!",
-					return -1);
-		}
-	}
-
-	return 0;
-}
-
-static int polaris10_enable_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t soft_register_value = 0;
-	uint32_t handshake_disables_offset = data->soft_regs_start
-				+ offsetof(SMU74_SoftRegisters, HandshakeDisables);
-
-	/* enable SCLK dpm */
-	if (!data->sclk_dpm_key_disabled)
-		PP_ASSERT_WITH_CODE(
-		(0 == smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_DPM_Enable)),
-		"Failed to enable SCLK DPM during DPM Start Function!",
-		return -1);
-
-	/* enable MCLK dpm */
-	if (0 == data->mclk_dpm_key_disabled) {
-/* Disable UVD - SMU handshake for MCLK. */
-		soft_register_value = cgs_read_ind_register(hwmgr->device,
-					CGS_IND_REG__SMC, handshake_disables_offset);
-		soft_register_value |= SMU7_UVD_MCLK_HANDSHAKE_DISABLE;
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-				handshake_disables_offset, soft_register_value);
-
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_MCLKDPM_Enable)),
-				"Failed to enable MCLK DPM during DPM Start Function!",
-				return -1);
-
-		PHM_WRITE_FIELD(hwmgr->device, MC_SEQ_CNTL_3, CAC_EN, 0x1);
-
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC0_CNTL, 0x5);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC1_CNTL, 0x5);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_CPL_CNTL, 0x100005);
-		udelay(10);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC0_CNTL, 0x400005);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC1_CNTL, 0x400005);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_CPL_CNTL, 0x500005);
-	}
-
-	return 0;
-}
-
-static int polaris10_start_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	/*enable general power management */
-
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-			GLOBAL_PWRMGT_EN, 1);
-
-	/* enable sclk deep sleep */
-
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL,
-			DYNAMIC_PM_EN, 1);
-
-	/* prepare for PCIE DPM */
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			data->soft_regs_start + offsetof(SMU74_SoftRegisters,
-					VoltageChangeTimeout), 0x1000);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__PCIE,
-			SWRST_COMMAND_1, RESETLC, 0x0);
-/*
-	PP_ASSERT_WITH_CODE(
-			(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					PPSMC_MSG_Voltage_Cntl_Enable)),
-			"Failed to enable voltage DPM during DPM Start Function!",
-			return -1);
-*/
-
-	if (polaris10_enable_sclk_mclk_dpm(hwmgr)) {
-		printk(KERN_ERR "Failed to enable Sclk DPM and Mclk DPM!");
-		return -1;
-	}
-
-	/* enable PCIE dpm */
-	if (0 == data->pcie_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_PCIeDPM_Enable)),
-				"Failed to enable pcie DPM during DPM Start Function!",
-				return -1);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_Falcon_QuickTransition)) {
-		PP_ASSERT_WITH_CODE((0 == smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_EnableACDCGPIOInterrupt)),
-				"Failed to enable AC DC GPIO Interrupt!",
-				);
-	}
-
-	return 0;
-}
-
-static int polaris10_disable_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	/* disable SCLK dpm */
-	if (!data->sclk_dpm_key_disabled)
-		PP_ASSERT_WITH_CODE(
-				(smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_DPM_Disable) == 0),
-				"Failed to disable SCLK DPM!",
-				return -1);
-
-	/* disable MCLK dpm */
-	if (!data->mclk_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_MCLKDPM_Disable) == 0),
-				"Failed to disable MCLK DPM!",
-				return -1);
-	}
-
-	return 0;
-}
-
-static int polaris10_stop_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	/* disable general power management */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-			GLOBAL_PWRMGT_EN, 0);
-	/* disable sclk deep sleep */
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL,
-			DYNAMIC_PM_EN, 0);
-
-	/* disable PCIE dpm */
-	if (!data->pcie_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_PCIeDPM_Disable) == 0),
-				"Failed to disable pcie DPM during DPM Stop Function!",
-				return -1);
-	}
-
-	if (polaris10_disable_sclk_mclk_dpm(hwmgr)) {
-		printk(KERN_ERR "Failed to disable Sclk DPM and Mclk DPM!");
-		return -1;
-	}
-
-	return 0;
-}
-
-static void polaris10_set_dpm_event_sources(struct pp_hwmgr *hwmgr, uint32_t sources)
-{
-	bool protection;
-	enum DPM_EVENT_SRC src;
-
-	switch (sources) {
-	default:
-		printk(KERN_ERR "Unknown throttling event sources.");
-		/* fall through */
-	case 0:
-		protection = false;
-		/* src is unused */
-		break;
-	case (1 << PHM_AutoThrottleSource_Thermal):
-		protection = true;
-		src = DPM_EVENT_SRC_DIGITAL;
-		break;
-	case (1 << PHM_AutoThrottleSource_External):
-		protection = true;
-		src = DPM_EVENT_SRC_EXTERNAL;
-		break;
-	case (1 << PHM_AutoThrottleSource_External) |
-			(1 << PHM_AutoThrottleSource_Thermal):
-		protection = true;
-		src = DPM_EVENT_SRC_DIGITAL_OR_EXTERNAL;
-		break;
-	}
-	/* Order matters - don't enable thermal protection for the wrong source. */
-	if (protection) {
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_CTRL,
-				DPM_EVENT_SRC, src);
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-				THERMAL_PROTECTION_DIS,
-				!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_ThermalController));
-	} else
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-				THERMAL_PROTECTION_DIS, 1);
-}
-
-static int polaris10_enable_auto_throttle_source(struct pp_hwmgr *hwmgr,
-		PHM_AutoThrottleSource source)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (!(data->active_auto_throttle_sources & (1 << source))) {
-		data->active_auto_throttle_sources |= 1 << source;
-		polaris10_set_dpm_event_sources(hwmgr, data->active_auto_throttle_sources);
-	}
-	return 0;
-}
-
-static int polaris10_enable_thermal_auto_throttle(struct pp_hwmgr *hwmgr)
-{
-	return polaris10_enable_auto_throttle_source(hwmgr, PHM_AutoThrottleSource_Thermal);
-}
-
-static int polaris10_disable_auto_throttle_source(struct pp_hwmgr *hwmgr,
-		PHM_AutoThrottleSource source)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (data->active_auto_throttle_sources & (1 << source)) {
-		data->active_auto_throttle_sources &= ~(1 << source);
-		polaris10_set_dpm_event_sources(hwmgr, data->active_auto_throttle_sources);
-	}
-	return 0;
-}
-
-static int polaris10_disable_thermal_auto_throttle(struct pp_hwmgr *hwmgr)
-{
-	return polaris10_disable_auto_throttle_source(hwmgr, PHM_AutoThrottleSource_Thermal);
-}
-
-static int polaris10_pcie_performance_request(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	data->pcie_performance_request = true;
-
-	return 0;
-}
-
-static int polaris10_enable_dpm_tasks(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-	tmp_result = (!polaris10_is_dpm_running(hwmgr)) ? 0 : -1;
-	PP_ASSERT_WITH_CODE(result == 0,
-			"DPM is already running right now, no need to enable DPM!",
-			return 0);
-
-	if (polaris10_voltage_control(hwmgr)) {
-		tmp_result = polaris10_enable_voltage_control(hwmgr);
-		PP_ASSERT_WITH_CODE(tmp_result == 0,
-				"Failed to enable voltage control!",
-				result = tmp_result);
-
-		tmp_result = polaris10_construct_voltage_tables(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-				"Failed to contruct voltage tables!",
-				result = tmp_result);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EngineSpreadSpectrumSupport))
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				GENERAL_PWRMGT, DYN_SPREAD_SPECTRUM_EN, 1);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ThermalController))
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				GENERAL_PWRMGT, THERMAL_PROTECTION_DIS, 0);
-
-	tmp_result = polaris10_program_static_screen_threshold_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to program static screen threshold parameters!",
-			result = tmp_result);
-
-	tmp_result = polaris10_enable_display_gap(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable display gap!", result = tmp_result);
-
-	tmp_result = polaris10_program_voting_clients(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to program voting clients!", result = tmp_result);
-
-	tmp_result = polaris10_process_firmware_header(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to process firmware header!", result = tmp_result);
-
-	tmp_result = polaris10_initial_switch_from_arbf0_to_f1(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to initialize switch from ArbF0 to F1!",
-			result = tmp_result);
-
-	tmp_result = polaris10_init_smc_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to initialize SMC table!", result = tmp_result);
-
-	tmp_result = polaris10_init_arb_table_index(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to initialize ARB table index!", result = tmp_result);
-
-	tmp_result = polaris10_populate_pm_fuses(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to populate PM fuses!", result = tmp_result);
-
-	tmp_result = polaris10_enable_vrhot_gpio_interrupt(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable VR hot GPIO interrupt!", result = tmp_result);
-
-	smum_send_msg_to_smc(hwmgr->smumgr, (PPSMC_Msg)PPSMC_HasDisplay);
-
-	tmp_result = polaris10_enable_sclk_control(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable SCLK control!", result = tmp_result);
-
-	tmp_result = polaris10_enable_smc_voltage_controller(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable voltage control!", result = tmp_result);
-
-	tmp_result = polaris10_enable_ulv(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable ULV!", result = tmp_result);
-
-	tmp_result = polaris10_enable_deep_sleep_master_switch(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable deep sleep master switch!", result = tmp_result);
-
-	tmp_result = polaris10_enable_didt_config(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to enable deep sleep master switch!", result = tmp_result);
-
-	tmp_result = polaris10_start_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to start DPM!", result = tmp_result);
-
-	tmp_result = polaris10_enable_smc_cac(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable SMC CAC!", result = tmp_result);
-
-	tmp_result = polaris10_enable_power_containment(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable power containment!", result = tmp_result);
-
-	tmp_result = polaris10_power_control_set_level(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to power control set level!", result = tmp_result);
-
-	tmp_result = polaris10_enable_thermal_auto_throttle(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable thermal auto throttle!", result = tmp_result);
-
-	tmp_result = polaris10_pcie_performance_request(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"pcie performance request failed!", result = tmp_result);
-
-	return result;
-}
-
-int polaris10_disable_dpm_tasks(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = (polaris10_is_dpm_running(hwmgr)) ? 0 : -1;
-	PP_ASSERT_WITH_CODE(tmp_result == 0,
-			"DPM is not running right now, no need to disable DPM!",
-			return 0);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ThermalController))
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				GENERAL_PWRMGT, THERMAL_PROTECTION_DIS, 1);
-
-	tmp_result = polaris10_disable_power_containment(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable power containment!", result = tmp_result);
-
-	tmp_result = polaris10_disable_smc_cac(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable SMC CAC!", result = tmp_result);
-
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_SPLL_SPREAD_SPECTRUM, SSEN, 0);
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			GENERAL_PWRMGT, DYN_SPREAD_SPECTRUM_EN, 0);
-
-	tmp_result = polaris10_disable_thermal_auto_throttle(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable thermal auto throttle!", result = tmp_result);
-
-	tmp_result = polaris10_stop_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to stop DPM!", result = tmp_result);
-
-	tmp_result = polaris10_disable_deep_sleep_master_switch(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable deep sleep master switch!", result = tmp_result);
-
-	tmp_result = polaris10_disable_ulv(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable ULV!", result = tmp_result);
-
-	tmp_result = polaris10_clear_voting_clients(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to clear voting clients!", result = tmp_result);
-
-	tmp_result = polaris10_reset_to_default(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to reset to default!", result = tmp_result);
-
-	tmp_result = polaris10_force_switch_to_arbf0(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to force to switch arbf0!", result = tmp_result);
-
-	return result;
-}
-
-int polaris10_reset_asic_tasks(struct pp_hwmgr *hwmgr)
-{
-
-	return 0;
-}
-
-static int polaris10_hwmgr_backend_fini(struct pp_hwmgr *hwmgr)
-{
-	return phm_hwmgr_backend_fini(hwmgr);
-}
-
-static int polaris10_set_features_platform_caps(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_DynamicPatchPowerState);
-
-	if (data->mvdd_control == POLARIS10_VOLTAGE_CONTROL_NONE)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_EnableMVDDControl);
-
-	if (data->vddci_control == POLARIS10_VOLTAGE_CONTROL_NONE)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_ControlVDDCI);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			 PHM_PlatformCaps_TablelessHardwareInterface);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EnableSMU7ThermalManagement);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DynamicPowerManagement);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_UnTabledHardwareInterface);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_TablelessHardwareInterface);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_SMC);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_NonABMSupportInPPLib);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_DynamicUVDState);
-
-	/* power tune caps Assume disabled */
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_SQRamping);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_DBRamping);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_TDRamping);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_TCPRamping);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-							PHM_PlatformCaps_CAC);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_RegulatorHot);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_AutomaticDCTransition);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_ODFuzzyFanControlSupport);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_FanSpeedInTableIsRPM);
-
-	if (hwmgr->chip_id == CHIP_POLARIS11)
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_SPLLShutdownSupport);
-	return 0;
-}
-
-static void polaris10_init_dpm_defaults(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	polaris10_initialize_power_tune_defaults(hwmgr);
-
-	data->pcie_gen_performance.max = PP_PCIEGen1;
-	data->pcie_gen_performance.min = PP_PCIEGen3;
-	data->pcie_gen_power_saving.max = PP_PCIEGen1;
-	data->pcie_gen_power_saving.min = PP_PCIEGen3;
-	data->pcie_lane_performance.max = 0;
-	data->pcie_lane_performance.min = 16;
-	data->pcie_lane_power_saving.max = 0;
-	data->pcie_lane_power_saving.min = 16;
-}
-
-/**
-* Get Leakage VDDC based on leakage ID.
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always 0
-*/
-static int polaris10_get_evv_voltages(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint16_t vv_id;
-	uint32_t vddc = 0;
-	uint16_t i, j;
-	uint32_t sclk = 0;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)hwmgr->pptable;
-	struct phm_ppt_v1_clock_voltage_dependency_table *sclk_table =
-			table_info->vdd_dep_on_sclk;
-	int result;
-
-	for (i = 0; i < POLARIS10_MAX_LEAKAGE_COUNT; i++) {
-		vv_id = ATOM_VIRTUAL_VOLTAGE_ID0 + i;
-		if (!phm_get_sclk_for_voltage_evv(hwmgr,
-				table_info->vddc_lookup_table, vv_id, &sclk)) {
-			if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_ClockStretcher)) {
-				for (j = 1; j < sclk_table->count; j++) {
-					if (sclk_table->entries[j].clk == sclk &&
-							sclk_table->entries[j].cks_enable == 0) {
-						sclk += 5000;
-						break;
-					}
-				}
-			}
-
-			if (atomctrl_get_voltage_evv_on_sclk_ai(hwmgr,
-						VOLTAGE_TYPE_VDDC,
-						sclk, vv_id, &vddc) != 0) {
-				printk(KERN_WARNING "failed to retrieving EVV voltage!\n");
-				continue;
-			}
-
-			/* need to make sure vddc is less than 2V or else, it could burn the ASIC.
-			 * real voltage level in unit of 0.01mV */
-			PP_ASSERT_WITH_CODE((vddc < 200000 && vddc != 0),
-					"Invalid VDDC value", result = -EINVAL;);
-
-			/* the voltage should not be zero nor equal to leakage ID */
-			if (vddc != 0 && vddc != vv_id) {
-				data->vddc_leakage.actual_voltage[data->vddc_leakage.count] = (uint16_t)(vddc/100);
-				data->vddc_leakage.leakage_id[data->vddc_leakage.count] = vv_id;
-				data->vddc_leakage.count++;
-			}
-		}
-	}
-
-	return 0;
-}
-
-/**
- * Change virtual leakage voltage to actual value.
- *
- * @param     hwmgr  the address of the powerplay hardware manager.
- * @param     pointer to changing voltage
- * @param     pointer to leakage table
- */
-static void polaris10_patch_with_vdd_leakage(struct pp_hwmgr *hwmgr,
-		uint16_t *voltage, struct polaris10_leakage_voltage *leakage_table)
-{
-	uint32_t index;
-
-	/* search for leakage voltage ID 0xff01 ~ 0xff08 */
-	for (index = 0; index < leakage_table->count; index++) {
-		/* if this voltage matches a leakage voltage ID */
-		/* patch with actual leakage voltage */
-		if (leakage_table->leakage_id[index] == *voltage) {
-			*voltage = leakage_table->actual_voltage[index];
-			break;
-		}
-	}
-
-	if (*voltage > ATOM_VIRTUAL_VOLTAGE_ID0)
-		printk(KERN_ERR "Voltage value looks like a Leakage ID but it's not patched \n");
-}
-
-/**
-* Patch voltage lookup table by EVV leakages.
-*
-* @param     hwmgr  the address of the powerplay hardware manager.
-* @param     pointer to voltage lookup table
-* @param     pointer to leakage table
-* @return     always 0
-*/
-static int polaris10_patch_lookup_table_with_leakage(struct pp_hwmgr *hwmgr,
-		phm_ppt_v1_voltage_lookup_table *lookup_table,
-		struct polaris10_leakage_voltage *leakage_table)
-{
-	uint32_t i;
-
-	for (i = 0; i < lookup_table->count; i++)
-		polaris10_patch_with_vdd_leakage(hwmgr,
-				&lookup_table->entries[i].us_vdd, leakage_table);
-
-	return 0;
-}
-
-static int polaris10_patch_clock_voltage_limits_with_vddc_leakage(
-		struct pp_hwmgr *hwmgr, struct polaris10_leakage_voltage *leakage_table,
-		uint16_t *vddc)
-{
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	polaris10_patch_with_vdd_leakage(hwmgr, (uint16_t *)vddc, leakage_table);
-	hwmgr->dyn_state.max_clock_voltage_on_dc.vddc =
-			table_info->max_clock_voltage_on_dc.vddc;
-	return 0;
-}
-
-static int polaris10_patch_voltage_dependency_tables_with_lookup_table(
-		struct pp_hwmgr *hwmgr)
-{
-	uint8_t entryId;
-	uint8_t voltageId;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	struct phm_ppt_v1_clock_voltage_dependency_table *sclk_table =
-			table_info->vdd_dep_on_sclk;
-	struct phm_ppt_v1_clock_voltage_dependency_table *mclk_table =
-			table_info->vdd_dep_on_mclk;
-	struct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =
-			table_info->mm_dep_table;
-
-	for (entryId = 0; entryId < sclk_table->count; ++entryId) {
-		voltageId = sclk_table->entries[entryId].vddInd;
-		sclk_table->entries[entryId].vddc =
-				table_info->vddc_lookup_table->entries[voltageId].us_vdd;
-	}
-
-	for (entryId = 0; entryId < mclk_table->count; ++entryId) {
-		voltageId = mclk_table->entries[entryId].vddInd;
-		mclk_table->entries[entryId].vddc =
-			table_info->vddc_lookup_table->entries[voltageId].us_vdd;
-	}
-
-	for (entryId = 0; entryId < mm_table->count; ++entryId) {
-		voltageId = mm_table->entries[entryId].vddcInd;
-		mm_table->entries[entryId].vddc =
-			table_info->vddc_lookup_table->entries[voltageId].us_vdd;
-	}
-
-	return 0;
-
-}
-
-static int polaris10_calc_voltage_dependency_tables(struct pp_hwmgr *hwmgr)
-{
-	/* Need to determine if we need calculated voltage. */
-	return 0;
-}
-
-static int polaris10_calc_mm_voltage_dependency_table(struct pp_hwmgr *hwmgr)
-{
-	/* Need to determine if we need calculated voltage from mm table. */
-	return 0;
-}
-
-static int polaris10_sort_lookup_table(struct pp_hwmgr *hwmgr,
-		struct phm_ppt_v1_voltage_lookup_table *lookup_table)
-{
-	uint32_t table_size, i, j;
-	struct phm_ppt_v1_voltage_lookup_record tmp_voltage_lookup_record;
-	table_size = lookup_table->count;
-
-	PP_ASSERT_WITH_CODE(0 != lookup_table->count,
-		"Lookup table is empty", return -EINVAL);
-
-	/* Sorting voltages */
-	for (i = 0; i < table_size - 1; i++) {
-		for (j = i + 1; j > 0; j--) {
-			if (lookup_table->entries[j].us_vdd <
-					lookup_table->entries[j - 1].us_vdd) {
-				tmp_voltage_lookup_record = lookup_table->entries[j - 1];
-				lookup_table->entries[j - 1] = lookup_table->entries[j];
-				lookup_table->entries[j] = tmp_voltage_lookup_record;
-			}
-		}
-	}
-
-	return 0;
-}
-
-static int polaris10_complete_dependency_tables(struct pp_hwmgr *hwmgr)
-{
-	int result = 0;
-	int tmp_result;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	tmp_result = polaris10_patch_lookup_table_with_leakage(hwmgr,
-			table_info->vddc_lookup_table, &(data->vddc_leakage));
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = polaris10_patch_clock_voltage_limits_with_vddc_leakage(hwmgr,
-			&(data->vddc_leakage), &table_info->max_clock_voltage_on_dc.vddc);
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = polaris10_patch_voltage_dependency_tables_with_lookup_table(hwmgr);
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = polaris10_calc_voltage_dependency_tables(hwmgr);
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = polaris10_calc_mm_voltage_dependency_table(hwmgr);
-	if (tmp_result)
-		result = tmp_result;
-
-	tmp_result = polaris10_sort_lookup_table(hwmgr, table_info->vddc_lookup_table);
-	if (tmp_result)
-		result = tmp_result;
-
-	return result;
-}
-
-static int polaris10_set_private_data_based_on_pptable(struct pp_hwmgr *hwmgr)
-{
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	struct phm_ppt_v1_clock_voltage_dependency_table *allowed_sclk_vdd_table =
-						table_info->vdd_dep_on_sclk;
-	struct phm_ppt_v1_clock_voltage_dependency_table *allowed_mclk_vdd_table =
-						table_info->vdd_dep_on_mclk;
-
-	PP_ASSERT_WITH_CODE(allowed_sclk_vdd_table != NULL,
-		"VDD dependency on SCLK table is missing.	\
-		This table is mandatory", return -EINVAL);
-	PP_ASSERT_WITH_CODE(allowed_sclk_vdd_table->count >= 1,
-		"VDD dependency on SCLK table has to have is missing.	\
-		This table is mandatory", return -EINVAL);
-
-	PP_ASSERT_WITH_CODE(allowed_mclk_vdd_table != NULL,
-		"VDD dependency on MCLK table is missing.	\
-		This table is mandatory", return -EINVAL);
-	PP_ASSERT_WITH_CODE(allowed_mclk_vdd_table->count >= 1,
-		"VDD dependency on MCLK table has to have is missing.	 \
-		This table is mandatory", return -EINVAL);
-
-	table_info->max_clock_voltage_on_ac.sclk =
-		allowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].clk;
-	table_info->max_clock_voltage_on_ac.mclk =
-		allowed_mclk_vdd_table->entries[allowed_mclk_vdd_table->count - 1].clk;
-	table_info->max_clock_voltage_on_ac.vddc =
-		allowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].vddc;
-	table_info->max_clock_voltage_on_ac.vddci =
-		allowed_mclk_vdd_table->entries[allowed_mclk_vdd_table->count - 1].vddci;
-
-	hwmgr->dyn_state.max_clock_voltage_on_ac.sclk = table_info->max_clock_voltage_on_ac.sclk;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.mclk = table_info->max_clock_voltage_on_ac.mclk;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.vddc = table_info->max_clock_voltage_on_ac.vddc;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.vddci =table_info->max_clock_voltage_on_ac.vddci;
-
-	return 0;
-}
-
-static int polaris10_patch_voltage_workaround(struct pp_hwmgr *hwmgr)
-{
-	struct phm_ppt_v1_information *table_info =
-		       (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table =
-			table_info->vdd_dep_on_mclk;
-	struct phm_ppt_v1_voltage_lookup_table *lookup_table =
-			table_info->vddc_lookup_table;
-	uint32_t i;
-	uint32_t hw_revision, sub_vendor_id, sub_sys_id;
-	struct cgs_system_info sys_info = {0};
-
-	sys_info.size = sizeof(struct cgs_system_info);
-
-	sys_info.info_id = CGS_SYSTEM_INFO_PCIE_REV;
-	cgs_query_system_info(hwmgr->device, &sys_info);
-	hw_revision = (uint32_t)sys_info.value;
-
-	sys_info.info_id = CGS_SYSTEM_INFO_PCIE_SUB_SYS_ID;
-	cgs_query_system_info(hwmgr->device, &sys_info);
-	sub_sys_id = (uint32_t)sys_info.value;
-
-	sys_info.info_id = CGS_SYSTEM_INFO_PCIE_SUB_SYS_VENDOR_ID;
-	cgs_query_system_info(hwmgr->device, &sys_info);
-	sub_vendor_id = (uint32_t)sys_info.value;
-
-	if (hwmgr->chip_id == CHIP_POLARIS10 && hw_revision == 0xC7 &&
-			((sub_sys_id == 0xb37 && sub_vendor_id == 0x1002) ||
-		    (sub_sys_id == 0x4a8 && sub_vendor_id == 0x1043) ||
-		    (sub_sys_id == 0x9480 && sub_vendor_id == 0x1682))) {
-		if (lookup_table->entries[dep_mclk_table->entries[dep_mclk_table->count-1].vddInd].us_vdd >= 1000)
-			return 0;
-
-		for (i = 0; i < lookup_table->count; i++) {
-			if (lookup_table->entries[i].us_vdd < 0xff01 && lookup_table->entries[i].us_vdd >= 1000) {
-				dep_mclk_table->entries[dep_mclk_table->count-1].vddInd = (uint8_t) i;
-				return 0;
-			}
-		}
-	}
-	return 0;
-}
-
-
-static int polaris10_hwmgr_backend_init(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data;
-	struct pp_atomctrl_gpio_pin_assignment gpio_pin_assignment;
-	uint32_t temp_reg;
-	int result;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	data = kzalloc(sizeof(struct polaris10_hwmgr), GFP_KERNEL);
-	if (data == NULL)
-		return -ENOMEM;
-
-	hwmgr->backend = data;
-
-	data->dll_default_on = false;
-	data->sram_end = SMC_RAM_END;
-	data->mclk_dpm0_activity_target = 0xa;
-	data->disable_dpm_mask = 0xFF;
-	data->static_screen_threshold = PPPOLARIS10_STATICSCREENTHRESHOLD_DFLT;
-	data->static_screen_threshold_unit = PPPOLARIS10_STATICSCREENTHRESHOLD_DFLT;
-	data->activity_target[0] = PPPOLARIS10_TARGETACTIVITY_DFLT;
-	data->activity_target[1] = PPPOLARIS10_TARGETACTIVITY_DFLT;
-	data->activity_target[2] = PPPOLARIS10_TARGETACTIVITY_DFLT;
-	data->activity_target[3] = PPPOLARIS10_TARGETACTIVITY_DFLT;
-	data->activity_target[4] = PPPOLARIS10_TARGETACTIVITY_DFLT;
-	data->activity_target[5] = PPPOLARIS10_TARGETACTIVITY_DFLT;
-	data->activity_target[6] = PPPOLARIS10_TARGETACTIVITY_DFLT;
-	data->activity_target[7] = PPPOLARIS10_TARGETACTIVITY_DFLT;
-
-	data->voting_rights_clients0 = PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT0;
-	data->voting_rights_clients1 = PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT1;
-	data->voting_rights_clients2 = PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT2;
-	data->voting_rights_clients3 = PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT3;
-	data->voting_rights_clients4 = PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT4;
-	data->voting_rights_clients5 = PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT5;
-	data->voting_rights_clients6 = PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT6;
-	data->voting_rights_clients7 = PPPOLARIS10_VOTINGRIGHTSCLIENTS_DFLT7;
-
-	data->vddc_vddci_delta = VDDC_VDDCI_DELTA;
-
-	data->mclk_activity_target = PPPOLARIS10_MCLK_TARGETACTIVITY_DFLT;
-
-	/* need to set voltage control types before EVV patching */
-	data->voltage_control = POLARIS10_VOLTAGE_CONTROL_NONE;
-	data->vddci_control = POLARIS10_VOLTAGE_CONTROL_NONE;
-	data->mvdd_control = POLARIS10_VOLTAGE_CONTROL_NONE;
-
-	data->enable_tdc_limit_feature = true;
-	data->enable_pkg_pwr_tracking_feature = true;
-	data->force_pcie_gen = PP_PCIEGenInvalid;
-	data->mclk_stutter_mode_threshold = 40000;
-
-	if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-			VOLTAGE_TYPE_VDDC, VOLTAGE_OBJ_SVID2))
-		data->voltage_control = POLARIS10_VOLTAGE_CONTROL_BY_SVID2;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EnableMVDDControl)) {
-		if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-				VOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_GPIO_LUT))
-			data->mvdd_control = POLARIS10_VOLTAGE_CONTROL_BY_GPIO;
-		else if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-				VOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_SVID2))
-			data->mvdd_control = POLARIS10_VOLTAGE_CONTROL_BY_SVID2;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ControlVDDCI)) {
-		if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-				VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_GPIO_LUT))
-			data->vddci_control = POLARIS10_VOLTAGE_CONTROL_BY_GPIO;
-		else if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-				VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_SVID2))
-			data->vddci_control = POLARIS10_VOLTAGE_CONTROL_BY_SVID2;
-	}
-
-	if (table_info->cac_dtp_table->usClockStretchAmount != 0)
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_ClockStretcher);
-
-	polaris10_set_features_platform_caps(hwmgr);
-
-	polaris10_patch_voltage_workaround(hwmgr);
-	polaris10_init_dpm_defaults(hwmgr);
-
-	/* Get leakage voltage based on leakage ID. */
-	result = polaris10_get_evv_voltages(hwmgr);
-
-	if (result) {
-		printk("Get EVV Voltage Failed.  Abort Driver loading!\n");
-		return -1;
-	}
-
-	polaris10_complete_dependency_tables(hwmgr);
-	polaris10_set_private_data_based_on_pptable(hwmgr);
-
-	/* Initalize Dynamic State Adjustment Rule Settings */
-	result = phm_initializa_dynamic_state_adjustment_rule_settings(hwmgr);
-
-	if (0 == result) {
-		struct cgs_system_info sys_info = {0};
-
-		data->is_tlu_enabled = false;
-
-		hwmgr->platform_descriptor.hardwareActivityPerformanceLevels =
-							POLARIS10_MAX_HARDWARE_POWERLEVELS;
-		hwmgr->platform_descriptor.hardwarePerformanceLevels = 2;
-		hwmgr->platform_descriptor.minimumClocksReductionPercentage = 50;
-
-
-		if (atomctrl_get_pp_assign_pin(hwmgr, VDDC_PCC_GPIO_PINID, &gpio_pin_assignment)) {
-			temp_reg = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCNB_PWRMGT_CNTL);
-			switch (gpio_pin_assignment.uc_gpio_pin_bit_shift) {
-			case 0:
-				temp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, GNB_SLOW_MODE, 0x1);
-				break;
-			case 1:
-				temp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, GNB_SLOW_MODE, 0x2);
-				break;
-			case 2:
-				temp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, GNB_SLOW, 0x1);
-				break;
-			case 3:
-				temp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, FORCE_NB_PS1, 0x1);
-				break;
-			case 4:
-				temp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, DPM_ENABLED, 0x1);
-				break;
-			default:
-				PP_ASSERT_WITH_CODE(0,
-				"Failed to setup PCC HW register! Wrong GPIO assigned for VDDC_PCC_GPIO_PINID!",
-				);
-				break;
-			}
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCNB_PWRMGT_CNTL, temp_reg);
-		}
-
-		if (table_info->cac_dtp_table->usDefaultTargetOperatingTemp != 0 &&
-			hwmgr->thermal_controller.advanceFanControlParameters.ucFanControlMode) {
-			hwmgr->thermal_controller.advanceFanControlParameters.usFanPWMMinLimit =
-				(uint16_t)hwmgr->thermal_controller.advanceFanControlParameters.ucMinimumPWMLimit;
-
-			hwmgr->thermal_controller.advanceFanControlParameters.usFanPWMMaxLimit =
-				(uint16_t)hwmgr->thermal_controller.advanceFanControlParameters.usDefaultMaxFanPWM;
-
-			hwmgr->thermal_controller.advanceFanControlParameters.usFanPWMStep = 1;
-
-			hwmgr->thermal_controller.advanceFanControlParameters.usFanRPMMaxLimit = 100;
-
-			hwmgr->thermal_controller.advanceFanControlParameters.usFanRPMMinLimit =
-				(uint16_t)hwmgr->thermal_controller.advanceFanControlParameters.ucMinimumPWMLimit;
-
-			hwmgr->thermal_controller.advanceFanControlParameters.usFanRPMStep = 1;
-
-			table_info->cac_dtp_table->usDefaultTargetOperatingTemp = (table_info->cac_dtp_table->usDefaultTargetOperatingTemp >= 50) ?
-									(table_info->cac_dtp_table->usDefaultTargetOperatingTemp -50) : 0;
-
-			table_info->cac_dtp_table->usOperatingTempMaxLimit = table_info->cac_dtp_table->usDefaultTargetOperatingTemp;
-			table_info->cac_dtp_table->usOperatingTempStep = 1;
-			table_info->cac_dtp_table->usOperatingTempHyst = 1;
-
-			hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanPWM =
-				       hwmgr->thermal_controller.advanceFanControlParameters.usDefaultMaxFanPWM;
-
-			hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanRPM =
-				       hwmgr->thermal_controller.advanceFanControlParameters.usDefaultMaxFanRPM;
-
-			hwmgr->dyn_state.cac_dtp_table->usOperatingTempMinLimit =
-				       table_info->cac_dtp_table->usOperatingTempMinLimit;
-
-			hwmgr->dyn_state.cac_dtp_table->usOperatingTempMaxLimit =
-				       table_info->cac_dtp_table->usOperatingTempMaxLimit;
-
-			hwmgr->dyn_state.cac_dtp_table->usDefaultTargetOperatingTemp =
-				       table_info->cac_dtp_table->usDefaultTargetOperatingTemp;
-
-			hwmgr->dyn_state.cac_dtp_table->usOperatingTempStep =
-				       table_info->cac_dtp_table->usOperatingTempStep;
-
-			hwmgr->dyn_state.cac_dtp_table->usTargetOperatingTemp =
-				       table_info->cac_dtp_table->usTargetOperatingTemp;
-		}
-
-		sys_info.size = sizeof(struct cgs_system_info);
-		sys_info.info_id = CGS_SYSTEM_INFO_PCIE_GEN_INFO;
-		result = cgs_query_system_info(hwmgr->device, &sys_info);
-		if (result)
-			data->pcie_gen_cap = AMDGPU_DEFAULT_PCIE_GEN_MASK;
-		else
-			data->pcie_gen_cap = (uint32_t)sys_info.value;
-		if (data->pcie_gen_cap & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)
-			data->pcie_spc_cap = 20;
-		sys_info.size = sizeof(struct cgs_system_info);
-		sys_info.info_id = CGS_SYSTEM_INFO_PCIE_MLW;
-		result = cgs_query_system_info(hwmgr->device, &sys_info);
-		if (result)
-			data->pcie_lane_cap = AMDGPU_DEFAULT_PCIE_MLW_MASK;
-		else
-			data->pcie_lane_cap = (uint32_t)sys_info.value;
-
-		hwmgr->platform_descriptor.vbiosInterruptId = 0x20000400; /* IRQ_SOURCE1_SW_INT */
-/* The true clock step depends on the frequency, typically 4.5 or 9 MHz. Here we use 5. */
-		hwmgr->platform_descriptor.clockStep.engineClock = 500;
-		hwmgr->platform_descriptor.clockStep.memoryClock = 500;
-	} else {
-		/* Ignore return value in here, we are cleaning up a mess. */
-		polaris10_hwmgr_backend_fini(hwmgr);
-	}
-
-	return 0;
-}
-
-static int polaris10_force_dpm_highest(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t level, tmp;
-
-	if (!data->pcie_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.pcie_dpm_enable_mask) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.pcie_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++;
-
-			if (level)
-				smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-						PPSMC_MSG_PCIeDPM_ForceLevel, level);
-		}
-	}
-
-	if (!data->sclk_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.sclk_dpm_enable_mask) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.sclk_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++;
-
-			if (level)
-				smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-						PPSMC_MSG_SCLKDPM_SetEnabledMask,
-						(1 << level));
-		}
-	}
-
-	if (!data->mclk_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.mclk_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++;
-
-			if (level)
-				smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-						PPSMC_MSG_MCLKDPM_SetEnabledMask,
-						(1 << level));
-		}
-	}
-
-	return 0;
-}
-
-static int polaris10_upload_dpm_level_enable_mask(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	phm_apply_dal_min_voltage_request(hwmgr);
-
-	if (!data->sclk_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.sclk_dpm_enable_mask)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_SCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.sclk_dpm_enable_mask);
-	}
-
-	if (!data->mclk_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_MCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.mclk_dpm_enable_mask);
-	}
-
-	return 0;
-}
-
-static int polaris10_unforce_dpm_levels(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (!polaris10_is_dpm_running(hwmgr))
-		return -EINVAL;
-
-	if (!data->pcie_dpm_key_disabled) {
-		smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_PCIeDPM_UnForceLevel);
-	}
-
-	return polaris10_upload_dpm_level_enable_mask(hwmgr);
-}
-
-static int polaris10_force_dpm_lowest(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data =
-			(struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t level;
-
-	if (!data->sclk_dpm_key_disabled)
-		if (data->dpm_level_enable_mask.sclk_dpm_enable_mask) {
-			level = phm_get_lowest_enabled_level(hwmgr,
-							      data->dpm_level_enable_mask.sclk_dpm_enable_mask);
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-							    PPSMC_MSG_SCLKDPM_SetEnabledMask,
-							    (1 << level));
-
-	}
-
-	if (!data->mclk_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask) {
-			level = phm_get_lowest_enabled_level(hwmgr,
-							      data->dpm_level_enable_mask.mclk_dpm_enable_mask);
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-							    PPSMC_MSG_MCLKDPM_SetEnabledMask,
-							    (1 << level));
-		}
-	}
-
-	if (!data->pcie_dpm_key_disabled) {
-		if (data->dpm_level_enable_mask.pcie_dpm_enable_mask) {
-			level = phm_get_lowest_enabled_level(hwmgr,
-							      data->dpm_level_enable_mask.pcie_dpm_enable_mask);
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-							    PPSMC_MSG_PCIeDPM_ForceLevel,
-							    (level));
-		}
-	}
-
-	return 0;
-
-}
-static int polaris10_force_dpm_level(struct pp_hwmgr *hwmgr,
-				enum amd_dpm_forced_level level)
-{
-	int ret = 0;
-
-	switch (level) {
-	case AMD_DPM_FORCED_LEVEL_HIGH:
-		ret = polaris10_force_dpm_highest(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	case AMD_DPM_FORCED_LEVEL_LOW:
-		ret = polaris10_force_dpm_lowest(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	case AMD_DPM_FORCED_LEVEL_AUTO:
-		ret = polaris10_unforce_dpm_levels(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	default:
-		break;
-	}
-
-	hwmgr->dpm_level = level;
-
-	return ret;
-}
-
-static int polaris10_get_power_state_size(struct pp_hwmgr *hwmgr)
-{
-	return sizeof(struct polaris10_power_state);
-}
-
-
-static int polaris10_apply_state_adjust_rules(struct pp_hwmgr *hwmgr,
-				struct pp_power_state *request_ps,
-			const struct pp_power_state *current_ps)
-{
-
-	struct polaris10_power_state *polaris10_ps =
-				cast_phw_polaris10_power_state(&request_ps->hardware);
-	uint32_t sclk;
-	uint32_t mclk;
-	struct PP_Clocks minimum_clocks = {0};
-	bool disable_mclk_switching;
-	bool disable_mclk_switching_for_frame_lock;
-	struct cgs_display_info info = {0};
-	const struct phm_clock_and_voltage_limits *max_limits;
-	uint32_t i;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	int32_t count;
-	int32_t stable_pstate_sclk = 0, stable_pstate_mclk = 0;
-
-	data->battery_state = (PP_StateUILabel_Battery ==
-			request_ps->classification.ui_label);
-
-	PP_ASSERT_WITH_CODE(polaris10_ps->performance_level_count == 2,
-				 "VI should always have 2 performance levels",
-				);
-
-	max_limits = (PP_PowerSource_AC == hwmgr->power_source) ?
-			&(hwmgr->dyn_state.max_clock_voltage_on_ac) :
-			&(hwmgr->dyn_state.max_clock_voltage_on_dc);
-
-	/* Cap clock DPM tables at DC MAX if it is in DC. */
-	if (PP_PowerSource_DC == hwmgr->power_source) {
-		for (i = 0; i < polaris10_ps->performance_level_count; i++) {
-			if (polaris10_ps->performance_levels[i].memory_clock > max_limits->mclk)
-				polaris10_ps->performance_levels[i].memory_clock = max_limits->mclk;
-			if (polaris10_ps->performance_levels[i].engine_clock > max_limits->sclk)
-				polaris10_ps->performance_levels[i].engine_clock = max_limits->sclk;
-		}
-	}
-
-	polaris10_ps->vce_clks.evclk = hwmgr->vce_arbiter.evclk;
-	polaris10_ps->vce_clks.ecclk = hwmgr->vce_arbiter.ecclk;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	/*TO DO result = PHM_CheckVBlankTime(hwmgr, &vblankTooShort);*/
-
-	/* TO DO GetMinClockSettings(hwmgr->pPECI, &minimum_clocks); */
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_StablePState)) {
-		max_limits = &(hwmgr->dyn_state.max_clock_voltage_on_ac);
-		stable_pstate_sclk = (max_limits->sclk * 75) / 100;
-
-		for (count = table_info->vdd_dep_on_sclk->count - 1;
-				count >= 0; count--) {
-			if (stable_pstate_sclk >=
-					table_info->vdd_dep_on_sclk->entries[count].clk) {
-				stable_pstate_sclk =
-						table_info->vdd_dep_on_sclk->entries[count].clk;
-				break;
-			}
-		}
-
-		if (count < 0)
-			stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[0].clk;
-
-		stable_pstate_mclk = max_limits->mclk;
-
-		minimum_clocks.engineClock = stable_pstate_sclk;
-		minimum_clocks.memoryClock = stable_pstate_mclk;
-	}
-
-	if (minimum_clocks.engineClock < hwmgr->gfx_arbiter.sclk)
-		minimum_clocks.engineClock = hwmgr->gfx_arbiter.sclk;
-
-	if (minimum_clocks.memoryClock < hwmgr->gfx_arbiter.mclk)
-		minimum_clocks.memoryClock = hwmgr->gfx_arbiter.mclk;
-
-	polaris10_ps->sclk_threshold = hwmgr->gfx_arbiter.sclk_threshold;
-
-	if (0 != hwmgr->gfx_arbiter.sclk_over_drive) {
-		PP_ASSERT_WITH_CODE((hwmgr->gfx_arbiter.sclk_over_drive <=
-				hwmgr->platform_descriptor.overdriveLimit.engineClock),
-				"Overdrive sclk exceeds limit",
-				hwmgr->gfx_arbiter.sclk_over_drive =
-						hwmgr->platform_descriptor.overdriveLimit.engineClock);
-
-		if (hwmgr->gfx_arbiter.sclk_over_drive >= hwmgr->gfx_arbiter.sclk)
-			polaris10_ps->performance_levels[1].engine_clock =
-					hwmgr->gfx_arbiter.sclk_over_drive;
-	}
-
-	if (0 != hwmgr->gfx_arbiter.mclk_over_drive) {
-		PP_ASSERT_WITH_CODE((hwmgr->gfx_arbiter.mclk_over_drive <=
-				hwmgr->platform_descriptor.overdriveLimit.memoryClock),
-				"Overdrive mclk exceeds limit",
-				hwmgr->gfx_arbiter.mclk_over_drive =
-						hwmgr->platform_descriptor.overdriveLimit.memoryClock);
-
-		if (hwmgr->gfx_arbiter.mclk_over_drive >= hwmgr->gfx_arbiter.mclk)
-			polaris10_ps->performance_levels[1].memory_clock =
-					hwmgr->gfx_arbiter.mclk_over_drive;
-	}
-
-	disable_mclk_switching_for_frame_lock = phm_cap_enabled(
-				    hwmgr->platform_descriptor.platformCaps,
-				    PHM_PlatformCaps_DisableMclkSwitchingForFrameLock);
-
-
-	disable_mclk_switching = (1 < info.display_count) ||
-				    disable_mclk_switching_for_frame_lock;
-
-	sclk = polaris10_ps->performance_levels[0].engine_clock;
-	mclk = polaris10_ps->performance_levels[0].memory_clock;
-
-	if (disable_mclk_switching)
-		mclk = polaris10_ps->performance_levels
-		[polaris10_ps->performance_level_count - 1].memory_clock;
-
-	if (sclk < minimum_clocks.engineClock)
-		sclk = (minimum_clocks.engineClock > max_limits->sclk) ?
-				max_limits->sclk : minimum_clocks.engineClock;
-
-	if (mclk < minimum_clocks.memoryClock)
-		mclk = (minimum_clocks.memoryClock > max_limits->mclk) ?
-				max_limits->mclk : minimum_clocks.memoryClock;
-
-	polaris10_ps->performance_levels[0].engine_clock = sclk;
-	polaris10_ps->performance_levels[0].memory_clock = mclk;
-
-	polaris10_ps->performance_levels[1].engine_clock =
-		(polaris10_ps->performance_levels[1].engine_clock >=
-				polaris10_ps->performance_levels[0].engine_clock) ?
-						polaris10_ps->performance_levels[1].engine_clock :
-						polaris10_ps->performance_levels[0].engine_clock;
-
-	if (disable_mclk_switching) {
-		if (mclk < polaris10_ps->performance_levels[1].memory_clock)
-			mclk = polaris10_ps->performance_levels[1].memory_clock;
-
-		polaris10_ps->performance_levels[0].memory_clock = mclk;
-		polaris10_ps->performance_levels[1].memory_clock = mclk;
-	} else {
-		if (polaris10_ps->performance_levels[1].memory_clock <
-				polaris10_ps->performance_levels[0].memory_clock)
-			polaris10_ps->performance_levels[1].memory_clock =
-					polaris10_ps->performance_levels[0].memory_clock;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_StablePState)) {
-		for (i = 0; i < polaris10_ps->performance_level_count; i++) {
-			polaris10_ps->performance_levels[i].engine_clock = stable_pstate_sclk;
-			polaris10_ps->performance_levels[i].memory_clock = stable_pstate_mclk;
-			polaris10_ps->performance_levels[i].pcie_gen = data->pcie_gen_performance.max;
-			polaris10_ps->performance_levels[i].pcie_lane = data->pcie_gen_performance.max;
-		}
-	}
-	return 0;
-}
-
-
-static int polaris10_dpm_get_mclk(struct pp_hwmgr *hwmgr, bool low)
-{
-	struct pp_power_state  *ps;
-	struct polaris10_power_state  *polaris10_ps;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	polaris10_ps = cast_phw_polaris10_power_state(&ps->hardware);
-
-	if (low)
-		return polaris10_ps->performance_levels[0].memory_clock;
-	else
-		return polaris10_ps->performance_levels
-				[polaris10_ps->performance_level_count-1].memory_clock;
-}
-
-static int polaris10_dpm_get_sclk(struct pp_hwmgr *hwmgr, bool low)
-{
-	struct pp_power_state  *ps;
-	struct polaris10_power_state  *polaris10_ps;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	polaris10_ps = cast_phw_polaris10_power_state(&ps->hardware);
-
-	if (low)
-		return polaris10_ps->performance_levels[0].engine_clock;
-	else
-		return polaris10_ps->performance_levels
-				[polaris10_ps->performance_level_count-1].engine_clock;
-}
-
-static int polaris10_dpm_patch_boot_state(struct pp_hwmgr *hwmgr,
-					struct pp_hw_power_state *hw_ps)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_power_state *ps = (struct polaris10_power_state *)hw_ps;
-	ATOM_FIRMWARE_INFO_V2_2 *fw_info;
-	uint16_t size;
-	uint8_t frev, crev;
-	int index = GetIndexIntoMasterTable(DATA, FirmwareInfo);
-
-	/* First retrieve the Boot clocks and VDDC from the firmware info table.
-	 * We assume here that fw_info is unchanged if this call fails.
-	 */
-	fw_info = (ATOM_FIRMWARE_INFO_V2_2 *)cgs_atom_get_data_table(
-			hwmgr->device, index,
-			&size, &frev, &crev);
-	if (!fw_info)
-		/* During a test, there is no firmware info table. */
-		return 0;
-
-	/* Patch the state. */
-	data->vbios_boot_state.sclk_bootup_value =
-			le32_to_cpu(fw_info->ulDefaultEngineClock);
-	data->vbios_boot_state.mclk_bootup_value =
-			le32_to_cpu(fw_info->ulDefaultMemoryClock);
-	data->vbios_boot_state.mvdd_bootup_value =
-			le16_to_cpu(fw_info->usBootUpMVDDCVoltage);
-	data->vbios_boot_state.vddc_bootup_value =
-			le16_to_cpu(fw_info->usBootUpVDDCVoltage);
-	data->vbios_boot_state.vddci_bootup_value =
-			le16_to_cpu(fw_info->usBootUpVDDCIVoltage);
-	data->vbios_boot_state.pcie_gen_bootup_value =
-			phm_get_current_pcie_speed(hwmgr);
-
-	data->vbios_boot_state.pcie_lane_bootup_value =
-			(uint16_t)phm_get_current_pcie_lane_number(hwmgr);
-
-	/* set boot power state */
-	ps->performance_levels[0].memory_clock = data->vbios_boot_state.mclk_bootup_value;
-	ps->performance_levels[0].engine_clock = data->vbios_boot_state.sclk_bootup_value;
-	ps->performance_levels[0].pcie_gen = data->vbios_boot_state.pcie_gen_bootup_value;
-	ps->performance_levels[0].pcie_lane = data->vbios_boot_state.pcie_lane_bootup_value;
-
-	return 0;
-}
-
-static int polaris10_get_pp_table_entry_callback_func(struct pp_hwmgr *hwmgr,
-		void *state, struct pp_power_state *power_state,
-		void *pp_table, uint32_t classification_flag)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_power_state  *polaris10_power_state =
-			(struct polaris10_power_state *)(&(power_state->hardware));
-	struct polaris10_performance_level *performance_level;
-	ATOM_Tonga_State *state_entry = (ATOM_Tonga_State *)state;
-	ATOM_Tonga_POWERPLAYTABLE *powerplay_table =
-			(ATOM_Tonga_POWERPLAYTABLE *)pp_table;
-	PPTable_Generic_SubTable_Header *sclk_dep_table =
-			(PPTable_Generic_SubTable_Header *)
-			(((unsigned long)powerplay_table) +
-				le16_to_cpu(powerplay_table->usSclkDependencyTableOffset));
-
-	ATOM_Tonga_MCLK_Dependency_Table *mclk_dep_table =
-			(ATOM_Tonga_MCLK_Dependency_Table *)
-			(((unsigned long)powerplay_table) +
-				le16_to_cpu(powerplay_table->usMclkDependencyTableOffset));
-
-	/* The following fields are not initialized here: id orderedList allStatesList */
-	power_state->classification.ui_label =
-			(le16_to_cpu(state_entry->usClassification) &
-			ATOM_PPLIB_CLASSIFICATION_UI_MASK) >>
-			ATOM_PPLIB_CLASSIFICATION_UI_SHIFT;
-	power_state->classification.flags = classification_flag;
-	/* NOTE: There is a classification2 flag in BIOS that is not being used right now */
-
-	power_state->classification.temporary_state = false;
-	power_state->classification.to_be_deleted = false;
-
-	power_state->validation.disallowOnDC =
-			(0 != (le32_to_cpu(state_entry->ulCapsAndSettings) &
-					ATOM_Tonga_DISALLOW_ON_DC));
-
-	power_state->pcie.lanes = 0;
-
-	power_state->display.disableFrameModulation = false;
-	power_state->display.limitRefreshrate = false;
-	power_state->display.enableVariBright =
-			(0 != (le32_to_cpu(state_entry->ulCapsAndSettings) &
-					ATOM_Tonga_ENABLE_VARIBRIGHT));
-
-	power_state->validation.supportedPowerLevels = 0;
-	power_state->uvd_clocks.VCLK = 0;
-	power_state->uvd_clocks.DCLK = 0;
-	power_state->temperatures.min = 0;
-	power_state->temperatures.max = 0;
-
-	performance_level = &(polaris10_power_state->performance_levels
-			[polaris10_power_state->performance_level_count++]);
-
-	PP_ASSERT_WITH_CODE(
-			(polaris10_power_state->performance_level_count < SMU74_MAX_LEVELS_GRAPHICS),
-			"Performance levels exceeds SMC limit!",
-			return -1);
-
-	PP_ASSERT_WITH_CODE(
-			(polaris10_power_state->performance_level_count <=
-					hwmgr->platform_descriptor.hardwareActivityPerformanceLevels),
-			"Performance levels exceeds Driver limit!",
-			return -1);
-
-	/* Performance levels are arranged from low to high. */
-	performance_level->memory_clock = mclk_dep_table->entries
-			[state_entry->ucMemoryClockIndexLow].ulMclk;
-	if (sclk_dep_table->ucRevId == 0)
-		performance_level->engine_clock = ((ATOM_Tonga_SCLK_Dependency_Table *)sclk_dep_table)->entries
-			[state_entry->ucEngineClockIndexLow].ulSclk;
-	else if (sclk_dep_table->ucRevId == 1)
-		performance_level->engine_clock = ((ATOM_Polaris_SCLK_Dependency_Table *)sclk_dep_table)->entries
-			[state_entry->ucEngineClockIndexLow].ulSclk;
-	performance_level->pcie_gen = get_pcie_gen_support(data->pcie_gen_cap,
-			state_entry->ucPCIEGenLow);
-	performance_level->pcie_lane = get_pcie_lane_support(data->pcie_lane_cap,
-			state_entry->ucPCIELaneHigh);
-
-	performance_level = &(polaris10_power_state->performance_levels
-			[polaris10_power_state->performance_level_count++]);
-	performance_level->memory_clock = mclk_dep_table->entries
-			[state_entry->ucMemoryClockIndexHigh].ulMclk;
-
-	if (sclk_dep_table->ucRevId == 0)
-		performance_level->engine_clock = ((ATOM_Tonga_SCLK_Dependency_Table *)sclk_dep_table)->entries
-			[state_entry->ucEngineClockIndexHigh].ulSclk;
-	else if (sclk_dep_table->ucRevId == 1)
-		performance_level->engine_clock = ((ATOM_Polaris_SCLK_Dependency_Table *)sclk_dep_table)->entries
-			[state_entry->ucEngineClockIndexHigh].ulSclk;
-
-	performance_level->pcie_gen = get_pcie_gen_support(data->pcie_gen_cap,
-			state_entry->ucPCIEGenHigh);
-	performance_level->pcie_lane = get_pcie_lane_support(data->pcie_lane_cap,
-			state_entry->ucPCIELaneHigh);
-
-	return 0;
-}
-
-static int polaris10_get_pp_table_entry(struct pp_hwmgr *hwmgr,
-		unsigned long entry_index, struct pp_power_state *state)
-{
-	int result;
-	struct polaris10_power_state *ps;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table =
-			table_info->vdd_dep_on_mclk;
-
-	state->hardware.magic = PHM_VIslands_Magic;
-
-	ps = (struct polaris10_power_state *)(&state->hardware);
-
-	result = get_powerplay_table_entry_v1_0(hwmgr, entry_index, state,
-			polaris10_get_pp_table_entry_callback_func);
-
-	/* This is the earliest time we have all the dependency table and the VBIOS boot state
-	 * as PP_Tables_GetPowerPlayTableEntry retrieves the VBIOS boot state
-	 * if there is only one VDDCI/MCLK level, check if it's the same as VBIOS boot state
-	 */
-	if (dep_mclk_table != NULL && dep_mclk_table->count == 1) {
-		if (dep_mclk_table->entries[0].clk !=
-				data->vbios_boot_state.mclk_bootup_value)
-			printk(KERN_ERR "Single MCLK entry VDDCI/MCLK dependency table "
-					"does not match VBIOS boot MCLK level");
-		if (dep_mclk_table->entries[0].vddci !=
-				data->vbios_boot_state.vddci_bootup_value)
-			printk(KERN_ERR "Single VDDCI entry VDDCI/MCLK dependency table "
-					"does not match VBIOS boot VDDCI level");
-	}
-
-	/* set DC compatible flag if this state supports DC */
-	if (!state->validation.disallowOnDC)
-		ps->dc_compatible = true;
-
-	if (state->classification.flags & PP_StateClassificationFlag_ACPI)
-		data->acpi_pcie_gen = ps->performance_levels[0].pcie_gen;
-
-	ps->uvd_clks.vclk = state->uvd_clocks.VCLK;
-	ps->uvd_clks.dclk = state->uvd_clocks.DCLK;
-
-	if (!result) {
-		uint32_t i;
-
-		switch (state->classification.ui_label) {
-		case PP_StateUILabel_Performance:
-			data->use_pcie_performance_levels = true;
-			for (i = 0; i < ps->performance_level_count; i++) {
-				if (data->pcie_gen_performance.max <
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_performance.max =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_gen_performance.min >
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_performance.min =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_lane_performance.max <
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_performance.max =
-							ps->performance_levels[i].pcie_lane;
-				if (data->pcie_lane_performance.min >
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_performance.min =
-							ps->performance_levels[i].pcie_lane;
-			}
-			break;
-		case PP_StateUILabel_Battery:
-			data->use_pcie_power_saving_levels = true;
-
-			for (i = 0; i < ps->performance_level_count; i++) {
-				if (data->pcie_gen_power_saving.max <
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_power_saving.max =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_gen_power_saving.min >
-						ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_power_saving.min =
-							ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_lane_power_saving.max <
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_power_saving.max =
-							ps->performance_levels[i].pcie_lane;
-
-				if (data->pcie_lane_power_saving.min >
-						ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_power_saving.min =
-							ps->performance_levels[i].pcie_lane;
-			}
-			break;
-		default:
-			break;
-		}
-	}
-	return 0;
-}
-
-static void
-polaris10_print_current_perforce_level(struct pp_hwmgr *hwmgr, struct seq_file *m)
-{
-	uint32_t sclk, mclk, activity_percent;
-	uint32_t offset;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetSclkFrequency);
-
-	sclk = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-	smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetMclkFrequency);
-
-	mclk = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-	seq_printf(m, "\n [  mclk  ]: %u MHz\n\n [  sclk  ]: %u MHz\n",
-			mclk / 100, sclk / 100);
-
-	offset = data->soft_regs_start + offsetof(SMU74_SoftRegisters, AverageGraphicsActivity);
-	activity_percent = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, offset);
-	activity_percent += 0x80;
-	activity_percent >>= 8;
-
-	seq_printf(m, "\n [GPU load]: %u%%\n\n", activity_percent > 100 ? 100 : activity_percent);
-
-	seq_printf(m, "uvd    %sabled\n", data->uvd_power_gated ? "dis" : "en");
-
-	seq_printf(m, "vce    %sabled\n", data->vce_power_gated ? "dis" : "en");
-}
-
-static int polaris10_find_dpm_states_clocks_in_dpm_table(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	const struct polaris10_power_state *polaris10_ps =
-			cast_const_phw_polaris10_power_state(states->pnew_state);
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	uint32_t sclk = polaris10_ps->performance_levels
-			[polaris10_ps->performance_level_count - 1].engine_clock;
-	struct polaris10_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	uint32_t mclk = polaris10_ps->performance_levels
-			[polaris10_ps->performance_level_count - 1].memory_clock;
-	struct PP_Clocks min_clocks = {0};
-	uint32_t i;
-	struct cgs_display_info info = {0};
-
-	data->need_update_smu7_dpm_table = 0;
-
-	for (i = 0; i < sclk_table->count; i++) {
-		if (sclk == sclk_table->dpm_levels[i].value)
-			break;
-	}
-
-	if (i >= sclk_table->count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_SCLK;
-	else {
-	/* TODO: Check SCLK in DAL's minimum clocks
-	 * in case DeepSleep divider update is required.
-	 */
-		if (data->display_timing.min_clock_in_sr != min_clocks.engineClockInSR &&
-			(min_clocks.engineClockInSR >= POLARIS10_MINIMUM_ENGINE_CLOCK ||
-				data->display_timing.min_clock_in_sr >= POLARIS10_MINIMUM_ENGINE_CLOCK))
-			data->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_SCLK;
-	}
-
-	for (i = 0; i < mclk_table->count; i++) {
-		if (mclk == mclk_table->dpm_levels[i].value)
-			break;
-	}
-
-	if (i >= mclk_table->count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_MCLK;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (data->display_timing.num_existing_displays != info.display_count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_MCLK;
-
-	return 0;
-}
-
-static uint16_t polaris10_get_maximum_link_speed(struct pp_hwmgr *hwmgr,
-		const struct polaris10_power_state *polaris10_ps)
-{
-	uint32_t i;
-	uint32_t sclk, max_sclk = 0;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_dpm_table *dpm_table = &data->dpm_table;
-
-	for (i = 0; i < polaris10_ps->performance_level_count; i++) {
-		sclk = polaris10_ps->performance_levels[i].engine_clock;
-		if (max_sclk < sclk)
-			max_sclk = sclk;
-	}
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		if (dpm_table->sclk_table.dpm_levels[i].value == max_sclk)
-			return (uint16_t) ((i >= dpm_table->pcie_speed_table.count) ?
-					dpm_table->pcie_speed_table.dpm_levels
-					[dpm_table->pcie_speed_table.count - 1].value :
-					dpm_table->pcie_speed_table.dpm_levels[i].value);
-	}
-
-	return 0;
-}
-
-static int polaris10_request_link_speed_change_before_state_change(
-		struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	const struct polaris10_power_state *polaris10_nps =
-			cast_const_phw_polaris10_power_state(states->pnew_state);
-	const struct polaris10_power_state *polaris10_cps =
-			cast_const_phw_polaris10_power_state(states->pcurrent_state);
-
-	uint16_t target_link_speed = polaris10_get_maximum_link_speed(hwmgr, polaris10_nps);
-	uint16_t current_link_speed;
-
-	if (data->force_pcie_gen == PP_PCIEGenInvalid)
-		current_link_speed = polaris10_get_maximum_link_speed(hwmgr, polaris10_cps);
-	else
-		current_link_speed = data->force_pcie_gen;
-
-	data->force_pcie_gen = PP_PCIEGenInvalid;
-	data->pspp_notify_required = false;
-
-	if (target_link_speed > current_link_speed) {
-		switch (target_link_speed) {
-		case PP_PCIEGen3:
-			if (0 == acpi_pcie_perf_request(hwmgr->device, PCIE_PERF_REQ_GEN3, false))
-				break;
-			data->force_pcie_gen = PP_PCIEGen2;
-			if (current_link_speed == PP_PCIEGen2)
-				break;
-		case PP_PCIEGen2:
-			if (0 == acpi_pcie_perf_request(hwmgr->device, PCIE_PERF_REQ_GEN2, false))
-				break;
-		default:
-			data->force_pcie_gen = phm_get_current_pcie_speed(hwmgr);
-			break;
-		}
-	} else {
-		if (target_link_speed < current_link_speed)
-			data->pspp_notify_required = true;
-	}
-
-	return 0;
-}
-
-static int polaris10_freeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if ((0 == data->sclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-			(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK))) {
-		PP_ASSERT_WITH_CODE(polaris10_is_dpm_running(hwmgr),
-				    "Trying to freeze SCLK DPM when DPM is disabled",
-				);
-		PP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_SCLKDPM_FreezeLevel),
-				"Failed to freeze SCLK DPM during FreezeSclkMclkDPM Function!",
-				return -1);
-	}
-
-	if ((0 == data->mclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		 DPMTABLE_OD_UPDATE_MCLK)) {
-		PP_ASSERT_WITH_CODE(polaris10_is_dpm_running(hwmgr),
-				    "Trying to freeze MCLK DPM when DPM is disabled",
-				);
-		PP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_MCLKDPM_FreezeLevel),
-				"Failed to freeze MCLK DPM during FreezeSclkMclkDPM Function!",
-				return -1);
-	}
-
-	return 0;
-}
-
-static int polaris10_populate_and_upload_sclk_mclk_dpm_levels(
-		struct pp_hwmgr *hwmgr, const void *input)
-{
-	int result = 0;
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	const struct polaris10_power_state *polaris10_ps =
-			cast_const_phw_polaris10_power_state(states->pnew_state);
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t sclk = polaris10_ps->performance_levels
-			[polaris10_ps->performance_level_count - 1].engine_clock;
-	uint32_t mclk = polaris10_ps->performance_levels
-			[polaris10_ps->performance_level_count - 1].memory_clock;
-	struct polaris10_dpm_table *dpm_table = &data->dpm_table;
-
-	struct polaris10_dpm_table *golden_dpm_table = &data->golden_dpm_table;
-	uint32_t dpm_count, clock_percent;
-	uint32_t i;
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_SCLK) {
-		dpm_table->sclk_table.dpm_levels
-		[dpm_table->sclk_table.count - 1].value = sclk;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinACSupport) ||
-		    phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinDCSupport)) {
-		/* Need to do calculation based on the golden DPM table
-		 * as the Heatmap GPU Clock axis is also based on the default values
-		 */
-			PP_ASSERT_WITH_CODE(
-				(golden_dpm_table->sclk_table.dpm_levels
-						[golden_dpm_table->sclk_table.count - 1].value != 0),
-				"Divide by 0!",
-				return -1);
-			dpm_count = dpm_table->sclk_table.count < 2 ? 0 : dpm_table->sclk_table.count - 2;
-
-			for (i = dpm_count; i > 1; i--) {
-				if (sclk > golden_dpm_table->sclk_table.dpm_levels[golden_dpm_table->sclk_table.count-1].value) {
-					clock_percent =
-					      ((sclk
-						- golden_dpm_table->sclk_table.dpm_levels[golden_dpm_table->sclk_table.count-1].value
-						) * 100)
-						/ golden_dpm_table->sclk_table.dpm_levels[golden_dpm_table->sclk_table.count-1].value;
-
-					dpm_table->sclk_table.dpm_levels[i].value =
-							golden_dpm_table->sclk_table.dpm_levels[i].value +
-							(golden_dpm_table->sclk_table.dpm_levels[i].value *
-								clock_percent)/100;
-
-				} else if (golden_dpm_table->sclk_table.dpm_levels[dpm_table->sclk_table.count-1].value > sclk) {
-					clock_percent =
-						((golden_dpm_table->sclk_table.dpm_levels[golden_dpm_table->sclk_table.count - 1].value
-						- sclk) * 100)
-						/ golden_dpm_table->sclk_table.dpm_levels[golden_dpm_table->sclk_table.count-1].value;
-
-					dpm_table->sclk_table.dpm_levels[i].value =
-							golden_dpm_table->sclk_table.dpm_levels[i].value -
-							(golden_dpm_table->sclk_table.dpm_levels[i].value *
-									clock_percent) / 100;
-				} else
-					dpm_table->sclk_table.dpm_levels[i].value =
-							golden_dpm_table->sclk_table.dpm_levels[i].value;
-			}
-		}
-	}
-
-	if (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK) {
-		dpm_table->mclk_table.dpm_levels
-			[dpm_table->mclk_table.count - 1].value = mclk;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinACSupport) ||
-		    phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinDCSupport)) {
-
-			PP_ASSERT_WITH_CODE(
-					(golden_dpm_table->mclk_table.dpm_levels
-						[golden_dpm_table->mclk_table.count-1].value != 0),
-					"Divide by 0!",
-					return -1);
-			dpm_count = dpm_table->mclk_table.count < 2 ? 0 : dpm_table->mclk_table.count - 2;
-			for (i = dpm_count; i > 1; i--) {
-				if (golden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count-1].value < mclk) {
-					clock_percent = ((mclk -
-					golden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count-1].value) * 100)
-					/ golden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count-1].value;
-
-					dpm_table->mclk_table.dpm_levels[i].value =
-							golden_dpm_table->mclk_table.dpm_levels[i].value +
-							(golden_dpm_table->mclk_table.dpm_levels[i].value *
-							clock_percent) / 100;
-
-				} else if (golden_dpm_table->mclk_table.dpm_levels[dpm_table->mclk_table.count-1].value > mclk) {
-					clock_percent = (
-					 (golden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count-1].value - mclk)
-					* 100)
-					/ golden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count-1].value;
-
-					dpm_table->mclk_table.dpm_levels[i].value =
-							golden_dpm_table->mclk_table.dpm_levels[i].value -
-							(golden_dpm_table->mclk_table.dpm_levels[i].value *
-									clock_percent) / 100;
-				} else
-					dpm_table->mclk_table.dpm_levels[i].value =
-							golden_dpm_table->mclk_table.dpm_levels[i].value;
-			}
-		}
-	}
-
-	if (data->need_update_smu7_dpm_table &
-			(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK)) {
-		result = polaris10_populate_all_graphic_levels(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to populate SCLK during PopulateNewDPMClocksStates Function!",
-				return result);
-	}
-
-	if (data->need_update_smu7_dpm_table &
-			(DPMTABLE_OD_UPDATE_MCLK + DPMTABLE_UPDATE_MCLK)) {
-		/*populate MCLK dpm table to SMU7 */
-		result = polaris10_populate_all_memory_levels(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to populate MCLK during PopulateNewDPMClocksStates Function!",
-				return result);
-	}
-
-	return result;
-}
-
-static int polaris10_trim_single_dpm_states(struct pp_hwmgr *hwmgr,
-			  struct polaris10_single_dpm_table *dpm_table,
-			uint32_t low_limit, uint32_t high_limit)
-{
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->count; i++) {
-		if ((dpm_table->dpm_levels[i].value < low_limit)
-		|| (dpm_table->dpm_levels[i].value > high_limit))
-			dpm_table->dpm_levels[i].enabled = false;
-		else
-			dpm_table->dpm_levels[i].enabled = true;
-	}
-
-	return 0;
-}
-
-static int polaris10_trim_dpm_states(struct pp_hwmgr *hwmgr,
-		const struct polaris10_power_state *polaris10_ps)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t high_limit_count;
-
-	PP_ASSERT_WITH_CODE((polaris10_ps->performance_level_count >= 1),
-			"power state did not have any performance level",
-			return -1);
-
-	high_limit_count = (1 == polaris10_ps->performance_level_count) ? 0 : 1;
-
-	polaris10_trim_single_dpm_states(hwmgr,
-			&(data->dpm_table.sclk_table),
-			polaris10_ps->performance_levels[0].engine_clock,
-			polaris10_ps->performance_levels[high_limit_count].engine_clock);
-
-	polaris10_trim_single_dpm_states(hwmgr,
-			&(data->dpm_table.mclk_table),
-			polaris10_ps->performance_levels[0].memory_clock,
-			polaris10_ps->performance_levels[high_limit_count].memory_clock);
-
-	return 0;
-}
-
-static int polaris10_generate_dpm_level_enable_mask(
-		struct pp_hwmgr *hwmgr, const void *input)
-{
-	int result;
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	const struct polaris10_power_state *polaris10_ps =
-			cast_const_phw_polaris10_power_state(states->pnew_state);
-
-	result = polaris10_trim_dpm_states(hwmgr, polaris10_ps);
-	if (result)
-		return result;
-
-	data->dpm_level_enable_mask.sclk_dpm_enable_mask =
-			phm_get_dpm_level_enable_mask_value(&data->dpm_table.sclk_table);
-	data->dpm_level_enable_mask.mclk_dpm_enable_mask =
-			phm_get_dpm_level_enable_mask_value(&data->dpm_table.mclk_table);
-	data->dpm_level_enable_mask.pcie_dpm_enable_mask =
-			phm_get_dpm_level_enable_mask_value(&data->dpm_table.pcie_speed_table);
-
-	return 0;
-}
-
-static int
-polaris10_enable_disable_uvd_dpm(struct pp_hwmgr *hwmgr, bool enable)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, enable ?
-			PPSMC_MSG_UVDDPM_Enable :
-			PPSMC_MSG_UVDDPM_Disable);
-}
-
-int polaris10_enable_disable_vce_dpm(struct pp_hwmgr *hwmgr, bool enable)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, enable?
-			PPSMC_MSG_VCEDPM_Enable :
-			PPSMC_MSG_VCEDPM_Disable);
-}
-
-static int
-polaris10_enable_disable_samu_dpm(struct pp_hwmgr *hwmgr, bool enable)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, enable?
-			PPSMC_MSG_SAMUDPM_Enable :
-			PPSMC_MSG_SAMUDPM_Disable);
-}
-
-int polaris10_update_uvd_dpm(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t mm_boot_level_offset, mm_boot_level_value;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (!bgate) {
-		data->smc_state_table.UvdBootLevel = 0;
-		if (table_info->mm_dep_table->count > 0)
-			data->smc_state_table.UvdBootLevel =
-					(uint8_t) (table_info->mm_dep_table->count - 1);
-		mm_boot_level_offset = data->dpm_table_start +
-				offsetof(SMU74_Discrete_DpmTable, UvdBootLevel);
-		mm_boot_level_offset /= 4;
-		mm_boot_level_offset *= 4;
-		mm_boot_level_value = cgs_read_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset);
-		mm_boot_level_value &= 0x00FFFFFF;
-		mm_boot_level_value |= data->smc_state_table.UvdBootLevel << 24;
-		cgs_write_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset, mm_boot_level_value);
-
-		if (!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_UVDDPM) ||
-			phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_StablePState))
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_UVDDPM_SetEnabledMask,
-					(uint32_t)(1 << data->smc_state_table.UvdBootLevel));
-	}
-
-	return polaris10_enable_disable_uvd_dpm(hwmgr, !bgate);
-}
-
-int polaris10_update_vce_dpm(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t mm_boot_level_offset, mm_boot_level_value;
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (!bgate) {
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_StablePState))
-			data->smc_state_table.VceBootLevel =
-				(uint8_t) (table_info->mm_dep_table->count - 1);
-		else
-			data->smc_state_table.VceBootLevel = 0;
-
-		mm_boot_level_offset = data->dpm_table_start +
-				offsetof(SMU74_Discrete_DpmTable, VceBootLevel);
-		mm_boot_level_offset /= 4;
-		mm_boot_level_offset *= 4;
-		mm_boot_level_value = cgs_read_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset);
-		mm_boot_level_value &= 0xFF00FFFF;
-		mm_boot_level_value |= data->smc_state_table.VceBootLevel << 16;
-		cgs_write_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset, mm_boot_level_value);
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState))
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_VCEDPM_SetEnabledMask,
-					(uint32_t)1 << data->smc_state_table.VceBootLevel);
-	}
-
-	polaris10_enable_disable_vce_dpm(hwmgr, !bgate);
-
-	return 0;
-}
-
-int polaris10_update_samu_dpm(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t mm_boot_level_offset, mm_boot_level_value;
-
-	if (!bgate) {
-		data->smc_state_table.SamuBootLevel = 0;
-		mm_boot_level_offset = data->dpm_table_start +
-				offsetof(SMU74_Discrete_DpmTable, SamuBootLevel);
-		mm_boot_level_offset /= 4;
-		mm_boot_level_offset *= 4;
-		mm_boot_level_value = cgs_read_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset);
-		mm_boot_level_value &= 0xFFFFFF00;
-		mm_boot_level_value |= data->smc_state_table.SamuBootLevel << 0;
-		cgs_write_ind_register(hwmgr->device,
-				CGS_IND_REG__SMC, mm_boot_level_offset, mm_boot_level_value);
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_StablePState))
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_SAMUDPM_SetEnabledMask,
-					(uint32_t)(1 << data->smc_state_table.SamuBootLevel));
-	}
-
-	return polaris10_enable_disable_samu_dpm(hwmgr, !bgate);
-}
-
-static int polaris10_update_sclk_threshold(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	int result = 0;
-	uint32_t low_sclk_interrupt_threshold = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkThrottleLowNotification)
-		&& (hwmgr->gfx_arbiter.sclk_threshold !=
-				data->low_sclk_interrupt_threshold)) {
-		data->low_sclk_interrupt_threshold =
-				hwmgr->gfx_arbiter.sclk_threshold;
-		low_sclk_interrupt_threshold =
-				data->low_sclk_interrupt_threshold;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(low_sclk_interrupt_threshold);
-
-		result = polaris10_copy_bytes_to_smc(
-				hwmgr->smumgr,
-				data->dpm_table_start +
-				offsetof(SMU74_Discrete_DpmTable,
-					LowSclkInterruptThreshold),
-				(uint8_t *)&low_sclk_interrupt_threshold,
-				sizeof(uint32_t),
-				data->sram_end);
-	}
-
-	return result;
-}
-
-static int polaris10_program_mem_timing_parameters(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_OD_UPDATE_MCLK))
-		return polaris10_program_memory_timing_parameters(hwmgr);
-
-	return 0;
-}
-
-static int polaris10_unfreeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if ((0 == data->sclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK))) {
-
-		PP_ASSERT_WITH_CODE(polaris10_is_dpm_running(hwmgr),
-				    "Trying to Unfreeze SCLK DPM when DPM is disabled",
-				);
-		PP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_SCLKDPM_UnfreezeLevel),
-			"Failed to unfreeze SCLK DPM during UnFreezeSclkMclkDPM Function!",
-			return -1);
-	}
-
-	if ((0 == data->mclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK)) {
-
-		PP_ASSERT_WITH_CODE(polaris10_is_dpm_running(hwmgr),
-				    "Trying to Unfreeze MCLK DPM when DPM is disabled",
-				);
-		PP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-				PPSMC_MSG_SCLKDPM_UnfreezeLevel),
-		    "Failed to unfreeze MCLK DPM during UnFreezeSclkMclkDPM Function!",
-		    return -1);
-	}
-
-	data->need_update_smu7_dpm_table = 0;
-
-	return 0;
-}
-
-static int polaris10_notify_link_speed_change_after_state_change(
-		struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states =
-			(const struct phm_set_power_state_input *)input;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	const struct polaris10_power_state *polaris10_ps =
-			cast_const_phw_polaris10_power_state(states->pnew_state);
-	uint16_t target_link_speed = polaris10_get_maximum_link_speed(hwmgr, polaris10_ps);
-	uint8_t  request;
-
-	if (data->pspp_notify_required) {
-		if (target_link_speed == PP_PCIEGen3)
-			request = PCIE_PERF_REQ_GEN3;
-		else if (target_link_speed == PP_PCIEGen2)
-			request = PCIE_PERF_REQ_GEN2;
-		else
-			request = PCIE_PERF_REQ_GEN1;
-
-		if (request == PCIE_PERF_REQ_GEN1 &&
-				phm_get_current_pcie_speed(hwmgr) > 0)
-			return 0;
-
-		if (acpi_pcie_perf_request(hwmgr->device, request, false)) {
-			if (PP_PCIEGen2 == target_link_speed)
-				printk("PSPP request to switch to Gen2 from Gen3 Failed!");
-			else
-				printk("PSPP request to switch to Gen1 from Gen2 Failed!");
-		}
-	}
-
-	return 0;
-}
-
-static int polaris10_notify_smc_display(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-		(PPSMC_Msg)PPSMC_MSG_SetVBITimeout, data->frame_time_x2);
-	return (smum_send_msg_to_smc(hwmgr->smumgr, (PPSMC_Msg)PPSMC_HasDisplay) == 0) ?  0 : -EINVAL;
-}
-
-
-
-static int polaris10_set_power_state_tasks(struct pp_hwmgr *hwmgr, const void *input)
-{
-	int tmp_result, result = 0;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	tmp_result = polaris10_find_dpm_states_clocks_in_dpm_table(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to find DPM states clocks in DPM table!",
-			result = tmp_result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PCIEPerformanceRequest)) {
-		tmp_result =
-			polaris10_request_link_speed_change_before_state_change(hwmgr, input);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-				"Failed to request link speed change before state change!",
-				result = tmp_result);
-	}
-
-	tmp_result = polaris10_freeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to freeze SCLK MCLK DPM!", result = tmp_result);
-
-	tmp_result = polaris10_populate_and_upload_sclk_mclk_dpm_levels(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to populate and upload SCLK MCLK DPM levels!",
-			result = tmp_result);
-
-	tmp_result = polaris10_generate_dpm_level_enable_mask(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to generate DPM level enabled mask!",
-			result = tmp_result);
-
-	tmp_result = polaris10_update_sclk_threshold(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to update SCLK threshold!",
-			result = tmp_result);
-
-	tmp_result = polaris10_program_mem_timing_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to program memory timing parameters!",
-			result = tmp_result);
-
-	tmp_result = polaris10_notify_smc_display(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to notify smc display settings!",
-			result = tmp_result);
-
-	tmp_result = polaris10_unfreeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to unfreeze SCLK MCLK DPM!",
-			result = tmp_result);
-
-	tmp_result = polaris10_upload_dpm_level_enable_mask(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to upload DPM level enabled mask!",
-			result = tmp_result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PCIEPerformanceRequest)) {
-		tmp_result =
-			polaris10_notify_link_speed_change_after_state_change(hwmgr, input);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-				"Failed to notify link speed change after state change!",
-				result = tmp_result);
-	}
-	data->apply_optimized_settings = false;
-	return result;
-}
-
-static int polaris10_set_max_fan_pwm_output(struct pp_hwmgr *hwmgr, uint16_t us_max_fan_pwm)
-{
-	hwmgr->thermal_controller.
-	advanceFanControlParameters.usMaxFanPWM = us_max_fan_pwm;
-
-	if (phm_is_hw_access_blocked(hwmgr))
-		return 0;
-
-	return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-			PPSMC_MSG_SetFanPwmMax, us_max_fan_pwm);
-}
-
-
-static int
-polaris10_notify_smc_display_change(struct pp_hwmgr *hwmgr, bool has_display)
-{
-	PPSMC_Msg msg = has_display ? (PPSMC_Msg)PPSMC_HasDisplay : (PPSMC_Msg)PPSMC_NoDisplay;
-
-	return (smum_send_msg_to_smc(hwmgr->smumgr, msg) == 0) ?  0 : -1;
-}
-
-static int
-polaris10_notify_smc_display_config_after_ps_adjustment(struct pp_hwmgr *hwmgr)
-{
-	uint32_t num_active_displays = 0;
-	struct cgs_display_info info = {0};
-	info.mode_info = NULL;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	num_active_displays = info.display_count;
-
-	if (num_active_displays > 1)  /* to do && (pHwMgr->pPECI->displayConfiguration.bMultiMonitorInSync != TRUE)) */
-		polaris10_notify_smc_display_change(hwmgr, false);
-
-
-	return 0;
-}
-
-/**
-* Programs the display gap
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always OK
-*/
-static int polaris10_program_display_gap(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t num_active_displays = 0;
-	uint32_t display_gap = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL);
-	uint32_t display_gap2;
-	uint32_t pre_vbi_time_in_us;
-	uint32_t frame_time_in_us;
-	uint32_t ref_clock;
-	uint32_t refresh_rate = 0;
-	struct cgs_display_info info = {0};
-	struct cgs_mode_info mode_info;
-
-	info.mode_info = &mode_info;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-	num_active_displays = info.display_count;
-
-	display_gap = PHM_SET_FIELD(display_gap, CG_DISPLAY_GAP_CNTL, DISP_GAP, (num_active_displays > 0) ? DISPLAY_GAP_VBLANK_OR_WM : DISPLAY_GAP_IGNORE);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL, display_gap);
-
-	ref_clock = mode_info.ref_clock;
-	refresh_rate = mode_info.refresh_rate;
-
-	if (0 == refresh_rate)
-		refresh_rate = 60;
-
-	frame_time_in_us = 1000000 / refresh_rate;
-
-	pre_vbi_time_in_us = frame_time_in_us - 200 - mode_info.vblank_time_us;
-	data->frame_time_x2 = frame_time_in_us * 2 / 100;
-
-	display_gap2 = pre_vbi_time_in_us * (ref_clock / 100);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL2, display_gap2);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, data->soft_regs_start + offsetof(SMU74_SoftRegisters, PreVBlankGap), 0x64);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, data->soft_regs_start + offsetof(SMU74_SoftRegisters, VBlankTimeout), (frame_time_in_us - pre_vbi_time_in_us));
-
-
-	return 0;
-}
-
-
-static int polaris10_display_configuration_changed_task(struct pp_hwmgr *hwmgr)
-{
-	return polaris10_program_display_gap(hwmgr);
-}
-
-/**
-*  Set maximum target operating fan output RPM
-*
-* @param    hwmgr:  the address of the powerplay hardware manager.
-* @param    usMaxFanRpm:  max operating fan RPM value.
-* @return   The response that came from the SMC.
-*/
-static int polaris10_set_max_fan_rpm_output(struct pp_hwmgr *hwmgr, uint16_t us_max_fan_rpm)
-{
-	hwmgr->thermal_controller.
-	advanceFanControlParameters.usMaxFanRPM = us_max_fan_rpm;
-
-	if (phm_is_hw_access_blocked(hwmgr))
-		return 0;
-
-	return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-			PPSMC_MSG_SetFanRpmMax, us_max_fan_rpm);
-}
-
-static int
-polaris10_register_internal_thermal_interrupt(struct pp_hwmgr *hwmgr,
-					const void *thermal_interrupt_info)
-{
-	return 0;
-}
-
-static bool polaris10_check_smc_update_required_for_display_configuration(
-		struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	bool is_update_required = false;
-	struct cgs_display_info info = {0, 0, NULL};
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (data->display_timing.num_existing_displays != info.display_count)
-		is_update_required = true;
-/* TO DO NEED TO GET DEEP SLEEP CLOCK FROM DAL
-	if (phm_cap_enabled(hwmgr->hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_SclkDeepSleep)) {
-		cgs_get_min_clock_settings(hwmgr->device, &min_clocks);
-		if (min_clocks.engineClockInSR != data->display_timing.minClockInSR &&
-			(min_clocks.engineClockInSR >= POLARIS10_MINIMUM_ENGINE_CLOCK ||
-				data->display_timing.minClockInSR >= POLARIS10_MINIMUM_ENGINE_CLOCK))
-			is_update_required = true;
-*/
-	return is_update_required;
-}
-
-static inline bool polaris10_are_power_levels_equal(const struct polaris10_performance_level *pl1,
-							   const struct polaris10_performance_level *pl2)
-{
-	return ((pl1->memory_clock == pl2->memory_clock) &&
-		  (pl1->engine_clock == pl2->engine_clock) &&
-		  (pl1->pcie_gen == pl2->pcie_gen) &&
-		  (pl1->pcie_lane == pl2->pcie_lane));
-}
-
-static int polaris10_check_states_equal(struct pp_hwmgr *hwmgr,
-		const struct pp_hw_power_state *pstate1,
-		const struct pp_hw_power_state *pstate2, bool *equal)
-{
-	const struct polaris10_power_state *psa = cast_const_phw_polaris10_power_state(pstate1);
-	const struct polaris10_power_state *psb = cast_const_phw_polaris10_power_state(pstate2);
-	int i;
-
-	if (pstate1 == NULL || pstate2 == NULL || equal == NULL)
-		return -EINVAL;
-
-	/* If the two states don't even have the same number of performance levels they cannot be the same state. */
-	if (psa->performance_level_count != psb->performance_level_count) {
-		*equal = false;
-		return 0;
-	}
-
-	for (i = 0; i < psa->performance_level_count; i++) {
-		if (!polaris10_are_power_levels_equal(&(psa->performance_levels[i]), &(psb->performance_levels[i]))) {
-			/* If we have found even one performance level pair that is different the states are different. */
-			*equal = false;
-			return 0;
-		}
-	}
-
-	/* If all performance levels are the same try to use the UVD clocks to break the tie.*/
-	*equal = ((psa->uvd_clks.vclk == psb->uvd_clks.vclk) && (psa->uvd_clks.dclk == psb->uvd_clks.dclk));
-	*equal &= ((psa->vce_clks.evclk == psb->vce_clks.evclk) && (psa->vce_clks.ecclk == psb->vce_clks.ecclk));
-	*equal &= (psa->sclk_threshold == psb->sclk_threshold);
-
-	return 0;
-}
-
-static int polaris10_upload_mc_firmware(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	uint32_t vbios_version;
-
-	/*  Read MC indirect register offset 0x9F bits [3:0] to see if VBIOS has already loaded a full version of MC ucode or not.*/
-
-	phm_get_mc_microcode_version(hwmgr);
-	vbios_version = hwmgr->microcode_version_info.MC & 0xf;
-	/*  Full version of MC ucode has already been loaded. */
-	if (vbios_version == 0) {
-		data->need_long_memory_training = false;
-		return 0;
-	}
-
-	data->need_long_memory_training = false;
-
-/*
- *	PPMCME_FirmwareDescriptorEntry *pfd = NULL;
-	pfd = &tonga_mcmeFirmware;
-	if (0 == PHM_READ_FIELD(hwmgr->device, MC_SEQ_SUP_CNTL, RUN))
-		polaris10_load_mc_microcode(hwmgr, pfd->dpmThreshold,
-					pfd->cfgArray, pfd->cfgSize, pfd->ioDebugArray,
-					pfd->ioDebugSize, pfd->ucodeArray, pfd->ucodeSize);
-*/
-	return 0;
-}
-
-/**
- * Read clock related registers.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int polaris10_read_clock_registers(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	data->clock_registers.vCG_SPLL_FUNC_CNTL = cgs_read_ind_register(hwmgr->device,
-						CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL)
-						& CG_SPLL_FUNC_CNTL__SPLL_BYPASS_EN_MASK;
-
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_2 = cgs_read_ind_register(hwmgr->device,
-						CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_2)
-						& CG_SPLL_FUNC_CNTL_2__SCLK_MUX_SEL_MASK;
-
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_4 = cgs_read_ind_register(hwmgr->device,
-						CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_4)
-						& CG_SPLL_FUNC_CNTL_4__SPLL_SPARE_MASK;
-
-	return 0;
-}
-
-/**
- * Find out if memory is GDDR5.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int polaris10_get_memory_type(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t temp;
-
-	temp = cgs_read_register(hwmgr->device, mmMC_SEQ_MISC0);
-
-	data->is_memory_gddr5 = (MC_SEQ_MISC0_GDDR5_VALUE ==
-			((temp & MC_SEQ_MISC0_GDDR5_MASK) >>
-			 MC_SEQ_MISC0_GDDR5_SHIFT));
-
-	return 0;
-}
-
-/**
- * Enables Dynamic Power Management by SMC
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int polaris10_enable_acpi_power_management(struct pp_hwmgr *hwmgr)
-{
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			GENERAL_PWRMGT, STATIC_PM_EN, 1);
-
-	return 0;
-}
-
-/**
- * Initialize PowerGating States for different engines
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int polaris10_init_power_gate_state(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	data->uvd_power_gated = false;
-	data->vce_power_gated = false;
-	data->samu_power_gated = false;
-
-	return 0;
-}
-
-static int polaris10_init_sclk_threshold(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	data->low_sclk_interrupt_threshold = 0;
-
-	return 0;
-}
-
-static int polaris10_setup_asic_task(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	polaris10_upload_mc_firmware(hwmgr);
-
-	tmp_result = polaris10_read_clock_registers(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to read clock registers!", result = tmp_result);
-
-	tmp_result = polaris10_get_memory_type(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to get memory type!", result = tmp_result);
-
-	tmp_result = polaris10_enable_acpi_power_management(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable ACPI power management!", result = tmp_result);
-
-	tmp_result = polaris10_init_power_gate_state(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to init power gate state!", result = tmp_result);
-
-	tmp_result = phm_get_mc_microcode_version(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to get MC microcode version!", result = tmp_result);
-
-	tmp_result = polaris10_init_sclk_threshold(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to init sclk threshold!", result = tmp_result);
-
-	return result;
-}
-
-static int polaris10_force_clock_level(struct pp_hwmgr *hwmgr,
-		enum pp_clock_type type, uint32_t mask)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (hwmgr->dpm_level != AMD_DPM_FORCED_LEVEL_MANUAL)
-		return -EINVAL;
-
-	switch (type) {
-	case PP_SCLK:
-		if (!data->sclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_SCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.sclk_dpm_enable_mask & mask);
-		break;
-	case PP_MCLK:
-		if (!data->mclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_MCLKDPM_SetEnabledMask,
-					data->dpm_level_enable_mask.mclk_dpm_enable_mask & mask);
-		break;
-	case PP_PCIE:
-	{
-		uint32_t tmp = mask & data->dpm_level_enable_mask.pcie_dpm_enable_mask;
-		uint32_t level = 0;
-
-		while (tmp >>= 1)
-			level++;
-
-		if (!data->pcie_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_PCIeDPM_ForceLevel,
-					level);
-		break;
-	}
-	default:
-		break;
-	}
-
-	return 0;
-}
-
-static uint16_t polaris10_get_current_pcie_speed(struct pp_hwmgr *hwmgr)
-{
-	uint32_t speedCntl = 0;
-
-	/* mmPCIE_PORT_INDEX rename as mmPCIE_INDEX */
-	speedCntl = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__PCIE,
-			ixPCIE_LC_SPEED_CNTL);
-	return((uint16_t)PHM_GET_FIELD(speedCntl,
-			PCIE_LC_SPEED_CNTL, LC_CURRENT_DATA_RATE));
-}
-
-static int polaris10_print_clock_levels(struct pp_hwmgr *hwmgr,
-		enum pp_clock_type type, char *buf)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	struct polaris10_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	struct polaris10_single_dpm_table *pcie_table = &(data->dpm_table.pcie_speed_table);
-	int i, now, size = 0;
-	uint32_t clock, pcie_speed;
-
-	switch (type) {
-	case PP_SCLK:
-		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetSclkFrequency);
-		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-		for (i = 0; i < sclk_table->count; i++) {
-			if (clock > sclk_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < sclk_table->count; i++)
-			size += sprintf(buf + size, "%d: %uMhz %s\n",
-					i, sclk_table->dpm_levels[i].value / 100,
-					(i == now) ? "*" : "");
-		break;
-	case PP_MCLK:
-		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetMclkFrequency);
-		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-		for (i = 0; i < mclk_table->count; i++) {
-			if (clock > mclk_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < mclk_table->count; i++)
-			size += sprintf(buf + size, "%d: %uMhz %s\n",
-					i, mclk_table->dpm_levels[i].value / 100,
-					(i == now) ? "*" : "");
-		break;
-	case PP_PCIE:
-		pcie_speed = polaris10_get_current_pcie_speed(hwmgr);
-		for (i = 0; i < pcie_table->count; i++) {
-			if (pcie_speed != pcie_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < pcie_table->count; i++)
-			size += sprintf(buf + size, "%d: %s %s\n", i,
-					(pcie_table->dpm_levels[i].value == 0) ? "2.5GB, x8" :
-					(pcie_table->dpm_levels[i].value == 1) ? "5.0GB, x16" :
-					(pcie_table->dpm_levels[i].value == 2) ? "8.0GB, x16" : "",
-					(i == now) ? "*" : "");
-		break;
-	default:
-		break;
-	}
-	return size;
-}
-
-static int polaris10_set_fan_control_mode(struct pp_hwmgr *hwmgr, uint32_t mode)
-{
-	if (mode) {
-		/* stop auto-manage */
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl))
-			polaris10_fan_ctrl_stop_smc_fan_control(hwmgr);
-		polaris10_fan_ctrl_set_static_mode(hwmgr, mode);
-	} else
-		/* restart auto-manage */
-		polaris10_fan_ctrl_reset_fan_speed_to_default(hwmgr);
-
-	return 0;
-}
-
-static int polaris10_get_fan_control_mode(struct pp_hwmgr *hwmgr)
-{
-	if (hwmgr->fan_ctrl_is_in_default_mode)
-		return hwmgr->fan_ctrl_default_mode;
-	else
-		return PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_FDO_CTRL2, FDO_PWM_MODE);
-}
-
-static int polaris10_get_sclk_od(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	struct polaris10_single_dpm_table *golden_sclk_table =
-			&(data->golden_dpm_table.sclk_table);
-	int value;
-
-	value = (sclk_table->dpm_levels[sclk_table->count - 1].value -
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value) *
-			100 /
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
-
-	return value;
-}
-
-static int polaris10_set_sclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_single_dpm_table *golden_sclk_table =
-			&(data->golden_dpm_table.sclk_table);
-	struct pp_power_state  *ps;
-	struct polaris10_power_state  *polaris10_ps;
-
-	if (value > 20)
-		value = 20;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	polaris10_ps = cast_phw_polaris10_power_state(&ps->hardware);
-
-	polaris10_ps->performance_levels[polaris10_ps->performance_level_count - 1].engine_clock =
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value *
-			value / 100 +
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
-
-	return 0;
-}
-
-static int polaris10_get_mclk_od(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	struct polaris10_single_dpm_table *golden_mclk_table =
-			&(data->golden_dpm_table.mclk_table);
-	int value;
-
-	value = (mclk_table->dpm_levels[mclk_table->count - 1].value -
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value) *
-			100 /
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
-
-	return value;
-}
-
-static int polaris10_set_mclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_single_dpm_table *golden_mclk_table =
-			&(data->golden_dpm_table.mclk_table);
-	struct pp_power_state  *ps;
-	struct polaris10_power_state  *polaris10_ps;
-
-	if (value > 20)
-		value = 20;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	polaris10_ps = cast_phw_polaris10_power_state(&ps->hardware);
-
-	polaris10_ps->performance_levels[polaris10_ps->performance_level_count - 1].memory_clock =
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value *
-			value / 100 +
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
-
-	return 0;
-}
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static int polaris10_populate_requested_graphic_levels(struct pp_hwmgr *hwmgr,
-		struct pp_profile *request)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_dpm_table *dpm_table = &(data->dpm_table);
-	struct SMU74_Discrete_GraphicsLevel *levels =
-			data->smc_state_table.GraphicsLevel;
-	uint32_t array = data->dpm_table_start +
-			offsetof(SMU74_Discrete_DpmTable, GraphicsLevel);
-	uint32_t array_size = sizeof(struct SMU74_Discrete_GraphicsLevel) *
-			SMU74_MAX_LEVELS_GRAPHICS;
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		levels[i].ActivityLevel =
-				cpu_to_be16(request->activity_threshold);
-		levels[i].EnabledForActivity = 1;
-		levels[i].UpHyst = request->up_hyst;
-		levels[i].DownHyst = request->down_hyst;
-	}
-
-	return polaris10_copy_bytes_to_smc(hwmgr->smumgr, array,
-			(uint8_t *)levels,
-			array_size, data->sram_end);
-}
-
-static void polaris10_find_min_clock_masks(struct pp_hwmgr *hwmgr,
-		uint32_t *sclk_mask, uint32_t *mclk_mask,
-		uint32_t min_sclk, uint32_t min_mclk)
-{
-	struct polaris10_hwmgr *data =
-			(struct polaris10_hwmgr *)(hwmgr->backend);
-	struct polaris10_dpm_table *dpm_table = &(data->dpm_table);
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		if (dpm_table->sclk_table.dpm_levels[i].enabled &&
-			dpm_table->sclk_table.dpm_levels[i].value >= min_sclk)
-			*sclk_mask |= 1 << i;
-	}
-
-	for (i = 0; i < dpm_table->mclk_table.count; i++) {
-		if (dpm_table->mclk_table.dpm_levels[i].enabled &&
-			dpm_table->mclk_table.dpm_levels[i].value >= min_mclk)
-			*mclk_mask |= 1 << i;
-	}
-}
-
-static int polaris10_set_power_profile_state(struct pp_hwmgr *hwmgr,
-		struct pp_profile *request)
-{
-	struct polaris10_hwmgr *data =
-			(struct polaris10_hwmgr *)(hwmgr->backend);
-	int tmp_result, result = 0;
-	uint32_t sclk_mask = 0, mclk_mask = 0;
-
-	if (hwmgr->dpm_level != AMD_DPM_FORCED_LEVEL_AUTO)
-		return -EINVAL;
-
-	tmp_result = polaris10_freeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE(!tmp_result,
-			"Failed to freeze SCLK MCLK DPM!",
-			result = tmp_result);
-
-	tmp_result = polaris10_populate_requested_graphic_levels(hwmgr, request);
-	PP_ASSERT_WITH_CODE(!tmp_result,
-			"Failed to populate requested graphic levels!",
-			result = tmp_result);
-
-	tmp_result = polaris10_unfreeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE(!tmp_result,
-			"Failed to unfreeze SCLK MCLK DPM!",
-			result = tmp_result);
-
-	polaris10_find_min_clock_masks(hwmgr, &sclk_mask, &mclk_mask,
-			request->min_sclk, request->min_mclk);
-
-	if (sclk_mask) {
-		if (!data->sclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_SCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.
-				sclk_dpm_enable_mask &
-				sclk_mask);
-	}
-
-	if (mclk_mask) {
-		if (!data->mclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_MCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.
-				mclk_dpm_enable_mask &
-				mclk_mask);
-	}
-
-
-	return result;
-}
-#endif
-
-static const struct pp_hwmgr_func polaris10_hwmgr_funcs = {
-	.backend_init = &polaris10_hwmgr_backend_init,
-	.backend_fini = &polaris10_hwmgr_backend_fini,
-	.asic_setup = &polaris10_setup_asic_task,
-	.dynamic_state_management_enable = &polaris10_enable_dpm_tasks,
-	.apply_state_adjust_rules = polaris10_apply_state_adjust_rules,
-	.force_dpm_level = &polaris10_force_dpm_level,
-	.power_state_set = polaris10_set_power_state_tasks,
-	.get_power_state_size = polaris10_get_power_state_size,
-	.get_mclk = polaris10_dpm_get_mclk,
-	.get_sclk = polaris10_dpm_get_sclk,
-	.patch_boot_state = polaris10_dpm_patch_boot_state,
-	.get_pp_table_entry = polaris10_get_pp_table_entry,
-	.get_num_of_pp_table_entries = get_number_of_powerplay_table_entries_v1_0,
-	.print_current_perforce_level = polaris10_print_current_perforce_level,
-	.powerdown_uvd = polaris10_phm_powerdown_uvd,
-	.powergate_uvd = polaris10_phm_powergate_uvd,
-	.powergate_vce = polaris10_phm_powergate_vce,
-	.disable_clock_power_gating = polaris10_phm_disable_clock_power_gating,
-	.update_clock_gatings = polaris10_phm_update_clock_gatings,
-	.notify_smc_display_config_after_ps_adjustment = polaris10_notify_smc_display_config_after_ps_adjustment,
-	.display_config_changed = polaris10_display_configuration_changed_task,
-	.set_max_fan_pwm_output = polaris10_set_max_fan_pwm_output,
-	.set_max_fan_rpm_output = polaris10_set_max_fan_rpm_output,
-	.get_temperature = polaris10_thermal_get_temperature,
-	.stop_thermal_controller = polaris10_thermal_stop_thermal_controller,
-	.get_fan_speed_info = polaris10_fan_ctrl_get_fan_speed_info,
-	.get_fan_speed_percent = polaris10_fan_ctrl_get_fan_speed_percent,
-	.set_fan_speed_percent = polaris10_fan_ctrl_set_fan_speed_percent,
-	.reset_fan_speed_to_default = polaris10_fan_ctrl_reset_fan_speed_to_default,
-	.get_fan_speed_rpm = polaris10_fan_ctrl_get_fan_speed_rpm,
-	.set_fan_speed_rpm = polaris10_fan_ctrl_set_fan_speed_rpm,
-	.uninitialize_thermal_controller = polaris10_thermal_ctrl_uninitialize_thermal_controller,
-	.register_internal_thermal_interrupt = polaris10_register_internal_thermal_interrupt,
-	.check_smc_update_required_for_display_configuration = polaris10_check_smc_update_required_for_display_configuration,
-	.check_states_equal = polaris10_check_states_equal,
-	.set_fan_control_mode = polaris10_set_fan_control_mode,
-	.get_fan_control_mode = polaris10_get_fan_control_mode,
-	.force_clock_level = polaris10_force_clock_level,
-	.print_clock_levels = polaris10_print_clock_levels,
-	.enable_per_cu_power_gating = polaris10_phm_enable_per_cu_power_gating,
-	.get_sclk_od = polaris10_get_sclk_od,
-	.set_sclk_od = polaris10_set_sclk_od,
-	.get_mclk_od = polaris10_get_mclk_od,
-	.set_mclk_od = polaris10_set_mclk_od,
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
-	.set_power_profile_state = polaris10_set_power_profile_state,
-#endif	
-};
-
-int polaris10_hwmgr_init(struct pp_hwmgr *hwmgr)
-{
-	hwmgr->hwmgr_func = &polaris10_hwmgr_funcs;
-	hwmgr->pptable_func = &pptable_v1_0_funcs;
-	pp_polaris10_thermal_initialize(hwmgr);
-
-	return 0;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.h b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.h
deleted file mode 100644
index 378ab34..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_hwmgr.h
+++ /dev/null
@@ -1,354 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef POLARIS10_HWMGR_H
-#define POLARIS10_HWMGR_H
-
-#include "hwmgr.h"
-#include "smu74.h"
-#include "smu74_discrete.h"
-#include "ppatomctrl.h"
-#include "polaris10_ppsmc.h"
-#include "polaris10_powertune.h"
-#include "polaris10_smumgr.h"
-
-#define POLARIS10_MAX_HARDWARE_POWERLEVELS	2
-
-#define POLARIS10_VOLTAGE_CONTROL_NONE                   0x0
-#define POLARIS10_VOLTAGE_CONTROL_BY_GPIO                0x1
-#define POLARIS10_VOLTAGE_CONTROL_BY_SVID2               0x2
-#define POLARIS10_VOLTAGE_CONTROL_MERGED                 0x3
-
-#define DPMTABLE_OD_UPDATE_SCLK     0x00000001
-#define DPMTABLE_OD_UPDATE_MCLK     0x00000002
-#define DPMTABLE_UPDATE_SCLK        0x00000004
-#define DPMTABLE_UPDATE_MCLK        0x00000008
-
-struct polaris10_performance_level {
-	uint32_t  memory_clock;
-	uint32_t  engine_clock;
-	uint16_t  pcie_gen;
-	uint16_t  pcie_lane;
-};
-
-struct polaris10_uvd_clocks {
-	uint32_t  vclk;
-	uint32_t  dclk;
-};
-
-struct polaris10_vce_clocks {
-	uint32_t  evclk;
-	uint32_t  ecclk;
-};
-
-struct polaris10_power_state {
-	uint32_t                  magic;
-	struct polaris10_uvd_clocks    uvd_clks;
-	struct polaris10_vce_clocks    vce_clks;
-	uint32_t                  sam_clk;
-	uint16_t                  performance_level_count;
-	bool                      dc_compatible;
-	uint32_t                  sclk_threshold;
-	struct polaris10_performance_level  performance_levels[POLARIS10_MAX_HARDWARE_POWERLEVELS];
-};
-
-struct polaris10_dpm_level {
-	bool	enabled;
-	uint32_t	value;
-	uint32_t	param1;
-};
-
-#define POLARIS10_MAX_DEEPSLEEP_DIVIDER_ID 5
-#define MAX_REGULAR_DPM_NUMBER 8
-#define POLARIS10_MINIMUM_ENGINE_CLOCK 2500
-
-struct polaris10_single_dpm_table {
-	uint32_t		count;
-	struct polaris10_dpm_level	dpm_levels[MAX_REGULAR_DPM_NUMBER];
-};
-
-struct polaris10_dpm_table {
-	struct polaris10_single_dpm_table  sclk_table;
-	struct polaris10_single_dpm_table  mclk_table;
-	struct polaris10_single_dpm_table  pcie_speed_table;
-	struct polaris10_single_dpm_table  vddc_table;
-	struct polaris10_single_dpm_table  vddci_table;
-	struct polaris10_single_dpm_table  mvdd_table;
-};
-
-struct polaris10_clock_registers {
-	uint32_t  vCG_SPLL_FUNC_CNTL;
-	uint32_t  vCG_SPLL_FUNC_CNTL_2;
-	uint32_t  vCG_SPLL_FUNC_CNTL_3;
-	uint32_t  vCG_SPLL_FUNC_CNTL_4;
-	uint32_t  vCG_SPLL_SPREAD_SPECTRUM;
-	uint32_t  vCG_SPLL_SPREAD_SPECTRUM_2;
-	uint32_t  vDLL_CNTL;
-	uint32_t  vMCLK_PWRMGT_CNTL;
-	uint32_t  vMPLL_AD_FUNC_CNTL;
-	uint32_t  vMPLL_DQ_FUNC_CNTL;
-	uint32_t  vMPLL_FUNC_CNTL;
-	uint32_t  vMPLL_FUNC_CNTL_1;
-	uint32_t  vMPLL_FUNC_CNTL_2;
-	uint32_t  vMPLL_SS1;
-	uint32_t  vMPLL_SS2;
-};
-
-#define DISABLE_MC_LOADMICROCODE   1
-#define DISABLE_MC_CFGPROGRAMMING  2
-
-struct polaris10_voltage_smio_registers {
-	uint32_t vS0_VID_LOWER_SMIO_CNTL;
-};
-
-#define POLARIS10_MAX_LEAKAGE_COUNT  8
-
-struct polaris10_leakage_voltage {
-	uint16_t  count;
-	uint16_t  leakage_id[POLARIS10_MAX_LEAKAGE_COUNT];
-	uint16_t  actual_voltage[POLARIS10_MAX_LEAKAGE_COUNT];
-};
-
-struct polaris10_vbios_boot_state {
-	uint16_t    mvdd_bootup_value;
-	uint16_t    vddc_bootup_value;
-	uint16_t    vddci_bootup_value;
-	uint32_t    sclk_bootup_value;
-	uint32_t    mclk_bootup_value;
-	uint16_t    pcie_gen_bootup_value;
-	uint16_t    pcie_lane_bootup_value;
-};
-
-/* Ultra Low Voltage parameter structure */
-struct polaris10_ulv_parm {
-	bool                           ulv_supported;
-	uint32_t                       cg_ulv_parameter;
-	uint32_t                       ulv_volt_change_delay;
-	struct polaris10_performance_level  ulv_power_level;
-};
-
-struct polaris10_display_timing {
-	uint32_t  min_clock_in_sr;
-	uint32_t  num_existing_displays;
-};
-
-struct polaris10_dpmlevel_enable_mask {
-	uint32_t  uvd_dpm_enable_mask;
-	uint32_t  vce_dpm_enable_mask;
-	uint32_t  acp_dpm_enable_mask;
-	uint32_t  samu_dpm_enable_mask;
-	uint32_t  sclk_dpm_enable_mask;
-	uint32_t  mclk_dpm_enable_mask;
-	uint32_t  pcie_dpm_enable_mask;
-};
-
-struct polaris10_pcie_perf_range {
-	uint16_t  max;
-	uint16_t  min;
-};
-
-struct polaris10_hwmgr {
-	struct polaris10_dpm_table			dpm_table;
-	struct polaris10_dpm_table			golden_dpm_table;
-	SMU74_Discrete_DpmTable				smc_state_table;
-	struct SMU74_Discrete_Ulv            ulv_setting;
-
-	struct polaris10_range_table                range_table[NUM_SCLK_RANGE];
-	uint32_t						voting_rights_clients0;
-	uint32_t						voting_rights_clients1;
-	uint32_t						voting_rights_clients2;
-	uint32_t						voting_rights_clients3;
-	uint32_t						voting_rights_clients4;
-	uint32_t						voting_rights_clients5;
-	uint32_t						voting_rights_clients6;
-	uint32_t						voting_rights_clients7;
-	uint32_t						static_screen_threshold_unit;
-	uint32_t						static_screen_threshold;
-	uint32_t						voltage_control;
-	uint32_t						vddc_vddci_delta;
-
-	uint32_t						active_auto_throttle_sources;
-
-	struct polaris10_clock_registers            clock_registers;
-	struct polaris10_voltage_smio_registers      voltage_smio_registers;
-
-	bool                           is_memory_gddr5;
-	uint16_t                       acpi_vddc;
-	bool                           pspp_notify_required;
-	uint16_t                       force_pcie_gen;
-	uint16_t                       acpi_pcie_gen;
-	uint32_t                       pcie_gen_cap;
-	uint32_t                       pcie_lane_cap;
-	uint32_t                       pcie_spc_cap;
-	struct polaris10_leakage_voltage          vddc_leakage;
-	struct polaris10_leakage_voltage          Vddci_leakage;
-
-	uint32_t                             mvdd_control;
-	uint32_t                             vddc_mask_low;
-	uint32_t                             mvdd_mask_low;
-	uint16_t                            max_vddc_in_pptable;
-	uint16_t                            min_vddc_in_pptable;
-	uint16_t                            max_vddci_in_pptable;
-	uint16_t                            min_vddci_in_pptable;
-	uint32_t                             mclk_strobe_mode_threshold;
-	uint32_t                             mclk_stutter_mode_threshold;
-	uint32_t                             mclk_edc_enable_threshold;
-	uint32_t                             mclk_edcwr_enable_threshold;
-	bool                                is_uvd_enabled;
-	struct polaris10_vbios_boot_state        vbios_boot_state;
-
-	bool                           pcie_performance_request;
-	bool                           battery_state;
-	bool                           is_tlu_enabled;
-
-	/* ---- SMC SRAM Address of firmware header tables ---- */
-	uint32_t                             sram_end;
-	uint32_t                             dpm_table_start;
-	uint32_t                             soft_regs_start;
-	uint32_t                             mc_reg_table_start;
-	uint32_t                             fan_table_start;
-	uint32_t                             arb_table_start;
-
-	/* ---- Stuff originally coming from Evergreen ---- */
-	uint32_t                             vddci_control;
-	struct pp_atomctrl_voltage_table     vddc_voltage_table;
-	struct pp_atomctrl_voltage_table     vddci_voltage_table;
-	struct pp_atomctrl_voltage_table     mvdd_voltage_table;
-
-	uint32_t                             mgcg_cgtt_local2;
-	uint32_t                             mgcg_cgtt_local3;
-	uint32_t                             gpio_debug;
-	uint32_t                             mc_micro_code_feature;
-	uint32_t                             highest_mclk;
-	uint16_t                             acpi_vddci;
-	uint8_t                              mvdd_high_index;
-	uint8_t                              mvdd_low_index;
-	bool                                 dll_default_on;
-	bool                                 performance_request_registered;
-
-	/* ---- Low Power Features ---- */
-	struct polaris10_ulv_parm                 ulv;
-
-	/* ---- CAC Stuff ---- */
-	uint32_t                       cac_table_start;
-	bool                           cac_configuration_required;
-	bool                           driver_calculate_cac_leakage;
-	bool                           cac_enabled;
-
-	/* ---- DPM2 Parameters ---- */
-	uint32_t                       power_containment_features;
-	bool                           enable_dte_feature;
-	bool                           enable_tdc_limit_feature;
-	bool                           enable_pkg_pwr_tracking_feature;
-	bool                           disable_uvd_power_tune_feature;
-	const struct polaris10_pt_defaults       *power_tune_defaults;
-	struct SMU74_Discrete_PmFuses  power_tune_table;
-	uint32_t                       dte_tj_offset;
-	uint32_t                       fast_watermark_threshold;
-
-	/* ---- Phase Shedding ---- */
-	bool                           vddc_phase_shed_control;
-
-	/* ---- DI/DT ---- */
-	struct polaris10_display_timing        display_timing;
-	uint32_t                      bif_sclk_table[SMU74_MAX_LEVELS_LINK];
-
-	/* ---- Thermal Temperature Setting ---- */
-	struct polaris10_dpmlevel_enable_mask     dpm_level_enable_mask;
-	uint32_t                                  need_update_smu7_dpm_table;
-	uint32_t                                  sclk_dpm_key_disabled;
-	uint32_t                                  mclk_dpm_key_disabled;
-	uint32_t                                  pcie_dpm_key_disabled;
-	uint32_t                                  min_engine_clocks;
-	struct polaris10_pcie_perf_range          pcie_gen_performance;
-	struct polaris10_pcie_perf_range          pcie_lane_performance;
-	struct polaris10_pcie_perf_range          pcie_gen_power_saving;
-	struct polaris10_pcie_perf_range          pcie_lane_power_saving;
-	bool                                      use_pcie_performance_levels;
-	bool                                      use_pcie_power_saving_levels;
-	uint32_t                                  activity_target[SMU74_MAX_LEVELS_GRAPHICS];
-	uint32_t                                  mclk_activity_target;
-	uint32_t                                  mclk_dpm0_activity_target;
-	uint32_t                                  low_sclk_interrupt_threshold;
-	uint32_t                                  last_mclk_dpm_enable_mask;
-	bool                                      uvd_enabled;
-
-	/* ---- Power Gating States ---- */
-	bool                           uvd_power_gated;
-	bool                           vce_power_gated;
-	bool                           samu_power_gated;
-	bool                           need_long_memory_training;
-
-	/* Application power optimization parameters */
-	bool                               update_up_hyst;
-	bool                               update_down_hyst;
-	uint32_t                           down_hyst;
-	uint32_t                           up_hyst;
-	uint32_t disable_dpm_mask;
-	bool apply_optimized_settings;
-	uint32_t                              avfs_vdroop_override_setting;
-	bool                                  apply_avfs_cks_off_voltage;
-	uint32_t                              frame_time_x2;
-};
-
-/* To convert to Q8.8 format for firmware */
-#define POLARIS10_Q88_FORMAT_CONVERSION_UNIT             256
-
-enum Polaris10_I2CLineID {
-	Polaris10_I2CLineID_DDC1 = 0x90,
-	Polaris10_I2CLineID_DDC2 = 0x91,
-	Polaris10_I2CLineID_DDC3 = 0x92,
-	Polaris10_I2CLineID_DDC4 = 0x93,
-	Polaris10_I2CLineID_DDC5 = 0x94,
-	Polaris10_I2CLineID_DDC6 = 0x95,
-	Polaris10_I2CLineID_SCLSDA = 0x96,
-	Polaris10_I2CLineID_DDCVGA = 0x97
-};
-
-#define POLARIS10_I2C_DDC1DATA          0
-#define POLARIS10_I2C_DDC1CLK           1
-#define POLARIS10_I2C_DDC2DATA          2
-#define POLARIS10_I2C_DDC2CLK           3
-#define POLARIS10_I2C_DDC3DATA          4
-#define POLARIS10_I2C_DDC3CLK           5
-#define POLARIS10_I2C_SDA               40
-#define POLARIS10_I2C_SCL               41
-#define POLARIS10_I2C_DDC4DATA          65
-#define POLARIS10_I2C_DDC4CLK           66
-#define POLARIS10_I2C_DDC5DATA          0x48
-#define POLARIS10_I2C_DDC5CLK           0x49
-#define POLARIS10_I2C_DDC6DATA          0x4a
-#define POLARIS10_I2C_DDC6CLK           0x4b
-#define POLARIS10_I2C_DDCVGADATA        0x4c
-#define POLARIS10_I2C_DDCVGACLK         0x4d
-
-#define POLARIS10_UNUSED_GPIO_PIN       0x7F
-
-int polaris10_hwmgr_init(struct pp_hwmgr *hwmgr);
-
-int polaris10_update_uvd_dpm(struct pp_hwmgr *hwmgr, bool bgate);
-int polaris10_update_samu_dpm(struct pp_hwmgr *hwmgr, bool bgate);
-int polaris10_enable_disable_vce_dpm(struct pp_hwmgr *hwmgr, bool enable);
-int polaris10_update_vce_dpm(struct pp_hwmgr *hwmgr, bool bgate);
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_powertune.c b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_powertune.c
deleted file mode 100644
index b9cb240..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_powertune.c
+++ /dev/null
@@ -1,988 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#include "hwmgr.h"
-#include "smumgr.h"
-#include "polaris10_hwmgr.h"
-#include "polaris10_powertune.h"
-#include "polaris10_smumgr.h"
-#include "smu74_discrete.h"
-#include "pp_debug.h"
-#include "gca/gfx_8_0_d.h"
-#include "gca/gfx_8_0_sh_mask.h"
-#include "oss/oss_3_0_sh_mask.h"
-
-#define VOLTAGE_SCALE  4
-#define POWERTUNE_DEFAULT_SET_MAX    1
-
-uint32_t DIDTBlock_Info = SQ_IR_MASK | TCP_IR_MASK | TD_PCC_MASK;
-
-struct polaris10_pt_config_reg GCCACConfig_Polaris10[] = {
-/* ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- *      Offset                             Mask                                                Shift                                               Value       Type
- * ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- */
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x00060013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x00860013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x01060013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x01860013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x02060013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x02860013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x03060013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x03860013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x04060013, POLARIS10_CONFIGREG_GC_CAC_IND },
-
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x000E0013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x008E0013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x010E0013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x018E0013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x020E0013, POLARIS10_CONFIGREG_GC_CAC_IND },
-
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x00100013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x00900013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x01100013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x01900013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x02100013, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x02900013, POLARIS10_CONFIGREG_GC_CAC_IND },
-
-	{   0xFFFFFFFF  }
-};
-
-struct polaris10_pt_config_reg GCCACConfig_Polaris11[] = {
-/* ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- *      Offset                             Mask                                                Shift                                               Value       Type
- * ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- */
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x00060011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x00860011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x01060011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x01860011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x02060011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x02860011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x03060011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x03860011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x04060011, POLARIS10_CONFIGREG_GC_CAC_IND },
-
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x000E0011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x008E0011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x010E0011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x018E0011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x020E0011, POLARIS10_CONFIGREG_GC_CAC_IND },
-
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x00100011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x00900011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x01100011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x01900011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x02100011, POLARIS10_CONFIGREG_GC_CAC_IND },
-	{   ixGC_CAC_CNTL,                     0xFFFFFFFF,                                         0,                                                  0x02900011, POLARIS10_CONFIGREG_GC_CAC_IND },
-
-	{   0xFFFFFFFF  }
-};
-
-struct polaris10_pt_config_reg DIDTConfig_Polaris10[] = {
-/* ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- *      Offset                             Mask                                                Shift                                               Value       Type
- * ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- */
-	{   ixDIDT_SQ_WEIGHT0_3,               DIDT_SQ_WEIGHT0_3__WEIGHT0_MASK,                    DIDT_SQ_WEIGHT0_3__WEIGHT0__SHIFT,                  0x0073,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT0_3,               DIDT_SQ_WEIGHT0_3__WEIGHT1_MASK,                    DIDT_SQ_WEIGHT0_3__WEIGHT1__SHIFT,                  0x00ab,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT0_3,               DIDT_SQ_WEIGHT0_3__WEIGHT2_MASK,                    DIDT_SQ_WEIGHT0_3__WEIGHT2__SHIFT,                  0x0084,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT0_3,               DIDT_SQ_WEIGHT0_3__WEIGHT3_MASK,                    DIDT_SQ_WEIGHT0_3__WEIGHT3__SHIFT,                  0x005a,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_WEIGHT4_7,               DIDT_SQ_WEIGHT4_7__WEIGHT4_MASK,                    DIDT_SQ_WEIGHT4_7__WEIGHT4__SHIFT,                  0x0067,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT4_7,               DIDT_SQ_WEIGHT4_7__WEIGHT5_MASK,                    DIDT_SQ_WEIGHT4_7__WEIGHT5__SHIFT,                  0x0084,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT4_7,               DIDT_SQ_WEIGHT4_7__WEIGHT6_MASK,                    DIDT_SQ_WEIGHT4_7__WEIGHT6__SHIFT,                  0x0027,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT4_7,               DIDT_SQ_WEIGHT4_7__WEIGHT7_MASK,                    DIDT_SQ_WEIGHT4_7__WEIGHT7__SHIFT,                  0x0046,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_WEIGHT8_11,              DIDT_SQ_WEIGHT8_11__WEIGHT8_MASK,                   DIDT_SQ_WEIGHT8_11__WEIGHT8__SHIFT,                 0x00aa,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT8_11,              DIDT_SQ_WEIGHT8_11__WEIGHT9_MASK,                   DIDT_SQ_WEIGHT8_11__WEIGHT9__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT8_11,              DIDT_SQ_WEIGHT8_11__WEIGHT10_MASK,                  DIDT_SQ_WEIGHT8_11__WEIGHT10__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT8_11,              DIDT_SQ_WEIGHT8_11__WEIGHT11_MASK,                  DIDT_SQ_WEIGHT8_11__WEIGHT11__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_CTRL1,                   DIDT_SQ_CTRL1__MIN_POWER_MASK,                      DIDT_SQ_CTRL1__MIN_POWER__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL1,                   DIDT_SQ_CTRL1__MAX_POWER_MASK,                      DIDT_SQ_CTRL1__MAX_POWER__SHIFT,                    0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_CTRL_OCP,                DIDT_SQ_CTRL_OCP__UNUSED_0_MASK,                    DIDT_SQ_CTRL_OCP__UNUSED_0__SHIFT,                  0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL_OCP,                DIDT_SQ_CTRL_OCP__OCP_MAX_POWER_MASK,               DIDT_SQ_CTRL_OCP__OCP_MAX_POWER__SHIFT,             0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__MAX_POWER_DELTA_MASK,                DIDT_SQ_CTRL2__MAX_POWER_DELTA__SHIFT,              0x3853,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__UNUSED_0_MASK,                       DIDT_SQ_CTRL2__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__SHORT_TERM_INTERVAL_SIZE_MASK,       DIDT_SQ_CTRL2__SHORT_TERM_INTERVAL_SIZE__SHIFT,     0x005a,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__UNUSED_1_MASK,                       DIDT_SQ_CTRL2__UNUSED_1__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__LONG_TERM_INTERVAL_RATIO_MASK,       DIDT_SQ_CTRL2__LONG_TERM_INTERVAL_RATIO__SHIFT,     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__UNUSED_2_MASK,                       DIDT_SQ_CTRL2__UNUSED_2__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__DIDT_STALL_CTRL_ENABLE_MASK,    DIDT_SQ_STALL_CTRL__DIDT_STALL_CTRL_ENABLE__SHIFT,  0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__DIDT_STALL_DELAY_HI_MASK,       DIDT_SQ_STALL_CTRL__DIDT_STALL_DELAY_HI__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__DIDT_STALL_DELAY_LO_MASK,       DIDT_SQ_STALL_CTRL__DIDT_STALL_DELAY_LO__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__DIDT_HI_POWER_THRESHOLD_MASK,   DIDT_SQ_STALL_CTRL__DIDT_HI_POWER_THRESHOLD__SHIFT, 0x0ebb,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__UNUSED_0_MASK,                  DIDT_SQ_STALL_CTRL__UNUSED_0__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_TUNING_CTRL,             DIDT_SQ_TUNING_CTRL__DIDT_TUNING_ENABLE_MASK,       DIDT_SQ_TUNING_CTRL__DIDT_TUNING_ENABLE__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_TUNING_CTRL,             DIDT_SQ_TUNING_CTRL__MAX_POWER_DELTA_HI_MASK,       DIDT_SQ_TUNING_CTRL__MAX_POWER_DELTA_HI__SHIFT,     0x3853,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_TUNING_CTRL,             DIDT_SQ_TUNING_CTRL__MAX_POWER_DELTA_LO_MASK,       DIDT_SQ_TUNING_CTRL__MAX_POWER_DELTA_LO__SHIFT,     0x3153,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_TUNING_CTRL,             DIDT_SQ_TUNING_CTRL__UNUSED_0_MASK,                 DIDT_SQ_TUNING_CTRL__UNUSED_0__SHIFT,               0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_CTRL_EN_MASK,                   DIDT_SQ_CTRL0__DIDT_CTRL_EN__SHIFT,                 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__USE_REF_CLOCK_MASK,                  DIDT_SQ_CTRL0__USE_REF_CLOCK__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__PHASE_OFFSET_MASK,                   DIDT_SQ_CTRL0__PHASE_OFFSET__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_CTRL_RST_MASK,                  DIDT_SQ_CTRL0__DIDT_CTRL_RST__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_CLK_EN_OVERRIDE_MASK,           DIDT_SQ_CTRL0__DIDT_CLK_EN_OVERRIDE__SHIFT,         0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI_MASK,     DIDT_SQ_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI__SHIFT,   0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO_MASK,     DIDT_SQ_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO__SHIFT,   0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__UNUSED_0_MASK,                       DIDT_SQ_CTRL0__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_WEIGHT0_3,               DIDT_TD_WEIGHT0_3__WEIGHT0_MASK,                    DIDT_TD_WEIGHT0_3__WEIGHT0__SHIFT,                  0x000a,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT0_3,               DIDT_TD_WEIGHT0_3__WEIGHT1_MASK,                    DIDT_TD_WEIGHT0_3__WEIGHT1__SHIFT,                  0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT0_3,               DIDT_TD_WEIGHT0_3__WEIGHT2_MASK,                    DIDT_TD_WEIGHT0_3__WEIGHT2__SHIFT,                  0x0017,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT0_3,               DIDT_TD_WEIGHT0_3__WEIGHT3_MASK,                    DIDT_TD_WEIGHT0_3__WEIGHT3__SHIFT,                  0x002f,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_WEIGHT4_7,               DIDT_TD_WEIGHT4_7__WEIGHT4_MASK,                    DIDT_TD_WEIGHT4_7__WEIGHT4__SHIFT,                  0x0046,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT4_7,               DIDT_TD_WEIGHT4_7__WEIGHT5_MASK,                    DIDT_TD_WEIGHT4_7__WEIGHT5__SHIFT,                  0x005d,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT4_7,               DIDT_TD_WEIGHT4_7__WEIGHT6_MASK,                    DIDT_TD_WEIGHT4_7__WEIGHT6__SHIFT,                  0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT4_7,               DIDT_TD_WEIGHT4_7__WEIGHT7_MASK,                    DIDT_TD_WEIGHT4_7__WEIGHT7__SHIFT,                  0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_CTRL1,                   DIDT_TD_CTRL1__MIN_POWER_MASK,                      DIDT_TD_CTRL1__MIN_POWER__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL1,                   DIDT_TD_CTRL1__MAX_POWER_MASK,                      DIDT_TD_CTRL1__MAX_POWER__SHIFT,                    0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_CTRL_OCP,                DIDT_TD_CTRL_OCP__UNUSED_0_MASK,                    DIDT_TD_CTRL_OCP__UNUSED_0__SHIFT,                  0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL_OCP,                DIDT_TD_CTRL_OCP__OCP_MAX_POWER_MASK,               DIDT_TD_CTRL_OCP__OCP_MAX_POWER__SHIFT,             0x00ff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__MAX_POWER_DELTA_MASK,                DIDT_TD_CTRL2__MAX_POWER_DELTA__SHIFT,              0x3fff,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__UNUSED_0_MASK,                       DIDT_TD_CTRL2__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__SHORT_TERM_INTERVAL_SIZE_MASK,       DIDT_TD_CTRL2__SHORT_TERM_INTERVAL_SIZE__SHIFT,     0x000f,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__UNUSED_1_MASK,                       DIDT_TD_CTRL2__UNUSED_1__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__LONG_TERM_INTERVAL_RATIO_MASK,       DIDT_TD_CTRL2__LONG_TERM_INTERVAL_RATIO__SHIFT,     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__UNUSED_2_MASK,                       DIDT_TD_CTRL2__UNUSED_2__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__DIDT_STALL_CTRL_ENABLE_MASK,    DIDT_TD_STALL_CTRL__DIDT_STALL_CTRL_ENABLE__SHIFT,  0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__DIDT_STALL_DELAY_HI_MASK,       DIDT_TD_STALL_CTRL__DIDT_STALL_DELAY_HI__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__DIDT_STALL_DELAY_LO_MASK,       DIDT_TD_STALL_CTRL__DIDT_STALL_DELAY_LO__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__DIDT_HI_POWER_THRESHOLD_MASK,   DIDT_TD_STALL_CTRL__DIDT_HI_POWER_THRESHOLD__SHIFT, 0x01aa,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__UNUSED_0_MASK,                  DIDT_TD_STALL_CTRL__UNUSED_0__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_TUNING_CTRL,             DIDT_TD_TUNING_CTRL__DIDT_TUNING_ENABLE_MASK,       DIDT_TD_TUNING_CTRL__DIDT_TUNING_ENABLE__SHIFT,     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_TUNING_CTRL,             DIDT_TD_TUNING_CTRL__MAX_POWER_DELTA_HI_MASK,       DIDT_TD_TUNING_CTRL__MAX_POWER_DELTA_HI__SHIFT,     0x0dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_TUNING_CTRL,             DIDT_TD_TUNING_CTRL__MAX_POWER_DELTA_LO_MASK,       DIDT_TD_TUNING_CTRL__MAX_POWER_DELTA_LO__SHIFT,     0x0dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_TUNING_CTRL,             DIDT_TD_TUNING_CTRL__UNUSED_0_MASK,                 DIDT_TD_TUNING_CTRL__UNUSED_0__SHIFT,               0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_CTRL_EN_MASK,                   DIDT_TD_CTRL0__DIDT_CTRL_EN__SHIFT,                 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__USE_REF_CLOCK_MASK,                  DIDT_TD_CTRL0__USE_REF_CLOCK__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__PHASE_OFFSET_MASK,                   DIDT_TD_CTRL0__PHASE_OFFSET__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_CTRL_RST_MASK,                  DIDT_TD_CTRL0__DIDT_CTRL_RST__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_CLK_EN_OVERRIDE_MASK,           DIDT_TD_CTRL0__DIDT_CLK_EN_OVERRIDE__SHIFT,         0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI_MASK,     DIDT_TD_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI__SHIFT,   0x0009,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO_MASK,     DIDT_TD_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO__SHIFT,   0x0009,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__UNUSED_0_MASK,                       DIDT_TD_CTRL0__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_WEIGHT0_3,              DIDT_TCP_WEIGHT0_3__WEIGHT0_MASK,                   DIDT_TCP_WEIGHT0_3__WEIGHT0__SHIFT,                 0x0004,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT0_3,              DIDT_TCP_WEIGHT0_3__WEIGHT1_MASK,                   DIDT_TCP_WEIGHT0_3__WEIGHT1__SHIFT,                 0x0037,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT0_3,              DIDT_TCP_WEIGHT0_3__WEIGHT2_MASK,                   DIDT_TCP_WEIGHT0_3__WEIGHT2__SHIFT,                 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT0_3,              DIDT_TCP_WEIGHT0_3__WEIGHT3_MASK,                   DIDT_TCP_WEIGHT0_3__WEIGHT3__SHIFT,                 0x00ff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_WEIGHT4_7,              DIDT_TCP_WEIGHT4_7__WEIGHT4_MASK,                   DIDT_TCP_WEIGHT4_7__WEIGHT4__SHIFT,                 0x0054,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT4_7,              DIDT_TCP_WEIGHT4_7__WEIGHT5_MASK,                   DIDT_TCP_WEIGHT4_7__WEIGHT5__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT4_7,              DIDT_TCP_WEIGHT4_7__WEIGHT6_MASK,                   DIDT_TCP_WEIGHT4_7__WEIGHT6__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT4_7,              DIDT_TCP_WEIGHT4_7__WEIGHT7_MASK,                   DIDT_TCP_WEIGHT4_7__WEIGHT7__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_CTRL1,                  DIDT_TCP_CTRL1__MIN_POWER_MASK,                     DIDT_TCP_CTRL1__MIN_POWER__SHIFT,                   0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL1,                  DIDT_TCP_CTRL1__MAX_POWER_MASK,                     DIDT_TCP_CTRL1__MAX_POWER__SHIFT,                   0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_CTRL_OCP,               DIDT_TCP_CTRL_OCP__UNUSED_0_MASK,                   DIDT_TCP_CTRL_OCP__UNUSED_0__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL_OCP,               DIDT_TCP_CTRL_OCP__OCP_MAX_POWER_MASK,              DIDT_TCP_CTRL_OCP__OCP_MAX_POWER__SHIFT,            0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__MAX_POWER_DELTA_MASK,               DIDT_TCP_CTRL2__MAX_POWER_DELTA__SHIFT,             0x3dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__UNUSED_0_MASK,                      DIDT_TCP_CTRL2__UNUSED_0__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__SHORT_TERM_INTERVAL_SIZE_MASK,      DIDT_TCP_CTRL2__SHORT_TERM_INTERVAL_SIZE__SHIFT,    0x0032,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__UNUSED_1_MASK,                      DIDT_TCP_CTRL2__UNUSED_1__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__LONG_TERM_INTERVAL_RATIO_MASK,      DIDT_TCP_CTRL2__LONG_TERM_INTERVAL_RATIO__SHIFT,    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__UNUSED_2_MASK,                      DIDT_TCP_CTRL2__UNUSED_2__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__DIDT_STALL_CTRL_ENABLE_MASK,   DIDT_TCP_STALL_CTRL__DIDT_STALL_CTRL_ENABLE__SHIFT, 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__DIDT_STALL_DELAY_HI_MASK,      DIDT_TCP_STALL_CTRL__DIDT_STALL_DELAY_HI__SHIFT,    0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__DIDT_STALL_DELAY_LO_MASK,      DIDT_TCP_STALL_CTRL__DIDT_STALL_DELAY_LO__SHIFT,    0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__DIDT_HI_POWER_THRESHOLD_MASK,  DIDT_TCP_STALL_CTRL__DIDT_HI_POWER_THRESHOLD__SHIFT, 0x01aa,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__UNUSED_0_MASK,                 DIDT_TCP_STALL_CTRL__UNUSED_0__SHIFT,               0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_TUNING_CTRL,            DIDT_TCP_TUNING_CTRL__DIDT_TUNING_ENABLE_MASK,      DIDT_TCP_TUNING_CTRL__DIDT_TUNING_ENABLE__SHIFT,    0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_TUNING_CTRL,            DIDT_TCP_TUNING_CTRL__MAX_POWER_DELTA_HI_MASK,      DIDT_TCP_TUNING_CTRL__MAX_POWER_DELTA_HI__SHIFT,    0x3dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_TUNING_CTRL,            DIDT_TCP_TUNING_CTRL__MAX_POWER_DELTA_LO_MASK,      DIDT_TCP_TUNING_CTRL__MAX_POWER_DELTA_LO__SHIFT,    0x3dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_TUNING_CTRL,            DIDT_TCP_TUNING_CTRL__UNUSED_0_MASK,                DIDT_TCP_TUNING_CTRL__UNUSED_0__SHIFT,              0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_CTRL_EN_MASK,                   DIDT_TCP_CTRL0__DIDT_CTRL_EN__SHIFT,                 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__USE_REF_CLOCK_MASK,                  DIDT_TCP_CTRL0__USE_REF_CLOCK__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__PHASE_OFFSET_MASK,                   DIDT_TCP_CTRL0__PHASE_OFFSET__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_CTRL_RST_MASK,                  DIDT_TCP_CTRL0__DIDT_CTRL_RST__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_CLK_EN_OVERRIDE_MASK,           DIDT_TCP_CTRL0__DIDT_CLK_EN_OVERRIDE__SHIFT,         0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI_MASK,     DIDT_TCP_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI__SHIFT,   0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO_MASK,     DIDT_TCP_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO__SHIFT,   0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__UNUSED_0_MASK,                       DIDT_TCP_CTRL0__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   0xFFFFFFFF  }
-};
-
-struct polaris10_pt_config_reg DIDTConfig_Polaris11[] = {
-/* ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- *      Offset                             Mask                                                Shift                                               Value       Type
- * ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- */
-	{   ixDIDT_SQ_WEIGHT0_3,               DIDT_SQ_WEIGHT0_3__WEIGHT0_MASK,                    DIDT_SQ_WEIGHT0_3__WEIGHT0__SHIFT,                  0x0073,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT0_3,               DIDT_SQ_WEIGHT0_3__WEIGHT1_MASK,                    DIDT_SQ_WEIGHT0_3__WEIGHT1__SHIFT,                  0x00ab,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT0_3,               DIDT_SQ_WEIGHT0_3__WEIGHT2_MASK,                    DIDT_SQ_WEIGHT0_3__WEIGHT2__SHIFT,                  0x0084,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT0_3,               DIDT_SQ_WEIGHT0_3__WEIGHT3_MASK,                    DIDT_SQ_WEIGHT0_3__WEIGHT3__SHIFT,                  0x005a,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_WEIGHT4_7,               DIDT_SQ_WEIGHT4_7__WEIGHT4_MASK,                    DIDT_SQ_WEIGHT4_7__WEIGHT4__SHIFT,                  0x0067,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT4_7,               DIDT_SQ_WEIGHT4_7__WEIGHT5_MASK,                    DIDT_SQ_WEIGHT4_7__WEIGHT5__SHIFT,                  0x0084,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT4_7,               DIDT_SQ_WEIGHT4_7__WEIGHT6_MASK,                    DIDT_SQ_WEIGHT4_7__WEIGHT6__SHIFT,                  0x0027,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT4_7,               DIDT_SQ_WEIGHT4_7__WEIGHT7_MASK,                    DIDT_SQ_WEIGHT4_7__WEIGHT7__SHIFT,                  0x0046,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_WEIGHT8_11,              DIDT_SQ_WEIGHT8_11__WEIGHT8_MASK,                   DIDT_SQ_WEIGHT8_11__WEIGHT8__SHIFT,                 0x00aa,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT8_11,              DIDT_SQ_WEIGHT8_11__WEIGHT9_MASK,                   DIDT_SQ_WEIGHT8_11__WEIGHT9__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT8_11,              DIDT_SQ_WEIGHT8_11__WEIGHT10_MASK,                  DIDT_SQ_WEIGHT8_11__WEIGHT10__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_WEIGHT8_11,              DIDT_SQ_WEIGHT8_11__WEIGHT11_MASK,                  DIDT_SQ_WEIGHT8_11__WEIGHT11__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_CTRL1,                   DIDT_SQ_CTRL1__MIN_POWER_MASK,                      DIDT_SQ_CTRL1__MIN_POWER__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL1,                   DIDT_SQ_CTRL1__MAX_POWER_MASK,                      DIDT_SQ_CTRL1__MAX_POWER__SHIFT,                    0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_CTRL_OCP,                DIDT_SQ_CTRL_OCP__UNUSED_0_MASK,                    DIDT_SQ_CTRL_OCP__UNUSED_0__SHIFT,                  0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL_OCP,                DIDT_SQ_CTRL_OCP__OCP_MAX_POWER_MASK,               DIDT_SQ_CTRL_OCP__OCP_MAX_POWER__SHIFT,             0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__MAX_POWER_DELTA_MASK,                DIDT_SQ_CTRL2__MAX_POWER_DELTA__SHIFT,              0x3853,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__UNUSED_0_MASK,                       DIDT_SQ_CTRL2__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__SHORT_TERM_INTERVAL_SIZE_MASK,       DIDT_SQ_CTRL2__SHORT_TERM_INTERVAL_SIZE__SHIFT,     0x005a,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__UNUSED_1_MASK,                       DIDT_SQ_CTRL2__UNUSED_1__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__LONG_TERM_INTERVAL_RATIO_MASK,       DIDT_SQ_CTRL2__LONG_TERM_INTERVAL_RATIO__SHIFT,     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL2,                   DIDT_SQ_CTRL2__UNUSED_2_MASK,                       DIDT_SQ_CTRL2__UNUSED_2__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__DIDT_STALL_CTRL_ENABLE_MASK,    DIDT_SQ_STALL_CTRL__DIDT_STALL_CTRL_ENABLE__SHIFT,  0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__DIDT_STALL_DELAY_HI_MASK,       DIDT_SQ_STALL_CTRL__DIDT_STALL_DELAY_HI__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__DIDT_STALL_DELAY_LO_MASK,       DIDT_SQ_STALL_CTRL__DIDT_STALL_DELAY_LO__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__DIDT_HI_POWER_THRESHOLD_MASK,   DIDT_SQ_STALL_CTRL__DIDT_HI_POWER_THRESHOLD__SHIFT, 0x0ebb,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_STALL_CTRL,              DIDT_SQ_STALL_CTRL__UNUSED_0_MASK,                  DIDT_SQ_STALL_CTRL__UNUSED_0__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_TUNING_CTRL,             DIDT_SQ_TUNING_CTRL__DIDT_TUNING_ENABLE_MASK,       DIDT_SQ_TUNING_CTRL__DIDT_TUNING_ENABLE__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_TUNING_CTRL,             DIDT_SQ_TUNING_CTRL__MAX_POWER_DELTA_HI_MASK,       DIDT_SQ_TUNING_CTRL__MAX_POWER_DELTA_HI__SHIFT,     0x3853,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_TUNING_CTRL,             DIDT_SQ_TUNING_CTRL__MAX_POWER_DELTA_LO_MASK,       DIDT_SQ_TUNING_CTRL__MAX_POWER_DELTA_LO__SHIFT,     0x3153,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_TUNING_CTRL,             DIDT_SQ_TUNING_CTRL__UNUSED_0_MASK,                 DIDT_SQ_TUNING_CTRL__UNUSED_0__SHIFT,               0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_CTRL_EN_MASK,                   DIDT_SQ_CTRL0__DIDT_CTRL_EN__SHIFT,                 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__USE_REF_CLOCK_MASK,                  DIDT_SQ_CTRL0__USE_REF_CLOCK__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__PHASE_OFFSET_MASK,                   DIDT_SQ_CTRL0__PHASE_OFFSET__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_CTRL_RST_MASK,                  DIDT_SQ_CTRL0__DIDT_CTRL_RST__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_CLK_EN_OVERRIDE_MASK,           DIDT_SQ_CTRL0__DIDT_CLK_EN_OVERRIDE__SHIFT,         0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI_MASK,     DIDT_SQ_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI__SHIFT,   0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO_MASK,     DIDT_SQ_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO__SHIFT,   0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_SQ_CTRL0,                   DIDT_SQ_CTRL0__UNUSED_0_MASK,                       DIDT_SQ_CTRL0__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_WEIGHT0_3,               DIDT_TD_WEIGHT0_3__WEIGHT0_MASK,                    DIDT_TD_WEIGHT0_3__WEIGHT0__SHIFT,                  0x000a,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT0_3,               DIDT_TD_WEIGHT0_3__WEIGHT1_MASK,                    DIDT_TD_WEIGHT0_3__WEIGHT1__SHIFT,                  0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT0_3,               DIDT_TD_WEIGHT0_3__WEIGHT2_MASK,                    DIDT_TD_WEIGHT0_3__WEIGHT2__SHIFT,                  0x0017,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT0_3,               DIDT_TD_WEIGHT0_3__WEIGHT3_MASK,                    DIDT_TD_WEIGHT0_3__WEIGHT3__SHIFT,                  0x002f,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_WEIGHT4_7,               DIDT_TD_WEIGHT4_7__WEIGHT4_MASK,                    DIDT_TD_WEIGHT4_7__WEIGHT4__SHIFT,                  0x0046,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT4_7,               DIDT_TD_WEIGHT4_7__WEIGHT5_MASK,                    DIDT_TD_WEIGHT4_7__WEIGHT5__SHIFT,                  0x005d,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT4_7,               DIDT_TD_WEIGHT4_7__WEIGHT6_MASK,                    DIDT_TD_WEIGHT4_7__WEIGHT6__SHIFT,                  0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_WEIGHT4_7,               DIDT_TD_WEIGHT4_7__WEIGHT7_MASK,                    DIDT_TD_WEIGHT4_7__WEIGHT7__SHIFT,                  0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_CTRL1,                   DIDT_TD_CTRL1__MIN_POWER_MASK,                      DIDT_TD_CTRL1__MIN_POWER__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL1,                   DIDT_TD_CTRL1__MAX_POWER_MASK,                      DIDT_TD_CTRL1__MAX_POWER__SHIFT,                    0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_CTRL_OCP,                DIDT_TD_CTRL_OCP__UNUSED_0_MASK,                    DIDT_TD_CTRL_OCP__UNUSED_0__SHIFT,                  0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL_OCP,                DIDT_TD_CTRL_OCP__OCP_MAX_POWER_MASK,               DIDT_TD_CTRL_OCP__OCP_MAX_POWER__SHIFT,             0x00ff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__MAX_POWER_DELTA_MASK,                DIDT_TD_CTRL2__MAX_POWER_DELTA__SHIFT,              0x3fff,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__UNUSED_0_MASK,                       DIDT_TD_CTRL2__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__SHORT_TERM_INTERVAL_SIZE_MASK,       DIDT_TD_CTRL2__SHORT_TERM_INTERVAL_SIZE__SHIFT,     0x000f,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__UNUSED_1_MASK,                       DIDT_TD_CTRL2__UNUSED_1__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__LONG_TERM_INTERVAL_RATIO_MASK,       DIDT_TD_CTRL2__LONG_TERM_INTERVAL_RATIO__SHIFT,     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL2,                   DIDT_TD_CTRL2__UNUSED_2_MASK,                       DIDT_TD_CTRL2__UNUSED_2__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__DIDT_STALL_CTRL_ENABLE_MASK,    DIDT_TD_STALL_CTRL__DIDT_STALL_CTRL_ENABLE__SHIFT,  0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__DIDT_STALL_DELAY_HI_MASK,       DIDT_TD_STALL_CTRL__DIDT_STALL_DELAY_HI__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__DIDT_STALL_DELAY_LO_MASK,       DIDT_TD_STALL_CTRL__DIDT_STALL_DELAY_LO__SHIFT,     0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__DIDT_HI_POWER_THRESHOLD_MASK,   DIDT_TD_STALL_CTRL__DIDT_HI_POWER_THRESHOLD__SHIFT, 0x01aa,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_STALL_CTRL,              DIDT_TD_STALL_CTRL__UNUSED_0_MASK,                  DIDT_TD_STALL_CTRL__UNUSED_0__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_TUNING_CTRL,             DIDT_TD_TUNING_CTRL__DIDT_TUNING_ENABLE_MASK,       DIDT_TD_TUNING_CTRL__DIDT_TUNING_ENABLE__SHIFT,     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_TUNING_CTRL,             DIDT_TD_TUNING_CTRL__MAX_POWER_DELTA_HI_MASK,       DIDT_TD_TUNING_CTRL__MAX_POWER_DELTA_HI__SHIFT,     0x0dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_TUNING_CTRL,             DIDT_TD_TUNING_CTRL__MAX_POWER_DELTA_LO_MASK,       DIDT_TD_TUNING_CTRL__MAX_POWER_DELTA_LO__SHIFT,     0x0dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_TUNING_CTRL,             DIDT_TD_TUNING_CTRL__UNUSED_0_MASK,                 DIDT_TD_TUNING_CTRL__UNUSED_0__SHIFT,               0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_CTRL_EN_MASK,                   DIDT_TD_CTRL0__DIDT_CTRL_EN__SHIFT,                 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__USE_REF_CLOCK_MASK,                  DIDT_TD_CTRL0__USE_REF_CLOCK__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__PHASE_OFFSET_MASK,                   DIDT_TD_CTRL0__PHASE_OFFSET__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_CTRL_RST_MASK,                  DIDT_TD_CTRL0__DIDT_CTRL_RST__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_CLK_EN_OVERRIDE_MASK,           DIDT_TD_CTRL0__DIDT_CLK_EN_OVERRIDE__SHIFT,         0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI_MASK,     DIDT_TD_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI__SHIFT,   0x0008,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO_MASK,     DIDT_TD_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO__SHIFT,   0x0008,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TD_CTRL0,                   DIDT_TD_CTRL0__UNUSED_0_MASK,                       DIDT_TD_CTRL0__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_WEIGHT0_3,              DIDT_TCP_WEIGHT0_3__WEIGHT0_MASK,                   DIDT_TCP_WEIGHT0_3__WEIGHT0__SHIFT,                 0x0004,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT0_3,              DIDT_TCP_WEIGHT0_3__WEIGHT1_MASK,                   DIDT_TCP_WEIGHT0_3__WEIGHT1__SHIFT,                 0x0037,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT0_3,              DIDT_TCP_WEIGHT0_3__WEIGHT2_MASK,                   DIDT_TCP_WEIGHT0_3__WEIGHT2__SHIFT,                 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT0_3,              DIDT_TCP_WEIGHT0_3__WEIGHT3_MASK,                   DIDT_TCP_WEIGHT0_3__WEIGHT3__SHIFT,                 0x00ff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_WEIGHT4_7,              DIDT_TCP_WEIGHT4_7__WEIGHT4_MASK,                   DIDT_TCP_WEIGHT4_7__WEIGHT4__SHIFT,                 0x0054,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT4_7,              DIDT_TCP_WEIGHT4_7__WEIGHT5_MASK,                   DIDT_TCP_WEIGHT4_7__WEIGHT5__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT4_7,              DIDT_TCP_WEIGHT4_7__WEIGHT6_MASK,                   DIDT_TCP_WEIGHT4_7__WEIGHT6__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_WEIGHT4_7,              DIDT_TCP_WEIGHT4_7__WEIGHT7_MASK,                   DIDT_TCP_WEIGHT4_7__WEIGHT7__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_CTRL1,                  DIDT_TCP_CTRL1__MIN_POWER_MASK,                     DIDT_TCP_CTRL1__MIN_POWER__SHIFT,                   0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL1,                  DIDT_TCP_CTRL1__MAX_POWER_MASK,                     DIDT_TCP_CTRL1__MAX_POWER__SHIFT,                   0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_CTRL_OCP,               DIDT_TCP_CTRL_OCP__UNUSED_0_MASK,                   DIDT_TCP_CTRL_OCP__UNUSED_0__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL_OCP,               DIDT_TCP_CTRL_OCP__OCP_MAX_POWER_MASK,              DIDT_TCP_CTRL_OCP__OCP_MAX_POWER__SHIFT,            0xffff,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__MAX_POWER_DELTA_MASK,               DIDT_TCP_CTRL2__MAX_POWER_DELTA__SHIFT,             0x3dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__UNUSED_0_MASK,                      DIDT_TCP_CTRL2__UNUSED_0__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__SHORT_TERM_INTERVAL_SIZE_MASK,      DIDT_TCP_CTRL2__SHORT_TERM_INTERVAL_SIZE__SHIFT,    0x0032,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__UNUSED_1_MASK,                      DIDT_TCP_CTRL2__UNUSED_1__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__LONG_TERM_INTERVAL_RATIO_MASK,      DIDT_TCP_CTRL2__LONG_TERM_INTERVAL_RATIO__SHIFT,    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL2,                  DIDT_TCP_CTRL2__UNUSED_2_MASK,                      DIDT_TCP_CTRL2__UNUSED_2__SHIFT,                    0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__DIDT_STALL_CTRL_ENABLE_MASK,   DIDT_TCP_STALL_CTRL__DIDT_STALL_CTRL_ENABLE__SHIFT, 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__DIDT_STALL_DELAY_HI_MASK,      DIDT_TCP_STALL_CTRL__DIDT_STALL_DELAY_HI__SHIFT,    0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__DIDT_STALL_DELAY_LO_MASK,      DIDT_TCP_STALL_CTRL__DIDT_STALL_DELAY_LO__SHIFT,    0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__DIDT_HI_POWER_THRESHOLD_MASK,  DIDT_TCP_STALL_CTRL__DIDT_HI_POWER_THRESHOLD__SHIFT, 0x01aa,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_STALL_CTRL,             DIDT_TCP_STALL_CTRL__UNUSED_0_MASK,                 DIDT_TCP_STALL_CTRL__UNUSED_0__SHIFT,               0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_TUNING_CTRL,            DIDT_TCP_TUNING_CTRL__DIDT_TUNING_ENABLE_MASK,      DIDT_TCP_TUNING_CTRL__DIDT_TUNING_ENABLE__SHIFT,    0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_TUNING_CTRL,            DIDT_TCP_TUNING_CTRL__MAX_POWER_DELTA_HI_MASK,      DIDT_TCP_TUNING_CTRL__MAX_POWER_DELTA_HI__SHIFT,    0x3dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_TUNING_CTRL,            DIDT_TCP_TUNING_CTRL__MAX_POWER_DELTA_LO_MASK,      DIDT_TCP_TUNING_CTRL__MAX_POWER_DELTA_LO__SHIFT,    0x3dde,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_TUNING_CTRL,            DIDT_TCP_TUNING_CTRL__UNUSED_0_MASK,                DIDT_TCP_TUNING_CTRL__UNUSED_0__SHIFT,              0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_CTRL_EN_MASK,                   DIDT_TCP_CTRL0__DIDT_CTRL_EN__SHIFT,                 0x0001,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__USE_REF_CLOCK_MASK,                  DIDT_TCP_CTRL0__USE_REF_CLOCK__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__PHASE_OFFSET_MASK,                   DIDT_TCP_CTRL0__PHASE_OFFSET__SHIFT,                 0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_CTRL_RST_MASK,                  DIDT_TCP_CTRL0__DIDT_CTRL_RST__SHIFT,                0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_CLK_EN_OVERRIDE_MASK,           DIDT_TCP_CTRL0__DIDT_CLK_EN_OVERRIDE__SHIFT,         0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI_MASK,     DIDT_TCP_CTRL0__DIDT_MAX_STALLS_ALLOWED_HI__SHIFT,   0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO_MASK,     DIDT_TCP_CTRL0__DIDT_MAX_STALLS_ALLOWED_LO__SHIFT,   0x0010,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   ixDIDT_TCP_CTRL0,                   DIDT_TCP_CTRL0__UNUSED_0_MASK,                       DIDT_TCP_CTRL0__UNUSED_0__SHIFT,                     0x0000,     POLARIS10_CONFIGREG_DIDT_IND },
-	{   0xFFFFFFFF  }
-};
-
-static const struct polaris10_pt_defaults polaris10_power_tune_data_set_array[POWERTUNE_DEFAULT_SET_MAX] = {
-	/* sviLoadLIneEn, SviLoadLineVddC, TDC_VDDC_ThrottleReleaseLimitPerc, TDC_MAWt,
-	 * TdcWaterfallCtl, DTEAmbientTempBase, DisplayCac, BAPM_TEMP_GRADIENT */
-	{ 1, 0xF, 0xFD, 0x19, 5, 45, 0, 0xB0000,
-	{ 0x79, 0x253, 0x25D, 0xAE, 0x72, 0x80, 0x83, 0x86, 0x6F, 0xC8, 0xC9, 0xC9, 0x2F, 0x4D, 0x61},
-	{ 0x17C, 0x172, 0x180, 0x1BC, 0x1B3, 0x1BD, 0x206, 0x200, 0x203, 0x25D, 0x25A, 0x255, 0x2C3, 0x2C5, 0x2B4 } },
-};
-
-void polaris10_initialize_power_tune_defaults(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *polaris10_hwmgr = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct  phm_ppt_v1_information *table_info =
-			(struct  phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (table_info &&
-			table_info->cac_dtp_table->usPowerTuneDataSetID <= POWERTUNE_DEFAULT_SET_MAX &&
-			table_info->cac_dtp_table->usPowerTuneDataSetID)
-		polaris10_hwmgr->power_tune_defaults =
-				&polaris10_power_tune_data_set_array
-				[table_info->cac_dtp_table->usPowerTuneDataSetID - 1];
-	else
-		polaris10_hwmgr->power_tune_defaults = &polaris10_power_tune_data_set_array[0];
-
-}
-
-static uint16_t scale_fan_gain_settings(uint16_t raw_setting)
-{
-	uint32_t tmp;
-	tmp = raw_setting * 4096 / 100;
-	return (uint16_t)tmp;
-}
-
-int polaris10_populate_bapm_parameters_in_dpm_table(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	const struct polaris10_pt_defaults *defaults = data->power_tune_defaults;
-	SMU74_Discrete_DpmTable  *dpm_table = &(data->smc_state_table);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_cac_tdp_table *cac_dtp_table = table_info->cac_dtp_table;
-	struct pp_advance_fan_control_parameters *fan_table=
-			&hwmgr->thermal_controller.advanceFanControlParameters;
-	int i, j, k;
-	const uint16_t *pdef1;
-	const uint16_t *pdef2;
-
-	dpm_table->DefaultTdp = PP_HOST_TO_SMC_US((uint16_t)(cac_dtp_table->usTDP * 128));
-	dpm_table->TargetTdp  = PP_HOST_TO_SMC_US((uint16_t)(cac_dtp_table->usTDP * 128));
-
-	PP_ASSERT_WITH_CODE(cac_dtp_table->usTargetOperatingTemp <= 255,
-				"Target Operating Temp is out of Range!",
-				);
-
-	dpm_table->TemperatureLimitEdge = PP_HOST_TO_SMC_US(
-			cac_dtp_table->usTargetOperatingTemp * 256);
-	dpm_table->TemperatureLimitHotspot = PP_HOST_TO_SMC_US(
-			cac_dtp_table->usTemperatureLimitHotspot * 256);
-	dpm_table->FanGainEdge = PP_HOST_TO_SMC_US(
-			scale_fan_gain_settings(fan_table->usFanGainEdge));
-	dpm_table->FanGainHotspot = PP_HOST_TO_SMC_US(
-			scale_fan_gain_settings(fan_table->usFanGainHotspot));
-
-	pdef1 = defaults->BAPMTI_R;
-	pdef2 = defaults->BAPMTI_RC;
-
-	for (i = 0; i < SMU74_DTE_ITERATIONS; i++) {
-		for (j = 0; j < SMU74_DTE_SOURCES; j++) {
-			for (k = 0; k < SMU74_DTE_SINKS; k++) {
-				dpm_table->BAPMTI_R[i][j][k] = PP_HOST_TO_SMC_US(*pdef1);
-				dpm_table->BAPMTI_RC[i][j][k] = PP_HOST_TO_SMC_US(*pdef2);
-				pdef1++;
-				pdef2++;
-			}
-		}
-	}
-
-	return 0;
-}
-
-static int polaris10_populate_svi_load_line(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	const struct polaris10_pt_defaults *defaults = data->power_tune_defaults;
-
-	data->power_tune_table.SviLoadLineEn = defaults->SviLoadLineEn;
-	data->power_tune_table.SviLoadLineVddC = defaults->SviLoadLineVddC;
-	data->power_tune_table.SviLoadLineTrimVddC = 3;
-	data->power_tune_table.SviLoadLineOffsetVddC = 0;
-
-	return 0;
-}
-
-static int polaris10_populate_tdc_limit(struct pp_hwmgr *hwmgr)
-{
-	uint16_t tdc_limit;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	const struct polaris10_pt_defaults *defaults = data->power_tune_defaults;
-
-	tdc_limit = (uint16_t)(table_info->cac_dtp_table->usTDC * 128);
-	data->power_tune_table.TDC_VDDC_PkgLimit =
-			CONVERT_FROM_HOST_TO_SMC_US(tdc_limit);
-	data->power_tune_table.TDC_VDDC_ThrottleReleaseLimitPerc =
-			defaults->TDC_VDDC_ThrottleReleaseLimitPerc;
-	data->power_tune_table.TDC_MAWt = defaults->TDC_MAWt;
-
-	return 0;
-}
-
-static int polaris10_populate_dw8(struct pp_hwmgr *hwmgr, uint32_t fuse_table_offset)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	const struct polaris10_pt_defaults *defaults = data->power_tune_defaults;
-	uint32_t temp;
-
-	if (polaris10_read_smc_sram_dword(hwmgr->smumgr,
-			fuse_table_offset +
-			offsetof(SMU74_Discrete_PmFuses, TdcWaterfallCtl),
-			(uint32_t *)&temp, data->sram_end))
-		PP_ASSERT_WITH_CODE(false,
-				"Attempt to read PmFuses.DW6 (SviLoadLineEn) from SMC Failed!",
-				return -EINVAL);
-	else {
-		data->power_tune_table.TdcWaterfallCtl = defaults->TdcWaterfallCtl;
-		data->power_tune_table.LPMLTemperatureMin =
-				(uint8_t)((temp >> 16) & 0xff);
-		data->power_tune_table.LPMLTemperatureMax =
-				(uint8_t)((temp >> 8) & 0xff);
-		data->power_tune_table.Reserved = (uint8_t)(temp & 0xff);
-	}
-	return 0;
-}
-
-static int polaris10_populate_temperature_scaler(struct pp_hwmgr *hwmgr)
-{
-	int i;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	/* Currently not used. Set all to zero. */
-	for (i = 0; i < 16; i++)
-		data->power_tune_table.LPMLTemperatureScaler[i] = 0;
-
-	return 0;
-}
-
-static int polaris10_populate_fuzzy_fan(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if ((hwmgr->thermal_controller.advanceFanControlParameters.usFanOutputSensitivity & (1 << 15))
-		|| 0 == hwmgr->thermal_controller.advanceFanControlParameters.usFanOutputSensitivity)
-		hwmgr->thermal_controller.advanceFanControlParameters.usFanOutputSensitivity =
-			hwmgr->thermal_controller.advanceFanControlParameters.usDefaultFanOutputSensitivity;
-
-	data->power_tune_table.FuzzyFan_PwmSetDelta = PP_HOST_TO_SMC_US(
-				hwmgr->thermal_controller.advanceFanControlParameters.usFanOutputSensitivity);
-	return 0;
-}
-
-static int polaris10_populate_gnb_lpml(struct pp_hwmgr *hwmgr)
-{
-	int i;
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	/* Currently not used. Set all to zero. */
-	for (i = 0; i < 16; i++)
-		data->power_tune_table.GnbLPML[i] = 0;
-
-	return 0;
-}
-
-static int polaris10_min_max_vgnb_lpml_id_from_bapm_vddc(struct pp_hwmgr *hwmgr)
-{
-	return 0;
-}
-
-static int polaris10_enable_didt(struct pp_hwmgr *hwmgr, const bool enable)
-{
-
-	uint32_t en = enable ? 1 : 0;
-	int32_t result = 0;
-	uint32_t data;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_SQRamping)) {
-		data = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__DIDT, ixDIDT_SQ_CTRL0);
-		data &= ~DIDT_SQ_CTRL0__DIDT_CTRL_EN_MASK;
-		data |= ((en << DIDT_SQ_CTRL0__DIDT_CTRL_EN__SHIFT) & DIDT_SQ_CTRL0__DIDT_CTRL_EN_MASK);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__DIDT, ixDIDT_SQ_CTRL0, data);
-		DIDTBlock_Info &= ~SQ_Enable_MASK;
-		DIDTBlock_Info |= en << SQ_Enable_SHIFT;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_DBRamping)) {
-		data = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__DIDT, ixDIDT_DB_CTRL0);
-		data &= ~DIDT_DB_CTRL0__DIDT_CTRL_EN_MASK;
-		data |= ((en << DIDT_DB_CTRL0__DIDT_CTRL_EN__SHIFT) & DIDT_DB_CTRL0__DIDT_CTRL_EN_MASK);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__DIDT, ixDIDT_DB_CTRL0, data);
-		DIDTBlock_Info &= ~DB_Enable_MASK;
-		DIDTBlock_Info |= en << DB_Enable_SHIFT;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_TDRamping)) {
-		data = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__DIDT, ixDIDT_TD_CTRL0);
-		data &= ~DIDT_TD_CTRL0__DIDT_CTRL_EN_MASK;
-		data |= ((en << DIDT_TD_CTRL0__DIDT_CTRL_EN__SHIFT) & DIDT_TD_CTRL0__DIDT_CTRL_EN_MASK);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__DIDT, ixDIDT_TD_CTRL0, data);
-		DIDTBlock_Info &= ~TD_Enable_MASK;
-		DIDTBlock_Info |= en << TD_Enable_SHIFT;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_TCPRamping)) {
-		data = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__DIDT, ixDIDT_TCP_CTRL0);
-		data &= ~DIDT_TCP_CTRL0__DIDT_CTRL_EN_MASK;
-		data |= ((en << DIDT_TCP_CTRL0__DIDT_CTRL_EN__SHIFT) & DIDT_TCP_CTRL0__DIDT_CTRL_EN_MASK);
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__DIDT, ixDIDT_TCP_CTRL0, data);
-		DIDTBlock_Info &= ~TCP_Enable_MASK;
-		DIDTBlock_Info |= en << TCP_Enable_SHIFT;
-	}
-
-	if (enable)
-		result = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_Didt_Block_Function, DIDTBlock_Info);
-
-	return result;
-}
-
-static int polaris10_program_pt_config_registers(struct pp_hwmgr *hwmgr,
-				struct polaris10_pt_config_reg *cac_config_regs)
-{
-	struct polaris10_pt_config_reg *config_regs = cac_config_regs;
-	uint32_t cache = 0;
-	uint32_t data = 0;
-
-	PP_ASSERT_WITH_CODE((config_regs != NULL), "Invalid config register table.", return -EINVAL);
-
-	while (config_regs->offset != 0xFFFFFFFF) {
-		if (config_regs->type == POLARIS10_CONFIGREG_CACHE)
-			cache |= ((config_regs->value << config_regs->shift) & config_regs->mask);
-		else {
-			switch (config_regs->type) {
-			case POLARIS10_CONFIGREG_SMC_IND:
-				data = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, config_regs->offset);
-				break;
-
-			case POLARIS10_CONFIGREG_DIDT_IND:
-				data = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__DIDT, config_regs->offset);
-				break;
-
-			case POLARIS10_CONFIGREG_GC_CAC_IND:
-				data = cgs_read_ind_register(hwmgr->device, CGS_IND_REG_GC_CAC, config_regs->offset);
-				break;
-
-			default:
-				data = cgs_read_register(hwmgr->device, config_regs->offset);
-				break;
-			}
-
-			data &= ~config_regs->mask;
-			data |= ((config_regs->value << config_regs->shift) & config_regs->mask);
-			data |= cache;
-
-			switch (config_regs->type) {
-			case POLARIS10_CONFIGREG_SMC_IND:
-				cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, config_regs->offset, data);
-				break;
-
-			case POLARIS10_CONFIGREG_DIDT_IND:
-				cgs_write_ind_register(hwmgr->device, CGS_IND_REG__DIDT, config_regs->offset, data);
-				break;
-
-			case POLARIS10_CONFIGREG_GC_CAC_IND:
-				cgs_write_ind_register(hwmgr->device, CGS_IND_REG_GC_CAC, config_regs->offset, data);
-				break;
-
-			default:
-				cgs_write_register(hwmgr->device, config_regs->offset, data);
-				break;
-			}
-			cache = 0;
-		}
-
-		config_regs++;
-	}
-
-	return 0;
-}
-
-int polaris10_enable_didt_config(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	uint32_t num_se = 0;
-	uint32_t count, value, value2;
-	struct cgs_system_info sys_info = {0};
-
-	sys_info.size = sizeof(struct cgs_system_info);
-	sys_info.info_id = CGS_SYSTEM_INFO_GFX_SE_INFO;
-	result = cgs_query_system_info(hwmgr->device, &sys_info);
-
-
-	if (result == 0)
-		num_se = sys_info.value;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_SQRamping) ||
-		phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_DBRamping) ||
-		phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_TDRamping) ||
-		phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_TCPRamping)) {
-
-		/* TO DO Pre DIDT disable clock gating */
-		value = 0;
-		value2 = cgs_read_register(hwmgr->device, mmGRBM_GFX_INDEX);
-		for (count = 0; count < num_se; count++) {
-			value = SYS_GRBM_GFX_INDEX_DATA__INSTANCE_BROADCAST_WRITES_MASK
-				| SYS_GRBM_GFX_INDEX_DATA__SH_BROADCAST_WRITES_MASK
-				| (count << SYS_GRBM_GFX_INDEX_DATA__SE_INDEX__SHIFT);
-			cgs_write_register(hwmgr->device, mmGRBM_GFX_INDEX, value);
-
-			if (hwmgr->chip_id == CHIP_POLARIS10) {
-				result = polaris10_program_pt_config_registers(hwmgr, GCCACConfig_Polaris10);
-				PP_ASSERT_WITH_CODE((result == 0), "DIDT Config failed.", return result);
-				result = polaris10_program_pt_config_registers(hwmgr, DIDTConfig_Polaris10);
-				PP_ASSERT_WITH_CODE((result == 0), "DIDT Config failed.", return result);
-			} else if (hwmgr->chip_id == CHIP_POLARIS11) {
-				result = polaris10_program_pt_config_registers(hwmgr, GCCACConfig_Polaris11);
-				PP_ASSERT_WITH_CODE((result == 0), "DIDT Config failed.", return result);
-				result = polaris10_program_pt_config_registers(hwmgr, DIDTConfig_Polaris11);
-				PP_ASSERT_WITH_CODE((result == 0), "DIDT Config failed.", return result);
-			}
-		}
-		cgs_write_register(hwmgr->device, mmGRBM_GFX_INDEX, value2);
-
-		result = polaris10_enable_didt(hwmgr, true);
-		PP_ASSERT_WITH_CODE((result == 0), "EnableDiDt failed.", return result);
-
-		/* TO DO Post DIDT enable clock gating */
-	}
-
-	return 0;
-}
-
-int polaris10_disable_didt_config(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_SQRamping) ||
-		phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_DBRamping) ||
-		phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_TDRamping) ||
-		phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_TCPRamping)) {
-		/* TO DO Pre DIDT disable clock gating */
-
-		result = polaris10_enable_didt(hwmgr, false);
-		PP_ASSERT_WITH_CODE((result == 0), "Post DIDT enable clock gating failed.", return result);
-		/* TO DO Post DIDT enable clock gating */
-	}
-
-	return 0;
-}
-
-
-static int polaris10_populate_bapm_vddc_base_leakage_sidd(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint16_t hi_sidd = data->power_tune_table.BapmVddCBaseLeakageHiSidd;
-	uint16_t lo_sidd = data->power_tune_table.BapmVddCBaseLeakageLoSidd;
-	struct phm_cac_tdp_table *cac_table = table_info->cac_dtp_table;
-
-	hi_sidd = (uint16_t)(cac_table->usHighCACLeakage / 100 * 256);
-	lo_sidd = (uint16_t)(cac_table->usLowCACLeakage / 100 * 256);
-
-	data->power_tune_table.BapmVddCBaseLeakageHiSidd =
-			CONVERT_FROM_HOST_TO_SMC_US(hi_sidd);
-	data->power_tune_table.BapmVddCBaseLeakageLoSidd =
-			CONVERT_FROM_HOST_TO_SMC_US(lo_sidd);
-
-	return 0;
-}
-
-int polaris10_populate_pm_fuses(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	uint32_t pm_fuse_table_offset;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		if (polaris10_read_smc_sram_dword(hwmgr->smumgr,
-				SMU7_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU74_Firmware_Header, PmFuseTable),
-				&pm_fuse_table_offset, data->sram_end))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to get pm_fuse_table_offset Failed!",
-					return -EINVAL);
-
-		if (polaris10_populate_svi_load_line(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate SviLoadLine Failed!",
-					return -EINVAL);
-
-		if (polaris10_populate_tdc_limit(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate TDCLimit Failed!", return -EINVAL);
-
-		if (polaris10_populate_dw8(hwmgr, pm_fuse_table_offset))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate TdcWaterfallCtl, "
-					"LPMLTemperature Min and Max Failed!",
-					return -EINVAL);
-
-		if (0 != polaris10_populate_temperature_scaler(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate LPMLTemperatureScaler Failed!",
-					return -EINVAL);
-
-		if (polaris10_populate_fuzzy_fan(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate Fuzzy Fan Control parameters Failed!",
-					return -EINVAL);
-
-		if (polaris10_populate_gnb_lpml(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate GnbLPML Failed!",
-					return -EINVAL);
-
-		if (polaris10_min_max_vgnb_lpml_id_from_bapm_vddc(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate GnbLPML Min and Max Vid Failed!",
-					return -EINVAL);
-
-		if (polaris10_populate_bapm_vddc_base_leakage_sidd(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate BapmVddCBaseLeakage Hi and Lo "
-					"Sidd Failed!", return -EINVAL);
-
-		if (polaris10_copy_bytes_to_smc(hwmgr->smumgr, pm_fuse_table_offset,
-				(uint8_t *)&data->power_tune_table,
-				(sizeof(struct SMU74_Discrete_PmFuses) - 92), data->sram_end))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to download PmFuseTable Failed!",
-					return -EINVAL);
-	}
-	return 0;
-}
-
-int polaris10_enable_smc_cac(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC)) {
-		int smc_result;
-		smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-				(uint16_t)(PPSMC_MSG_EnableCac));
-		PP_ASSERT_WITH_CODE((0 == smc_result),
-				"Failed to enable CAC in SMC.", result = -1);
-
-		data->cac_enabled = (0 == smc_result) ? true : false;
-	}
-	return result;
-}
-
-int polaris10_disable_smc_cac(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC) && data->cac_enabled) {
-		int smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-				(uint16_t)(PPSMC_MSG_DisableCac));
-		PP_ASSERT_WITH_CODE((smc_result == 0),
-				"Failed to disable CAC in SMC.", result = -1);
-
-		data->cac_enabled = false;
-	}
-	return result;
-}
-
-int polaris10_set_power_limit(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (data->power_containment_features &
-			POWERCONTAINMENT_FEATURE_PkgPwrLimit)
-		return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_PkgPwrSetLimit, n);
-	return 0;
-}
-
-static int polaris10_set_overdriver_target_tdp(struct pp_hwmgr *pHwMgr, uint32_t target_tdp)
-{
-	return smum_send_msg_to_smc_with_parameter(pHwMgr->smumgr,
-			PPSMC_MSG_OverDriveSetTargetTdp, target_tdp);
-}
-
-int polaris10_enable_power_containment(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	int smc_result;
-	int result = 0;
-
-	data->power_containment_features = 0;
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-
-		if (data->enable_tdc_limit_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_TDCLimitEnable));
-			PP_ASSERT_WITH_CODE((0 == smc_result),
-					"Failed to enable TDCLimit in SMC.", result = -1;);
-			if (0 == smc_result)
-				data->power_containment_features |=
-						POWERCONTAINMENT_FEATURE_TDCLimit;
-		}
-
-		if (data->enable_pkg_pwr_tracking_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_PkgPwrLimitEnable));
-			PP_ASSERT_WITH_CODE((0 == smc_result),
-					"Failed to enable PkgPwrTracking in SMC.", result = -1;);
-			if (0 == smc_result) {
-				struct phm_cac_tdp_table *cac_table =
-						table_info->cac_dtp_table;
-				uint32_t default_limit =
-					(uint32_t)(cac_table->usMaximumPowerDeliveryLimit * 256);
-
-				data->power_containment_features |=
-						POWERCONTAINMENT_FEATURE_PkgPwrLimit;
-
-				if (polaris10_set_power_limit(hwmgr, default_limit))
-					printk(KERN_ERR "Failed to set Default Power Limit in SMC!");
-			}
-		}
-	}
-	return result;
-}
-
-int polaris10_disable_power_containment(struct pp_hwmgr *hwmgr)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment) &&
-			data->power_containment_features) {
-		int smc_result;
-
-		if (data->power_containment_features &
-				POWERCONTAINMENT_FEATURE_TDCLimit) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_TDCLimitDisable));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to disable TDCLimit in SMC.",
-					result = smc_result);
-		}
-
-		if (data->power_containment_features &
-				POWERCONTAINMENT_FEATURE_DTE) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_DisableDTE));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to disable DTE in SMC.",
-					result = smc_result);
-		}
-
-		if (data->power_containment_features &
-				POWERCONTAINMENT_FEATURE_PkgPwrLimit) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_PkgPwrLimitDisable));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to disable PkgPwrTracking in SMC.",
-					result = smc_result);
-		}
-		data->power_containment_features = 0;
-	}
-
-	return result;
-}
-
-int polaris10_power_control_set_level(struct pp_hwmgr *hwmgr)
-{
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_cac_tdp_table *cac_table = table_info->cac_dtp_table;
-	int adjust_percent, target_tdp;
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		/* adjustment percentage has already been validated */
-		adjust_percent = hwmgr->platform_descriptor.TDPAdjustmentPolarity ?
-				hwmgr->platform_descriptor.TDPAdjustment :
-				(-1 * hwmgr->platform_descriptor.TDPAdjustment);
-		/* SMC requested that target_tdp to be 7 bit fraction in DPM table
-		 * but message to be 8 bit fraction for messages
-		 */
-		target_tdp = ((100 + adjust_percent) * (int)(cac_table->usTDP * 256)) / 100;
-		result = polaris10_set_overdriver_target_tdp(hwmgr, (uint32_t)target_tdp);
-	}
-
-	return result;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_powertune.h b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_powertune.h
deleted file mode 100644
index 329119d..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_powertune.h
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-#ifndef POLARIS10_POWERTUNE_H
-#define POLARIS10_POWERTUNE_H
-
-enum polaris10_pt_config_reg_type {
-	POLARIS10_CONFIGREG_MMR = 0,
-	POLARIS10_CONFIGREG_SMC_IND,
-	POLARIS10_CONFIGREG_DIDT_IND,
-	POLARIS10_CONFIGREG_GC_CAC_IND,
-	POLARIS10_CONFIGREG_CACHE,
-	POLARIS10_CONFIGREG_MAX
-};
-
-#define DIDT_SQ_CTRL0__UNUSED_0_MASK    0xfffc0000
-#define DIDT_SQ_CTRL0__UNUSED_0__SHIFT  0x12
-#define DIDT_TD_CTRL0__UNUSED_0_MASK    0xfffc0000
-#define DIDT_TD_CTRL0__UNUSED_0__SHIFT  0x12
-#define DIDT_TCP_CTRL0__UNUSED_0_MASK   0xfffc0000
-#define DIDT_TCP_CTRL0__UNUSED_0__SHIFT 0x12
-#define DIDT_SQ_TUNING_CTRL__UNUSED_0_MASK                 0xc0000000
-#define DIDT_SQ_TUNING_CTRL__UNUSED_0__SHIFT               0x0000001e
-#define DIDT_TD_TUNING_CTRL__UNUSED_0_MASK                 0xc0000000
-#define DIDT_TD_TUNING_CTRL__UNUSED_0__SHIFT               0x0000001e
-#define DIDT_TCP_TUNING_CTRL__UNUSED_0_MASK                0xc0000000
-#define DIDT_TCP_TUNING_CTRL__UNUSED_0__SHIFT              0x0000001e
-
-/* PowerContainment Features */
-#define POWERCONTAINMENT_FEATURE_DTE             0x00000001
-#define POWERCONTAINMENT_FEATURE_TDCLimit        0x00000002
-#define POWERCONTAINMENT_FEATURE_PkgPwrLimit     0x00000004
-
-#define ixGC_CAC_CNTL 0x0000
-#define ixDIDT_SQ_STALL_CTRL 0x0004
-#define  ixDIDT_SQ_TUNING_CTRL 0x0005
-#define ixDIDT_TD_STALL_CTRL 0x0044
-#define ixDIDT_TD_TUNING_CTRL 0x0045
-#define ixDIDT_TCP_STALL_CTRL 0x0064
-#define ixDIDT_TCP_TUNING_CTRL 0x0065
-
-struct polaris10_pt_config_reg {
-	uint32_t                           offset;
-	uint32_t                           mask;
-	uint32_t                           shift;
-	uint32_t                           value;
-	enum polaris10_pt_config_reg_type       type;
-};
-
-
-void polaris10_initialize_power_tune_defaults(struct pp_hwmgr *hwmgr);
-int polaris10_populate_bapm_parameters_in_dpm_table(struct pp_hwmgr *hwmgr);
-int polaris10_populate_pm_fuses(struct pp_hwmgr *hwmgr);
-int polaris10_enable_smc_cac(struct pp_hwmgr *hwmgr);
-int polaris10_disable_smc_cac(struct pp_hwmgr *hwmgr);
-int polaris10_enable_power_containment(struct pp_hwmgr *hwmgr);
-int polaris10_disable_power_containment(struct pp_hwmgr *hwmgr);
-int polaris10_set_power_limit(struct pp_hwmgr *hwmgr, uint32_t n);
-int polaris10_power_control_set_level(struct pp_hwmgr *hwmgr);
-int polaris10_enable_didt_config(struct pp_hwmgr *hwmgr);
-#endif  /* POLARIS10_POWERTUNE_H */
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_thermal.c b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_thermal.c
deleted file mode 100644
index 41f835a..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_thermal.c
+++ /dev/null
@@ -1,716 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#include <asm/div64.h>
-#include "polaris10_thermal.h"
-#include "polaris10_hwmgr.h"
-#include "polaris10_smumgr.h"
-#include "polaris10_ppsmc.h"
-#include "smu/smu_7_1_3_d.h"
-#include "smu/smu_7_1_3_sh_mask.h"
-
-int polaris10_fan_ctrl_get_fan_speed_info(struct pp_hwmgr *hwmgr,
-		struct phm_fan_speed_info *fan_speed_info)
-{
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	fan_speed_info->supports_percent_read = true;
-	fan_speed_info->supports_percent_write = true;
-	fan_speed_info->min_percent = 0;
-	fan_speed_info->max_percent = 100;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_FanSpeedInTableIsRPM) &&
-		hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution) {
-		fan_speed_info->supports_rpm_read = true;
-		fan_speed_info->supports_rpm_write = true;
-		fan_speed_info->min_rpm = hwmgr->thermal_controller.fanInfo.ulMinRPM;
-		fan_speed_info->max_rpm = hwmgr->thermal_controller.fanInfo.ulMaxRPM;
-	} else {
-		fan_speed_info->min_rpm = 0;
-		fan_speed_info->max_rpm = 0;
-	}
-
-	return 0;
-}
-
-int polaris10_fan_ctrl_get_fan_speed_percent(struct pp_hwmgr *hwmgr,
-		uint32_t *speed)
-{
-	uint32_t duty100;
-	uint32_t duty;
-	uint64_t tmp64;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL1, FMAX_DUTY100);
-	duty = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_STATUS, FDO_PWM_DUTY);
-
-	if (duty100 == 0)
-		return -EINVAL;
-
-
-	tmp64 = (uint64_t)duty * 100;
-	do_div(tmp64, duty100);
-	*speed = (uint32_t)tmp64;
-
-	if (*speed > 100)
-		*speed = 100;
-
-	return 0;
-}
-
-int polaris10_fan_ctrl_get_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t *speed)
-{
-	uint32_t tach_period;
-	uint32_t crystal_clock_freq;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan ||
-			(hwmgr->thermal_controller.fanInfo.
-				ucTachometerPulsesPerRevolution == 0))
-		return 0;
-
-	tach_period = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_TACH_STATUS, TACH_PERIOD);
-
-	if (tach_period == 0)
-		return -EINVAL;
-
-	crystal_clock_freq = tonga_get_xclk(hwmgr);
-
-	*speed = 60 * crystal_clock_freq * 10000 / tach_period;
-
-	return 0;
-}
-
-/**
-* Set Fan Speed Control to static mode, so that the user can decide what speed to use.
-* @param    hwmgr  the address of the powerplay hardware manager.
-*           mode    the fan control mode, 0 default, 1 by percent, 5, by RPM
-* @exception Should always succeed.
-*/
-int polaris10_fan_ctrl_set_static_mode(struct pp_hwmgr *hwmgr, uint32_t mode)
-{
-
-	if (hwmgr->fan_ctrl_is_in_default_mode) {
-		hwmgr->fan_ctrl_default_mode =
-				PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device,	CGS_IND_REG__SMC,
-						CG_FDO_CTRL2, FDO_PWM_MODE);
-		hwmgr->tmin =
-				PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-						CG_FDO_CTRL2, TMIN);
-		hwmgr->fan_ctrl_is_in_default_mode = false;
-	}
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL2, TMIN, 0);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL2, FDO_PWM_MODE, mode);
-
-	return 0;
-}
-
-/**
-* Reset Fan Speed Control to default mode.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @exception Should always succeed.
-*/
-int polaris10_fan_ctrl_set_default_mode(struct pp_hwmgr *hwmgr)
-{
-	if (!hwmgr->fan_ctrl_is_in_default_mode) {
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_FDO_CTRL2, FDO_PWM_MODE, hwmgr->fan_ctrl_default_mode);
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_FDO_CTRL2, TMIN, hwmgr->tmin);
-		hwmgr->fan_ctrl_is_in_default_mode = true;
-	}
-
-	return 0;
-}
-
-static int polaris10_fan_ctrl_start_smc_fan_control(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ODFuzzyFanControlSupport)) {
-		cgs_write_register(hwmgr->device, mmSMC_MSG_ARG_0, FAN_CONTROL_FUZZY);
-		result = smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StartFanControl);
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_FanSpeedInTableIsRPM))
-			hwmgr->hwmgr_func->set_max_fan_rpm_output(hwmgr,
-					hwmgr->thermal_controller.
-					advanceFanControlParameters.usMaxFanRPM);
-		else
-			hwmgr->hwmgr_func->set_max_fan_pwm_output(hwmgr,
-					hwmgr->thermal_controller.
-					advanceFanControlParameters.usMaxFanPWM);
-
-	} else {
-		cgs_write_register(hwmgr->device, mmSMC_MSG_ARG_0, FAN_CONTROL_TABLE);
-		result = smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StartFanControl);
-	}
-
-	if (!result && hwmgr->thermal_controller.
-			advanceFanControlParameters.ucTargetTemperature)
-		result = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_SetFanTemperatureTarget,
-				hwmgr->thermal_controller.
-				advanceFanControlParameters.ucTargetTemperature);
-
-	return result;
-}
-
-
-int polaris10_fan_ctrl_stop_smc_fan_control(struct pp_hwmgr *hwmgr)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StopFanControl);
-}
-
-/**
-* Set Fan Speed in percent.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the percentage value (0% - 100%) to be set.
-* @exception Fails is the 100% setting appears to be 0.
-*/
-int polaris10_fan_ctrl_set_fan_speed_percent(struct pp_hwmgr *hwmgr,
-		uint32_t speed)
-{
-	uint32_t duty100;
-	uint32_t duty;
-	uint64_t tmp64;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	if (speed > 100)
-		speed = 100;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MicrocodeFanControl))
-		polaris10_fan_ctrl_stop_smc_fan_control(hwmgr);
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL1, FMAX_DUTY100);
-
-	if (duty100 == 0)
-		return -EINVAL;
-
-	tmp64 = (uint64_t)speed * duty100;
-	do_div(tmp64, 100);
-	duty = (uint32_t)tmp64;
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL0, FDO_STATIC_DUTY, duty);
-
-	return polaris10_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-}
-
-/**
-* Reset Fan Speed to default.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @exception Always succeeds.
-*/
-int polaris10_fan_ctrl_reset_fan_speed_to_default(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MicrocodeFanControl)) {
-		result = polaris10_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-		if (!result)
-			result = polaris10_fan_ctrl_start_smc_fan_control(hwmgr);
-	} else
-		result = polaris10_fan_ctrl_set_default_mode(hwmgr);
-
-	return result;
-}
-
-/**
-* Set Fan Speed in RPM.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the percentage value (min - max) to be set.
-* @exception Fails is the speed not lie between min and max.
-*/
-int polaris10_fan_ctrl_set_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t speed)
-{
-	uint32_t tach_period;
-	uint32_t crystal_clock_freq;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan ||
-			(hwmgr->thermal_controller.fanInfo.
-			ucTachometerPulsesPerRevolution == 0) ||
-			(speed < hwmgr->thermal_controller.fanInfo.ulMinRPM) ||
-			(speed > hwmgr->thermal_controller.fanInfo.ulMaxRPM))
-		return 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MicrocodeFanControl))
-		polaris10_fan_ctrl_stop_smc_fan_control(hwmgr);
-
-	crystal_clock_freq = tonga_get_xclk(hwmgr);
-
-	tach_period = 60 * crystal_clock_freq * 10000 / (8 * speed);
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_TACH_STATUS, TACH_PERIOD, tach_period);
-
-	return polaris10_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-}
-
-/**
-* Reads the remote temperature from the SIslands thermal controller.
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-int polaris10_thermal_get_temperature(struct pp_hwmgr *hwmgr)
-{
-	int temp;
-
-	temp = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_MULT_THERMAL_STATUS, CTF_TEMP);
-
-	/* Bit 9 means the reading is lower than the lowest usable value. */
-	if (temp & 0x200)
-		temp = POLARIS10_THERMAL_MAXIMUM_TEMP_READING;
-	else
-		temp = temp & 0x1ff;
-
-	temp *= PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-
-	return temp;
-}
-
-/**
-* Set the requested temperature range for high and low alert signals
-*
-* @param    hwmgr The address of the hardware manager.
-* @param    range Temperature range to be programmed for high and low alert signals
-* @exception PP_Result_BadInput if the input data is not valid.
-*/
-static int polaris10_thermal_set_temperature_range(struct pp_hwmgr *hwmgr,
-		uint32_t low_temp, uint32_t high_temp)
-{
-	uint32_t low = POLARIS10_THERMAL_MINIMUM_ALERT_TEMP *
-			PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-	uint32_t high = POLARIS10_THERMAL_MAXIMUM_ALERT_TEMP *
-			PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-
-	if (low < low_temp)
-		low = low_temp;
-	if (high > high_temp)
-		high = high_temp;
-
-	if (low > high)
-		return -EINVAL;
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, DIG_THERM_INTH,
-			(high / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, DIG_THERM_INTL,
-			(low / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_CTRL, DIG_THERM_DPM,
-			(high / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-
-	return 0;
-}
-
-/**
-* Programs thermal controller one-time setting registers
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-static int polaris10_thermal_initialize(struct pp_hwmgr *hwmgr)
-{
-	if (hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution)
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_TACH_CTRL, EDGE_PER_REV,
-				hwmgr->thermal_controller.fanInfo.
-				ucTachometerPulsesPerRevolution - 1);
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL2, TACH_PWM_RESP_RATE, 0x28);
-
-	return 0;
-}
-
-/**
-* Enable thermal alerts on the RV770 thermal controller.
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-static int polaris10_thermal_enable_alert(struct pp_hwmgr *hwmgr)
-{
-	uint32_t alert;
-
-	alert = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, THERM_INT_MASK);
-	alert &= ~(POLARIS10_THERMAL_HIGH_ALERT_MASK | POLARIS10_THERMAL_LOW_ALERT_MASK);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, THERM_INT_MASK, alert);
-
-	/* send message to SMU to enable internal thermal interrupts */
-	return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_Thermal_Cntl_Enable);
-}
-
-/**
-* Disable thermal alerts on the RV770 thermal controller.
-* @param    hwmgr The address of the hardware manager.
-*/
-static int polaris10_thermal_disable_alert(struct pp_hwmgr *hwmgr)
-{
-	uint32_t alert;
-
-	alert = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, THERM_INT_MASK);
-	alert |= (POLARIS10_THERMAL_HIGH_ALERT_MASK | POLARIS10_THERMAL_LOW_ALERT_MASK);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_THERMAL_INT, THERM_INT_MASK, alert);
-
-	/* send message to SMU to disable internal thermal interrupts */
-	return smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_Thermal_Cntl_Disable);
-}
-
-/**
-* Uninitialize the thermal controller.
-* Currently just disables alerts.
-* @param    hwmgr The address of the hardware manager.
-*/
-int polaris10_thermal_stop_thermal_controller(struct pp_hwmgr *hwmgr)
-{
-	int result = polaris10_thermal_disable_alert(hwmgr);
-
-	if (!hwmgr->thermal_controller.fanInfo.bNoFan)
-		polaris10_fan_ctrl_set_default_mode(hwmgr);
-
-	return result;
-}
-
-/**
-* Set up the fan table to control the fan using the SMC.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-static int tf_polaris10_thermal_setup_fan_table(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-	SMU74_Discrete_FanTable fan_table = { FDO_MODE_HARDWARE };
-	uint32_t duty100;
-	uint32_t t_diff1, t_diff2, pwm_diff1, pwm_diff2;
-	uint16_t fdo_min, slope1, slope2;
-	uint32_t reference_clock;
-	int res;
-	uint64_t tmp64;
-
-	if (data->fan_table_start == 0) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl);
-		return 0;
-	}
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-			CG_FDO_CTRL1, FMAX_DUTY100);
-
-	if (duty100 == 0) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl);
-		return 0;
-	}
-
-	tmp64 = hwmgr->thermal_controller.advanceFanControlParameters.
-			usPWMMin * duty100;
-	do_div(tmp64, 10000);
-	fdo_min = (uint16_t)tmp64;
-
-	t_diff1 = hwmgr->thermal_controller.advanceFanControlParameters.usTMed -
-			hwmgr->thermal_controller.advanceFanControlParameters.usTMin;
-	t_diff2 = hwmgr->thermal_controller.advanceFanControlParameters.usTHigh -
-			hwmgr->thermal_controller.advanceFanControlParameters.usTMed;
-
-	pwm_diff1 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMMed -
-			hwmgr->thermal_controller.advanceFanControlParameters.usPWMMin;
-	pwm_diff2 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMHigh -
-			hwmgr->thermal_controller.advanceFanControlParameters.usPWMMed;
-
-	slope1 = (uint16_t)((50 + ((16 * duty100 * pwm_diff1) / t_diff1)) / 100);
-	slope2 = (uint16_t)((50 + ((16 * duty100 * pwm_diff2) / t_diff2)) / 100);
-
-	fan_table.TempMin = cpu_to_be16((50 + hwmgr->
-			thermal_controller.advanceFanControlParameters.usTMin) / 100);
-	fan_table.TempMed = cpu_to_be16((50 + hwmgr->
-			thermal_controller.advanceFanControlParameters.usTMed) / 100);
-	fan_table.TempMax = cpu_to_be16((50 + hwmgr->
-			thermal_controller.advanceFanControlParameters.usTMax) / 100);
-
-	fan_table.Slope1 = cpu_to_be16(slope1);
-	fan_table.Slope2 = cpu_to_be16(slope2);
-
-	fan_table.FdoMin = cpu_to_be16(fdo_min);
-
-	fan_table.HystDown = cpu_to_be16(hwmgr->
-			thermal_controller.advanceFanControlParameters.ucTHyst);
-
-	fan_table.HystUp = cpu_to_be16(1);
-
-	fan_table.HystSlope = cpu_to_be16(1);
-
-	fan_table.TempRespLim = cpu_to_be16(5);
-
-	reference_clock = tonga_get_xclk(hwmgr);
-
-	fan_table.RefreshPeriod = cpu_to_be32((hwmgr->
-			thermal_controller.advanceFanControlParameters.ulCycleDelay *
-			reference_clock) / 1600);
-
-	fan_table.FdoMax = cpu_to_be16((uint16_t)duty100);
-
-	fan_table.TempSrc = (uint8_t)PHM_READ_VFPF_INDIRECT_FIELD(
-			hwmgr->device, CGS_IND_REG__SMC,
-			CG_MULT_THERMAL_CTRL, TEMP_SEL);
-
-	res = polaris10_copy_bytes_to_smc(hwmgr->smumgr, data->fan_table_start,
-			(uint8_t *)&fan_table, (uint32_t)sizeof(fan_table),
-			data->sram_end);
-
-	if (!res && hwmgr->thermal_controller.
-			advanceFanControlParameters.ucMinimumPWMLimit)
-		res = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_SetFanMinPwm,
-				hwmgr->thermal_controller.
-				advanceFanControlParameters.ucMinimumPWMLimit);
-
-	if (!res && hwmgr->thermal_controller.
-			advanceFanControlParameters.ulMinFanSCLKAcousticLimit)
-		res = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_SetFanSclkTarget,
-				hwmgr->thermal_controller.
-				advanceFanControlParameters.ulMinFanSCLKAcousticLimit);
-
-	if (res)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl);
-
-	return 0;
-}
-
-/**
-* Start the fan control on the SMC.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-static int tf_polaris10_thermal_start_smc_fan_control(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-/* If the fantable setup has failed we could have disabled
- * PHM_PlatformCaps_MicrocodeFanControl even after
- * this function was included in the table.
- * Make sure that we still think controlling the fan is OK.
-*/
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MicrocodeFanControl)) {
-		polaris10_fan_ctrl_start_smc_fan_control(hwmgr);
-		polaris10_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-	}
-
-	return 0;
-}
-
-/**
-* Set temperature range for high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-int tf_polaris10_thermal_set_temperature_range(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	struct PP_TemperatureRange *range = (struct PP_TemperatureRange *)input;
-
-	if (range == NULL)
-		return -EINVAL;
-
-	return polaris10_thermal_set_temperature_range(hwmgr, range->min, range->max);
-}
-
-/**
-* Programs one-time setting registers
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from initialize thermal controller routine
-*/
-int tf_polaris10_thermal_initialize(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-    return polaris10_thermal_initialize(hwmgr);
-}
-
-/**
-* Enable high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from enable alert routine
-*/
-int tf_polaris10_thermal_enable_alert(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	return polaris10_thermal_enable_alert(hwmgr);
-}
-
-/**
-* Disable high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from disable alert routine
-*/
-static int tf_polaris10_thermal_disable_alert(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	return polaris10_thermal_disable_alert(hwmgr);
-}
-
-static int tf_polaris10_thermal_avfs_enable(struct pp_hwmgr *hwmgr,
-		void *input, void *output, void *storage, int result)
-{
-	int ret;
-	struct pp_smumgr *smumgr = (struct pp_smumgr *)(hwmgr->smumgr);
-	struct polaris10_smumgr *smu_data = (struct polaris10_smumgr *)(smumgr->backend);
-	struct polaris10_hwmgr *data = (struct polaris10_hwmgr *)(hwmgr->backend);
-
-	if (smu_data->avfs.avfs_btc_status == AVFS_BTC_NOTSUPPORTED)
-		return 0;
-
-	ret = smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-			PPSMC_MSG_SetGBDroopSettings, data->avfs_vdroop_override_setting);
-
-	ret = (smum_send_msg_to_smc(smumgr, PPSMC_MSG_EnableAvfs) == 0) ?
-			0 : -1;
-
-	if (!ret)
-		/* If this param is not changed, this function could fire unnecessarily */
-		smu_data->avfs.avfs_btc_status = AVFS_BTC_COMPLETED_PREVIOUSLY;
-
-	return ret;
-}
-
-static const struct phm_master_table_item
-polaris10_thermal_start_thermal_controller_master_list[] = {
-	{NULL, tf_polaris10_thermal_initialize},
-	{NULL, tf_polaris10_thermal_set_temperature_range},
-	{NULL, tf_polaris10_thermal_enable_alert},
-	{NULL, tf_polaris10_thermal_avfs_enable},
-/* We should restrict performance levels to low before we halt the SMC.
- * On the other hand we are still in boot state when we do this
- * so it would be pointless.
- * If this assumption changes we have to revisit this table.
- */
-	{NULL, tf_polaris10_thermal_setup_fan_table},
-	{NULL, tf_polaris10_thermal_start_smc_fan_control},
-	{NULL, NULL}
-};
-
-static const struct phm_master_table_header
-polaris10_thermal_start_thermal_controller_master = {
-	0,
-	PHM_MasterTableFlag_None,
-	polaris10_thermal_start_thermal_controller_master_list
-};
-
-static const struct phm_master_table_item
-polaris10_thermal_set_temperature_range_master_list[] = {
-	{NULL, tf_polaris10_thermal_disable_alert},
-	{NULL, tf_polaris10_thermal_set_temperature_range},
-	{NULL, tf_polaris10_thermal_enable_alert},
-	{NULL, NULL}
-};
-
-static const struct phm_master_table_header
-polaris10_thermal_set_temperature_range_master = {
-	0,
-	PHM_MasterTableFlag_None,
-	polaris10_thermal_set_temperature_range_master_list
-};
-
-int polaris10_thermal_ctrl_uninitialize_thermal_controller(struct pp_hwmgr *hwmgr)
-{
-	if (!hwmgr->thermal_controller.fanInfo.bNoFan)
-		polaris10_fan_ctrl_set_default_mode(hwmgr);
-	return 0;
-}
-
-/**
-* Initializes the thermal controller related functions in the Hardware Manager structure.
-* @param    hwmgr The address of the hardware manager.
-* @exception Any error code from the low-level communication.
-*/
-int pp_polaris10_thermal_initialize(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	result = phm_construct_table(hwmgr,
-			&polaris10_thermal_set_temperature_range_master,
-			&(hwmgr->set_temperature_range));
-
-	if (!result) {
-		result = phm_construct_table(hwmgr,
-				&polaris10_thermal_start_thermal_controller_master,
-				&(hwmgr->start_thermal_controller));
-		if (result)
-			phm_destroy_table(hwmgr, &(hwmgr->set_temperature_range));
-	}
-
-	if (!result)
-		hwmgr->fan_ctrl_is_in_default_mode = true;
-	return result;
-}
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_thermal.h b/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_thermal.h
deleted file mode 100644
index 62f8cbc..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/polaris10_thermal.h
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Copyright 2016 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef _POLARIS10_THERMAL_H_
-#define _POLARIS10_THERMAL_H_
-
-#include "hwmgr.h"
-
-#define POLARIS10_THERMAL_HIGH_ALERT_MASK         0x1
-#define POLARIS10_THERMAL_LOW_ALERT_MASK          0x2
-
-#define POLARIS10_THERMAL_MINIMUM_TEMP_READING    -256
-#define POLARIS10_THERMAL_MAXIMUM_TEMP_READING    255
-
-#define POLARIS10_THERMAL_MINIMUM_ALERT_TEMP      0
-#define POLARIS10_THERMAL_MAXIMUM_ALERT_TEMP      255
-
-#define FDO_PWM_MODE_STATIC  1
-#define FDO_PWM_MODE_STATIC_RPM 5
-
-
-extern int tf_polaris10_thermal_initialize(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result);
-extern int tf_polaris10_thermal_set_temperature_range(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result);
-extern int tf_polaris10_thermal_enable_alert(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result);
-
-extern int polaris10_thermal_get_temperature(struct pp_hwmgr *hwmgr);
-extern int polaris10_thermal_stop_thermal_controller(struct pp_hwmgr *hwmgr);
-extern int polaris10_fan_ctrl_get_fan_speed_info(struct pp_hwmgr *hwmgr, struct phm_fan_speed_info *fan_speed_info);
-extern int polaris10_fan_ctrl_get_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t *speed);
-extern int polaris10_fan_ctrl_set_default_mode(struct pp_hwmgr *hwmgr);
-extern int polaris10_fan_ctrl_set_static_mode(struct pp_hwmgr *hwmgr, uint32_t mode);
-extern int polaris10_fan_ctrl_set_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t speed);
-extern int polaris10_fan_ctrl_reset_fan_speed_to_default(struct pp_hwmgr *hwmgr);
-extern int pp_polaris10_thermal_initialize(struct pp_hwmgr *hwmgr);
-extern int polaris10_thermal_ctrl_uninitialize_thermal_controller(struct pp_hwmgr *hwmgr);
-extern int polaris10_fan_ctrl_set_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t speed);
-extern int polaris10_fan_ctrl_get_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t *speed);
-extern int polaris10_fan_ctrl_stop_smc_fan_control(struct pp_hwmgr *hwmgr);
-extern uint32_t tonga_get_xclk(struct pp_hwmgr *hwmgr);
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/ppatomctrl.c b/drivers/gpu/drm/amd/powerplay/hwmgr/ppatomctrl.c
index ee53394..4b0a94c 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/ppatomctrl.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/ppatomctrl.c
@@ -23,7 +23,6 @@
 #include "pp_debug.h"
 #include <linux/module.h>
 #include <linux/slab.h>
-#include <linux/fb.h>
 
 #include "ppatomctrl.h"
 #include "atombios.h"
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/process_pptables_v1_0.c b/drivers/gpu/drm/amd/powerplay/hwmgr/process_pptables_v1_0.c
index e4acf51..84f01fd3 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/process_pptables_v1_0.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/process_pptables_v1_0.c
@@ -23,7 +23,6 @@
 #include "pp_debug.h"
 #include <linux/module.h>
 #include <linux/slab.h>
-#include <linux/fb.h>
 
 #include "process_pptables_v1_0.h"
 #include "ppatomctrl.h"
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/smu7_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/smu7_hwmgr.c
index 5e62ced..9232c11 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/smu7_hwmgr.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/smu7_hwmgr.c
@@ -1469,19 +1469,17 @@ static int smu7_get_evv_voltages(struct pp_hwmgr *hwmgr)
 	struct phm_ppt_v1_clock_voltage_dependency_table *sclk_table = NULL;
 
 
-	if (table_info == NULL)
-		return -EINVAL;
-
-	sclk_table = table_info->vdd_dep_on_sclk;
-
 	for (i = 0; i < SMU7_MAX_LEAKAGE_COUNT; i++) {
 		vv_id = ATOM_VIRTUAL_VOLTAGE_ID0 + i;
 
 		if (data->vdd_gfx_control == SMU7_VOLTAGE_CONTROL_BY_SVID2) {
-			if (0 == phm_get_sclk_for_voltage_evv(hwmgr,
+			if ((hwmgr->pp_table_version == PP_TABLE_V1)
+			    && !phm_get_sclk_for_voltage_evv(hwmgr,
 						table_info->vddgfx_lookup_table, vv_id, &sclk)) {
 				if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
 							PHM_PlatformCaps_ClockStretcher)) {
+					sclk_table = table_info->vdd_dep_on_sclk;
+
 					for (j = 1; j < sclk_table->count; j++) {
 						if (sclk_table->entries[j].clk == sclk &&
 								sclk_table->entries[j].cks_enable == 0) {
@@ -1507,12 +1505,15 @@ static int smu7_get_evv_voltages(struct pp_hwmgr *hwmgr)
 				}
 			}
 		} else {
-
 			if ((hwmgr->pp_table_version == PP_TABLE_V0)
 				|| !phm_get_sclk_for_voltage_evv(hwmgr,
 					table_info->vddc_lookup_table, vv_id, &sclk)) {
 				if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
 						PHM_PlatformCaps_ClockStretcher)) {
+					if (table_info == NULL)
+						return -EINVAL;
+					sclk_table = table_info->vdd_dep_on_sclk;
+
 					for (j = 1; j < sclk_table->count; j++) {
 						if (sclk_table->entries[j].clk == sclk &&
 								sclk_table->entries[j].cks_enable == 0) {
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_clockpowergating.c b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_clockpowergating.c
deleted file mode 100644
index e58d038..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_clockpowergating.c
+++ /dev/null
@@ -1,350 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#include "hwmgr.h"
-#include "tonga_clockpowergating.h"
-#include "tonga_ppsmc.h"
-#include "tonga_hwmgr.h"
-
-int tonga_phm_powerdown_uvd(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cf_want_uvd_power_gating(hwmgr))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-						     PPSMC_MSG_UVDPowerOFF);
-	return 0;
-}
-
-int tonga_phm_powerup_uvd(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cf_want_uvd_power_gating(hwmgr)) {
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				  PHM_PlatformCaps_UVDDynamicPowerGating)) {
-			return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-								PPSMC_MSG_UVDPowerON, 1);
-		} else {
-			return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-								PPSMC_MSG_UVDPowerON, 0);
-		}
-	}
-
-	return 0;
-}
-
-int tonga_phm_powerdown_vce(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cf_want_vce_power_gating(hwmgr))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-						  PPSMC_MSG_VCEPowerOFF);
-	return 0;
-}
-
-int tonga_phm_powerup_vce(struct pp_hwmgr *hwmgr)
-{
-	if (phm_cf_want_vce_power_gating(hwmgr))
-		return smum_send_msg_to_smc(hwmgr->smumgr,
-						  PPSMC_MSG_VCEPowerON);
-	return 0;
-}
-
-int tonga_phm_set_asic_block_gating(struct pp_hwmgr *hwmgr, enum PHM_AsicBlock block, enum PHM_ClockGateSetting gating)
-{
-	int ret = 0;
-
-	switch (block) {
-	case PHM_AsicBlock_UVD_MVC:
-	case PHM_AsicBlock_UVD:
-	case PHM_AsicBlock_UVD_HD:
-	case PHM_AsicBlock_UVD_SD:
-		if (gating == PHM_ClockGateSetting_StaticOff)
-			ret = tonga_phm_powerdown_uvd(hwmgr);
-		else
-			ret = tonga_phm_powerup_uvd(hwmgr);
-		break;
-	case PHM_AsicBlock_GFX:
-	default:
-		break;
-	}
-
-	return ret;
-}
-
-int tonga_phm_disable_clock_power_gating(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	data->uvd_power_gated = false;
-	data->vce_power_gated = false;
-
-	tonga_phm_powerup_uvd(hwmgr);
-	tonga_phm_powerup_vce(hwmgr);
-
-	return 0;
-}
-
-int tonga_phm_powergate_uvd(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	if (data->uvd_power_gated == bgate)
-		return 0;
-
-	data->uvd_power_gated = bgate;
-
-	if (bgate) {
-		cgs_set_clockgating_state(hwmgr->device,
-						AMD_IP_BLOCK_TYPE_UVD,
-						AMD_CG_STATE_UNGATE);
-		cgs_set_powergating_state(hwmgr->device,
-						AMD_IP_BLOCK_TYPE_UVD,
-						AMD_PG_STATE_GATE);
-		tonga_update_uvd_dpm(hwmgr, true);
-		tonga_phm_powerdown_uvd(hwmgr);
-	} else {
-		tonga_phm_powerup_uvd(hwmgr);
-		cgs_set_powergating_state(hwmgr->device,
-						AMD_IP_BLOCK_TYPE_UVD,
-						AMD_PG_STATE_UNGATE);
-		cgs_set_clockgating_state(hwmgr->device,
-						AMD_IP_BLOCK_TYPE_UVD,
-						AMD_PG_STATE_GATE);
-
-		tonga_update_uvd_dpm(hwmgr, false);
-	}
-
-	return 0;
-}
-
-int tonga_phm_powergate_vce(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct phm_set_power_state_input states;
-	const struct pp_power_state  *pcurrent;
-	struct pp_power_state  *requested;
-
-	pcurrent = hwmgr->current_ps;
-	requested = hwmgr->request_ps;
-
-	states.pcurrent_state = &(pcurrent->hardware);
-	states.pnew_state = &(requested->hardware);
-
-	if (phm_cf_want_vce_power_gating(hwmgr)) {
-		if (data->vce_power_gated != bgate) {
-			if (bgate) {
-				cgs_set_clockgating_state(
-							hwmgr->device,
-							AMD_IP_BLOCK_TYPE_VCE,
-							AMD_CG_STATE_UNGATE);
-				cgs_set_powergating_state(
-							hwmgr->device,
-							AMD_IP_BLOCK_TYPE_VCE,
-							AMD_PG_STATE_GATE);
-				tonga_enable_disable_vce_dpm(hwmgr, false);
-				data->vce_power_gated = true;
-			} else {
-				tonga_phm_powerup_vce(hwmgr);
-				data->vce_power_gated = false;
-				cgs_set_powergating_state(
-							hwmgr->device,
-							AMD_IP_BLOCK_TYPE_VCE,
-							AMD_PG_STATE_UNGATE);
-				cgs_set_clockgating_state(
-							hwmgr->device,
-							AMD_IP_BLOCK_TYPE_VCE,
-							AMD_PG_STATE_GATE);
-
-				tonga_update_vce_dpm(hwmgr, &states);
-				tonga_enable_disable_vce_dpm(hwmgr, true);
-				return 0;
-			}
-		}
-	} else {
-		tonga_update_vce_dpm(hwmgr, &states);
-		tonga_enable_disable_vce_dpm(hwmgr, true);
-		return 0;
-	}
-
-	if (!data->vce_power_gated)
-		tonga_update_vce_dpm(hwmgr, &states);
-
-	return 0;
-}
-
-int tonga_phm_update_clock_gatings(struct pp_hwmgr *hwmgr,
-					const uint32_t *msg_id)
-{
-	PPSMC_Msg msg;
-	uint32_t value;
-
-	switch ((*msg_id & PP_GROUP_MASK) >> PP_GROUP_SHIFT) {
-	case PP_GROUP_GFX:
-		switch ((*msg_id & PP_BLOCK_MASK) >> PP_BLOCK_SHIFT) {
-		case PP_BLOCK_GFX_CG:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG)
-				  ? PPSMC_MSG_EnableClockGatingFeature
-				  : PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_GFX_CGCG_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS
-					? PPSMC_MSG_EnableClockGatingFeature
-					: PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_GFX_CGLS_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_GFX_MG:
-			/* For GFX MGCG, there are three different ones;
-			 * CPF, RLC, and all others.  CPF MGCG will not be used for Tonga.
-			 * For GFX MGLS, Tonga will not support it.
-			 * */
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG)
-				? PPSMC_MSG_EnableClockGatingFeature
-				: PPSMC_MSG_DisableClockGatingFeature;
-				value = (CG_RLC_MGCG_MASK | CG_GFX_OTHERS_MGCG_MASK);
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		default:
-			return -1;
-		}
-		break;
-
-	case PP_GROUP_SYS:
-		switch ((*msg_id & PP_BLOCK_MASK) >> PP_BLOCK_SHIFT) {
-		case PP_BLOCK_SYS_BIF:
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS
-				? PPSMC_MSG_EnableClockGatingFeature
-				: PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_BIF_MGLS_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_SYS_MC:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG)
-				? PPSMC_MSG_EnableClockGatingFeature
-				: PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_MC_MGCG_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS
-				? PPSMC_MSG_EnableClockGatingFeature
-				: PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_MC_MGLS_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-
-			}
-			break;
-
-		case PP_BLOCK_SYS_HDP:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG)
-					? PPSMC_MSG_EnableClockGatingFeature
-					: PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_HDP_MGCG_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS
-					? PPSMC_MSG_EnableClockGatingFeature
-					: PPSMC_MSG_DisableClockGatingFeature;
-
-				value = CG_SYS_HDP_MGLS_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_SYS_SDMA:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG)
-				? PPSMC_MSG_EnableClockGatingFeature
-				: PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_SDMA_MGCG_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-
-			if (PP_STATE_SUPPORT_LS & *msg_id) {
-				msg = (*msg_id & PP_STATE_MASK) & PP_STATE_LS
-				? PPSMC_MSG_EnableClockGatingFeature
-				: PPSMC_MSG_DisableClockGatingFeature;
-
-				value = CG_SYS_SDMA_MGLS_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		case PP_BLOCK_SYS_ROM:
-			if (PP_STATE_SUPPORT_CG & *msg_id) {
-				msg = ((*msg_id & PP_STATE_MASK) & PP_STATE_CG)
-				? PPSMC_MSG_EnableClockGatingFeature
-				: PPSMC_MSG_DisableClockGatingFeature;
-				value = CG_SYS_ROM_MASK;
-
-				if (0 != smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, msg, value))
-					return -1;
-			}
-			break;
-
-		default:
-			return -1;
-
-		}
-		break;
-
-	default:
-		return -1;
-
-	}
-
-	return 0;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_clockpowergating.h b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_clockpowergating.h
deleted file mode 100644
index 8bc38cb..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_clockpowergating.h
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef _TONGA_CLOCK_POWER_GATING_H_
-#define _TONGA_CLOCK_POWER_GATING_H_
-
-#include "tonga_hwmgr.h"
-#include "pp_asicblocks.h"
-
-extern int tonga_phm_set_asic_block_gating(struct pp_hwmgr *hwmgr, enum PHM_AsicBlock block, enum PHM_ClockGateSetting gating);
-extern int tonga_phm_powergate_vce(struct pp_hwmgr *hwmgr, bool bgate);
-extern int tonga_phm_powergate_uvd(struct pp_hwmgr *hwmgr, bool bgate);
-extern int tonga_phm_powerdown_uvd(struct pp_hwmgr *hwmgr);
-extern int tonga_phm_disable_clock_power_gating(struct pp_hwmgr *hwmgr);
-extern int tonga_phm_update_clock_gatings(struct pp_hwmgr *hwmgr, const uint32_t *msg_id);
-#endif /* _TONGA_CLOCK_POWER_GATING_H_ */
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_dyn_defaults.h b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_dyn_defaults.h
deleted file mode 100644
index 080d69d..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_dyn_defaults.h
+++ /dev/null
@@ -1,107 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-#ifndef TONGA_DYN_DEFAULTS_H
-#define TONGA_DYN_DEFAULTS_H
-
-
-/** \file
- * Volcanic Islands Dynamic default parameters.
- */
-
-enum TONGAdpm_TrendDetection {
-	TONGAdpm_TrendDetection_AUTO,
-	TONGAdpm_TrendDetection_UP,
-	TONGAdpm_TrendDetection_DOWN
-};
-typedef enum TONGAdpm_TrendDetection TONGAdpm_TrendDetection;
-
-/* Bit vector representing same fields as hardware register. */
-#define PPTONGA_VOTINGRIGHTSCLIENTS_DFLT0              0x3FFFC102  /* CP_Gfx_busy */
-/* HDP_busy */
-/* IH_busy */
-/* DRM_busy */
-/* DRMDMA_busy */
-/* UVD_busy */
-/* VCE_busy */
-/* ACP_busy */
-/* SAMU_busy */
-/* AVP_busy  */
-/* SDMA enabled */
-#define PPTONGA_VOTINGRIGHTSCLIENTS_DFLT1              0x000400  /* FE_Gfx_busy  - Intended for primary usage.   Rest are for flexibility. */
-/* SH_Gfx_busy */
-/* RB_Gfx_busy */
-/* VCE_busy */
-
-#define PPTONGA_VOTINGRIGHTSCLIENTS_DFLT2              0xC00080  /* SH_Gfx_busy - Intended for primary usage.   Rest are for flexibility. */
-/* FE_Gfx_busy */
-/* RB_Gfx_busy */
-/* ACP_busy */
-
-#define PPTONGA_VOTINGRIGHTSCLIENTS_DFLT3              0xC00200  /* RB_Gfx_busy - Intended for primary usage.   Rest are for flexibility. */
-/* FE_Gfx_busy */
-/* SH_Gfx_busy */
-/* UVD_busy */
-
-#define PPTONGA_VOTINGRIGHTSCLIENTS_DFLT4              0xC01680  /* UVD_busy */
-/* VCE_busy */
-/* ACP_busy */
-/* SAMU_busy */
-
-#define PPTONGA_VOTINGRIGHTSCLIENTS_DFLT5              0xC00033  /* GFX, HDP, DRMDMA */
-#define PPTONGA_VOTINGRIGHTSCLIENTS_DFLT6              0xC00033  /* GFX, HDP, DRMDMA */
-#define PPTONGA_VOTINGRIGHTSCLIENTS_DFLT7              0x3FFFC000  /* GFX, HDP, DRMDMA */
-
-
-/* thermal protection counter (units).*/
-#define PPTONGA_THERMALPROTECTCOUNTER_DFLT            0x200 /* ~19us */
-
-/* static screen threshold unit */
-#define PPTONGA_STATICSCREENTHRESHOLDUNIT_DFLT        0
-
-/* static screen threshold */
-#define PPTONGA_STATICSCREENTHRESHOLD_DFLT            0x00C8
-
-/* gfx idle clock stop threshold */
-#define PPTONGA_GFXIDLECLOCKSTOPTHRESHOLD_DFLT        0x200 /* ~19us with static screen threshold unit of 0 */
-
-/* Fixed reference divider to use when building baby stepping tables. */
-#define PPTONGA_REFERENCEDIVIDER_DFLT                  4
-
-/*
- * ULV voltage change delay time
- * Used to be delay_vreg in N.I. split for S.I.
- * Using N.I. delay_vreg value as default
- * ReferenceClock = 2700
- * VoltageResponseTime = 1000
- * VDDCDelayTime = (VoltageResponseTime * ReferenceClock) / 1600 = 1687
- */
-
-#define PPTONGA_ULVVOLTAGECHANGEDELAY_DFLT             1687
-
-#define PPTONGA_CGULVPARAMETER_DFLT                    0x00040035
-#define PPTONGA_CGULVCONTROL_DFLT                      0x00007450
-#define PPTONGA_TARGETACTIVITY_DFLT                     30  /*30%  */
-#define PPTONGA_MCLK_TARGETACTIVITY_DFLT                10  /*10%  */
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c
deleted file mode 100644
index 82d6200..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c
+++ /dev/null
@@ -1,6504 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-#include <linux/module.h>
-#include <linux/slab.h>
-#include <linux/fb.h>
-#include "linux/delay.h"
-#include "pp_acpi.h"
-#include "hwmgr.h"
-#include <atombios.h>
-#include "tonga_hwmgr.h"
-#include "pptable.h"
-#include "processpptables.h"
-#include "process_pptables_v1_0.h"
-#include "pptable_v1_0.h"
-#include "pp_debug.h"
-#include "tonga_ppsmc.h"
-#include "cgs_common.h"
-#include "pppcielanes.h"
-#include "tonga_dyn_defaults.h"
-#include "smumgr.h"
-#include "tonga_smumgr.h"
-#include "tonga_clockpowergating.h"
-#include "tonga_thermal.h"
-
-#include "smu/smu_7_1_2_d.h"
-#include "smu/smu_7_1_2_sh_mask.h"
-
-#include "gmc/gmc_8_1_d.h"
-#include "gmc/gmc_8_1_sh_mask.h"
-
-#include "bif/bif_5_0_d.h"
-#include "bif/bif_5_0_sh_mask.h"
-
-#include "dce/dce_10_0_d.h"
-#include "dce/dce_10_0_sh_mask.h"
-
-#include "cgs_linux.h"
-#include "eventmgr.h"
-#include "amd_pcie_helpers.h"
-
-#define MC_CG_ARB_FREQ_F0           0x0a
-#define MC_CG_ARB_FREQ_F1           0x0b
-#define MC_CG_ARB_FREQ_F2           0x0c
-#define MC_CG_ARB_FREQ_F3           0x0d
-
-#define MC_CG_SEQ_DRAMCONF_S0       0x05
-#define MC_CG_SEQ_DRAMCONF_S1       0x06
-#define MC_CG_SEQ_YCLK_SUSPEND      0x04
-#define MC_CG_SEQ_YCLK_RESUME       0x0a
-
-#define PCIE_BUS_CLK                10000
-#define TCLK                        (PCIE_BUS_CLK / 10)
-
-#define SMC_RAM_END 0x40000
-#define SMC_CG_IND_START            0xc0030000
-#define SMC_CG_IND_END              0xc0040000  /* First byte after SMC_CG_IND*/
-
-#define VOLTAGE_SCALE               4
-#define VOLTAGE_VID_OFFSET_SCALE1   625
-#define VOLTAGE_VID_OFFSET_SCALE2   100
-
-#define VDDC_VDDCI_DELTA            200
-#define VDDC_VDDGFX_DELTA           300
-
-#define MC_SEQ_MISC0_GDDR5_SHIFT 28
-#define MC_SEQ_MISC0_GDDR5_MASK  0xf0000000
-#define MC_SEQ_MISC0_GDDR5_VALUE 5
-
-typedef uint32_t PECI_RegistryValue;
-
-/* [2.5%,~2.5%] Clock stretched is multiple of 2.5% vs not and [Fmin, Fmax, LDO_REFSEL, USE_FOR_LOW_FREQ] */
-static const uint16_t PP_ClockStretcherLookupTable[2][4] = {
-	{600, 1050, 3, 0},
-	{600, 1050, 6, 1} };
-
-/* [FF, SS] type, [] 4 voltage ranges, and [Floor Freq, Boundary Freq, VID min , VID max] */
-static const uint32_t PP_ClockStretcherDDTTable[2][4][4] = {
-	{ {265, 529, 120, 128}, {325, 650, 96, 119}, {430, 860, 32, 95}, {0, 0, 0, 31} },
-	{ {275, 550, 104, 112}, {319, 638, 96, 103}, {360, 720, 64, 95}, {384, 768, 32, 63} } };
-
-/* [Use_For_Low_freq] value, [0%, 5%, 10%, 7.14%, 14.28%, 20%] (coming from PWR_CKS_CNTL.stretch_amount reg spec) */
-static const uint8_t PP_ClockStretchAmountConversion[2][6] = {
-	{0, 1, 3, 2, 4, 5},
-	{0, 2, 4, 5, 6, 5} };
-
-/* Values for the CG_THERMAL_CTRL::DPM_EVENT_SRC field. */
-enum DPM_EVENT_SRC {
-	DPM_EVENT_SRC_ANALOG = 0,               /* Internal analog trip point */
-	DPM_EVENT_SRC_EXTERNAL = 1,             /* External (GPIO 17) signal */
-	DPM_EVENT_SRC_DIGITAL = 2,              /* Internal digital trip point (DIG_THERM_DPM) */
-	DPM_EVENT_SRC_ANALOG_OR_EXTERNAL = 3,   /* Internal analog or external */
-	DPM_EVENT_SRC_DIGITAL_OR_EXTERNAL = 4   /* Internal digital or external */
-};
-typedef enum DPM_EVENT_SRC DPM_EVENT_SRC;
-
-static const unsigned long PhwTonga_Magic = (unsigned long)(PHM_VIslands_Magic);
-
-struct tonga_power_state *cast_phw_tonga_power_state(
-				  struct pp_hw_power_state *hw_ps)
-{
-	if (hw_ps == NULL)
-		return NULL;
-
-	PP_ASSERT_WITH_CODE((PhwTonga_Magic == hw_ps->magic),
-				"Invalid Powerstate Type!",
-				 return NULL);
-
-	return (struct tonga_power_state *)hw_ps;
-}
-
-const struct tonga_power_state *cast_const_phw_tonga_power_state(
-				 const struct pp_hw_power_state *hw_ps)
-{
-	if (hw_ps == NULL)
-		return NULL;
-
-	PP_ASSERT_WITH_CODE((PhwTonga_Magic == hw_ps->magic),
-				"Invalid Powerstate Type!",
-				 return NULL);
-
-	return (const struct tonga_power_state *)hw_ps;
-}
-
-int tonga_add_voltage(struct pp_hwmgr *hwmgr,
-	phm_ppt_v1_voltage_lookup_table *look_up_table,
-	phm_ppt_v1_voltage_lookup_record *record)
-{
-	uint32_t i;
-	PP_ASSERT_WITH_CODE((NULL != look_up_table),
-		"Lookup Table empty.", return -1;);
-	PP_ASSERT_WITH_CODE((0 != look_up_table->count),
-		"Lookup Table empty.", return -1;);
-	PP_ASSERT_WITH_CODE((SMU72_MAX_LEVELS_VDDGFX >= look_up_table->count),
-		"Lookup Table is full.", return -1;);
-
-	/* This is to avoid entering duplicate calculated records. */
-	for (i = 0; i < look_up_table->count; i++) {
-		if (look_up_table->entries[i].us_vdd == record->us_vdd) {
-			if (look_up_table->entries[i].us_calculated == 1)
-				return 0;
-			else
-				break;
-		}
-	}
-
-	look_up_table->entries[i].us_calculated = 1;
-	look_up_table->entries[i].us_vdd = record->us_vdd;
-	look_up_table->entries[i].us_cac_low = record->us_cac_low;
-	look_up_table->entries[i].us_cac_mid = record->us_cac_mid;
-	look_up_table->entries[i].us_cac_high = record->us_cac_high;
-	/* Only increment the count when we're appending, not replacing duplicate entry. */
-	if (i == look_up_table->count)
-		look_up_table->count++;
-
-	return 0;
-}
-
-int tonga_notify_smc_display_change(struct pp_hwmgr *hwmgr, bool has_display)
-{
-	PPSMC_Msg msg = has_display? (PPSMC_Msg)PPSMC_HasDisplay : (PPSMC_Msg)PPSMC_NoDisplay;
-
-	return (smum_send_msg_to_smc(hwmgr->smumgr, msg) == 0) ?  0 : -1;
-}
-
-uint8_t tonga_get_voltage_id(pp_atomctrl_voltage_table *voltage_table,
-		uint32_t voltage)
-{
-	uint8_t count = (uint8_t) (voltage_table->count);
-	uint8_t i = 0;
-
-	PP_ASSERT_WITH_CODE((NULL != voltage_table),
-		"Voltage Table empty.", return 0;);
-	PP_ASSERT_WITH_CODE((0 != count),
-		"Voltage Table empty.", return 0;);
-
-	for (i = 0; i < count; i++) {
-		/* find first voltage bigger than requested */
-		if (voltage_table->entries[i].value >= voltage)
-			return i;
-	}
-
-	/* voltage is bigger than max voltage in the table */
-	return i - 1;
-}
-
-
-/**
- * @brief PhwTonga_GetVoltageOrder
- *  Returns index of requested voltage record in lookup(table)
- * @param hwmgr - pointer to hardware manager
- * @param lookupTable - lookup list to search in
- * @param voltage - voltage to look for
- * @return 0 on success
- */
-uint8_t tonga_get_voltage_index(phm_ppt_v1_voltage_lookup_table *look_up_table,
-		uint16_t voltage)
-{
-	uint8_t count = (uint8_t) (look_up_table->count);
-	uint8_t i;
-
-	PP_ASSERT_WITH_CODE((NULL != look_up_table), "Lookup Table empty.", return 0;);
-	PP_ASSERT_WITH_CODE((0 != count), "Lookup Table empty.", return 0;);
-
-	for (i = 0; i < count; i++) {
-		/* find first voltage equal or bigger than requested */
-		if (look_up_table->entries[i].us_vdd >= voltage)
-			return i;
-	}
-
-	/* voltage is bigger than max voltage in the table */
-	return i-1;
-}
-
-static bool tonga_is_dpm_running(struct pp_hwmgr *hwmgr)
-{
-	/*
-	 * We return the status of Voltage Control instead of checking SCLK/MCLK DPM
-	 * because we may have test scenarios that need us intentionly disable SCLK/MCLK DPM,
-	 * whereas voltage control is a fundemental change that will not be disabled
-	 */
-
-	return (0 == PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-					FEATURE_STATUS, VOLTAGE_CONTROLLER_ON) ? 1 : 0);
-}
-
-/**
- * Re-generate the DPM level mask value
- * @param    hwmgr      the address of the hardware manager
- */
-static uint32_t tonga_get_dpm_level_enable_mask_value(
-			struct tonga_single_dpm_table * dpm_table)
-{
-	uint32_t i;
-	uint32_t mask_value = 0;
-
-	for (i = dpm_table->count; i > 0; i--) {
-		mask_value = mask_value << 1;
-
-		if (dpm_table->dpm_levels[i-1].enabled)
-			mask_value |= 0x1;
-		else
-			mask_value &= 0xFFFFFFFE;
-	}
-	return mask_value;
-}
-
-/**
- * Retrieve DPM default values from registry (if available)
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- */
-void tonga_initialize_dpm_defaults(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	phw_tonga_ulv_parm *ulv = &(data->ulv);
-	uint32_t tmp;
-
-	ulv->ch_ulv_parameter = PPTONGA_CGULVPARAMETER_DFLT;
-	data->voting_rights_clients0 = PPTONGA_VOTINGRIGHTSCLIENTS_DFLT0;
-	data->voting_rights_clients1 = PPTONGA_VOTINGRIGHTSCLIENTS_DFLT1;
-	data->voting_rights_clients2 = PPTONGA_VOTINGRIGHTSCLIENTS_DFLT2;
-	data->voting_rights_clients3 = PPTONGA_VOTINGRIGHTSCLIENTS_DFLT3;
-	data->voting_rights_clients4 = PPTONGA_VOTINGRIGHTSCLIENTS_DFLT4;
-	data->voting_rights_clients5 = PPTONGA_VOTINGRIGHTSCLIENTS_DFLT5;
-	data->voting_rights_clients6 = PPTONGA_VOTINGRIGHTSCLIENTS_DFLT6;
-	data->voting_rights_clients7 = PPTONGA_VOTINGRIGHTSCLIENTS_DFLT7;
-
-	data->static_screen_threshold_unit = PPTONGA_STATICSCREENTHRESHOLDUNIT_DFLT;
-	data->static_screen_threshold = PPTONGA_STATICSCREENTHRESHOLD_DFLT;
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_ABM);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_NonABMSupportInPPLib);
-
-	tmp = 0;
-	if (tmp == 0)
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DynamicACTiming);
-
-	tmp = 0;
-	if (0 != tmp)
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DisableMemoryTransition);
-
-	tonga_initialize_power_tune_defaults(hwmgr);
-
-	data->mclk_strobe_mode_threshold = 40000;
-	data->mclk_stutter_mode_threshold = 30000;
-	data->mclk_edc_enable_threshold = 40000;
-	data->mclk_edc_wr_enable_threshold = 40000;
-
-	tmp = 0;
-	if (tmp != 0)
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DisableMCLS);
-
-	data->pcie_gen_performance.max = PP_PCIEGen1;
-	data->pcie_gen_performance.min = PP_PCIEGen3;
-	data->pcie_gen_power_saving.max = PP_PCIEGen1;
-	data->pcie_gen_power_saving.min = PP_PCIEGen3;
-
-	data->pcie_lane_performance.max = 0;
-	data->pcie_lane_performance.min = 16;
-	data->pcie_lane_power_saving.max = 0;
-	data->pcie_lane_power_saving.min = 16;
-
-	tmp = 0;
-
-	if (tmp)
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkThrottleLowNotification);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_DynamicUVDState);
-
-}
-
-static int tonga_update_sclk_threshold(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	int result = 0;
-	uint32_t low_sclk_interrupt_threshold = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkThrottleLowNotification)
-		&& (hwmgr->gfx_arbiter.sclk_threshold != data->low_sclk_interrupt_threshold)) {
-		data->low_sclk_interrupt_threshold = hwmgr->gfx_arbiter.sclk_threshold;
-		low_sclk_interrupt_threshold = data->low_sclk_interrupt_threshold;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(low_sclk_interrupt_threshold);
-
-		result = tonga_copy_bytes_to_smc(
-				hwmgr->smumgr,
-				data->dpm_table_start + offsetof(SMU72_Discrete_DpmTable,
-				LowSclkInterruptThreshold),
-				(uint8_t *)&low_sclk_interrupt_threshold,
-				sizeof(uint32_t),
-				data->sram_end
-				);
-	}
-
-	return result;
-}
-
-/**
- * Find SCLK value that is associated with specified virtual_voltage_Id.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @param    virtual_voltage_Id  voltageId to look for.
- * @param    sclk output value .
- * @return   always 0 if success and 2 if association not found
- */
-static int tonga_get_sclk_for_voltage_evv(struct pp_hwmgr *hwmgr,
-	phm_ppt_v1_voltage_lookup_table *lookup_table,
-	uint16_t virtual_voltage_id, uint32_t *sclk)
-{
-	uint8_t entryId;
-	uint8_t voltageId;
-	struct phm_ppt_v1_information *pptable_info =
-					(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	PP_ASSERT_WITH_CODE(lookup_table->count != 0, "Lookup table is empty", return -1);
-
-	/* search for leakage voltage ID 0xff01 ~ 0xff08 and sckl */
-	for (entryId = 0; entryId < pptable_info->vdd_dep_on_sclk->count; entryId++) {
-		voltageId = pptable_info->vdd_dep_on_sclk->entries[entryId].vddInd;
-		if (lookup_table->entries[voltageId].us_vdd == virtual_voltage_id)
-			break;
-	}
-
-	PP_ASSERT_WITH_CODE(entryId < pptable_info->vdd_dep_on_sclk->count,
-			"Can't find requested voltage id in vdd_dep_on_sclk table!",
-			return -1;
-			);
-
-	*sclk = pptable_info->vdd_dep_on_sclk->entries[entryId].clk;
-
-	return 0;
-}
-
-/**
- * Get Leakage VDDC based on leakage ID.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   2 if vddgfx returned is greater than 2V or if BIOS
- */
-int tonga_get_evv_voltage(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	phm_ppt_v1_clock_voltage_dependency_table *sclk_table = pptable_info->vdd_dep_on_sclk;
-	uint16_t    virtual_voltage_id;
-	uint16_t    vddc = 0;
-	uint16_t    vddgfx = 0;
-	uint16_t    i, j;
-	uint32_t  sclk = 0;
-
-	/* retrieve voltage for leakage ID (0xff01 + i) */
-	for (i = 0; i < TONGA_MAX_LEAKAGE_COUNT; i++) {
-		virtual_voltage_id = ATOM_VIRTUAL_VOLTAGE_ID0 + i;
-
-		/* in split mode we should have only vddgfx EVV leakages */
-		if (data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2) {
-			if (0 == tonga_get_sclk_for_voltage_evv(hwmgr,
-						pptable_info->vddgfx_lookup_table, virtual_voltage_id, &sclk)) {
-				if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-							PHM_PlatformCaps_ClockStretcher)) {
-					for (j = 1; j < sclk_table->count; j++) {
-						if (sclk_table->entries[j].clk == sclk &&
-								sclk_table->entries[j].cks_enable == 0) {
-							sclk += 5000;
-							break;
-						}
-					}
-				}
-				if (0 == atomctrl_get_voltage_evv_on_sclk
-				    (hwmgr, VOLTAGE_TYPE_VDDGFX, sclk,
-				     virtual_voltage_id, &vddgfx)) {
-					/* need to make sure vddgfx is less than 2v or else, it could burn the ASIC. */
-					PP_ASSERT_WITH_CODE((vddgfx < 2000 && vddgfx != 0), "Invalid VDDGFX value!", return -1);
-
-					/* the voltage should not be zero nor equal to leakage ID */
-					if (vddgfx != 0 && vddgfx != virtual_voltage_id) {
-						data->vddcgfx_leakage.actual_voltage[data->vddcgfx_leakage.count] = vddgfx;
-						data->vddcgfx_leakage.leakage_id[data->vddcgfx_leakage.count] = virtual_voltage_id;
-						data->vddcgfx_leakage.count++;
-					}
-				} else {
-					printk("Error retrieving EVV voltage value!\n");
-				}
-			}
-		} else {
-			/*  in merged mode we have only vddc EVV leakages */
-			if (0 == tonga_get_sclk_for_voltage_evv(hwmgr,
-						pptable_info->vddc_lookup_table,
-						virtual_voltage_id, &sclk)) {
-				if (0 == atomctrl_get_voltage_evv_on_sclk
-				    (hwmgr, VOLTAGE_TYPE_VDDC, sclk,
-				     virtual_voltage_id, &vddc)) {
-					/* need to make sure vddc is less than 2v or else, it could burn the ASIC. */
-					PP_ASSERT_WITH_CODE(vddc < 2000, "Invalid VDDC value!", return -1);
-
-					/* the voltage should not be zero nor equal to leakage ID */
-					if (vddc != 0 && vddc != virtual_voltage_id) {
-						data->vddc_leakage.actual_voltage[data->vddc_leakage.count] = vddc;
-						data->vddc_leakage.leakage_id[data->vddc_leakage.count] = virtual_voltage_id;
-						data->vddc_leakage.count++;
-					}
-				} else {
-					printk("Error retrieving EVV voltage value!\n");
-				}
-			}
-		}
-	}
-
-	return 0;
-}
-
-int tonga_enable_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	/* enable SCLK dpm */
-	if (0 == data->sclk_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						   PPSMC_MSG_DPM_Enable)),
-				"Failed to enable SCLK DPM during DPM Start Function!",
-				return -1);
-	}
-
-	/* enable MCLK dpm */
-	if (0 == data->mclk_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					     PPSMC_MSG_MCLKDPM_Enable)),
-				"Failed to enable MCLK DPM during DPM Start Function!",
-				return -1);
-
-		PHM_WRITE_FIELD(hwmgr->device, MC_SEQ_CNTL_3, CAC_EN, 0x1);
-
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_MC0_CNTL, 0x05);/* CH0,1 read */
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_MC1_CNTL, 0x05);/* CH2,3 read */
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_CPL_CNTL, 0x100005);/*Read */
-
-		udelay(10);
-
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_MC0_CNTL, 0x400005);/* CH0,1 write */
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_MC1_CNTL, 0x400005);/* CH2,3 write */
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixLCAC_CPL_CNTL, 0x500005);/* write */
-
-	}
-
-	return 0;
-}
-
-int tonga_start_dpm(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	/* enable general power management */
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT, GLOBAL_PWRMGT_EN, 1);
-	/* enable sclk deep sleep */
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL, DYNAMIC_PM_EN, 1);
-
-	/* prepare for PCIE DPM */
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, data->soft_regs_start +
-			offsetof(SMU72_SoftRegisters, VoltageChangeTimeout), 0x1000);
-
-	PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__PCIE, SWRST_COMMAND_1, RESETLC, 0x0);
-
-	PP_ASSERT_WITH_CODE(
-			(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					PPSMC_MSG_Voltage_Cntl_Enable)),
-			"Failed to enable voltage DPM during DPM Start Function!",
-			return -1);
-
-	if (0 != tonga_enable_sclk_mclk_dpm(hwmgr)) {
-		PP_ASSERT_WITH_CODE(0, "Failed to enable Sclk DPM and Mclk DPM!", return -1);
-	}
-
-	/* enable PCIE dpm */
-	if (0 == data->pcie_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_PCIeDPM_Enable)),
-				"Failed to enable pcie DPM during DPM Start Function!",
-				return -1
-				);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_Falcon_QuickTransition)) {
-				   smum_send_msg_to_smc(hwmgr->smumgr,
-				    PPSMC_MSG_EnableACDCGPIOInterrupt);
-	}
-
-	return 0;
-}
-
-int tonga_disable_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	/* disable SCLK dpm */
-	if (0 == data->sclk_dpm_key_disabled) {
-		/* Checking if DPM is running.  If we discover hang because of this, we should skip this message.*/
-		PP_ASSERT_WITH_CODE(
-				!tonga_is_dpm_running(hwmgr),
-				"Trying to Disable SCLK DPM when DPM is disabled",
-				return -1
-				);
-
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						  PPSMC_MSG_DPM_Disable)),
-				"Failed to disable SCLK DPM during DPM stop Function!",
-				return -1);
-	}
-
-	/* disable MCLK dpm */
-	if (0 == data->mclk_dpm_key_disabled) {
-		/* Checking if DPM is running.  If we discover hang because of this, we should skip this message. */
-		PP_ASSERT_WITH_CODE(
-				!tonga_is_dpm_running(hwmgr),
-				"Trying to Disable MCLK DPM when DPM is disabled",
-				return -1
-				);
-
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					    PPSMC_MSG_MCLKDPM_Disable)),
-				"Failed to Disable MCLK DPM during DPM stop Function!",
-				return -1);
-	}
-
-	return 0;
-}
-
-int tonga_stop_dpm(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT, GLOBAL_PWRMGT_EN, 0);
-	/* disable sclk deep sleep*/
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL, DYNAMIC_PM_EN, 0);
-
-	/* disable PCIE dpm */
-	if (0 == data->pcie_dpm_key_disabled) {
-		/* Checking if DPM is running.  If we discover hang because of this, we should skip this message.*/
-		PP_ASSERT_WITH_CODE(
-				!tonga_is_dpm_running(hwmgr),
-				"Trying to Disable PCIE DPM when DPM is disabled",
-				return -1
-				);
-		PP_ASSERT_WITH_CODE(
-				(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-						PPSMC_MSG_PCIeDPM_Disable)),
-				"Failed to disable pcie DPM during DPM stop Function!",
-				return -1);
-	}
-
-	if (0 != tonga_disable_sclk_mclk_dpm(hwmgr))
-		PP_ASSERT_WITH_CODE(0, "Failed to disable Sclk DPM and Mclk DPM!", return -1);
-
-	/* Checking if DPM is running.  If we discover hang because of this, we should skip this message.*/
-	PP_ASSERT_WITH_CODE(
-			!tonga_is_dpm_running(hwmgr),
-			"Trying to Disable Voltage CNTL when DPM is disabled",
-			return -1
-			);
-
-	PP_ASSERT_WITH_CODE(
-			(0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					PPSMC_MSG_Voltage_Cntl_Disable)),
-			"Failed to disable voltage DPM during DPM stop Function!",
-			return -1);
-
-	return 0;
-}
-
-int tonga_enable_sclk_control(struct pp_hwmgr *hwmgr)
-{
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL, SCLK_PWRMGT_OFF, 0);
-
-	return 0;
-}
-
-/**
- * Send a message to the SMC and return a parameter
- *
- * @param    hwmgr:  the address of the powerplay hardware manager.
- * @param    msg: the message to send.
- * @param    parameter: pointer to the received parameter
- * @return   The response that came from the SMC.
- */
-PPSMC_Result tonga_send_msg_to_smc_return_parameter(
-		struct pp_hwmgr *hwmgr,
-		PPSMC_Msg msg,
-		uint32_t *parameter)
-{
-	int result;
-
-	result = smum_send_msg_to_smc(hwmgr->smumgr, msg);
-
-	if ((0 == result) && parameter) {
-		*parameter = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-	}
-
-	return result;
-}
-
-/**
- * force DPM power State
- *
- * @param    hwmgr:  the address of the powerplay hardware manager.
- * @param    n     :  DPM level
- * @return   The response that came from the SMC.
- */
-int tonga_dpm_force_state(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	uint32_t level_mask = 1 << n;
-
-	/* Checking if DPM is running.  If we discover hang because of this, we should skip this message. */
-	PP_ASSERT_WITH_CODE(!tonga_is_dpm_running(hwmgr),
-			    "Trying to force SCLK when DPM is disabled",
-			    return -1;);
-	if (0 == data->sclk_dpm_key_disabled)
-		return (0 == smum_send_msg_to_smc_with_parameter(
-							     hwmgr->smumgr,
-		     (PPSMC_Msg)(PPSMC_MSG_SCLKDPM_SetEnabledMask),
-						    level_mask) ? 0 : 1);
-
-	return 0;
-}
-
-/**
- * force DPM power State
- *
- * @param    hwmgr:  the address of the powerplay hardware manager.
- * @param    n     :  DPM level
- * @return   The response that came from the SMC.
- */
-int tonga_dpm_force_state_mclk(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	uint32_t level_mask = 1 << n;
-
-	/* Checking if DPM is running.  If we discover hang because of this, we should skip this message. */
-	PP_ASSERT_WITH_CODE(!tonga_is_dpm_running(hwmgr),
-			    "Trying to Force MCLK when DPM is disabled",
-			    return -1;);
-	if (0 == data->mclk_dpm_key_disabled)
-		return (0 == smum_send_msg_to_smc_with_parameter(
-								hwmgr->smumgr,
-								(PPSMC_Msg)(PPSMC_MSG_MCLKDPM_SetEnabledMask),
-								level_mask) ? 0 : 1);
-
-	return 0;
-}
-
-/**
- * force DPM power State
- *
- * @param    hwmgr:  the address of the powerplay hardware manager.
- * @param    n     :  DPM level
- * @return   The response that came from the SMC.
- */
-int tonga_dpm_force_state_pcie(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	/* Checking if DPM is running.  If we discover hang because of this, we should skip this message.*/
-	PP_ASSERT_WITH_CODE(!tonga_is_dpm_running(hwmgr),
-			    "Trying to Force PCIE level when DPM is disabled",
-			    return -1;);
-	if (0 == data->pcie_dpm_key_disabled)
-		return (0 == smum_send_msg_to_smc_with_parameter(
-							     hwmgr->smumgr,
-			   (PPSMC_Msg)(PPSMC_MSG_PCIeDPM_ForceLevel),
-								n) ? 0 : 1);
-
-	return 0;
-}
-
-/**
- * Set the initial state by calling SMC to switch to this state directly
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_set_boot_state(struct pp_hwmgr *hwmgr)
-{
-	/*
-	* SMC only stores one state that SW will ask to switch too,
-	* so we switch the the just uploaded one
-	*/
-	return (0 == tonga_disable_sclk_mclk_dpm(hwmgr)) ? 0 : 1;
-}
-
-/**
- * Get the location of various tables inside the FW image.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-static int tonga_process_firmware_header(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_smumgr *tonga_smu = (struct tonga_smumgr *)(hwmgr->smumgr->backend);
-
-	uint32_t tmp;
-	int result;
-	bool error = false;
-
-	result = tonga_read_smc_sram_dword(hwmgr->smumgr,
-				SMU72_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU72_Firmware_Header, DpmTable),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->dpm_table_start = tmp;
-	}
-
-	error |= (0 != result);
-
-	result = tonga_read_smc_sram_dword(hwmgr->smumgr,
-				SMU72_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU72_Firmware_Header, SoftRegisters),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->soft_regs_start = tmp;
-		tonga_smu->soft_regs_start = tmp;
-	}
-
-	error |= (0 != result);
-
-
-	result = tonga_read_smc_sram_dword(hwmgr->smumgr,
-				SMU72_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU72_Firmware_Header, mcRegisterTable),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->mc_reg_table_start = tmp;
-	}
-
-	result = tonga_read_smc_sram_dword(hwmgr->smumgr,
-				SMU72_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU72_Firmware_Header, FanTable),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->fan_table_start = tmp;
-	}
-
-	error |= (0 != result);
-
-	result = tonga_read_smc_sram_dword(hwmgr->smumgr,
-				SMU72_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU72_Firmware_Header, mcArbDramTimingTable),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		data->arb_table_start = tmp;
-	}
-
-	error |= (0 != result);
-
-
-	result = tonga_read_smc_sram_dword(hwmgr->smumgr,
-				SMU72_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU72_Firmware_Header, Version),
-				&tmp, data->sram_end);
-
-	if (0 == result) {
-		hwmgr->microcode_version_info.SMC = tmp;
-	}
-
-	error |= (0 != result);
-
-	return error ? 1 : 0;
-}
-
-/**
- * Read clock related registers.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_read_clock_registers(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	data->clock_registers.vCG_SPLL_FUNC_CNTL         =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL);
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_2       =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_2);
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_3       =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_3);
-	data->clock_registers.vCG_SPLL_FUNC_CNTL_4       =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_4);
-	data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM   =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_SPREAD_SPECTRUM);
-	data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2 =
-		cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_SPREAD_SPECTRUM_2);
-	data->clock_registers.vDLL_CNTL                  =
-		cgs_read_register(hwmgr->device, mmDLL_CNTL);
-	data->clock_registers.vMCLK_PWRMGT_CNTL          =
-		cgs_read_register(hwmgr->device, mmMCLK_PWRMGT_CNTL);
-	data->clock_registers.vMPLL_AD_FUNC_CNTL         =
-		cgs_read_register(hwmgr->device, mmMPLL_AD_FUNC_CNTL);
-	data->clock_registers.vMPLL_DQ_FUNC_CNTL         =
-		cgs_read_register(hwmgr->device, mmMPLL_DQ_FUNC_CNTL);
-	data->clock_registers.vMPLL_FUNC_CNTL            =
-		cgs_read_register(hwmgr->device, mmMPLL_FUNC_CNTL);
-	data->clock_registers.vMPLL_FUNC_CNTL_1          =
-		cgs_read_register(hwmgr->device, mmMPLL_FUNC_CNTL_1);
-	data->clock_registers.vMPLL_FUNC_CNTL_2          =
-		cgs_read_register(hwmgr->device, mmMPLL_FUNC_CNTL_2);
-	data->clock_registers.vMPLL_SS1                  =
-		cgs_read_register(hwmgr->device, mmMPLL_SS1);
-	data->clock_registers.vMPLL_SS2                  =
-		cgs_read_register(hwmgr->device, mmMPLL_SS2);
-
-	return 0;
-}
-
-/**
- * Find out if memory is GDDR5.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_get_memory_type(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	uint32_t temp;
-
-	temp = cgs_read_register(hwmgr->device, mmMC_SEQ_MISC0);
-
-	data->is_memory_GDDR5 = (MC_SEQ_MISC0_GDDR5_VALUE ==
-			((temp & MC_SEQ_MISC0_GDDR5_MASK) >>
-			 MC_SEQ_MISC0_GDDR5_SHIFT));
-
-	return 0;
-}
-
-/**
- * Enables Dynamic Power Management by SMC
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_enable_acpi_power_management(struct pp_hwmgr *hwmgr)
-{
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT, STATIC_PM_EN, 1);
-
-	return 0;
-}
-
-/**
- * Initialize PowerGating States for different engines
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_init_power_gate_state(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	data->uvd_power_gated = false;
-	data->vce_power_gated = false;
-	data->samu_power_gated = false;
-	data->acp_power_gated = false;
-	data->pg_acp_init = true;
-
-	return 0;
-}
-
-/**
- * Checks if DPM is enabled
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_check_for_dpm_running(struct pp_hwmgr *hwmgr)
-{
-	/*
-	 * We return the status of Voltage Control instead of checking SCLK/MCLK DPM
-	 * because we may have test scenarios that need us intentionly disable SCLK/MCLK DPM,
-	 * whereas voltage control is a fundemental change that will not be disabled
-	 */
-	return (!tonga_is_dpm_running(hwmgr) ? 0 : 1);
-}
-
-/**
- * Checks if DPM is stopped
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_check_for_dpm_stopped(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	if (tonga_is_dpm_running(hwmgr)) {
-		/* If HW Virtualization is enabled, dpm_table_start will not have a valid value */
-		if (!data->dpm_table_start) {
-			return 1;
-		}
-	}
-
-	return 0;
-}
-
-/**
- * Remove repeated voltage values and create table with unique values.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @param    voltage_table  the pointer to changing voltage table
- * @return    1 in success
- */
-
-static int tonga_trim_voltage_table(struct pp_hwmgr *hwmgr,
-			pp_atomctrl_voltage_table *voltage_table)
-{
-	uint32_t table_size, i, j;
-	uint16_t vvalue;
-	bool bVoltageFound = false;
-	pp_atomctrl_voltage_table *table;
-
-	PP_ASSERT_WITH_CODE((NULL != voltage_table), "Voltage Table empty.", return -1;);
-	table_size = sizeof(pp_atomctrl_voltage_table);
-	table = kzalloc(table_size, GFP_KERNEL);
-
-	if (NULL == table)
-		return -ENOMEM;
-
-	memset(table, 0x00, table_size);
-	table->mask_low = voltage_table->mask_low;
-	table->phase_delay = voltage_table->phase_delay;
-
-	for (i = 0; i < voltage_table->count; i++) {
-		vvalue = voltage_table->entries[i].value;
-		bVoltageFound = false;
-
-		for (j = 0; j < table->count; j++) {
-			if (vvalue == table->entries[j].value) {
-				bVoltageFound = true;
-				break;
-			}
-		}
-
-		if (!bVoltageFound) {
-			table->entries[table->count].value = vvalue;
-			table->entries[table->count].smio_low =
-				voltage_table->entries[i].smio_low;
-			table->count++;
-		}
-	}
-
-	memcpy(table, voltage_table, sizeof(pp_atomctrl_voltage_table));
-
-	kfree(table);
-
-	return 0;
-}
-
-static int tonga_get_svi2_vdd_ci_voltage_table(
-		struct pp_hwmgr *hwmgr,
-		phm_ppt_v1_clock_voltage_dependency_table *voltage_dependency_table)
-{
-	uint32_t i;
-	int result;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	pp_atomctrl_voltage_table *vddci_voltage_table = &(data->vddci_voltage_table);
-
-	PP_ASSERT_WITH_CODE((0 != voltage_dependency_table->count),
-			"Voltage Dependency Table empty.", return -1;);
-
-	vddci_voltage_table->mask_low = 0;
-	vddci_voltage_table->phase_delay = 0;
-	vddci_voltage_table->count = voltage_dependency_table->count;
-
-	for (i = 0; i < voltage_dependency_table->count; i++) {
-		vddci_voltage_table->entries[i].value =
-			voltage_dependency_table->entries[i].vddci;
-		vddci_voltage_table->entries[i].smio_low = 0;
-	}
-
-	result = tonga_trim_voltage_table(hwmgr, vddci_voltage_table);
-	PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to trim VDDCI table.", return result;);
-
-	return 0;
-}
-
-
-
-static int tonga_get_svi2_vdd_voltage_table(
-		struct pp_hwmgr *hwmgr,
-		phm_ppt_v1_voltage_lookup_table *look_up_table,
-		pp_atomctrl_voltage_table *voltage_table)
-{
-	uint8_t i = 0;
-
-	PP_ASSERT_WITH_CODE((0 != look_up_table->count),
-			"Voltage Lookup Table empty.", return -1;);
-
-	voltage_table->mask_low = 0;
-	voltage_table->phase_delay = 0;
-
-	voltage_table->count = look_up_table->count;
-
-	for (i = 0; i < voltage_table->count; i++) {
-		voltage_table->entries[i].value = look_up_table->entries[i].us_vdd;
-		voltage_table->entries[i].smio_low = 0;
-	}
-
-	return 0;
-}
-
-/*
- * -------------------------------------------------------- Voltage Tables --------------------------------------------------------------------------
- * If the voltage table would be bigger than what will fit into the state table on the SMC keep only the higher entries.
- */
-
-static void tonga_trim_voltage_table_to_fit_state_table(
-		struct pp_hwmgr *hwmgr,
-		uint32_t max_voltage_steps,
-		pp_atomctrl_voltage_table *voltage_table)
-{
-	unsigned int i, diff;
-
-	if (voltage_table->count <= max_voltage_steps) {
-		return;
-	}
-
-	diff = voltage_table->count - max_voltage_steps;
-
-	for (i = 0; i < max_voltage_steps; i++) {
-		voltage_table->entries[i] = voltage_table->entries[i + diff];
-	}
-
-	voltage_table->count = max_voltage_steps;
-
-	return;
-}
-
-/**
- * Create Voltage Tables.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_construct_voltage_tables(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	int result;
-
-	/* MVDD has only GPIO voltage control */
-	if (TONGA_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control) {
-		result = atomctrl_get_voltage_table_v3(hwmgr,
-					VOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_GPIO_LUT, &(data->mvdd_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve MVDD table.", return result;);
-	}
-
-	if (TONGA_VOLTAGE_CONTROL_BY_GPIO == data->vdd_ci_control) {
-		/* GPIO voltage */
-		result = atomctrl_get_voltage_table_v3(hwmgr,
-					VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_GPIO_LUT, &(data->vddci_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve VDDCI table.", return result;);
-	} else if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->vdd_ci_control) {
-		/* SVI2 voltage */
-		result = tonga_get_svi2_vdd_ci_voltage_table(hwmgr,
-					pptable_info->vdd_dep_on_mclk);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve SVI2 VDDCI table from dependancy table.", return result;);
-	}
-
-	if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->vdd_gfx_control) {
-		/* VDDGFX has only SVI2 voltage control */
-		result = tonga_get_svi2_vdd_voltage_table(hwmgr,
-					pptable_info->vddgfx_lookup_table, &(data->vddgfx_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve SVI2 VDDGFX table from lookup table.", return result;);
-	}
-
-	if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {
-		/* VDDC has only SVI2 voltage control */
-		result = tonga_get_svi2_vdd_voltage_table(hwmgr,
-					pptable_info->vddc_lookup_table, &(data->vddc_voltage_table));
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to retrieve SVI2 VDDC table from lookup table.", return result;);
-	}
-
-	PP_ASSERT_WITH_CODE(
-			(data->vddc_voltage_table.count <= (SMU72_MAX_LEVELS_VDDC)),
-			"Too many voltage values for VDDC. Trimming to fit state table.",
-			tonga_trim_voltage_table_to_fit_state_table(hwmgr,
-			SMU72_MAX_LEVELS_VDDC, &(data->vddc_voltage_table));
-			);
-
-	PP_ASSERT_WITH_CODE(
-			(data->vddgfx_voltage_table.count <= (SMU72_MAX_LEVELS_VDDGFX)),
-			"Too many voltage values for VDDGFX. Trimming to fit state table.",
-			tonga_trim_voltage_table_to_fit_state_table(hwmgr,
-			SMU72_MAX_LEVELS_VDDGFX, &(data->vddgfx_voltage_table));
-			);
-
-	PP_ASSERT_WITH_CODE(
-			(data->vddci_voltage_table.count <= (SMU72_MAX_LEVELS_VDDCI)),
-			"Too many voltage values for VDDCI. Trimming to fit state table.",
-			tonga_trim_voltage_table_to_fit_state_table(hwmgr,
-			SMU72_MAX_LEVELS_VDDCI, &(data->vddci_voltage_table));
-			);
-
-	PP_ASSERT_WITH_CODE(
-			(data->mvdd_voltage_table.count <= (SMU72_MAX_LEVELS_MVDD)),
-			"Too many voltage values for MVDD. Trimming to fit state table.",
-			tonga_trim_voltage_table_to_fit_state_table(hwmgr,
-			SMU72_MAX_LEVELS_MVDD, &(data->mvdd_voltage_table));
-			);
-
-	return 0;
-}
-
-/**
- * Vddc table preparation for SMC.
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    table     the SMC DPM table structure to be populated
- * @return   always 0
- */
-static int tonga_populate_smc_vddc_table(struct pp_hwmgr *hwmgr,
-			SMU72_Discrete_DpmTable *table)
-{
-	unsigned int count;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {
-		table->VddcLevelCount = data->vddc_voltage_table.count;
-		for (count = 0; count < table->VddcLevelCount; count++) {
-			table->VddcTable[count] =
-				PP_HOST_TO_SMC_US(data->vddc_voltage_table.entries[count].value * VOLTAGE_SCALE);
-		}
-		CONVERT_FROM_HOST_TO_SMC_UL(table->VddcLevelCount);
-	}
-	return 0;
-}
-
-/**
- * VddGfx table preparation for SMC.
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    table     the SMC DPM table structure to be populated
- * @return   always 0
- */
-static int tonga_populate_smc_vdd_gfx_table(struct pp_hwmgr *hwmgr,
-			SMU72_Discrete_DpmTable *table)
-{
-	unsigned int count;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->vdd_gfx_control) {
-		table->VddGfxLevelCount = data->vddgfx_voltage_table.count;
-		for (count = 0; count < data->vddgfx_voltage_table.count; count++) {
-			table->VddGfxTable[count] =
-				PP_HOST_TO_SMC_US(data->vddgfx_voltage_table.entries[count].value * VOLTAGE_SCALE);
-		}
-		CONVERT_FROM_HOST_TO_SMC_UL(table->VddGfxLevelCount);
-	}
-	return 0;
-}
-
-/**
- * Vddci table preparation for SMC.
- *
- * @param    *hwmgr The address of the hardware manager.
- * @param    *table The SMC DPM table structure to be populated.
- * @return   0
- */
-static int tonga_populate_smc_vdd_ci_table(struct pp_hwmgr *hwmgr,
-			SMU72_Discrete_DpmTable *table)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	uint32_t count;
-
-	table->VddciLevelCount = data->vddci_voltage_table.count;
-	for (count = 0; count < table->VddciLevelCount; count++) {
-		if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->vdd_ci_control) {
-			table->VddciTable[count] =
-				PP_HOST_TO_SMC_US(data->vddci_voltage_table.entries[count].value * VOLTAGE_SCALE);
-		} else if (TONGA_VOLTAGE_CONTROL_BY_GPIO == data->vdd_ci_control) {
-			table->SmioTable1.Pattern[count].Voltage =
-				PP_HOST_TO_SMC_US(data->vddci_voltage_table.entries[count].value * VOLTAGE_SCALE);
-			/* Index into DpmTable.Smio. Drive bits from Smio entry to get this voltage level. */
-			table->SmioTable1.Pattern[count].Smio =
-				(uint8_t) count;
-			table->Smio[count] |=
-				data->vddci_voltage_table.entries[count].smio_low;
-			table->VddciTable[count] =
-				PP_HOST_TO_SMC_US(data->vddci_voltage_table.entries[count].value * VOLTAGE_SCALE);
-		}
-	}
-
-	table->SmioMask1 = data->vddci_voltage_table.mask_low;
-	CONVERT_FROM_HOST_TO_SMC_UL(table->VddciLevelCount);
-
-	return 0;
-}
-
-/**
- * Mvdd table preparation for SMC.
- *
- * @param    *hwmgr The address of the hardware manager.
- * @param    *table The SMC DPM table structure to be populated.
- * @return   0
- */
-static int tonga_populate_smc_mvdd_table(struct pp_hwmgr *hwmgr,
-			SMU72_Discrete_DpmTable *table)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	uint32_t count;
-
-	if (TONGA_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control) {
-		table->MvddLevelCount = data->mvdd_voltage_table.count;
-		for (count = 0; count < table->MvddLevelCount; count++) {
-			table->SmioTable2.Pattern[count].Voltage =
-				PP_HOST_TO_SMC_US(data->mvdd_voltage_table.entries[count].value * VOLTAGE_SCALE);
-			/* Index into DpmTable.Smio. Drive bits from Smio entry to get this voltage level.*/
-			table->SmioTable2.Pattern[count].Smio =
-				(uint8_t) count;
-			table->Smio[count] |=
-				data->mvdd_voltage_table.entries[count].smio_low;
-		}
-		table->SmioMask2 = data->mvdd_voltage_table.mask_low;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->MvddLevelCount);
-	}
-
-	return 0;
-}
-
-/**
- * Preparation of vddc and vddgfx CAC tables for SMC.
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    table     the SMC DPM table structure to be populated
- * @return   always 0
- */
-static int tonga_populate_cac_tables(struct pp_hwmgr *hwmgr,
-			SMU72_Discrete_DpmTable *table)
-{
-	uint32_t count;
-	uint8_t index;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_ppt_v1_voltage_lookup_table *vddgfx_lookup_table = pptable_info->vddgfx_lookup_table;
-	struct phm_ppt_v1_voltage_lookup_table *vddc_lookup_table = pptable_info->vddc_lookup_table;
-
-	/* pTables is already swapped, so in order to use the value from it, we need to swap it back. */
-	uint32_t vddcLevelCount = PP_SMC_TO_HOST_UL(table->VddcLevelCount);
-	uint32_t vddgfxLevelCount = PP_SMC_TO_HOST_UL(table->VddGfxLevelCount);
-
-	for (count = 0; count < vddcLevelCount; count++) {
-		/* We are populating vddc CAC data to BapmVddc table in split and merged mode */
-		index = tonga_get_voltage_index(vddc_lookup_table,
-			data->vddc_voltage_table.entries[count].value);
-		table->BapmVddcVidLoSidd[count] =
-			convert_to_vid(vddc_lookup_table->entries[index].us_cac_low);
-		table->BapmVddcVidHiSidd[count] =
-			convert_to_vid(vddc_lookup_table->entries[index].us_cac_mid);
-		table->BapmVddcVidHiSidd2[count] =
-			convert_to_vid(vddc_lookup_table->entries[index].us_cac_high);
-	}
-
-	if ((data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2)) {
-		/* We are populating vddgfx CAC data to BapmVddgfx table in split mode */
-		for (count = 0; count < vddgfxLevelCount; count++) {
-			index = tonga_get_voltage_index(vddgfx_lookup_table,
-				data->vddgfx_voltage_table.entries[count].value);
-			table->BapmVddGfxVidLoSidd[count] =
-				convert_to_vid(vddgfx_lookup_table->entries[index].us_cac_low);
-			table->BapmVddGfxVidHiSidd[count] =
-				convert_to_vid(vddgfx_lookup_table->entries[index].us_cac_mid);
-			table->BapmVddGfxVidHiSidd2[count] =
-				convert_to_vid(vddgfx_lookup_table->entries[index].us_cac_high);
-		}
-	} else {
-		for (count = 0; count < vddcLevelCount; count++) {
-			index = tonga_get_voltage_index(vddc_lookup_table,
-				data->vddc_voltage_table.entries[count].value);
-			table->BapmVddGfxVidLoSidd[count] =
-				convert_to_vid(vddc_lookup_table->entries[index].us_cac_low);
-			table->BapmVddGfxVidHiSidd[count] =
-				convert_to_vid(vddc_lookup_table->entries[index].us_cac_mid);
-			table->BapmVddGfxVidHiSidd2[count] =
-				convert_to_vid(vddc_lookup_table->entries[index].us_cac_high);
-		}
-	}
-
-	return 0;
-}
-
-
-/**
- * Preparation of voltage tables for SMC.
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    table     the SMC DPM table structure to be populated
- * @return   always 0
- */
-
-int tonga_populate_smc_voltage_tables(struct pp_hwmgr *hwmgr,
-	SMU72_Discrete_DpmTable *table)
-{
-	int result;
-
-	result = tonga_populate_smc_vddc_table(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"can not populate VDDC voltage table to SMC", return -1);
-
-	result = tonga_populate_smc_vdd_ci_table(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"can not populate VDDCI voltage table to SMC", return -1);
-
-	result = tonga_populate_smc_vdd_gfx_table(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"can not populate VDDGFX voltage table to SMC", return -1);
-
-	result = tonga_populate_smc_mvdd_table(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"can not populate MVDD voltage table to SMC", return -1);
-
-	result = tonga_populate_cac_tables(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-			"can not populate CAC voltage tables to SMC", return -1);
-
-	return 0;
-}
-
-/**
- * Populates the SMC VRConfig field in DPM table.
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    table     the SMC DPM table structure to be populated
- * @return   always 0
- */
-static int tonga_populate_vr_config(struct pp_hwmgr *hwmgr,
-			SMU72_Discrete_DpmTable *table)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	uint16_t config;
-
-	if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->vdd_gfx_control) {
-		/*  Splitted mode */
-		config = VR_SVI2_PLANE_1;
-		table->VRConfig |= (config<<VRCONF_VDDGFX_SHIFT);
-
-		if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {
-			config = VR_SVI2_PLANE_2;
-			table->VRConfig |= config;
-		} else {
-			printk(KERN_ERR "[ powerplay ] VDDC and VDDGFX should be both on SVI2 control in splitted mode! \n");
-		}
-	} else {
-		/* Merged mode  */
-		config = VR_MERGED_WITH_VDDC;
-		table->VRConfig |= (config<<VRCONF_VDDGFX_SHIFT);
-
-		/* Set Vddc Voltage Controller  */
-		if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {
-			config = VR_SVI2_PLANE_1;
-			table->VRConfig |= config;
-		} else {
-			printk(KERN_ERR "[ powerplay ] VDDC should be on SVI2 control in merged mode! \n");
-		}
-	}
-
-	/* Set Vddci Voltage Controller  */
-	if (TONGA_VOLTAGE_CONTROL_BY_SVID2 == data->vdd_ci_control) {
-		config = VR_SVI2_PLANE_2;  /* only in merged mode */
-		table->VRConfig |= (config<<VRCONF_VDDCI_SHIFT);
-	} else if (TONGA_VOLTAGE_CONTROL_BY_GPIO == data->vdd_ci_control) {
-		config = VR_SMIO_PATTERN_1;
-		table->VRConfig |= (config<<VRCONF_VDDCI_SHIFT);
-	}
-
-	/* Set Mvdd Voltage Controller */
-	if (TONGA_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control) {
-		config = VR_SMIO_PATTERN_2;
-		table->VRConfig |= (config<<VRCONF_MVDD_SHIFT);
-	}
-
-	return 0;
-}
-
-static int tonga_get_dependecy_volt_by_clk(struct pp_hwmgr *hwmgr,
-	phm_ppt_v1_clock_voltage_dependency_table *allowed_clock_voltage_table,
-	uint32_t clock, SMU_VoltageLevel *voltage, uint32_t *mvdd)
-{
-	uint32_t i = 0;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	/* clock - voltage dependency table is empty table */
-	if (allowed_clock_voltage_table->count == 0)
-		return -1;
-
-	for (i = 0; i < allowed_clock_voltage_table->count; i++) {
-		/* find first sclk bigger than request */
-		if (allowed_clock_voltage_table->entries[i].clk >= clock) {
-			voltage->VddGfx = tonga_get_voltage_index(pptable_info->vddgfx_lookup_table,
-								allowed_clock_voltage_table->entries[i].vddgfx);
-
-			voltage->Vddc = tonga_get_voltage_index(pptable_info->vddc_lookup_table,
-								allowed_clock_voltage_table->entries[i].vddc);
-
-			if (allowed_clock_voltage_table->entries[i].vddci) {
-				voltage->Vddci = tonga_get_voltage_id(&data->vddci_voltage_table,
-									allowed_clock_voltage_table->entries[i].vddci);
-			} else {
-				voltage->Vddci = tonga_get_voltage_id(&data->vddci_voltage_table,
-									allowed_clock_voltage_table->entries[i].vddc - data->vddc_vddci_delta);
-			}
-
-			if (allowed_clock_voltage_table->entries[i].mvdd) {
-				*mvdd = (uint32_t) allowed_clock_voltage_table->entries[i].mvdd;
-			}
-
-			voltage->Phases = 1;
-			return 0;
-		}
-	}
-
-	/* sclk is bigger than max sclk in the dependence table */
-	voltage->VddGfx = tonga_get_voltage_index(pptable_info->vddgfx_lookup_table,
-		allowed_clock_voltage_table->entries[i-1].vddgfx);
-	voltage->Vddc = tonga_get_voltage_index(pptable_info->vddc_lookup_table,
-		allowed_clock_voltage_table->entries[i-1].vddc);
-
-	if (allowed_clock_voltage_table->entries[i-1].vddci) {
-		voltage->Vddci = tonga_get_voltage_id(&data->vddci_voltage_table,
-			allowed_clock_voltage_table->entries[i-1].vddci);
-	}
-	if (allowed_clock_voltage_table->entries[i-1].mvdd) {
-		*mvdd = (uint32_t) allowed_clock_voltage_table->entries[i-1].mvdd;
-	}
-
-	return 0;
-}
-
-/**
- * Call SMC to reset S0/S1 to S1 and Reset SMIO to initial value
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_reset_to_default(struct pp_hwmgr *hwmgr)
-{
-	return (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_ResetToDefaults) == 0) ? 0 : 1;
-}
-
-int tonga_populate_memory_timing_parameters(
-		struct pp_hwmgr *hwmgr,
-		uint32_t engine_clock,
-		uint32_t memory_clock,
-		struct SMU72_Discrete_MCArbDramTimingTableEntry *arb_regs
-		)
-{
-	uint32_t dramTiming;
-	uint32_t dramTiming2;
-	uint32_t burstTime;
-	int result;
-
-	result = atomctrl_set_engine_dram_timings_rv770(hwmgr,
-				engine_clock, memory_clock);
-
-	PP_ASSERT_WITH_CODE(result == 0,
-		"Error calling VBIOS to set DRAM_TIMING.", return result);
-
-	dramTiming  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING);
-	dramTiming2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2);
-	burstTime = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0);
-
-	arb_regs->McArbDramTiming  = PP_HOST_TO_SMC_UL(dramTiming);
-	arb_regs->McArbDramTiming2 = PP_HOST_TO_SMC_UL(dramTiming2);
-	arb_regs->McArbBurstTime = (uint8_t)burstTime;
-
-	return 0;
-}
-
-/**
- * Setup parameters for the MC ARB.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- * This function is to be called from the SetPowerState table.
- */
-int tonga_program_memory_timing_parameters(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	int result = 0;
-	SMU72_Discrete_MCArbDramTimingTable  arb_regs;
-	uint32_t i, j;
-
-	memset(&arb_regs, 0x00, sizeof(SMU72_Discrete_MCArbDramTimingTable));
-
-	for (i = 0; i < data->dpm_table.sclk_table.count; i++) {
-		for (j = 0; j < data->dpm_table.mclk_table.count; j++) {
-			result = tonga_populate_memory_timing_parameters
-				(hwmgr, data->dpm_table.sclk_table.dpm_levels[i].value,
-				 data->dpm_table.mclk_table.dpm_levels[j].value,
-				 &arb_regs.entries[i][j]);
-
-			if (0 != result) {
-				break;
-			}
-		}
-	}
-
-	if (0 == result) {
-		result = tonga_copy_bytes_to_smc(
-				hwmgr->smumgr,
-				data->arb_table_start,
-				(uint8_t *)&arb_regs,
-				sizeof(SMU72_Discrete_MCArbDramTimingTable),
-				data->sram_end
-				);
-	}
-
-	return result;
-}
-
-static int tonga_populate_smc_link_level(struct pp_hwmgr *hwmgr, SMU72_Discrete_DpmTable *table)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_dpm_table *dpm_table = &data->dpm_table;
-	uint32_t i;
-
-	/* Index (dpm_table->pcie_speed_table.count) is reserved for PCIE boot level. */
-	for (i = 0; i <= dpm_table->pcie_speed_table.count; i++) {
-		table->LinkLevel[i].PcieGenSpeed  =
-			(uint8_t)dpm_table->pcie_speed_table.dpm_levels[i].value;
-		table->LinkLevel[i].PcieLaneCount =
-			(uint8_t)encode_pcie_lane_width(dpm_table->pcie_speed_table.dpm_levels[i].param1);
-		table->LinkLevel[i].EnabledForActivity =
-			1;
-		table->LinkLevel[i].SPC =
-			(uint8_t)(data->pcie_spc_cap & 0xff);
-		table->LinkLevel[i].DownThreshold =
-			PP_HOST_TO_SMC_UL(5);
-		table->LinkLevel[i].UpThreshold =
-			PP_HOST_TO_SMC_UL(30);
-	}
-
-	data->smc_state_table.LinkLevelCount =
-		(uint8_t)dpm_table->pcie_speed_table.count;
-	data->dpm_level_enable_mask.pcie_dpm_enable_mask =
-		tonga_get_dpm_level_enable_mask_value(&dpm_table->pcie_speed_table);
-
-	return 0;
-}
-
-static int tonga_populate_smc_uvd_level(struct pp_hwmgr *hwmgr,
-					SMU72_Discrete_DpmTable *table)
-{
-	int result = 0;
-
-	uint8_t count;
-	pp_atomctrl_clock_dividers_vi dividers;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table = pptable_info->mm_dep_table;
-
-	table->UvdLevelCount = (uint8_t) (mm_table->count);
-	table->UvdBootLevel = 0;
-
-	for (count = 0; count < table->UvdLevelCount; count++) {
-		table->UvdLevel[count].VclkFrequency = mm_table->entries[count].vclk;
-		table->UvdLevel[count].DclkFrequency = mm_table->entries[count].dclk;
-		table->UvdLevel[count].MinVoltage.Vddc =
-			tonga_get_voltage_index(pptable_info->vddc_lookup_table,
-						mm_table->entries[count].vddc);
-		table->UvdLevel[count].MinVoltage.VddGfx =
-			(data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2) ?
-			tonga_get_voltage_index(pptable_info->vddgfx_lookup_table,
-						mm_table->entries[count].vddgfx) : 0;
-		table->UvdLevel[count].MinVoltage.Vddci =
-			tonga_get_voltage_id(&data->vddci_voltage_table,
-					     mm_table->entries[count].vddc - data->vddc_vddci_delta);
-		table->UvdLevel[count].MinVoltage.Phases = 1;
-
-		/* retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-							  table->UvdLevel[count].VclkFrequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				    "can not find divide id for Vclk clock", return result);
-
-		table->UvdLevel[count].VclkDivider = (uint8_t)dividers.pll_post_divider;
-
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-							  table->UvdLevel[count].DclkFrequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				    "can not find divide id for Dclk clock", return result);
-
-		table->UvdLevel[count].DclkDivider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->UvdLevel[count].VclkFrequency);
-		CONVERT_FROM_HOST_TO_SMC_UL(table->UvdLevel[count].DclkFrequency);
-		//CONVERT_FROM_HOST_TO_SMC_UL((uint32_t)table->UvdLevel[count].MinVoltage);
-	}
-
-	return result;
-
-}
-
-static int tonga_populate_smc_vce_level(struct pp_hwmgr *hwmgr,
-		SMU72_Discrete_DpmTable *table)
-{
-	int result = 0;
-
-	uint8_t count;
-	pp_atomctrl_clock_dividers_vi dividers;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table = pptable_info->mm_dep_table;
-
-	table->VceLevelCount = (uint8_t) (mm_table->count);
-	table->VceBootLevel = 0;
-
-	for (count = 0; count < table->VceLevelCount; count++) {
-		table->VceLevel[count].Frequency =
-			mm_table->entries[count].eclk;
-		table->VceLevel[count].MinVoltage.Vddc =
-			tonga_get_voltage_index(pptable_info->vddc_lookup_table,
-				mm_table->entries[count].vddc);
-		table->VceLevel[count].MinVoltage.VddGfx =
-			(data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2) ?
-			tonga_get_voltage_index(pptable_info->vddgfx_lookup_table,
-				mm_table->entries[count].vddgfx) : 0;
-		table->VceLevel[count].MinVoltage.Vddci =
-			tonga_get_voltage_id(&data->vddci_voltage_table,
-				mm_table->entries[count].vddc - data->vddc_vddci_delta);
-		table->VceLevel[count].MinVoltage.Phases = 1;
-
-		/* retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-					table->VceLevel[count].Frequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"can not find divide id for VCE engine clock", return result);
-
-		table->VceLevel[count].Divider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->VceLevel[count].Frequency);
-	}
-
-	return result;
-}
-
-static int tonga_populate_smc_acp_level(struct pp_hwmgr *hwmgr,
-		SMU72_Discrete_DpmTable *table)
-{
-	int result = 0;
-	uint8_t count;
-	pp_atomctrl_clock_dividers_vi dividers;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table = pptable_info->mm_dep_table;
-
-	table->AcpLevelCount = (uint8_t) (mm_table->count);
-	table->AcpBootLevel = 0;
-
-	for (count = 0; count < table->AcpLevelCount; count++) {
-		table->AcpLevel[count].Frequency =
-			pptable_info->mm_dep_table->entries[count].aclk;
-		table->AcpLevel[count].MinVoltage.Vddc =
-			tonga_get_voltage_index(pptable_info->vddc_lookup_table,
-			mm_table->entries[count].vddc);
-		table->AcpLevel[count].MinVoltage.VddGfx =
-			(data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2) ?
-			tonga_get_voltage_index(pptable_info->vddgfx_lookup_table,
-				mm_table->entries[count].vddgfx) : 0;
-		table->AcpLevel[count].MinVoltage.Vddci =
-			tonga_get_voltage_id(&data->vddci_voltage_table,
-				mm_table->entries[count].vddc - data->vddc_vddci_delta);
-		table->AcpLevel[count].MinVoltage.Phases = 1;
-
-		/* retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-			table->AcpLevel[count].Frequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"can not find divide id for engine clock", return result);
-
-		table->AcpLevel[count].Divider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->AcpLevel[count].Frequency);
-	}
-
-	return result;
-}
-
-static int tonga_populate_smc_samu_level(struct pp_hwmgr *hwmgr,
-	SMU72_Discrete_DpmTable *table)
-{
-	int result = 0;
-	uint8_t count;
-	pp_atomctrl_clock_dividers_vi dividers;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table = pptable_info->mm_dep_table;
-
-	table->SamuBootLevel = 0;
-	table->SamuLevelCount = (uint8_t) (mm_table->count);
-
-	for (count = 0; count < table->SamuLevelCount; count++) {
-		/* not sure whether we need evclk or not */
-		table->SamuLevel[count].Frequency =
-			pptable_info->mm_dep_table->entries[count].samclock;
-		table->SamuLevel[count].MinVoltage.Vddc =
-			tonga_get_voltage_index(pptable_info->vddc_lookup_table,
-				mm_table->entries[count].vddc);
-		table->SamuLevel[count].MinVoltage.VddGfx =
-			(data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2) ?
-			tonga_get_voltage_index(pptable_info->vddgfx_lookup_table,
-				mm_table->entries[count].vddgfx) : 0;
-		table->SamuLevel[count].MinVoltage.Vddci =
-			tonga_get_voltage_id(&data->vddci_voltage_table,
-				mm_table->entries[count].vddc - data->vddc_vddci_delta);
-		table->SamuLevel[count].MinVoltage.Phases = 1;
-
-		/* retrieve divider value for VBIOS */
-		result = atomctrl_get_dfs_pll_dividers_vi(hwmgr,
-					table->SamuLevel[count].Frequency, &dividers);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"can not find divide id for samu clock", return result);
-
-		table->SamuLevel[count].Divider = (uint8_t)dividers.pll_post_divider;
-
-		CONVERT_FROM_HOST_TO_SMC_UL(table->SamuLevel[count].Frequency);
-	}
-
-	return result;
-}
-
-/**
- * Populates the SMC MCLK structure using the provided memory clock
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    memory_clock the memory clock to use to populate the structure
- * @param    sclk        the SMC SCLK structure to be populated
- */
-static int tonga_calculate_mclk_params(
-		struct pp_hwmgr *hwmgr,
-		uint32_t memory_clock,
-		SMU72_Discrete_MemoryLevel *mclk,
-		bool strobe_mode,
-		bool dllStateOn
-		)
-{
-	const tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	uint32_t  dll_cntl = data->clock_registers.vDLL_CNTL;
-	uint32_t  mclk_pwrmgt_cntl = data->clock_registers.vMCLK_PWRMGT_CNTL;
-	uint32_t  mpll_ad_func_cntl = data->clock_registers.vMPLL_AD_FUNC_CNTL;
-	uint32_t  mpll_dq_func_cntl = data->clock_registers.vMPLL_DQ_FUNC_CNTL;
-	uint32_t  mpll_func_cntl = data->clock_registers.vMPLL_FUNC_CNTL;
-	uint32_t  mpll_func_cntl_1 = data->clock_registers.vMPLL_FUNC_CNTL_1;
-	uint32_t  mpll_func_cntl_2 = data->clock_registers.vMPLL_FUNC_CNTL_2;
-	uint32_t  mpll_ss1 = data->clock_registers.vMPLL_SS1;
-	uint32_t  mpll_ss2 = data->clock_registers.vMPLL_SS2;
-
-	pp_atomctrl_memory_clock_param mpll_param;
-	int result;
-
-	result = atomctrl_get_memory_pll_dividers_si(hwmgr,
-				memory_clock, &mpll_param, strobe_mode);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Error retrieving Memory Clock Parameters from VBIOS.", return result);
-
-	/* MPLL_FUNC_CNTL setup*/
-	mpll_func_cntl = PHM_SET_FIELD(mpll_func_cntl, MPLL_FUNC_CNTL, BWCTRL, mpll_param.bw_ctrl);
-
-	/* MPLL_FUNC_CNTL_1 setup*/
-	mpll_func_cntl_1  = PHM_SET_FIELD(mpll_func_cntl_1,
-							MPLL_FUNC_CNTL_1, CLKF, mpll_param.mpll_fb_divider.cl_kf);
-	mpll_func_cntl_1  = PHM_SET_FIELD(mpll_func_cntl_1,
-							MPLL_FUNC_CNTL_1, CLKFRAC, mpll_param.mpll_fb_divider.clk_frac);
-	mpll_func_cntl_1  = PHM_SET_FIELD(mpll_func_cntl_1,
-							MPLL_FUNC_CNTL_1, VCO_MODE, mpll_param.vco_mode);
-
-	/* MPLL_AD_FUNC_CNTL setup*/
-	mpll_ad_func_cntl = PHM_SET_FIELD(mpll_ad_func_cntl,
-							MPLL_AD_FUNC_CNTL, YCLK_POST_DIV, mpll_param.mpll_post_divider);
-
-	if (data->is_memory_GDDR5) {
-		/* MPLL_DQ_FUNC_CNTL setup*/
-		mpll_dq_func_cntl  = PHM_SET_FIELD(mpll_dq_func_cntl,
-								MPLL_DQ_FUNC_CNTL, YCLK_SEL, mpll_param.yclk_sel);
-		mpll_dq_func_cntl  = PHM_SET_FIELD(mpll_dq_func_cntl,
-								MPLL_DQ_FUNC_CNTL, YCLK_POST_DIV, mpll_param.mpll_post_divider);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_MemorySpreadSpectrumSupport)) {
-		/*
-		 ************************************
-		 Fref = Reference Frequency
-		 NF = Feedback divider ratio
-		 NR = Reference divider ratio
-		 Fnom = Nominal VCO output frequency = Fref * NF / NR
-		 Fs = Spreading Rate
-		 D = Percentage down-spread / 2
-		 Fint = Reference input frequency to PFD = Fref / NR
-		 NS = Spreading rate divider ratio = int(Fint / (2 * Fs))
-		 CLKS = NS - 1 = ISS_STEP_NUM[11:0]
-		 NV = D * Fs / Fnom * 4 * ((Fnom/Fref * NR) ^ 2)
-		 CLKV = 65536 * NV = ISS_STEP_SIZE[25:0]
-		 *************************************
-		 */
-		pp_atomctrl_internal_ss_info ss_info;
-		uint32_t freq_nom;
-		uint32_t tmp;
-		uint32_t reference_clock = atomctrl_get_mpll_reference_clock(hwmgr);
-
-		/* for GDDR5 for all modes and DDR3 */
-		if (1 == mpll_param.qdr)
-			freq_nom = memory_clock * 4 * (1 << mpll_param.mpll_post_divider);
-		else
-			freq_nom = memory_clock * 2 * (1 << mpll_param.mpll_post_divider);
-
-		/* tmp = (freq_nom / reference_clock * reference_divider) ^ 2  Note: S.I. reference_divider = 1*/
-		tmp = (freq_nom / reference_clock);
-		tmp = tmp * tmp;
-
-		if (0 == atomctrl_get_memory_clock_spread_spectrum(hwmgr, freq_nom, &ss_info)) {
-			/* ss_info.speed_spectrum_percentage -- in unit of 0.01% */
-			/* ss.Info.speed_spectrum_rate -- in unit of khz */
-			/* CLKS = reference_clock / (2 * speed_spectrum_rate * reference_divider) * 10 */
-			/*     = reference_clock * 5 / speed_spectrum_rate */
-			uint32_t clks = reference_clock * 5 / ss_info.speed_spectrum_rate;
-
-			/* CLKV = 65536 * speed_spectrum_percentage / 2 * spreadSpecrumRate / freq_nom * 4 / 100000 * ((freq_nom / reference_clock) ^ 2) */
-			/*     = 131 * speed_spectrum_percentage * speed_spectrum_rate / 100 * ((freq_nom / reference_clock) ^ 2) / freq_nom */
-			uint32_t clkv =
-				(uint32_t)((((131 * ss_info.speed_spectrum_percentage *
-							ss_info.speed_spectrum_rate) / 100) * tmp) / freq_nom);
-
-			mpll_ss1 = PHM_SET_FIELD(mpll_ss1, MPLL_SS1, CLKV, clkv);
-			mpll_ss2 = PHM_SET_FIELD(mpll_ss2, MPLL_SS2, CLKS, clks);
-		}
-	}
-
-	/* MCLK_PWRMGT_CNTL setup */
-	mclk_pwrmgt_cntl = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, DLL_SPEED, mpll_param.dll_speed);
-	mclk_pwrmgt_cntl = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK0_PDNB, dllStateOn);
-	mclk_pwrmgt_cntl = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK1_PDNB, dllStateOn);
-
-
-	/* Save the result data to outpupt memory level structure */
-	mclk->MclkFrequency   = memory_clock;
-	mclk->MpllFuncCntl    = mpll_func_cntl;
-	mclk->MpllFuncCntl_1  = mpll_func_cntl_1;
-	mclk->MpllFuncCntl_2  = mpll_func_cntl_2;
-	mclk->MpllAdFuncCntl  = mpll_ad_func_cntl;
-	mclk->MpllDqFuncCntl  = mpll_dq_func_cntl;
-	mclk->MclkPwrmgtCntl  = mclk_pwrmgt_cntl;
-	mclk->DllCntl         = dll_cntl;
-	mclk->MpllSs1         = mpll_ss1;
-	mclk->MpllSs2         = mpll_ss2;
-
-	return 0;
-}
-
-static uint8_t tonga_get_mclk_frequency_ratio(uint32_t memory_clock,
-		bool strobe_mode)
-{
-	uint8_t mc_para_index;
-
-	if (strobe_mode) {
-		if (memory_clock < 12500) {
-			mc_para_index = 0x00;
-		} else if (memory_clock > 47500) {
-			mc_para_index = 0x0f;
-		} else {
-			mc_para_index = (uint8_t)((memory_clock - 10000) / 2500);
-		}
-	} else {
-		if (memory_clock < 65000) {
-			mc_para_index = 0x00;
-		} else if (memory_clock > 135000) {
-			mc_para_index = 0x0f;
-		} else {
-			mc_para_index = (uint8_t)((memory_clock - 60000) / 5000);
-		}
-	}
-
-	return mc_para_index;
-}
-
-static uint8_t tonga_get_ddr3_mclk_frequency_ratio(uint32_t memory_clock)
-{
-	uint8_t mc_para_index;
-
-	if (memory_clock < 10000) {
-		mc_para_index = 0;
-	} else if (memory_clock >= 80000) {
-		mc_para_index = 0x0f;
-	} else {
-		mc_para_index = (uint8_t)((memory_clock - 10000) / 5000 + 1);
-	}
-
-	return mc_para_index;
-}
-
-static int tonga_populate_single_memory_level(
-		struct pp_hwmgr *hwmgr,
-		uint32_t memory_clock,
-		SMU72_Discrete_MemoryLevel *memory_level
-		)
-{
-	uint32_t minMvdd = 0;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	int result = 0;
-	bool dllStateOn;
-	struct cgs_display_info info = {0};
-
-
-	if (NULL != pptable_info->vdd_dep_on_mclk) {
-		result = tonga_get_dependecy_volt_by_clk(hwmgr,
-			pptable_info->vdd_dep_on_mclk, memory_clock, &memory_level->MinVoltage, &minMvdd);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"can not find MinVddc voltage value from memory VDDC voltage dependency table", return result);
-	}
-
-	if (data->mvdd_control == TONGA_VOLTAGE_CONTROL_NONE) {
-		memory_level->MinMvdd = data->vbios_boot_state.mvdd_bootup_value;
-	} else {
-		memory_level->MinMvdd = minMvdd;
-	}
-	memory_level->EnabledForThrottle = 1;
-	memory_level->EnabledForActivity = 0;
-	memory_level->UpHyst = 0;
-	memory_level->DownHyst = 100;
-	memory_level->VoltageDownHyst = 0;
-
-	/* Indicates maximum activity level for this performance level.*/
-	memory_level->ActivityLevel = (uint16_t)data->mclk_activity_target;
-	memory_level->StutterEnable = 0;
-	memory_level->StrobeEnable = 0;
-	memory_level->EdcReadEnable = 0;
-	memory_level->EdcWriteEnable = 0;
-	memory_level->RttEnable = 0;
-
-	/* default set to low watermark. Highest level will be set to high later.*/
-	memory_level->DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-	data->display_timing.num_existing_displays = info.display_count;
-
-	if ((data->mclk_stutter_mode_threshold != 0) &&
-	    (memory_clock <= data->mclk_stutter_mode_threshold) &&
-	    (!data->is_uvd_enabled)
-	    && (PHM_READ_FIELD(hwmgr->device, DPG_PIPE_STUTTER_CONTROL, STUTTER_ENABLE) & 0x1)
-	    && (data->display_timing.num_existing_displays <= 2)
-	    && (data->display_timing.num_existing_displays != 0))
-		memory_level->StutterEnable = 1;
-
-	/* decide strobe mode*/
-	memory_level->StrobeEnable = (data->mclk_strobe_mode_threshold != 0) &&
-		(memory_clock <= data->mclk_strobe_mode_threshold);
-
-	/* decide EDC mode and memory clock ratio*/
-	if (data->is_memory_GDDR5) {
-		memory_level->StrobeRatio = tonga_get_mclk_frequency_ratio(memory_clock,
-					memory_level->StrobeEnable);
-
-		if ((data->mclk_edc_enable_threshold != 0) &&
-				(memory_clock > data->mclk_edc_enable_threshold)) {
-			memory_level->EdcReadEnable = 1;
-		}
-
-		if ((data->mclk_edc_wr_enable_threshold != 0) &&
-				(memory_clock > data->mclk_edc_wr_enable_threshold)) {
-			memory_level->EdcWriteEnable = 1;
-		}
-
-		if (memory_level->StrobeEnable) {
-			if (tonga_get_mclk_frequency_ratio(memory_clock, 1) >=
-					((cgs_read_register(hwmgr->device, mmMC_SEQ_MISC7) >> 16) & 0xf)) {
-				dllStateOn = ((cgs_read_register(hwmgr->device, mmMC_SEQ_MISC5) >> 1) & 0x1) ? 1 : 0;
-			} else {
-				dllStateOn = ((cgs_read_register(hwmgr->device, mmMC_SEQ_MISC6) >> 1) & 0x1) ? 1 : 0;
-			}
-
-		} else {
-			dllStateOn = data->dll_defaule_on;
-		}
-	} else {
-		memory_level->StrobeRatio =
-			tonga_get_ddr3_mclk_frequency_ratio(memory_clock);
-		dllStateOn = ((cgs_read_register(hwmgr->device, mmMC_SEQ_MISC5) >> 1) & 0x1) ? 1 : 0;
-	}
-
-	result = tonga_calculate_mclk_params(hwmgr,
-		memory_clock, memory_level, memory_level->StrobeEnable, dllStateOn);
-
-	if (0 == result) {
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MinMvdd);
-		/* MCLK frequency in units of 10KHz*/
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MclkFrequency);
-		/* Indicates maximum activity level for this performance level.*/
-		CONVERT_FROM_HOST_TO_SMC_US(memory_level->ActivityLevel);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllFuncCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllFuncCntl_1);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllFuncCntl_2);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllAdFuncCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllDqFuncCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MclkPwrmgtCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->DllCntl);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllSs1);
-		CONVERT_FROM_HOST_TO_SMC_UL(memory_level->MpllSs2);
-	}
-
-	return result;
-}
-
-/**
- * Populates the SMC MVDD structure using the provided memory clock.
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    mclk        the MCLK value to be used in the decision if MVDD should be high or low.
- * @param    voltage     the SMC VOLTAGE structure to be populated
- */
-int tonga_populate_mvdd_value(struct pp_hwmgr *hwmgr, uint32_t mclk, SMIO_Pattern *smio_pattern)
-{
-	const tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint32_t i = 0;
-
-	if (TONGA_VOLTAGE_CONTROL_NONE != data->mvdd_control) {
-		/* find mvdd value which clock is more than request */
-		for (i = 0; i < pptable_info->vdd_dep_on_mclk->count; i++) {
-			if (mclk <= pptable_info->vdd_dep_on_mclk->entries[i].clk) {
-				/* Always round to higher voltage. */
-				smio_pattern->Voltage = data->mvdd_voltage_table.entries[i].value;
-				break;
-			}
-		}
-
-		PP_ASSERT_WITH_CODE(i < pptable_info->vdd_dep_on_mclk->count,
-			"MVDD Voltage is outside the supported range.", return -1);
-
-	} else {
-		return -1;
-	}
-
-	return 0;
-}
-
-
-static int tonga_populate_smv_acpi_level(struct pp_hwmgr *hwmgr,
-	SMU72_Discrete_DpmTable *table)
-{
-	int result = 0;
-	const tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	pp_atomctrl_clock_dividers_vi dividers;
-	SMIO_Pattern voltage_level;
-	uint32_t spll_func_cntl    = data->clock_registers.vCG_SPLL_FUNC_CNTL;
-	uint32_t spll_func_cntl_2  = data->clock_registers.vCG_SPLL_FUNC_CNTL_2;
-	uint32_t dll_cntl          = data->clock_registers.vDLL_CNTL;
-	uint32_t mclk_pwrmgt_cntl  = data->clock_registers.vMCLK_PWRMGT_CNTL;
-
-	/* The ACPI state should not do DPM on DC (or ever).*/
-	table->ACPILevel.Flags &= ~PPSMC_SWSTATE_FLAG_DC;
-
-	table->ACPILevel.MinVoltage = data->smc_state_table.GraphicsLevel[0].MinVoltage;
-
-	/* assign zero for now*/
-	table->ACPILevel.SclkFrequency = atomctrl_get_reference_clock(hwmgr);
-
-	/* get the engine clock dividers for this clock value*/
-	result = atomctrl_get_engine_pll_dividers_vi(hwmgr,
-		table->ACPILevel.SclkFrequency,  &dividers);
-
-	PP_ASSERT_WITH_CODE(result == 0,
-		"Error retrieving Engine Clock dividers from VBIOS.", return result);
-
-	/* divider ID for required SCLK*/
-	table->ACPILevel.SclkDid = (uint8_t)dividers.pll_post_divider;
-	table->ACPILevel.DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-	table->ACPILevel.DeepSleepDivId = 0;
-
-	spll_func_cntl      = PHM_SET_FIELD(spll_func_cntl,
-							CG_SPLL_FUNC_CNTL,   SPLL_PWRON,     0);
-	spll_func_cntl      = PHM_SET_FIELD(spll_func_cntl,
-							CG_SPLL_FUNC_CNTL,   SPLL_RESET,     1);
-	spll_func_cntl_2    = PHM_SET_FIELD(spll_func_cntl_2,
-							CG_SPLL_FUNC_CNTL_2, SCLK_MUX_SEL,   4);
-
-	table->ACPILevel.CgSpllFuncCntl = spll_func_cntl;
-	table->ACPILevel.CgSpllFuncCntl2 = spll_func_cntl_2;
-	table->ACPILevel.CgSpllFuncCntl3 = data->clock_registers.vCG_SPLL_FUNC_CNTL_3;
-	table->ACPILevel.CgSpllFuncCntl4 = data->clock_registers.vCG_SPLL_FUNC_CNTL_4;
-	table->ACPILevel.SpllSpreadSpectrum = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM;
-	table->ACPILevel.SpllSpreadSpectrum2 = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2;
-	table->ACPILevel.CcPwrDynRm = 0;
-	table->ACPILevel.CcPwrDynRm1 = 0;
-
-
-	/* For various features to be enabled/disabled while this level is active.*/
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.Flags);
-	/* SCLK frequency in units of 10KHz*/
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SclkFrequency);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl2);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl3);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CgSpllFuncCntl4);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SpllSpreadSpectrum);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.SpllSpreadSpectrum2);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CcPwrDynRm);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->ACPILevel.CcPwrDynRm1);
-
-	/* table->MemoryACPILevel.MinVddcPhases = table->ACPILevel.MinVddcPhases;*/
-	table->MemoryACPILevel.MinVoltage = data->smc_state_table.MemoryLevel[0].MinVoltage;
-
-	/*  CONVERT_FROM_HOST_TO_SMC_UL(table->MemoryACPILevel.MinVoltage);*/
-
-	if (0 == tonga_populate_mvdd_value(hwmgr, 0, &voltage_level))
-		table->MemoryACPILevel.MinMvdd =
-			PP_HOST_TO_SMC_UL(voltage_level.Voltage * VOLTAGE_SCALE);
-	else
-		table->MemoryACPILevel.MinMvdd = 0;
-
-	/* Force reset on DLL*/
-	mclk_pwrmgt_cntl    = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK0_RESET, 0x1);
-	mclk_pwrmgt_cntl    = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK1_RESET, 0x1);
-
-	/* Disable DLL in ACPIState*/
-	mclk_pwrmgt_cntl    = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK0_PDNB, 0);
-	mclk_pwrmgt_cntl    = PHM_SET_FIELD(mclk_pwrmgt_cntl,
-		MCLK_PWRMGT_CNTL, MRDCK1_PDNB, 0);
-
-	/* Enable DLL bypass signal*/
-	dll_cntl            = PHM_SET_FIELD(dll_cntl,
-		DLL_CNTL, MRDCK0_BYPASS, 0);
-	dll_cntl            = PHM_SET_FIELD(dll_cntl,
-		DLL_CNTL, MRDCK1_BYPASS, 0);
-
-	table->MemoryACPILevel.DllCntl            =
-		PP_HOST_TO_SMC_UL(dll_cntl);
-	table->MemoryACPILevel.MclkPwrmgtCntl     =
-		PP_HOST_TO_SMC_UL(mclk_pwrmgt_cntl);
-	table->MemoryACPILevel.MpllAdFuncCntl     =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_AD_FUNC_CNTL);
-	table->MemoryACPILevel.MpllDqFuncCntl     =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_DQ_FUNC_CNTL);
-	table->MemoryACPILevel.MpllFuncCntl       =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_FUNC_CNTL);
-	table->MemoryACPILevel.MpllFuncCntl_1     =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_FUNC_CNTL_1);
-	table->MemoryACPILevel.MpllFuncCntl_2     =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_FUNC_CNTL_2);
-	table->MemoryACPILevel.MpllSs1            =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_SS1);
-	table->MemoryACPILevel.MpllSs2            =
-		PP_HOST_TO_SMC_UL(data->clock_registers.vMPLL_SS2);
-
-	table->MemoryACPILevel.EnabledForThrottle = 0;
-	table->MemoryACPILevel.EnabledForActivity = 0;
-	table->MemoryACPILevel.UpHyst = 0;
-	table->MemoryACPILevel.DownHyst = 100;
-	table->MemoryACPILevel.VoltageDownHyst = 0;
-	/* Indicates maximum activity level for this performance level.*/
-	table->MemoryACPILevel.ActivityLevel = PP_HOST_TO_SMC_US((uint16_t)data->mclk_activity_target);
-
-	table->MemoryACPILevel.StutterEnable = 0;
-	table->MemoryACPILevel.StrobeEnable = 0;
-	table->MemoryACPILevel.EdcReadEnable = 0;
-	table->MemoryACPILevel.EdcWriteEnable = 0;
-	table->MemoryACPILevel.RttEnable = 0;
-
-	return result;
-}
-
-static int tonga_find_boot_level(struct tonga_single_dpm_table *table, uint32_t value, uint32_t *boot_level)
-{
-	int result = 0;
-	uint32_t i;
-
-	for (i = 0; i < table->count; i++) {
-		if (value == table->dpm_levels[i].value) {
-			*boot_level = i;
-			result = 0;
-		}
-	}
-	return result;
-}
-
-static int tonga_populate_smc_boot_level(struct pp_hwmgr *hwmgr,
-			SMU72_Discrete_DpmTable *table)
-{
-	int result = 0;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	table->GraphicsBootLevel  = 0;        /* 0 == DPM[0] (low), etc. */
-	table->MemoryBootLevel    = 0;        /* 0 == DPM[0] (low), etc. */
-
-	/* find boot level from dpm table*/
-	result = tonga_find_boot_level(&(data->dpm_table.sclk_table),
-	data->vbios_boot_state.sclk_bootup_value,
-	(uint32_t *)&(data->smc_state_table.GraphicsBootLevel));
-
-	if (0 != result) {
-		data->smc_state_table.GraphicsBootLevel = 0;
-		printk(KERN_ERR "[ powerplay ] VBIOS did not find boot engine clock value \
-			in dependency table. Using Graphics DPM level 0!");
-		result = 0;
-	}
-
-	result = tonga_find_boot_level(&(data->dpm_table.mclk_table),
-		data->vbios_boot_state.mclk_bootup_value,
-		(uint32_t *)&(data->smc_state_table.MemoryBootLevel));
-
-	if (0 != result) {
-		data->smc_state_table.MemoryBootLevel = 0;
-		printk(KERN_ERR "[ powerplay ] VBIOS did not find boot engine clock value \
-			in dependency table. Using Memory DPM level 0!");
-		result = 0;
-	}
-
-	table->BootVoltage.Vddc =
-		tonga_get_voltage_id(&(data->vddc_voltage_table),
-			data->vbios_boot_state.vddc_bootup_value);
-	table->BootVoltage.VddGfx =
-		tonga_get_voltage_id(&(data->vddgfx_voltage_table),
-			data->vbios_boot_state.vddgfx_bootup_value);
-	table->BootVoltage.Vddci =
-		tonga_get_voltage_id(&(data->vddci_voltage_table),
-			data->vbios_boot_state.vddci_bootup_value);
-	table->BootMVdd = data->vbios_boot_state.mvdd_bootup_value;
-
-	CONVERT_FROM_HOST_TO_SMC_US(table->BootMVdd);
-
-	return result;
-}
-
-
-/**
- * Calculates the SCLK dividers using the provided engine clock
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    engine_clock the engine clock to use to populate the structure
- * @param    sclk        the SMC SCLK structure to be populated
- */
-int tonga_calculate_sclk_params(struct pp_hwmgr *hwmgr,
-		uint32_t engine_clock, SMU72_Discrete_GraphicsLevel *sclk)
-{
-	const tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	pp_atomctrl_clock_dividers_vi dividers;
-	uint32_t spll_func_cntl            = data->clock_registers.vCG_SPLL_FUNC_CNTL;
-	uint32_t spll_func_cntl_3          = data->clock_registers.vCG_SPLL_FUNC_CNTL_3;
-	uint32_t spll_func_cntl_4          = data->clock_registers.vCG_SPLL_FUNC_CNTL_4;
-	uint32_t cg_spll_spread_spectrum   = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM;
-	uint32_t cg_spll_spread_spectrum_2 = data->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2;
-	uint32_t    reference_clock;
-	uint32_t reference_divider;
-	uint32_t fbdiv;
-	int result;
-
-	/* get the engine clock dividers for this clock value*/
-	result = atomctrl_get_engine_pll_dividers_vi(hwmgr, engine_clock,  &dividers);
-
-	PP_ASSERT_WITH_CODE(result == 0,
-		"Error retrieving Engine Clock dividers from VBIOS.", return result);
-
-	/* To get FBDIV we need to multiply this by 16384 and divide it by Fref.*/
-	reference_clock = atomctrl_get_reference_clock(hwmgr);
-
-	reference_divider = 1 + dividers.uc_pll_ref_div;
-
-	/* low 14 bits is fraction and high 12 bits is divider*/
-	fbdiv = dividers.ul_fb_div.ul_fb_divider & 0x3FFFFFF;
-
-	/* SPLL_FUNC_CNTL setup*/
-	spll_func_cntl = PHM_SET_FIELD(spll_func_cntl,
-		CG_SPLL_FUNC_CNTL, SPLL_REF_DIV, dividers.uc_pll_ref_div);
-	spll_func_cntl = PHM_SET_FIELD(spll_func_cntl,
-		CG_SPLL_FUNC_CNTL, SPLL_PDIV_A,  dividers.uc_pll_post_div);
-
-	/* SPLL_FUNC_CNTL_3 setup*/
-	spll_func_cntl_3 = PHM_SET_FIELD(spll_func_cntl_3,
-		CG_SPLL_FUNC_CNTL_3, SPLL_FB_DIV, fbdiv);
-
-	/* set to use fractional accumulation*/
-	spll_func_cntl_3 = PHM_SET_FIELD(spll_func_cntl_3,
-		CG_SPLL_FUNC_CNTL_3, SPLL_DITHEN, 1);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EngineSpreadSpectrumSupport)) {
-		pp_atomctrl_internal_ss_info ss_info;
-
-		uint32_t vcoFreq = engine_clock * dividers.uc_pll_post_div;
-		if (0 == atomctrl_get_engine_clock_spread_spectrum(hwmgr, vcoFreq, &ss_info)) {
-			/*
-			* ss_info.speed_spectrum_percentage -- in unit of 0.01%
-			* ss_info.speed_spectrum_rate -- in unit of khz
-			*/
-			/* clks = reference_clock * 10 / (REFDIV + 1) / speed_spectrum_rate / 2 */
-			uint32_t clkS = reference_clock * 5 / (reference_divider * ss_info.speed_spectrum_rate);
-
-			/* clkv = 2 * D * fbdiv / NS */
-			uint32_t clkV = 4 * ss_info.speed_spectrum_percentage * fbdiv / (clkS * 10000);
-
-			cg_spll_spread_spectrum =
-				PHM_SET_FIELD(cg_spll_spread_spectrum, CG_SPLL_SPREAD_SPECTRUM, CLKS, clkS);
-			cg_spll_spread_spectrum =
-				PHM_SET_FIELD(cg_spll_spread_spectrum, CG_SPLL_SPREAD_SPECTRUM, SSEN, 1);
-			cg_spll_spread_spectrum_2 =
-				PHM_SET_FIELD(cg_spll_spread_spectrum_2, CG_SPLL_SPREAD_SPECTRUM_2, CLKV, clkV);
-		}
-	}
-
-	sclk->SclkFrequency        = engine_clock;
-	sclk->CgSpllFuncCntl3      = spll_func_cntl_3;
-	sclk->CgSpllFuncCntl4      = spll_func_cntl_4;
-	sclk->SpllSpreadSpectrum   = cg_spll_spread_spectrum;
-	sclk->SpllSpreadSpectrum2  = cg_spll_spread_spectrum_2;
-	sclk->SclkDid              = (uint8_t)dividers.pll_post_divider;
-
-	return 0;
-}
-
-static uint8_t tonga_get_sleep_divider_id_from_clock(uint32_t engine_clock,
-		uint32_t min_engine_clock_in_sr)
-{
-	uint32_t i, temp;
-	uint32_t min = max(min_engine_clock_in_sr, (uint32_t)TONGA_MINIMUM_ENGINE_CLOCK);
-
-	PP_ASSERT_WITH_CODE((engine_clock >= min),
-			"Engine clock can't satisfy stutter requirement!", return 0);
-
-	for (i = TONGA_MAX_DEEPSLEEP_DIVIDER_ID;; i--) {
-		temp = engine_clock >> i;
-
-		if(temp >= min || i == 0)
-			break;
-	}
-	return (uint8_t)i;
-}
-
-/**
- * Populates single SMC SCLK structure using the provided engine clock
- *
- * @param    hwmgr      the address of the hardware manager
- * @param    engine_clock the engine clock to use to populate the structure
- * @param    sclk        the SMC SCLK structure to be populated
- */
-static int tonga_populate_single_graphic_level(struct pp_hwmgr *hwmgr, uint32_t engine_clock, uint16_t sclk_activity_level_threshold, SMU72_Discrete_GraphicsLevel *graphic_level)
-{
-	int result;
-	uint32_t threshold;
-	uint32_t mvdd;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	result = tonga_calculate_sclk_params(hwmgr, engine_clock, graphic_level);
-
-
-	/* populate graphics levels*/
-	result = tonga_get_dependecy_volt_by_clk(hwmgr,
-		pptable_info->vdd_dep_on_sclk, engine_clock,
-		&graphic_level->MinVoltage, &mvdd);
-	PP_ASSERT_WITH_CODE((0 == result),
-		"can not find VDDC voltage value for VDDC	\
-		engine clock dependency table", return result);
-
-	/* SCLK frequency in units of 10KHz*/
-	graphic_level->SclkFrequency = engine_clock;
-
-	/* Indicates maximum activity level for this performance level. 50% for now*/
-	graphic_level->ActivityLevel = sclk_activity_level_threshold;
-
-	graphic_level->CcPwrDynRm = 0;
-	graphic_level->CcPwrDynRm1 = 0;
-	/* this level can be used if activity is high enough.*/
-	graphic_level->EnabledForActivity = 0;
-	/* this level can be used for throttling.*/
-	graphic_level->EnabledForThrottle = 1;
-	graphic_level->UpHyst = 0;
-	graphic_level->DownHyst = 0;
-	graphic_level->VoltageDownHyst = 0;
-	graphic_level->PowerThrottle = 0;
-
-	threshold = engine_clock * data->fast_watermark_threshold / 100;
-/*
-	*get the DAL clock. do it in funture.
-	PECI_GetMinClockSettings(hwmgr->peci, &minClocks);
-	data->display_timing.min_clock_insr = minClocks.engineClockInSR;
-*/
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SclkDeepSleep))
-		graphic_level->DeepSleepDivId =
-				tonga_get_sleep_divider_id_from_clock(engine_clock,
-						data->display_timing.min_clock_insr);
-
-	/* Default to slow, highest DPM level will be set to PPSMC_DISPLAY_WATERMARK_LOW later.*/
-	graphic_level->DisplayWatermark = PPSMC_DISPLAY_WATERMARK_LOW;
-
-	if (0 == result) {
-		/* CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->MinVoltage);*/
-		/* CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->MinVddcPhases);*/
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->SclkFrequency);
-		CONVERT_FROM_HOST_TO_SMC_US(graphic_level->ActivityLevel);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->CgSpllFuncCntl3);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->CgSpllFuncCntl4);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->SpllSpreadSpectrum);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->SpllSpreadSpectrum2);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->CcPwrDynRm);
-		CONVERT_FROM_HOST_TO_SMC_UL(graphic_level->CcPwrDynRm1);
-	}
-
-	return result;
-}
-
-/**
- * Populates all SMC SCLK levels' structure based on the trimmed allowed dpm engine clock states
- *
- * @param    hwmgr      the address of the hardware manager
- */
-static int tonga_populate_all_graphic_levels(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct tonga_dpm_table *dpm_table = &data->dpm_table;
-	phm_ppt_v1_pcie_table *pcie_table = pptable_info->pcie_table;
-	uint8_t pcie_entry_count = (uint8_t) data->dpm_table.pcie_speed_table.count;
-	int result = 0;
-	uint32_t level_array_adress = data->dpm_table_start +
-		offsetof(SMU72_Discrete_DpmTable, GraphicsLevel);
-	uint32_t level_array_size = sizeof(SMU72_Discrete_GraphicsLevel) *
-		SMU72_MAX_LEVELS_GRAPHICS;   /* 64 -> long; 32 -> int*/
-	SMU72_Discrete_GraphicsLevel *levels = data->smc_state_table.GraphicsLevel;
-	uint32_t i, maxEntry;
-	uint8_t highest_pcie_level_enabled = 0, lowest_pcie_level_enabled = 0, mid_pcie_level_enabled = 0, count = 0;
-	PECI_RegistryValue reg_value;
-	memset(levels, 0x00, level_array_size);
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		result = tonga_populate_single_graphic_level(hwmgr,
-					dpm_table->sclk_table.dpm_levels[i].value,
-					(uint16_t)data->activity_target[i],
-					&(data->smc_state_table.GraphicsLevel[i]));
-
-		if (0 != result)
-			return result;
-
-		/* Making sure only DPM level 0-1 have Deep Sleep Div ID populated. */
-		if (i > 1)
-			data->smc_state_table.GraphicsLevel[i].DeepSleepDivId = 0;
-
-		if (0 == i) {
-			reg_value = 0;
-			if (reg_value != 0)
-				data->smc_state_table.GraphicsLevel[0].UpHyst = (uint8_t)reg_value;
-		}
-
-		if (1 == i) {
-			reg_value = 0;
-			if (reg_value != 0)
-				data->smc_state_table.GraphicsLevel[1].UpHyst = (uint8_t)reg_value;
-		}
-	}
-
-	/* Only enable level 0 for now. */
-	data->smc_state_table.GraphicsLevel[0].EnabledForActivity = 1;
-
-	/* set highest level watermark to high */
-	if (dpm_table->sclk_table.count > 1)
-		data->smc_state_table.GraphicsLevel[dpm_table->sclk_table.count-1].DisplayWatermark =
-			PPSMC_DISPLAY_WATERMARK_HIGH;
-
-	data->smc_state_table.GraphicsDpmLevelCount =
-		(uint8_t)dpm_table->sclk_table.count;
-	data->dpm_level_enable_mask.sclk_dpm_enable_mask =
-		tonga_get_dpm_level_enable_mask_value(&dpm_table->sclk_table);
-
-	if (pcie_table != NULL) {
-		PP_ASSERT_WITH_CODE((pcie_entry_count >= 1),
-			"There must be 1 or more PCIE levels defined in PPTable.", return -1);
-		maxEntry = pcie_entry_count - 1; /* for indexing, we need to decrement by 1.*/
-		for (i = 0; i < dpm_table->sclk_table.count; i++) {
-			data->smc_state_table.GraphicsLevel[i].pcieDpmLevel =
-				(uint8_t) ((i < maxEntry) ? i : maxEntry);
-		}
-	} else {
-		if (0 == data->dpm_level_enable_mask.pcie_dpm_enable_mask)
-			printk(KERN_ERR "[ powerplay ] Pcie Dpm Enablemask is 0!");
-
-		while (data->dpm_level_enable_mask.pcie_dpm_enable_mask &&
-				((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-					(1<<(highest_pcie_level_enabled+1))) != 0)) {
-			highest_pcie_level_enabled++;
-		}
-
-		while (data->dpm_level_enable_mask.pcie_dpm_enable_mask &&
-				((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-					(1<<lowest_pcie_level_enabled)) == 0)) {
-			lowest_pcie_level_enabled++;
-		}
-
-		while ((count < highest_pcie_level_enabled) &&
-				((data->dpm_level_enable_mask.pcie_dpm_enable_mask &
-					(1<<(lowest_pcie_level_enabled+1+count))) == 0)) {
-			count++;
-		}
-		mid_pcie_level_enabled = (lowest_pcie_level_enabled+1+count) < highest_pcie_level_enabled ?
-			(lowest_pcie_level_enabled+1+count) : highest_pcie_level_enabled;
-
-
-		/* set pcieDpmLevel to highest_pcie_level_enabled*/
-		for (i = 2; i < dpm_table->sclk_table.count; i++) {
-			data->smc_state_table.GraphicsLevel[i].pcieDpmLevel = highest_pcie_level_enabled;
-		}
-
-		/* set pcieDpmLevel to lowest_pcie_level_enabled*/
-		data->smc_state_table.GraphicsLevel[0].pcieDpmLevel = lowest_pcie_level_enabled;
-
-		/* set pcieDpmLevel to mid_pcie_level_enabled*/
-		data->smc_state_table.GraphicsLevel[1].pcieDpmLevel = mid_pcie_level_enabled;
-	}
-	/* level count will send to smc once at init smc table and never change*/
-	result = tonga_copy_bytes_to_smc(hwmgr->smumgr, level_array_adress, (uint8_t *)levels, (uint32_t)level_array_size, data->sram_end);
-
-	if (0 != result)
-		return result;
-
-	return 0;
-}
-
-/**
- * Populates all SMC MCLK levels' structure based on the trimmed allowed dpm memory clock states
- *
- * @param    hwmgr      the address of the hardware manager
- */
-
-static int tonga_populate_all_memory_levels(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_dpm_table *dpm_table = &data->dpm_table;
-	int result;
-	/* populate MCLK dpm table to SMU7 */
-	uint32_t level_array_adress = data->dpm_table_start + offsetof(SMU72_Discrete_DpmTable, MemoryLevel);
-	uint32_t level_array_size = sizeof(SMU72_Discrete_MemoryLevel) * SMU72_MAX_LEVELS_MEMORY;
-	SMU72_Discrete_MemoryLevel *levels = data->smc_state_table.MemoryLevel;
-	uint32_t i;
-
-	memset(levels, 0x00, level_array_size);
-
-	for (i = 0; i < dpm_table->mclk_table.count; i++) {
-		PP_ASSERT_WITH_CODE((0 != dpm_table->mclk_table.dpm_levels[i].value),
-			"can not populate memory level as memory clock is zero", return -1);
-		result = tonga_populate_single_memory_level(hwmgr, dpm_table->mclk_table.dpm_levels[i].value,
-			&(data->smc_state_table.MemoryLevel[i]));
-		if (0 != result) {
-			return result;
-		}
-	}
-
-	/* Only enable level 0 for now.*/
-	data->smc_state_table.MemoryLevel[0].EnabledForActivity = 1;
-
-	/*
-	* in order to prevent MC activity from stutter mode to push DPM up.
-	* the UVD change complements this by putting the MCLK in a higher state
-	* by default such that we are not effected by up threshold or and MCLK DPM latency.
-	*/
-	data->smc_state_table.MemoryLevel[0].ActivityLevel = 0x1F;
-	CONVERT_FROM_HOST_TO_SMC_US(data->smc_state_table.MemoryLevel[0].ActivityLevel);
-
-	data->smc_state_table.MemoryDpmLevelCount = (uint8_t)dpm_table->mclk_table.count;
-	data->dpm_level_enable_mask.mclk_dpm_enable_mask = tonga_get_dpm_level_enable_mask_value(&dpm_table->mclk_table);
-	/* set highest level watermark to high*/
-	data->smc_state_table.MemoryLevel[dpm_table->mclk_table.count-1].DisplayWatermark = PPSMC_DISPLAY_WATERMARK_HIGH;
-
-	/* level count will send to smc once at init smc table and never change*/
-	result = tonga_copy_bytes_to_smc(hwmgr->smumgr,
-		level_array_adress, (uint8_t *)levels, (uint32_t)level_array_size, data->sram_end);
-
-	if (0 != result) {
-		return result;
-	}
-
-	return 0;
-}
-
-struct TONGA_DLL_SPEED_SETTING {
-	uint16_t            Min;                          /* Minimum Data Rate*/
-	uint16_t            Max;                          /* Maximum Data Rate*/
-	uint32_t			dll_speed;                     /* The desired DLL_SPEED setting*/
-};
-
-static int tonga_populate_clock_stretcher_data_table(struct pp_hwmgr *hwmgr)
-{
-	return 0;
-}
-
-/* ---------------------------------------- ULV related functions ----------------------------------------------------*/
-
-
-static int tonga_reset_single_dpm_table(
-	struct pp_hwmgr *hwmgr,
-	struct tonga_single_dpm_table *dpm_table,
-	uint32_t count)
-{
-	uint32_t i;
-	if (!(count <= MAX_REGULAR_DPM_NUMBER))
-		printk(KERN_ERR "[ powerplay ] Fatal error, can not set up single DPM \
-			table entries to exceed max number! \n");
-
-	dpm_table->count = count;
-	for (i = 0; i < MAX_REGULAR_DPM_NUMBER; i++) {
-		dpm_table->dpm_levels[i].enabled = false;
-	}
-
-	return 0;
-}
-
-static void tonga_setup_pcie_table_entry(
-	struct tonga_single_dpm_table *dpm_table,
-	uint32_t index, uint32_t pcie_gen,
-	uint32_t pcie_lanes)
-{
-	dpm_table->dpm_levels[index].value = pcie_gen;
-	dpm_table->dpm_levels[index].param1 = pcie_lanes;
-	dpm_table->dpm_levels[index].enabled = true;
-}
-
-static int tonga_setup_default_pcie_tables(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	phm_ppt_v1_pcie_table *pcie_table = pptable_info->pcie_table;
-	uint32_t i, maxEntry;
-
-	if (data->use_pcie_performance_levels && !data->use_pcie_power_saving_levels) {
-		data->pcie_gen_power_saving = data->pcie_gen_performance;
-		data->pcie_lane_power_saving = data->pcie_lane_performance;
-	} else if (!data->use_pcie_performance_levels && data->use_pcie_power_saving_levels) {
-		data->pcie_gen_performance = data->pcie_gen_power_saving;
-		data->pcie_lane_performance = data->pcie_lane_power_saving;
-	}
-
-	tonga_reset_single_dpm_table(hwmgr, &data->dpm_table.pcie_speed_table, SMU72_MAX_LEVELS_LINK);
-
-	if (pcie_table != NULL) {
-		/*
-		* maxEntry is used to make sure we reserve one PCIE level for boot level (fix for A+A PSPP issue).
-		* If PCIE table from PPTable have ULV entry + 8 entries, then ignore the last entry.
-		*/
-		maxEntry = (SMU72_MAX_LEVELS_LINK < pcie_table->count) ?
-						SMU72_MAX_LEVELS_LINK : pcie_table->count;
-		for (i = 1; i < maxEntry; i++) {
-			tonga_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, i-1,
-				get_pcie_gen_support(data->pcie_gen_cap, pcie_table->entries[i].gen_speed),
-				get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-		}
-		data->dpm_table.pcie_speed_table.count = maxEntry - 1;
-	} else {
-		/* Hardcode Pcie Table */
-		tonga_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 0,
-			get_pcie_gen_support(data->pcie_gen_cap, PP_Min_PCIEGen),
-			get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-		tonga_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 1,
-			get_pcie_gen_support(data->pcie_gen_cap, PP_Min_PCIEGen),
-			get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-		tonga_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 2,
-			get_pcie_gen_support(data->pcie_gen_cap, PP_Max_PCIEGen),
-			get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-		tonga_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 3,
-			get_pcie_gen_support(data->pcie_gen_cap, PP_Max_PCIEGen),
-			get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-		tonga_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 4,
-			get_pcie_gen_support(data->pcie_gen_cap, PP_Max_PCIEGen),
-			get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-		tonga_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 5,
-			get_pcie_gen_support(data->pcie_gen_cap, PP_Max_PCIEGen),
-			get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-		data->dpm_table.pcie_speed_table.count = 6;
-	}
-	/* Populate last level for boot PCIE level, but do not increment count. */
-	tonga_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table,
-		data->dpm_table.pcie_speed_table.count,
-		get_pcie_gen_support(data->pcie_gen_cap, PP_Min_PCIEGen),
-		get_pcie_lane_support(data->pcie_lane_cap, PP_Max_PCIELane));
-
-	return 0;
-
-}
-
-/*
- * This function is to initalize all DPM state tables for SMU7 based on the dependency table.
- * Dynamic state patching function will then trim these state tables to the allowed range based
- * on the power policy or external client requests, such as UVD request, etc.
- */
-static int tonga_setup_default_dpm_tables(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint32_t i;
-
-	phm_ppt_v1_clock_voltage_dependency_table *allowed_vdd_sclk_table =
-		pptable_info->vdd_dep_on_sclk;
-	phm_ppt_v1_clock_voltage_dependency_table *allowed_vdd_mclk_table =
-		pptable_info->vdd_dep_on_mclk;
-
-	PP_ASSERT_WITH_CODE(allowed_vdd_sclk_table != NULL,
-		"SCLK dependency table is missing. This table is mandatory", return -1);
-	PP_ASSERT_WITH_CODE(allowed_vdd_sclk_table->count >= 1,
-		"SCLK dependency table has to have is missing. This table is mandatory", return -1);
-
-	PP_ASSERT_WITH_CODE(allowed_vdd_mclk_table != NULL,
-		"MCLK dependency table is missing. This table is mandatory", return -1);
-	PP_ASSERT_WITH_CODE(allowed_vdd_mclk_table->count >= 1,
-		"VMCLK dependency table has to have is missing. This table is mandatory", return -1);
-
-	/* clear the state table to reset everything to default */
-	memset(&(data->dpm_table), 0x00, sizeof(data->dpm_table));
-	tonga_reset_single_dpm_table(hwmgr, &data->dpm_table.sclk_table, SMU72_MAX_LEVELS_GRAPHICS);
-	tonga_reset_single_dpm_table(hwmgr, &data->dpm_table.mclk_table, SMU72_MAX_LEVELS_MEMORY);
-	/* tonga_reset_single_dpm_table(hwmgr, &tonga_hwmgr->dpm_table.VddcTable, SMU72_MAX_LEVELS_VDDC); */
-	/* tonga_reset_single_dpm_table(hwmgr, &tonga_hwmgr->dpm_table.vdd_gfx_table, SMU72_MAX_LEVELS_VDDGFX);*/
-	/* tonga_reset_single_dpm_table(hwmgr, &tonga_hwmgr->dpm_table.vdd_ci_table, SMU72_MAX_LEVELS_VDDCI);*/
-	/* tonga_reset_single_dpm_table(hwmgr, &tonga_hwmgr->dpm_table.mvdd_table, SMU72_MAX_LEVELS_MVDD);*/
-
-	PP_ASSERT_WITH_CODE(allowed_vdd_sclk_table != NULL,
-		"SCLK dependency table is missing. This table is mandatory", return -1);
-	/* Initialize Sclk DPM table based on allow Sclk values*/
-	data->dpm_table.sclk_table.count = 0;
-
-	for (i = 0; i < allowed_vdd_sclk_table->count; i++) {
-		if (i == 0 || data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count-1].value !=
-				allowed_vdd_sclk_table->entries[i].clk) {
-			data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].value =
-				allowed_vdd_sclk_table->entries[i].clk;
-			data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].enabled = true; /*(i==0) ? 1 : 0; to do */
-			data->dpm_table.sclk_table.count++;
-		}
-	}
-
-	PP_ASSERT_WITH_CODE(allowed_vdd_mclk_table != NULL,
-		"MCLK dependency table is missing. This table is mandatory", return -1);
-	/* Initialize Mclk DPM table based on allow Mclk values */
-	data->dpm_table.mclk_table.count = 0;
-	for (i = 0; i < allowed_vdd_mclk_table->count; i++) {
-		if (i == 0 || data->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count-1].value !=
-			allowed_vdd_mclk_table->entries[i].clk) {
-			data->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].value =
-				allowed_vdd_mclk_table->entries[i].clk;
-			data->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].enabled = true; /*(i==0) ? 1 : 0; */
-			data->dpm_table.mclk_table.count++;
-		}
-	}
-
-	/* setup PCIE gen speed levels*/
-	tonga_setup_default_pcie_tables(hwmgr);
-
-	/* save a copy of the default DPM table*/
-	memcpy(&(data->golden_dpm_table), &(data->dpm_table), sizeof(struct tonga_dpm_table));
-
-	return 0;
-}
-
-int tonga_populate_smc_initial_state(struct pp_hwmgr *hwmgr,
-		const struct tonga_power_state *bootState)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint8_t count, level;
-
-	count = (uint8_t) (pptable_info->vdd_dep_on_sclk->count);
-	for (level = 0; level < count; level++) {
-		if (pptable_info->vdd_dep_on_sclk->entries[level].clk >=
-			bootState->performance_levels[0].engine_clock) {
-			data->smc_state_table.GraphicsBootLevel = level;
-			break;
-		}
-	}
-
-	count = (uint8_t) (pptable_info->vdd_dep_on_mclk->count);
-	for (level = 0; level < count; level++) {
-		if (pptable_info->vdd_dep_on_mclk->entries[level].clk >=
-			bootState->performance_levels[0].memory_clock) {
-			data->smc_state_table.MemoryBootLevel = level;
-			break;
-		}
-	}
-
-	return 0;
-}
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static void tonga_save_default_power_profile(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct SMU72_Discrete_GraphicsLevel *levels =
-				data->smc_state_table.GraphicsLevel;
-	unsigned min_level = 1;
-
-	hwmgr->default_gfx_power_profile.activity_threshold =
-			be16_to_cpu(levels[0].ActivityLevel);
-	hwmgr->default_gfx_power_profile.up_hyst = levels[0].UpHyst;
-	hwmgr->default_gfx_power_profile.down_hyst = levels[0].DownHyst;
-	hwmgr->default_gfx_power_profile.type = PP_GFX_PROFILE;
-
-	hwmgr->default_compute_power_profile = hwmgr->default_gfx_power_profile;
-	hwmgr->default_compute_power_profile.type = PP_COMPUTE_PROFILE;
-
-	/* Workaround compute SDMA instability: disable lowest SCLK
-	 * DPM level. Optimize compute power profile: Use only highest
-	 * 2 power levels (if more than 2 are available), Hysteresis:
-	 * 0ms up, 5ms down
-	 */
-	if (data->smc_state_table.GraphicsDpmLevelCount > 2)
-		min_level = data->smc_state_table.GraphicsDpmLevelCount - 2;
-	else if (data->smc_state_table.GraphicsDpmLevelCount == 2)
-		min_level = 1;
-	else
-		min_level = 0;
-	hwmgr->default_compute_power_profile.min_sclk =
-			be32_to_cpu(levels[min_level].SclkFrequency);
-	hwmgr->default_compute_power_profile.up_hyst = 0;
-	hwmgr->default_compute_power_profile.down_hyst = 5;
-
-	hwmgr->gfx_power_profile = hwmgr->default_gfx_power_profile;
-	hwmgr->compute_power_profile = hwmgr->default_compute_power_profile;
-}
-#endif
-
-/**
- * Initializes the SMC table and uploads it
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @param    pInput  the pointer to input data (PowerState)
- * @return   always 0
- */
-static int tonga_init_smc_table(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	SMU72_Discrete_DpmTable  *table = &(data->smc_state_table);
-	const phw_tonga_ulv_parm *ulv = &(data->ulv);
-	uint8_t i;
-	PECI_RegistryValue reg_value;
-	pp_atomctrl_gpio_pin_assignment gpio_pin_assignment;
-
-	result = tonga_setup_default_dpm_tables(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to setup default DPM tables!", return result;);
-	memset(&(data->smc_state_table), 0x00, sizeof(data->smc_state_table));
-	if (TONGA_VOLTAGE_CONTROL_NONE != data->voltage_control) {
-		tonga_populate_smc_voltage_tables(hwmgr, table);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_AutomaticDCTransition)) {
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_GPIO_DC;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_StepVddc)) {
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_STEPVDDC;
-	}
-
-	if (data->is_memory_GDDR5) {
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_GDDR5;
-	}
-
-	i = PHM_READ_FIELD(hwmgr->device, CC_MC_MAX_CHANNEL, NOOFCHAN);
-
-	if (i == 1 || i == 0) {
-		table->SystemFlags |= PPSMC_SYSTEMFLAG_12CHANNEL;
-	}
-
-	if (ulv->ulv_supported && pptable_info->us_ulv_voltage_offset) {
-		PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to initialize ULV state!", return result;);
-
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCG_ULV_PARAMETER, ulv->ch_ulv_parameter);
-	}
-
-	result = tonga_populate_smc_link_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize Link Level!", return result;);
-
-	result = tonga_populate_all_graphic_levels(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize Graphics Level!", return result;);
-
-	result = tonga_populate_all_memory_levels(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize Memory Level!", return result;);
-
-	result = tonga_populate_smv_acpi_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize ACPI Level!", return result;);
-
-	result = tonga_populate_smc_vce_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize VCE Level!", return result;);
-
-	result = tonga_populate_smc_acp_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize ACP Level!", return result;);
-
-	result = tonga_populate_smc_samu_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize SAMU Level!", return result;);
-
-	/* Since only the initial state is completely set up at this point (the other states are just copies of the boot state) we only */
-	/* need to populate the  ARB settings for the initial state. */
-	result = tonga_program_memory_timing_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to Write ARB settings for the initial state.", return result;);
-
-	result = tonga_populate_smc_uvd_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize UVD Level!", return result;);
-
-	result = tonga_populate_smc_boot_level(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize Boot Level!", return result;);
-
-	result = tonga_populate_bapm_parameters_in_dpm_table(hwmgr);
-	PP_ASSERT_WITH_CODE(result == 0,
-			"Failed to populate BAPM Parameters!", return result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ClockStretcher)) {
-		result = tonga_populate_clock_stretcher_data_table(hwmgr);
-		PP_ASSERT_WITH_CODE(0 == result,
-			"Failed to populate Clock Stretcher Data Table!", return result;);
-	}
-	table->GraphicsVoltageChangeEnable  = 1;
-	table->GraphicsThermThrottleEnable  = 1;
-	table->GraphicsInterval = 1;
-	table->VoltageInterval  = 1;
-	table->ThermalInterval  = 1;
-	table->TemperatureLimitHigh =
-		pptable_info->cac_dtp_table->usTargetOperatingTemp *
-		TONGA_Q88_FORMAT_CONVERSION_UNIT;
-	table->TemperatureLimitLow =
-		(pptable_info->cac_dtp_table->usTargetOperatingTemp - 1) *
-		TONGA_Q88_FORMAT_CONVERSION_UNIT;
-	table->MemoryVoltageChangeEnable  = 1;
-	table->MemoryInterval  = 1;
-	table->VoltageResponseTime  = 0;
-	table->PhaseResponseTime  = 0;
-	table->MemoryThermThrottleEnable  = 1;
-
-	/*
-	* Cail reads current link status and reports it as cap (we cannot change this due to some previous issues we had)
-	* SMC drops the link status to lowest level after enabling DPM by PowerPlay. After pnp or toggling CF, driver gets reloaded again
-	* but this time Cail reads current link status which was set to low by SMC and reports it as cap to powerplay
-	* To avoid it, we set PCIeBootLinkLevel to highest dpm level
-	*/
-	PP_ASSERT_WITH_CODE((1 <= data->dpm_table.pcie_speed_table.count),
-			"There must be 1 or more PCIE levels defined in PPTable.",
-			return -1);
-
-	table->PCIeBootLinkLevel = (uint8_t) (data->dpm_table.pcie_speed_table.count);
-
-	table->PCIeGenInterval  = 1;
-
-	result = tonga_populate_vr_config(hwmgr, table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to populate VRConfig setting!", return result);
-
-	table->ThermGpio  = 17;
-	table->SclkStepSize = 0x4000;
-
-	reg_value = 0;
-	if ((0 == reg_value) &&
-		(atomctrl_get_pp_assign_pin(hwmgr, VDDC_VRHOT_GPIO_PINID,
-						&gpio_pin_assignment))) {
-		table->VRHotGpio = gpio_pin_assignment.uc_gpio_pin_bit_shift;
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_RegulatorHot);
-	} else {
-		table->VRHotGpio = TONGA_UNUSED_GPIO_PIN;
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_RegulatorHot);
-	}
-
-	/* ACDC Switch GPIO */
-	reg_value = 0;
-	if ((0 == reg_value) &&
-		(atomctrl_get_pp_assign_pin(hwmgr, PP_AC_DC_SWITCH_GPIO_PINID,
-						&gpio_pin_assignment))) {
-		table->AcDcGpio = gpio_pin_assignment.uc_gpio_pin_bit_shift;
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_AutomaticDCTransition);
-	} else {
-		table->AcDcGpio = TONGA_UNUSED_GPIO_PIN;
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_AutomaticDCTransition);
-	}
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_Falcon_QuickTransition);
-
-	reg_value = 0;
-	if (1 == reg_value) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_AutomaticDCTransition);
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_Falcon_QuickTransition);
-	}
-
-	reg_value = 0;
-	if ((0 == reg_value) && (atomctrl_get_pp_assign_pin(hwmgr,
-			THERMAL_INT_OUTPUT_GPIO_PINID, &gpio_pin_assignment))) {
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ThermalOutGPIO);
-
-		table->ThermOutGpio = gpio_pin_assignment.uc_gpio_pin_bit_shift;
-
-		table->ThermOutPolarity =
-			(0 == (cgs_read_register(hwmgr->device, mmGPIOPAD_A) &
-			(1 << gpio_pin_assignment.uc_gpio_pin_bit_shift))) ? 1:0;
-
-		table->ThermOutMode = SMU7_THERM_OUT_MODE_THERM_ONLY;
-
-		/* if required, combine VRHot/PCC with thermal out GPIO*/
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_RegulatorHot) &&
-			phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CombinePCCWithThermalSignal)){
-			table->ThermOutMode = SMU7_THERM_OUT_MODE_THERM_VRHOT;
-		}
-	} else {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ThermalOutGPIO);
-
-		table->ThermOutGpio = 17;
-		table->ThermOutPolarity = 1;
-		table->ThermOutMode = SMU7_THERM_OUT_MODE_DISABLE;
-	}
-
-	for (i = 0; i < SMU72_MAX_ENTRIES_SMIO; i++) {
-		table->Smio[i] = PP_HOST_TO_SMC_UL(table->Smio[i]);
-	}
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SystemFlags);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->VRConfig);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMask1);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SmioMask2);
-	CONVERT_FROM_HOST_TO_SMC_UL(table->SclkStepSize);
-	CONVERT_FROM_HOST_TO_SMC_US(table->TemperatureLimitHigh);
-	CONVERT_FROM_HOST_TO_SMC_US(table->TemperatureLimitLow);
-	CONVERT_FROM_HOST_TO_SMC_US(table->VoltageResponseTime);
-	CONVERT_FROM_HOST_TO_SMC_US(table->PhaseResponseTime);
-
-	/* Upload all dpm data to SMC memory.(dpm level, dpm level count etc) */
-	result = tonga_copy_bytes_to_smc(hwmgr->smumgr, data->dpm_table_start +
-										offsetof(SMU72_Discrete_DpmTable, SystemFlags),
-										(uint8_t *)&(table->SystemFlags),
-										sizeof(SMU72_Discrete_DpmTable)-3 * sizeof(SMU72_PIDController),
-										data->sram_end);
-
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to upload dpm data to SMC memory!", return result;);
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-	tonga_save_default_power_profile(hwmgr);
-#endif
-	return result;
-}
-
-/* Look up the voltaged based on DAL's requested level. and then send the requested VDDC voltage to SMC*/
-static void tonga_apply_dal_minimum_voltage_request(struct pp_hwmgr *hwmgr)
-{
-	return;
-}
-
-int tonga_upload_dpm_level_enable_mask(struct pp_hwmgr *hwmgr)
-{
-	PPSMC_Result result;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	/* Apply minimum voltage based on DAL's request level */
-	tonga_apply_dal_minimum_voltage_request(hwmgr);
-
-	if (0 == data->sclk_dpm_key_disabled) {
-		/* Checking if DPM is running.  If we discover hang because of this, we should skip this message.*/
-		if (tonga_is_dpm_running(hwmgr))
-			printk(KERN_ERR "[ powerplay ] Trying to set Enable Mask when DPM is disabled \n");
-
-		if (0 != data->dpm_level_enable_mask.sclk_dpm_enable_mask) {
-			result = smum_send_msg_to_smc_with_parameter(
-								hwmgr->smumgr,
-				(PPSMC_Msg)PPSMC_MSG_SCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.sclk_dpm_enable_mask);
-			PP_ASSERT_WITH_CODE((0 == result),
-				"Set Sclk Dpm enable Mask failed", return -1);
-		}
-	}
-
-	if (0 == data->mclk_dpm_key_disabled) {
-		/* Checking if DPM is running.  If we discover hang because of this, we should skip this message.*/
-		if (tonga_is_dpm_running(hwmgr))
-			printk(KERN_ERR "[ powerplay ] Trying to set Enable Mask when DPM is disabled \n");
-
-		if (0 != data->dpm_level_enable_mask.mclk_dpm_enable_mask) {
-			result = smum_send_msg_to_smc_with_parameter(
-								hwmgr->smumgr,
-				(PPSMC_Msg)PPSMC_MSG_MCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.mclk_dpm_enable_mask);
-			PP_ASSERT_WITH_CODE((0 == result),
-				"Set Mclk Dpm enable Mask failed", return -1);
-		}
-	}
-
-	return 0;
-}
-
-
-int tonga_force_dpm_highest(struct pp_hwmgr *hwmgr)
-{
-	uint32_t level, tmp;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->pcie_dpm_key_disabled) {
-		/* PCIE */
-		if (data->dpm_level_enable_mask.pcie_dpm_enable_mask != 0) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.pcie_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++ ;
-
-			if (0 != level) {
-				PP_ASSERT_WITH_CODE((0 == tonga_dpm_force_state_pcie(hwmgr, level)),
-					"force highest pcie dpm state failed!", return -1);
-			}
-		}
-	}
-
-	if (0 == data->sclk_dpm_key_disabled) {
-		/* SCLK */
-		if (data->dpm_level_enable_mask.sclk_dpm_enable_mask != 0) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.sclk_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++ ;
-
-			if (0 != level) {
-				PP_ASSERT_WITH_CODE((0 == tonga_dpm_force_state(hwmgr, level)),
-					"force highest sclk dpm state failed!", return -1);
-				if (PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device,
-					CGS_IND_REG__SMC, TARGET_AND_CURRENT_PROFILE_INDEX, CURR_SCLK_INDEX) != level)
-					printk(KERN_ERR "[ powerplay ] Target_and_current_Profile_Index. \
-						Curr_Sclk_Index does not match the level \n");
-
-			}
-		}
-	}
-
-	if (0 == data->mclk_dpm_key_disabled) {
-		/* MCLK */
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask != 0) {
-			level = 0;
-			tmp = data->dpm_level_enable_mask.mclk_dpm_enable_mask;
-			while (tmp >>= 1)
-				level++ ;
-
-			if (0 != level) {
-				PP_ASSERT_WITH_CODE((0 == tonga_dpm_force_state_mclk(hwmgr, level)),
-					"force highest mclk dpm state failed!", return -1);
-				if (PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-					TARGET_AND_CURRENT_PROFILE_INDEX, CURR_MCLK_INDEX) != level)
-					printk(KERN_ERR "[ powerplay ] Target_and_current_Profile_Index. \
-						Curr_Mclk_Index does not match the level \n");
-			}
-		}
-	}
-
-	return 0;
-}
-
-/**
- * Find the MC microcode version and store it in the HwMgr struct
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_get_mc_microcode_version (struct pp_hwmgr *hwmgr)
-{
-	cgs_write_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_INDEX, 0x9F);
-
-	hwmgr->microcode_version_info.MC = cgs_read_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_DATA);
-
-	return 0;
-}
-
-/**
- * Initialize Dynamic State Adjustment Rule Settings
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- */
-int tonga_initializa_dynamic_state_adjustment_rule_settings(struct pp_hwmgr *hwmgr)
-{
-	uint32_t table_size;
-	struct phm_clock_voltage_dependency_table *table_clk_vlt;
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	hwmgr->dyn_state.mclk_sclk_ratio = 4;
-	hwmgr->dyn_state.sclk_mclk_delta = 15000;      /* 150 MHz */
-	hwmgr->dyn_state.vddc_vddci_delta = 200;       /* 200mV */
-
-	/* initialize vddc_dep_on_dal_pwrl table */
-	table_size = sizeof(uint32_t) + 4 * sizeof(struct phm_clock_voltage_dependency_record);
-	table_clk_vlt = kzalloc(table_size, GFP_KERNEL);
-
-	if (NULL == table_clk_vlt) {
-		printk(KERN_ERR "[ powerplay ] Can not allocate space for vddc_dep_on_dal_pwrl! \n");
-		return -ENOMEM;
-	} else {
-		table_clk_vlt->count = 4;
-		table_clk_vlt->entries[0].clk = PP_DAL_POWERLEVEL_ULTRALOW;
-		table_clk_vlt->entries[0].v = 0;
-		table_clk_vlt->entries[1].clk = PP_DAL_POWERLEVEL_LOW;
-		table_clk_vlt->entries[1].v = 720;
-		table_clk_vlt->entries[2].clk = PP_DAL_POWERLEVEL_NOMINAL;
-		table_clk_vlt->entries[2].v = 810;
-		table_clk_vlt->entries[3].clk = PP_DAL_POWERLEVEL_PERFORMANCE;
-		table_clk_vlt->entries[3].v = 900;
-		pptable_info->vddc_dep_on_dal_pwrl = table_clk_vlt;
-		hwmgr->dyn_state.vddc_dep_on_dal_pwrl = table_clk_vlt;
-	}
-
-	return 0;
-}
-
-static int tonga_set_private_var_based_on_pptale(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	phm_ppt_v1_clock_voltage_dependency_table *allowed_sclk_vdd_table =
-		pptable_info->vdd_dep_on_sclk;
-	phm_ppt_v1_clock_voltage_dependency_table *allowed_mclk_vdd_table =
-		pptable_info->vdd_dep_on_mclk;
-
-	PP_ASSERT_WITH_CODE(allowed_sclk_vdd_table != NULL,
-		"VDD dependency on SCLK table is missing.	\
-		This table is mandatory", return -1);
-	PP_ASSERT_WITH_CODE(allowed_sclk_vdd_table->count >= 1,
-		"VDD dependency on SCLK table has to have is missing.	\
-		This table is mandatory", return -1);
-
-	PP_ASSERT_WITH_CODE(allowed_mclk_vdd_table != NULL,
-		"VDD dependency on MCLK table is missing.	\
-		This table is mandatory", return -1);
-	PP_ASSERT_WITH_CODE(allowed_mclk_vdd_table->count >= 1,
-		"VDD dependency on MCLK table has to have is missing.	 \
-		This table is mandatory", return -1);
-
-	data->min_vddc_in_pp_table = (uint16_t)allowed_sclk_vdd_table->entries[0].vddc;
-	data->max_vddc_in_pp_table = (uint16_t)allowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].vddc;
-
-	pptable_info->max_clock_voltage_on_ac.sclk =
-		allowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].clk;
-	pptable_info->max_clock_voltage_on_ac.mclk =
-		allowed_mclk_vdd_table->entries[allowed_mclk_vdd_table->count - 1].clk;
-	pptable_info->max_clock_voltage_on_ac.vddc =
-		allowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].vddc;
-	pptable_info->max_clock_voltage_on_ac.vddci =
-		allowed_mclk_vdd_table->entries[allowed_mclk_vdd_table->count - 1].vddci;
-
-	hwmgr->dyn_state.max_clock_voltage_on_ac.sclk =
-		pptable_info->max_clock_voltage_on_ac.sclk;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.mclk =
-		pptable_info->max_clock_voltage_on_ac.mclk;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.vddc =
-		pptable_info->max_clock_voltage_on_ac.vddc;
-	hwmgr->dyn_state.max_clock_voltage_on_ac.vddci =
-		pptable_info->max_clock_voltage_on_ac.vddci;
-
-	return 0;
-}
-
-int tonga_unforce_dpm_levels(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	int result = 1;
-
-	PP_ASSERT_WITH_CODE (!tonga_is_dpm_running(hwmgr),
-			     "Trying to Unforce DPM when DPM is disabled. Returning without sending SMC message.",
-			     return result);
-
-	if (0 == data->pcie_dpm_key_disabled) {
-		PP_ASSERT_WITH_CODE((0 == smum_send_msg_to_smc(
-							     hwmgr->smumgr,
-					PPSMC_MSG_PCIeDPM_UnForceLevel)),
-					   "unforce pcie level failed!",
-								return -1);
-	}
-
-	result = tonga_upload_dpm_level_enable_mask(hwmgr);
-
-	return result;
-}
-
-static uint32_t tonga_get_lowest_enable_level(
-				struct pp_hwmgr *hwmgr, uint32_t level_mask)
-{
-	uint32_t level = 0;
-
-	while (0 == (level_mask & (1 << level)))
-		level++;
-
-	return level;
-}
-
-static int tonga_force_dpm_lowest(struct pp_hwmgr *hwmgr)
-{
-	uint32_t level;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->pcie_dpm_key_disabled) {
-		/* PCIE */
-		if (data->dpm_level_enable_mask.pcie_dpm_enable_mask != 0) {
-			level = tonga_get_lowest_enable_level(hwmgr,
-							      data->dpm_level_enable_mask.pcie_dpm_enable_mask);
-			PP_ASSERT_WITH_CODE((0 == tonga_dpm_force_state_pcie(hwmgr, level)),
-					    "force lowest pcie dpm state failed!", return -1);
-		}
-	}
-
-	if (0 == data->sclk_dpm_key_disabled) {
-		/* SCLK */
-		if (0 != data->dpm_level_enable_mask.sclk_dpm_enable_mask) {
-			level = tonga_get_lowest_enable_level(hwmgr,
-							      data->dpm_level_enable_mask.sclk_dpm_enable_mask);
-
-			PP_ASSERT_WITH_CODE((0 == tonga_dpm_force_state(hwmgr, level)),
-					    "force sclk dpm state failed!", return -1);
-
-			if (PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device,
-							 CGS_IND_REG__SMC, TARGET_AND_CURRENT_PROFILE_INDEX, CURR_SCLK_INDEX) != level)
-				printk(KERN_ERR "[ powerplay ] Target_and_current_Profile_Index.	\
-				Curr_Sclk_Index does not match the level \n");
-		}
-	}
-
-	if (0 == data->mclk_dpm_key_disabled) {
-		/* MCLK */
-		if (data->dpm_level_enable_mask.mclk_dpm_enable_mask != 0) {
-			level = tonga_get_lowest_enable_level(hwmgr,
-							      data->dpm_level_enable_mask.mclk_dpm_enable_mask);
-			PP_ASSERT_WITH_CODE((0 == tonga_dpm_force_state_mclk(hwmgr, level)),
-					    "force lowest mclk dpm state failed!", return -1);
-			if (PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-							 TARGET_AND_CURRENT_PROFILE_INDEX, CURR_MCLK_INDEX) != level)
-				printk(KERN_ERR "[ powerplay ] Target_and_current_Profile_Index. \
-						Curr_Mclk_Index does not match the level \n");
-		}
-	}
-
-	return 0;
-}
-
-static int tonga_patch_voltage_dependency_tables_with_lookup_table(struct pp_hwmgr *hwmgr)
-{
-	uint8_t entryId;
-	uint8_t voltageId;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	phm_ppt_v1_clock_voltage_dependency_table *sclk_table = pptable_info->vdd_dep_on_sclk;
-	phm_ppt_v1_clock_voltage_dependency_table *mclk_table = pptable_info->vdd_dep_on_mclk;
-	phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table = pptable_info->mm_dep_table;
-
-	if (data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2) {
-		for (entryId = 0; entryId < sclk_table->count; ++entryId) {
-			voltageId = sclk_table->entries[entryId].vddInd;
-			sclk_table->entries[entryId].vddgfx =
-				pptable_info->vddgfx_lookup_table->entries[voltageId].us_vdd;
-		}
-	} else {
-		for (entryId = 0; entryId < sclk_table->count; ++entryId) {
-			voltageId = sclk_table->entries[entryId].vddInd;
-			sclk_table->entries[entryId].vddc =
-				pptable_info->vddc_lookup_table->entries[voltageId].us_vdd;
-		}
-	}
-
-	for (entryId = 0; entryId < mclk_table->count; ++entryId) {
-		voltageId = mclk_table->entries[entryId].vddInd;
-		mclk_table->entries[entryId].vddc =
-			pptable_info->vddc_lookup_table->entries[voltageId].us_vdd;
-	}
-
-	for (entryId = 0; entryId < mm_table->count; ++entryId) {
-		voltageId = mm_table->entries[entryId].vddcInd;
-		mm_table->entries[entryId].vddc =
-			pptable_info->vddc_lookup_table->entries[voltageId].us_vdd;
-	}
-
-	return 0;
-
-}
-
-static int tonga_calc_voltage_dependency_tables(struct pp_hwmgr *hwmgr)
-{
-	uint8_t entryId;
-	phm_ppt_v1_voltage_lookup_record v_record;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	phm_ppt_v1_clock_voltage_dependency_table *sclk_table = pptable_info->vdd_dep_on_sclk;
-	phm_ppt_v1_clock_voltage_dependency_table *mclk_table = pptable_info->vdd_dep_on_mclk;
-
-	if (data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2) {
-		for (entryId = 0; entryId < sclk_table->count; ++entryId) {
-			if (sclk_table->entries[entryId].vdd_offset & (1 << 15))
-				v_record.us_vdd = sclk_table->entries[entryId].vddgfx +
-					sclk_table->entries[entryId].vdd_offset - 0xFFFF;
-			else
-				v_record.us_vdd = sclk_table->entries[entryId].vddgfx +
-					sclk_table->entries[entryId].vdd_offset;
-
-			sclk_table->entries[entryId].vddc =
-				v_record.us_cac_low = v_record.us_cac_mid =
-				v_record.us_cac_high = v_record.us_vdd;
-
-			tonga_add_voltage(hwmgr, pptable_info->vddc_lookup_table, &v_record);
-		}
-
-		for (entryId = 0; entryId < mclk_table->count; ++entryId) {
-			if (mclk_table->entries[entryId].vdd_offset & (1 << 15))
-				v_record.us_vdd = mclk_table->entries[entryId].vddc +
-					mclk_table->entries[entryId].vdd_offset - 0xFFFF;
-			else
-				v_record.us_vdd = mclk_table->entries[entryId].vddc +
-					mclk_table->entries[entryId].vdd_offset;
-
-			mclk_table->entries[entryId].vddgfx = v_record.us_cac_low =
-				v_record.us_cac_mid = v_record.us_cac_high = v_record.us_vdd;
-			tonga_add_voltage(hwmgr, pptable_info->vddgfx_lookup_table, &v_record);
-		}
-	}
-
-	return 0;
-
-}
-
-static int tonga_calc_mm_voltage_dependency_table(struct pp_hwmgr *hwmgr)
-{
-	uint32_t entryId;
-	phm_ppt_v1_voltage_lookup_record v_record;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table = pptable_info->mm_dep_table;
-
-	if (data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2) {
-		for (entryId = 0; entryId < mm_table->count; entryId++) {
-			if (mm_table->entries[entryId].vddgfx_offset & (1 << 15))
-				v_record.us_vdd = mm_table->entries[entryId].vddc +
-					mm_table->entries[entryId].vddgfx_offset - 0xFFFF;
-			else
-				v_record.us_vdd = mm_table->entries[entryId].vddc +
-					mm_table->entries[entryId].vddgfx_offset;
-
-			/* Add the calculated VDDGFX to the VDDGFX lookup table */
-			mm_table->entries[entryId].vddgfx = v_record.us_cac_low =
-				v_record.us_cac_mid = v_record.us_cac_high = v_record.us_vdd;
-			tonga_add_voltage(hwmgr, pptable_info->vddgfx_lookup_table, &v_record);
-		}
-	}
-	return 0;
-}
-
-
-/**
- * Change virtual leakage voltage to actual value.
- *
- * @param     hwmgr  the address of the powerplay hardware manager.
- * @param     pointer to changing voltage
- * @param     pointer to leakage table
- */
-static void tonga_patch_with_vdd_leakage(struct pp_hwmgr *hwmgr,
-		uint16_t *voltage, phw_tonga_leakage_voltage *pLeakageTable)
-{
-	uint32_t leakage_index;
-
-	/* search for leakage voltage ID 0xff01 ~ 0xff08 */
-	for (leakage_index = 0; leakage_index < pLeakageTable->count; leakage_index++) {
-		/* if this voltage matches a leakage voltage ID */
-		/* patch with actual leakage voltage */
-		if (pLeakageTable->leakage_id[leakage_index] == *voltage) {
-			*voltage = pLeakageTable->actual_voltage[leakage_index];
-			break;
-		}
-	}
-
-	if (*voltage > ATOM_VIRTUAL_VOLTAGE_ID0)
-		printk(KERN_ERR "[ powerplay ] Voltage value looks like a Leakage ID but it's not patched \n");
-}
-
-/**
- * Patch voltage lookup table by EVV leakages.
- *
- * @param     hwmgr  the address of the powerplay hardware manager.
- * @param     pointer to voltage lookup table
- * @param     pointer to leakage table
- * @return     always 0
- */
-static int tonga_patch_lookup_table_with_leakage(struct pp_hwmgr *hwmgr,
-		phm_ppt_v1_voltage_lookup_table *lookup_table,
-		phw_tonga_leakage_voltage *pLeakageTable)
-{
-	uint32_t i;
-
-	for (i = 0; i < lookup_table->count; i++) {
-		tonga_patch_with_vdd_leakage(hwmgr,
-			&lookup_table->entries[i].us_vdd, pLeakageTable);
-	}
-
-	return 0;
-}
-
-static int tonga_patch_clock_voltage_lomits_with_vddc_leakage(struct pp_hwmgr *hwmgr,
-		phw_tonga_leakage_voltage *pLeakageTable, uint16_t *Vddc)
-{
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	tonga_patch_with_vdd_leakage(hwmgr, (uint16_t *)Vddc, pLeakageTable);
-	hwmgr->dyn_state.max_clock_voltage_on_dc.vddc =
-		pptable_info->max_clock_voltage_on_dc.vddc;
-
-	return 0;
-}
-
-static int tonga_patch_clock_voltage_limits_with_vddgfx_leakage(
-		struct pp_hwmgr *hwmgr, phw_tonga_leakage_voltage *pLeakageTable,
-		uint16_t *Vddgfx)
-{
-	tonga_patch_with_vdd_leakage(hwmgr, (uint16_t *)Vddgfx, pLeakageTable);
-	return 0;
-}
-
-int tonga_sort_lookup_table(struct pp_hwmgr *hwmgr,
-		phm_ppt_v1_voltage_lookup_table *lookup_table)
-{
-	uint32_t table_size, i, j;
-	phm_ppt_v1_voltage_lookup_record tmp_voltage_lookup_record;
-	table_size = lookup_table->count;
-
-	PP_ASSERT_WITH_CODE(0 != lookup_table->count,
-		"Lookup table is empty", return -1);
-
-	/* Sorting voltages */
-	for (i = 0; i < table_size - 1; i++) {
-		for (j = i + 1; j > 0; j--) {
-			if (lookup_table->entries[j].us_vdd < lookup_table->entries[j-1].us_vdd) {
-				tmp_voltage_lookup_record = lookup_table->entries[j-1];
-				lookup_table->entries[j-1] = lookup_table->entries[j];
-				lookup_table->entries[j] = tmp_voltage_lookup_record;
-			}
-		}
-	}
-
-	return 0;
-}
-
-static int tonga_complete_dependency_tables(struct pp_hwmgr *hwmgr)
-{
-	int result = 0;
-	int tmp_result;
-	tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (data->vdd_gfx_control == TONGA_VOLTAGE_CONTROL_BY_SVID2) {
-		tmp_result = tonga_patch_lookup_table_with_leakage(hwmgr,
-			pptable_info->vddgfx_lookup_table, &(data->vddcgfx_leakage));
-		if (tmp_result != 0)
-			result = tmp_result;
-
-		tmp_result = tonga_patch_clock_voltage_limits_with_vddgfx_leakage(hwmgr,
-			&(data->vddcgfx_leakage), &pptable_info->max_clock_voltage_on_dc.vddgfx);
-		if (tmp_result != 0)
-			result = tmp_result;
-	} else {
-		tmp_result = tonga_patch_lookup_table_with_leakage(hwmgr,
-			pptable_info->vddc_lookup_table, &(data->vddc_leakage));
-		if (tmp_result != 0)
-			result = tmp_result;
-
-		tmp_result = tonga_patch_clock_voltage_lomits_with_vddc_leakage(hwmgr,
-			&(data->vddc_leakage), &pptable_info->max_clock_voltage_on_dc.vddc);
-		if (tmp_result != 0)
-			result = tmp_result;
-	}
-
-	tmp_result = tonga_patch_voltage_dependency_tables_with_lookup_table(hwmgr);
-	if (tmp_result != 0)
-		result = tmp_result;
-
-	tmp_result = tonga_calc_voltage_dependency_tables(hwmgr);
-	if (tmp_result != 0)
-		result = tmp_result;
-
-	tmp_result = tonga_calc_mm_voltage_dependency_table(hwmgr);
-	if (tmp_result != 0)
-		result = tmp_result;
-
-	tmp_result = tonga_sort_lookup_table(hwmgr, pptable_info->vddgfx_lookup_table);
-	if (tmp_result != 0)
-		result = tmp_result;
-
-	tmp_result = tonga_sort_lookup_table(hwmgr, pptable_info->vddc_lookup_table);
-	if (tmp_result != 0)
-		result = tmp_result;
-
-	return result;
-}
-
-int tonga_init_sclk_threshold(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	data->low_sclk_interrupt_threshold = 0;
-
-	return 0;
-}
-
-int tonga_setup_asic_task(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = tonga_read_clock_registers(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to read clock registers!", result = tmp_result);
-
-	tmp_result = tonga_get_memory_type(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to get memory type!", result = tmp_result);
-
-	tmp_result = tonga_enable_acpi_power_management(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to enable ACPI power management!", result = tmp_result);
-
-	tmp_result = tonga_init_power_gate_state(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to init power gate state!", result = tmp_result);
-
-	tmp_result = tonga_get_mc_microcode_version(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to get MC microcode version!", result = tmp_result);
-
-	tmp_result = tonga_init_sclk_threshold(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to init sclk threshold!", result = tmp_result);
-
-	return result;
-}
-
-/**
- * Enable voltage control
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_enable_voltage_control(struct pp_hwmgr *hwmgr)
-{
-	/* enable voltage control */
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT, VOLT_PWRMGT_EN, 1);
-
-	return 0;
-}
-
-/**
- * Checks if we want to support voltage control
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- */
-bool cf_tonga_voltage_control(const struct pp_hwmgr *hwmgr)
-{
-	const struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	return(TONGA_VOLTAGE_CONTROL_NONE != data->voltage_control);
-}
-
-/*---------------------------MC----------------------------*/
-
-uint8_t tonga_get_memory_modile_index(struct pp_hwmgr *hwmgr)
-{
-	return (uint8_t) (0xFF & (cgs_read_register(hwmgr->device, mmBIOS_SCRATCH_4) >> 16));
-}
-
-bool tonga_check_s0_mc_reg_index(uint16_t inReg, uint16_t *outReg)
-{
-	bool result = true;
-
-	switch (inReg) {
-	case  mmMC_SEQ_RAS_TIMING:
-		*outReg = mmMC_SEQ_RAS_TIMING_LP;
-		break;
-
-	case  mmMC_SEQ_DLL_STBY:
-		*outReg = mmMC_SEQ_DLL_STBY_LP;
-		break;
-
-	case  mmMC_SEQ_G5PDX_CMD0:
-		*outReg = mmMC_SEQ_G5PDX_CMD0_LP;
-		break;
-
-	case  mmMC_SEQ_G5PDX_CMD1:
-		*outReg = mmMC_SEQ_G5PDX_CMD1_LP;
-		break;
-
-	case  mmMC_SEQ_G5PDX_CTRL:
-		*outReg = mmMC_SEQ_G5PDX_CTRL_LP;
-		break;
-
-	case mmMC_SEQ_CAS_TIMING:
-		*outReg = mmMC_SEQ_CAS_TIMING_LP;
-		break;
-
-	case mmMC_SEQ_MISC_TIMING:
-		*outReg = mmMC_SEQ_MISC_TIMING_LP;
-		break;
-
-	case mmMC_SEQ_MISC_TIMING2:
-		*outReg = mmMC_SEQ_MISC_TIMING2_LP;
-		break;
-
-	case mmMC_SEQ_PMG_DVS_CMD:
-		*outReg = mmMC_SEQ_PMG_DVS_CMD_LP;
-		break;
-
-	case mmMC_SEQ_PMG_DVS_CTL:
-		*outReg = mmMC_SEQ_PMG_DVS_CTL_LP;
-		break;
-
-	case mmMC_SEQ_RD_CTL_D0:
-		*outReg = mmMC_SEQ_RD_CTL_D0_LP;
-		break;
-
-	case mmMC_SEQ_RD_CTL_D1:
-		*outReg = mmMC_SEQ_RD_CTL_D1_LP;
-		break;
-
-	case mmMC_SEQ_WR_CTL_D0:
-		*outReg = mmMC_SEQ_WR_CTL_D0_LP;
-		break;
-
-	case mmMC_SEQ_WR_CTL_D1:
-		*outReg = mmMC_SEQ_WR_CTL_D1_LP;
-		break;
-
-	case mmMC_PMG_CMD_EMRS:
-		*outReg = mmMC_SEQ_PMG_CMD_EMRS_LP;
-		break;
-
-	case mmMC_PMG_CMD_MRS:
-		*outReg = mmMC_SEQ_PMG_CMD_MRS_LP;
-		break;
-
-	case mmMC_PMG_CMD_MRS1:
-		*outReg = mmMC_SEQ_PMG_CMD_MRS1_LP;
-		break;
-
-	case mmMC_SEQ_PMG_TIMING:
-		*outReg = mmMC_SEQ_PMG_TIMING_LP;
-		break;
-
-	case mmMC_PMG_CMD_MRS2:
-		*outReg = mmMC_SEQ_PMG_CMD_MRS2_LP;
-		break;
-
-	case mmMC_SEQ_WR_CTL_2:
-		*outReg = mmMC_SEQ_WR_CTL_2_LP;
-		break;
-
-	default:
-		result = false;
-		break;
-	}
-
-	return result;
-}
-
-int tonga_set_s0_mc_reg_index(phw_tonga_mc_reg_table *table)
-{
-	uint32_t i;
-	uint16_t address;
-
-	for (i = 0; i < table->last; i++) {
-		table->mc_reg_address[i].s0 =
-			tonga_check_s0_mc_reg_index(table->mc_reg_address[i].s1, &address)
-			? address : table->mc_reg_address[i].s1;
-	}
-	return 0;
-}
-
-int tonga_copy_vbios_smc_reg_table(const pp_atomctrl_mc_reg_table *table, phw_tonga_mc_reg_table *ni_table)
-{
-	uint8_t i, j;
-
-	PP_ASSERT_WITH_CODE((table->last <= SMU72_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-		"Invalid VramInfo table.", return -1);
-	PP_ASSERT_WITH_CODE((table->num_entries <= MAX_AC_TIMING_ENTRIES),
-		"Invalid VramInfo table.", return -1);
-
-	for (i = 0; i < table->last; i++) {
-		ni_table->mc_reg_address[i].s1 = table->mc_reg_address[i].s1;
-	}
-	ni_table->last = table->last;
-
-	for (i = 0; i < table->num_entries; i++) {
-		ni_table->mc_reg_table_entry[i].mclk_max =
-			table->mc_reg_table_entry[i].mclk_max;
-		for (j = 0; j < table->last; j++) {
-			ni_table->mc_reg_table_entry[i].mc_data[j] =
-				table->mc_reg_table_entry[i].mc_data[j];
-		}
-	}
-
-	ni_table->num_entries = table->num_entries;
-
-	return 0;
-}
-
-/**
- * VBIOS omits some information to reduce size, we need to recover them here.
- * 1.   when we see mmMC_SEQ_MISC1, bit[31:16] EMRS1, need to be write to  mmMC_PMG_CMD_EMRS /_LP[15:0].
- *      Bit[15:0] MRS, need to be update mmMC_PMG_CMD_MRS/_LP[15:0]
- * 2.   when we see mmMC_SEQ_RESERVE_M, bit[15:0] EMRS2, need to be write to mmMC_PMG_CMD_MRS1/_LP[15:0].
- * 3.   need to set these data for each clock range
- *
- * @param    hwmgr the address of the powerplay hardware manager.
- * @param    table the address of MCRegTable
- * @return   always 0
- */
-int tonga_set_mc_special_registers(struct pp_hwmgr *hwmgr, phw_tonga_mc_reg_table *table)
-{
-	uint8_t i, j, k;
-	uint32_t temp_reg;
-	const tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	for (i = 0, j = table->last; i < table->last; i++) {
-		PP_ASSERT_WITH_CODE((j < SMU72_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-			"Invalid VramInfo table.", return -1);
-		switch (table->mc_reg_address[i].s1) {
-		/*
-		* mmMC_SEQ_MISC1, bit[31:16] EMRS1, need to be write to  mmMC_PMG_CMD_EMRS /_LP[15:0].
-		* Bit[15:0] MRS, need to be update mmMC_PMG_CMD_MRS/_LP[15:0]
-		*/
-		case mmMC_SEQ_MISC1:
-			temp_reg = cgs_read_register(hwmgr->device, mmMC_PMG_CMD_EMRS);
-			table->mc_reg_address[j].s1 = mmMC_PMG_CMD_EMRS;
-			table->mc_reg_address[j].s0 = mmMC_SEQ_PMG_CMD_EMRS_LP;
-			for (k = 0; k < table->num_entries; k++) {
-				table->mc_reg_table_entry[k].mc_data[j] =
-					((temp_reg & 0xffff0000)) |
-					((table->mc_reg_table_entry[k].mc_data[i] & 0xffff0000) >> 16);
-			}
-			j++;
-			PP_ASSERT_WITH_CODE((j < SMU72_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-				"Invalid VramInfo table.", return -1);
-
-			temp_reg = cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS);
-			table->mc_reg_address[j].s1 = mmMC_PMG_CMD_MRS;
-			table->mc_reg_address[j].s0 = mmMC_SEQ_PMG_CMD_MRS_LP;
-			for (k = 0; k < table->num_entries; k++) {
-				table->mc_reg_table_entry[k].mc_data[j] =
-					(temp_reg & 0xffff0000) |
-					(table->mc_reg_table_entry[k].mc_data[i] & 0x0000ffff);
-
-				if (!data->is_memory_GDDR5) {
-					table->mc_reg_table_entry[k].mc_data[j] |= 0x100;
-				}
-			}
-			j++;
-			PP_ASSERT_WITH_CODE((j <= SMU72_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-				"Invalid VramInfo table.", return -1);
-
-			if (!data->is_memory_GDDR5) {
-				table->mc_reg_address[j].s1 = mmMC_PMG_AUTO_CMD;
-				table->mc_reg_address[j].s0 = mmMC_PMG_AUTO_CMD;
-				for (k = 0; k < table->num_entries; k++) {
-					table->mc_reg_table_entry[k].mc_data[j] =
-						(table->mc_reg_table_entry[k].mc_data[i] & 0xffff0000) >> 16;
-				}
-				j++;
-				PP_ASSERT_WITH_CODE((j <= SMU72_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-					"Invalid VramInfo table.", return -1);
-			}
-
-			break;
-
-		case mmMC_SEQ_RESERVE_M:
-			temp_reg = cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS1);
-			table->mc_reg_address[j].s1 = mmMC_PMG_CMD_MRS1;
-			table->mc_reg_address[j].s0 = mmMC_SEQ_PMG_CMD_MRS1_LP;
-			for (k = 0; k < table->num_entries; k++) {
-				table->mc_reg_table_entry[k].mc_data[j] =
-					(temp_reg & 0xffff0000) |
-					(table->mc_reg_table_entry[k].mc_data[i] & 0x0000ffff);
-			}
-			j++;
-			PP_ASSERT_WITH_CODE((j <= SMU72_DISCRETE_MC_REGISTER_ARRAY_SIZE),
-				"Invalid VramInfo table.", return -1);
-			break;
-
-		default:
-			break;
-		}
-
-	}
-
-	table->last = j;
-
-	return 0;
-}
-
-int tonga_set_valid_flag(phw_tonga_mc_reg_table *table)
-{
-	uint8_t i, j;
-	for (i = 0; i < table->last; i++) {
-		for (j = 1; j < table->num_entries; j++) {
-			if (table->mc_reg_table_entry[j-1].mc_data[i] !=
-				table->mc_reg_table_entry[j].mc_data[i]) {
-				table->validflag |= (1<<i);
-				break;
-			}
-		}
-	}
-
-	return 0;
-}
-
-static int tonga_initialize_mc_reg_table(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-	pp_atomctrl_mc_reg_table *table;
-	phw_tonga_mc_reg_table *ni_table = &data->tonga_mc_reg_table;
-	uint8_t module_index = tonga_get_memory_modile_index(hwmgr);
-
-	table = kzalloc(sizeof(pp_atomctrl_mc_reg_table), GFP_KERNEL);
-
-	if (NULL == table)
-		return -ENOMEM;
-
-	/* Program additional LP registers that are no longer programmed by VBIOS */
-	cgs_write_register(hwmgr->device, mmMC_SEQ_RAS_TIMING_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_RAS_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_CAS_TIMING_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_CAS_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_DLL_STBY_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_DLL_STBY));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_G5PDX_CMD0_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_G5PDX_CMD0));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_G5PDX_CMD1_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_G5PDX_CMD1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_G5PDX_CTRL_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_G5PDX_CTRL));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_DVS_CMD_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_PMG_DVS_CMD));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_DVS_CTL_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_PMG_DVS_CTL));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_MISC_TIMING_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_MISC_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_MISC_TIMING2_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_MISC_TIMING2));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_CMD_EMRS_LP, cgs_read_register(hwmgr->device, mmMC_PMG_CMD_EMRS));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_CMD_MRS_LP, cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_CMD_MRS1_LP, cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_WR_CTL_D0_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_WR_CTL_D0));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_WR_CTL_D1_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_WR_CTL_D1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_RD_CTL_D0_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_RD_CTL_D0));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_RD_CTL_D1_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_RD_CTL_D1));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_TIMING_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_PMG_TIMING));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_PMG_CMD_MRS2_LP, cgs_read_register(hwmgr->device, mmMC_PMG_CMD_MRS2));
-	cgs_write_register(hwmgr->device, mmMC_SEQ_WR_CTL_2_LP, cgs_read_register(hwmgr->device, mmMC_SEQ_WR_CTL_2));
-
-	memset(table, 0x00, sizeof(pp_atomctrl_mc_reg_table));
-
-	result = atomctrl_initialize_mc_reg_table(hwmgr, module_index, table);
-
-	if (0 == result)
-		result = tonga_copy_vbios_smc_reg_table(table, ni_table);
-
-	if (0 == result) {
-		tonga_set_s0_mc_reg_index(ni_table);
-		result = tonga_set_mc_special_registers(hwmgr, ni_table);
-	}
-
-	if (0 == result)
-		tonga_set_valid_flag(ni_table);
-
-	kfree(table);
-	return result;
-}
-
-/*
-* Copy one arb setting to another and then switch the active set.
-* arbFreqSrc and arbFreqDest is one of the MC_CG_ARB_FREQ_Fx constants.
-*/
-int tonga_copy_and_switch_arb_sets(struct pp_hwmgr *hwmgr,
-		uint32_t arbFreqSrc, uint32_t arbFreqDest)
-{
-	uint32_t mc_arb_dram_timing;
-	uint32_t mc_arb_dram_timing2;
-	uint32_t burst_time;
-	uint32_t mc_cg_config;
-
-	switch (arbFreqSrc) {
-	case MC_CG_ARB_FREQ_F0:
-		mc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING);
-		mc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2);
-		burst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0);
-		break;
-
-	case MC_CG_ARB_FREQ_F1:
-		mc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1);
-		mc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1);
-		burst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1);
-		break;
-
-	default:
-		return -1;
-	}
-
-	switch (arbFreqDest) {
-	case MC_CG_ARB_FREQ_F0:
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING, mc_arb_dram_timing);
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2, mc_arb_dram_timing2);
-		PHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0, burst_time);
-		break;
-
-	case MC_CG_ARB_FREQ_F1:
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1, mc_arb_dram_timing);
-		cgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1, mc_arb_dram_timing2);
-		PHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1, burst_time);
-		break;
-
-	default:
-		return -1;
-	}
-
-	mc_cg_config = cgs_read_register(hwmgr->device, mmMC_CG_CONFIG);
-	mc_cg_config |= 0x0000000F;
-	cgs_write_register(hwmgr->device, mmMC_CG_CONFIG, mc_cg_config);
-	PHM_WRITE_FIELD(hwmgr->device, MC_ARB_CG, CG_ARB_REQ, arbFreqDest);
-
-	return 0;
-}
-
-/**
- * Initial switch from ARB F0->F1
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- * This function is to be called from the SetPowerState table.
- */
-int tonga_initial_switch_from_arb_f0_to_f1(struct pp_hwmgr *hwmgr)
-{
-	return tonga_copy_and_switch_arb_sets(hwmgr, MC_CG_ARB_FREQ_F0, MC_CG_ARB_FREQ_F1);
-}
-
-/**
- * Initialize the ARB DRAM timing table's index field.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_init_arb_table_index(struct pp_hwmgr *hwmgr)
-{
-	const tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	uint32_t tmp;
-	int result;
-
-	/*
-	* This is a read-modify-write on the first byte of the ARB table.
-	* The first byte in the SMU72_Discrete_MCArbDramTimingTable structure is the field 'current'.
-	* This solution is ugly, but we never write the whole table only individual fields in it.
-	* In reality this field should not be in that structure but in a soft register.
-	*/
-	result = tonga_read_smc_sram_dword(hwmgr->smumgr,
-				data->arb_table_start, &tmp, data->sram_end);
-
-	if (0 != result)
-		return result;
-
-	tmp &= 0x00FFFFFF;
-	tmp |= ((uint32_t)MC_CG_ARB_FREQ_F1) << 24;
-
-	return tonga_write_smc_sram_dword(hwmgr->smumgr,
-			data->arb_table_start,  tmp, data->sram_end);
-}
-
-int tonga_populate_mc_reg_address(struct pp_hwmgr *hwmgr, SMU72_Discrete_MCRegisters *mc_reg_table)
-{
-	const struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	uint32_t i, j;
-
-	for (i = 0, j = 0; j < data->tonga_mc_reg_table.last; j++) {
-		if (data->tonga_mc_reg_table.validflag & 1<<j) {
-			PP_ASSERT_WITH_CODE(i < SMU72_DISCRETE_MC_REGISTER_ARRAY_SIZE,
-				"Index of mc_reg_table->address[] array out of boundary", return -1);
-			mc_reg_table->address[i].s0 =
-				PP_HOST_TO_SMC_US(data->tonga_mc_reg_table.mc_reg_address[j].s0);
-			mc_reg_table->address[i].s1 =
-				PP_HOST_TO_SMC_US(data->tonga_mc_reg_table.mc_reg_address[j].s1);
-			i++;
-		}
-	}
-
-	mc_reg_table->last = (uint8_t)i;
-
-	return 0;
-}
-
-/*convert register values from driver to SMC format */
-void tonga_convert_mc_registers(
-	const phw_tonga_mc_reg_entry * pEntry,
-	SMU72_Discrete_MCRegisterSet *pData,
-	uint32_t numEntries, uint32_t validflag)
-{
-	uint32_t i, j;
-
-	for (i = 0, j = 0; j < numEntries; j++) {
-		if (validflag & 1<<j) {
-			pData->value[i] = PP_HOST_TO_SMC_UL(pEntry->mc_data[j]);
-			i++;
-		}
-	}
-}
-
-/* find the entry in the memory range table, then populate the value to SMC's tonga_mc_reg_table */
-int tonga_convert_mc_reg_table_entry_to_smc(
-		struct pp_hwmgr *hwmgr,
-		const uint32_t memory_clock,
-		SMU72_Discrete_MCRegisterSet *mc_reg_table_data
-		)
-{
-	const tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	uint32_t i = 0;
-
-	for (i = 0; i < data->tonga_mc_reg_table.num_entries; i++) {
-		if (memory_clock <=
-			data->tonga_mc_reg_table.mc_reg_table_entry[i].mclk_max) {
-			break;
-		}
-	}
-
-	if ((i == data->tonga_mc_reg_table.num_entries) && (i > 0))
-		--i;
-
-	tonga_convert_mc_registers(&data->tonga_mc_reg_table.mc_reg_table_entry[i],
-		mc_reg_table_data, data->tonga_mc_reg_table.last, data->tonga_mc_reg_table.validflag);
-
-	return 0;
-}
-
-int tonga_convert_mc_reg_table_to_smc(struct pp_hwmgr *hwmgr,
-		SMU72_Discrete_MCRegisters *mc_reg_table)
-{
-	int result = 0;
-	tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	int res;
-	uint32_t i;
-
-	for (i = 0; i < data->dpm_table.mclk_table.count; i++) {
-		res = tonga_convert_mc_reg_table_entry_to_smc(
-				hwmgr,
-				data->dpm_table.mclk_table.dpm_levels[i].value,
-				&mc_reg_table->data[i]
-				);
-
-		if (0 != res)
-			result = res;
-	}
-
-	return result;
-}
-
-int tonga_populate_initial_mc_reg_table(struct pp_hwmgr *hwmgr)
-{
-	int result;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	memset(&data->mc_reg_table, 0x00, sizeof(SMU72_Discrete_MCRegisters));
-	result = tonga_populate_mc_reg_address(hwmgr, &(data->mc_reg_table));
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize MCRegTable for the MC register addresses!", return result;);
-
-	result = tonga_convert_mc_reg_table_to_smc(hwmgr, &data->mc_reg_table);
-	PP_ASSERT_WITH_CODE(0 == result,
-		"Failed to initialize MCRegTable for driver state!", return result;);
-
-	return tonga_copy_bytes_to_smc(hwmgr->smumgr, data->mc_reg_table_start,
-			(uint8_t *)&data->mc_reg_table, sizeof(SMU72_Discrete_MCRegisters), data->sram_end);
-}
-
-/**
- * Programs static screed detection parameters
- *
- * @param   hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_program_static_screen_threshold_parameters(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	/* Set static screen threshold unit*/
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device,
-		CGS_IND_REG__SMC, CG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD_UNIT,
-		data->static_screen_threshold_unit);
-	/* Set static screen threshold*/
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device,
-		CGS_IND_REG__SMC, CG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD,
-		data->static_screen_threshold);
-
-	return 0;
-}
-
-/**
- * Setup display gap for glitch free memory clock switching.
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_enable_display_gap(struct pp_hwmgr *hwmgr)
-{
-	uint32_t display_gap = cgs_read_ind_register(hwmgr->device,
-							CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL);
-
-	display_gap = PHM_SET_FIELD(display_gap,
-					CG_DISPLAY_GAP_CNTL, DISP_GAP, DISPLAY_GAP_IGNORE);
-
-	display_gap = PHM_SET_FIELD(display_gap,
-					CG_DISPLAY_GAP_CNTL, DISP_GAP_MCHG, DISPLAY_GAP_VBLANK);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_DISPLAY_GAP_CNTL, display_gap);
-
-	return 0;
-}
-
-/**
- * Programs activity state transition voting clients
- *
- * @param    hwmgr  the address of the powerplay hardware manager.
- * @return   always 0
- */
-int tonga_program_voting_clients(struct pp_hwmgr *hwmgr)
-{
-	tonga_hwmgr *data = (tonga_hwmgr *)(hwmgr->backend);
-
-	/* Clear reset for voting clients before enabling DPM */
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-		SCLK_PWRMGT_CNTL, RESET_SCLK_CNT, 0);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-		SCLK_PWRMGT_CNTL, RESET_BUSY_CNT, 0);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_0, data->voting_rights_clients0);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_1, data->voting_rights_clients1);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_2, data->voting_rights_clients2);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_3, data->voting_rights_clients3);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_4, data->voting_rights_clients4);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_5, data->voting_rights_clients5);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_6, data->voting_rights_clients6);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-		ixCG_FREQ_TRAN_VOTING_7, data->voting_rights_clients7);
-
-	return 0;
-}
-
-static void tonga_set_dpm_event_sources(struct pp_hwmgr *hwmgr, uint32_t sources)
-{
-	bool protection;
-	enum DPM_EVENT_SRC src;
-
-	switch (sources) {
-	default:
-		printk(KERN_ERR "Unknown throttling event sources.");
-		/* fall through */
-	case 0:
-		protection = false;
-		/* src is unused */
-		break;
-	case (1 << PHM_AutoThrottleSource_Thermal):
-		protection = true;
-		src = DPM_EVENT_SRC_DIGITAL;
-		break;
-	case (1 << PHM_AutoThrottleSource_External):
-		protection = true;
-		src = DPM_EVENT_SRC_EXTERNAL;
-		break;
-	case (1 << PHM_AutoThrottleSource_External) |
-			(1 << PHM_AutoThrottleSource_Thermal):
-		protection = true;
-		src = DPM_EVENT_SRC_DIGITAL_OR_EXTERNAL;
-		break;
-	}
-	/* Order matters - don't enable thermal protection for the wrong source. */
-	if (protection) {
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_CTRL,
-				DPM_EVENT_SRC, src);
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-				THERMAL_PROTECTION_DIS,
-				!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-						PHM_PlatformCaps_ThermalController));
-	} else
-		PHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,
-				THERMAL_PROTECTION_DIS, 1);
-}
-
-static int tonga_enable_auto_throttle_source(struct pp_hwmgr *hwmgr,
-		PHM_AutoThrottleSource source)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	if (!(data->active_auto_throttle_sources & (1 << source))) {
-		data->active_auto_throttle_sources |= 1 << source;
-		tonga_set_dpm_event_sources(hwmgr, data->active_auto_throttle_sources);
-	}
-	return 0;
-}
-
-static int tonga_enable_thermal_auto_throttle(struct pp_hwmgr *hwmgr)
-{
-	return tonga_enable_auto_throttle_source(hwmgr, PHM_AutoThrottleSource_Thermal);
-}
-
-static int tonga_disable_auto_throttle_source(struct pp_hwmgr *hwmgr,
-		PHM_AutoThrottleSource source)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	if (data->active_auto_throttle_sources & (1 << source)) {
-		data->active_auto_throttle_sources &= ~(1 << source);
-		tonga_set_dpm_event_sources(hwmgr, data->active_auto_throttle_sources);
-	}
-	return 0;
-}
-
-static int tonga_disable_thermal_auto_throttle(struct pp_hwmgr *hwmgr)
-{
-	return tonga_disable_auto_throttle_source(hwmgr, PHM_AutoThrottleSource_Thermal);
-}
-
-int tonga_enable_dpm_tasks(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = tonga_check_for_dpm_stopped(hwmgr);
-
-	if (cf_tonga_voltage_control(hwmgr)) {
-		tmp_result = tonga_enable_voltage_control(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable voltage control!", result = tmp_result);
-
-		tmp_result = tonga_construct_voltage_tables(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to contruct voltage tables!", result = tmp_result);
-	}
-
-	tmp_result = tonga_initialize_mc_reg_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to initialize MC reg table!", result = tmp_result);
-
-	tmp_result = tonga_program_static_screen_threshold_parameters(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to program static screen threshold parameters!", result = tmp_result);
-
-	tmp_result = tonga_enable_display_gap(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to enable display gap!", result = tmp_result);
-
-	tmp_result = tonga_program_voting_clients(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to program voting clients!", result = tmp_result);
-
-	tmp_result = tonga_process_firmware_header(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to process firmware header!", result = tmp_result);
-
-	tmp_result = tonga_initial_switch_from_arb_f0_to_f1(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to initialize switch from ArbF0 to F1!", result = tmp_result);
-
-	tmp_result = tonga_init_smc_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to initialize SMC table!", result = tmp_result);
-
-	tmp_result = tonga_init_arb_table_index(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to initialize ARB table index!", result = tmp_result);
-
-	tmp_result = tonga_populate_pm_fuses(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to populate PM fuses!", result = tmp_result);
-
-	tmp_result = tonga_populate_initial_mc_reg_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to populate initialize MC Reg table!", result = tmp_result);
-
-	tmp_result = tonga_notify_smc_display_change(hwmgr, false);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to notify no display!", result = tmp_result);
-
-	/* enable SCLK control */
-	tmp_result = tonga_enable_sclk_control(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to enable SCLK control!", result = tmp_result);
-
-	/* enable DPM */
-	tmp_result = tonga_start_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to start DPM!", result = tmp_result);
-
-	tmp_result = tonga_enable_smc_cac(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to enable SMC CAC!", result = tmp_result);
-
-	tmp_result = tonga_enable_power_containment(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to enable power containment!", result = tmp_result);
-
-	tmp_result = tonga_power_control_set_level(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to power control set level!", result = tmp_result);
-
-	tmp_result = tonga_enable_thermal_auto_throttle(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-			"Failed to enable thermal auto throttle!", result = tmp_result);
-
-	return result;
-}
-
-int tonga_disable_dpm_tasks(struct pp_hwmgr *hwmgr)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = tonga_check_for_dpm_running(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"SMC is still running!", return 0);
-
-	tmp_result = tonga_disable_thermal_auto_throttle(hwmgr);
-	PP_ASSERT_WITH_CODE((tmp_result == 0),
-			"Failed to disable thermal auto throttle!", result = tmp_result);
-
-	tmp_result = tonga_stop_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to stop DPM!", result = tmp_result);
-
-	tmp_result = tonga_reset_to_default(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result),
-		"Failed to reset to default!", result = tmp_result);
-
-	return result;
-}
-
-int tonga_reset_asic_tasks(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	result = tonga_set_boot_state(hwmgr);
-	if (0 != result)
-		printk(KERN_ERR "[ powerplay ] Failed to reset asic via set boot state! \n");
-
-	return result;
-}
-
-int tonga_hwmgr_backend_fini(struct pp_hwmgr *hwmgr)
-{
-	return phm_hwmgr_backend_fini(hwmgr);
-}
-
-/**
- * Initializes the Volcanic Islands Hardware Manager
- *
- * @param   hwmgr the address of the powerplay hardware manager.
- * @return   1 if success; otherwise appropriate error code.
- */
-int tonga_hwmgr_backend_init(struct pp_hwmgr *hwmgr)
-{
-	int result = 0;
-	SMU72_Discrete_DpmTable  *table = NULL;
-	tonga_hwmgr *data;
-	pp_atomctrl_gpio_pin_assignment gpio_pin_assignment;
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-	phw_tonga_ulv_parm *ulv;
-	struct cgs_system_info sys_info = {0};
-
-	PP_ASSERT_WITH_CODE((NULL != hwmgr),
-		"Invalid Parameter!", return -1;);
-
-	data = kzalloc(sizeof(struct tonga_hwmgr), GFP_KERNEL);
-	if (data == NULL)
-		return -ENOMEM;
-
-	hwmgr->backend = data;
-
-	data->dll_defaule_on = false;
-	data->sram_end = SMC_RAM_END;
-
-	data->activity_target[0] = PPTONGA_TARGETACTIVITY_DFLT;
-	data->activity_target[1] = PPTONGA_TARGETACTIVITY_DFLT;
-	data->activity_target[2] = PPTONGA_TARGETACTIVITY_DFLT;
-	data->activity_target[3] = PPTONGA_TARGETACTIVITY_DFLT;
-	data->activity_target[4] = PPTONGA_TARGETACTIVITY_DFLT;
-	data->activity_target[5] = PPTONGA_TARGETACTIVITY_DFLT;
-	data->activity_target[6] = PPTONGA_TARGETACTIVITY_DFLT;
-	data->activity_target[7] = PPTONGA_TARGETACTIVITY_DFLT;
-
-	data->vddc_vddci_delta = VDDC_VDDCI_DELTA;
-	data->vddc_vddgfx_delta = VDDC_VDDGFX_DELTA;
-	data->mclk_activity_target = PPTONGA_MCLK_TARGETACTIVITY_DFLT;
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_DisableVoltageIsland);
-
-	data->sclk_dpm_key_disabled = 0;
-	data->mclk_dpm_key_disabled = 0;
-	data->pcie_dpm_key_disabled = 0;
-	data->pcc_monitor_enabled = 0;
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_UnTabledHardwareInterface);
-
-	data->gpio_debug = 0;
-	data->engine_clock_data = 0;
-	data->memory_clock_data = 0;
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_DynamicPatchPowerState);
-
-	/* need to set voltage control types before EVV patching*/
-	data->voltage_control = TONGA_VOLTAGE_CONTROL_NONE;
-	data->vdd_ci_control = TONGA_VOLTAGE_CONTROL_NONE;
-	data->vdd_gfx_control = TONGA_VOLTAGE_CONTROL_NONE;
-	data->mvdd_control = TONGA_VOLTAGE_CONTROL_NONE;
-	data->force_pcie_gen = PP_PCIEGenInvalid;
-
-	if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-				VOLTAGE_TYPE_VDDC, VOLTAGE_OBJ_SVID2)) {
-		data->voltage_control = TONGA_VOLTAGE_CONTROL_BY_SVID2;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ControlVDDGFX)) {
-		if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-			VOLTAGE_TYPE_VDDGFX, VOLTAGE_OBJ_SVID2)) {
-			data->vdd_gfx_control = TONGA_VOLTAGE_CONTROL_BY_SVID2;
-		}
-	}
-
-	if (TONGA_VOLTAGE_CONTROL_NONE == data->vdd_gfx_control) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ControlVDDGFX);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EnableMVDDControl)) {
-		if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-					VOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_GPIO_LUT)) {
-			data->mvdd_control = TONGA_VOLTAGE_CONTROL_BY_GPIO;
-		}
-	}
-
-	if (TONGA_VOLTAGE_CONTROL_NONE == data->mvdd_control) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_EnableMVDDControl);
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ControlVDDCI)) {
-		if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-					VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_GPIO_LUT))
-			data->vdd_ci_control = TONGA_VOLTAGE_CONTROL_BY_GPIO;
-		else if (atomctrl_is_voltage_controled_by_gpio_v3(hwmgr,
-						VOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_SVID2))
-			data->vdd_ci_control = TONGA_VOLTAGE_CONTROL_BY_SVID2;
-	}
-
-	if (TONGA_VOLTAGE_CONTROL_NONE == data->vdd_ci_control)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_ControlVDDCI);
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_TablelessHardwareInterface);
-
-	if (pptable_info->cac_dtp_table->usClockStretchAmount != 0)
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_ClockStretcher);
-
-	/* Initializes DPM default values*/
-	tonga_initialize_dpm_defaults(hwmgr);
-
-	/* Get leakage voltage based on leakage ID.*/
-	PP_ASSERT_WITH_CODE((0 == tonga_get_evv_voltage(hwmgr)),
-		"Get EVV Voltage Failed.  Abort Driver loading!", return -1);
-
-	tonga_complete_dependency_tables(hwmgr);
-
-	/* Parse pptable data read from VBIOS*/
-	tonga_set_private_var_based_on_pptale(hwmgr);
-
-	/* ULV Support*/
-	ulv = &(data->ulv);
-	ulv->ulv_supported = false;
-
-	/* Initalize Dynamic State Adjustment Rule Settings*/
-	result = tonga_initializa_dynamic_state_adjustment_rule_settings(hwmgr);
-	if (result)
-		printk(KERN_ERR "[ powerplay ] tonga_initializa_dynamic_state_adjustment_rule_settings failed!\n");
-	data->uvd_enabled = false;
-
-	table = &(data->smc_state_table);
-
-	/*
-	* if ucGPIO_ID=VDDC_PCC_GPIO_PINID in GPIO_LUTable,
-	* Peak Current Control feature is enabled and we should program PCC HW register
-	*/
-	if (atomctrl_get_pp_assign_pin(hwmgr, VDDC_PCC_GPIO_PINID, &gpio_pin_assignment)) {
-		uint32_t temp_reg = cgs_read_ind_register(hwmgr->device,
-										CGS_IND_REG__SMC, ixCNB_PWRMGT_CNTL);
-
-		switch (gpio_pin_assignment.uc_gpio_pin_bit_shift) {
-		case 0:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, GNB_SLOW_MODE, 0x1);
-			break;
-		case 1:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, GNB_SLOW_MODE, 0x2);
-			break;
-		case 2:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, GNB_SLOW, 0x1);
-			break;
-		case 3:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, FORCE_NB_PS1, 0x1);
-			break;
-		case 4:
-			temp_reg = PHM_SET_FIELD(temp_reg,
-				CNB_PWRMGT_CNTL, DPM_ENABLED, 0x1);
-			break;
-		default:
-			printk(KERN_ERR "[ powerplay ] Failed to setup PCC HW register!	\
-				Wrong GPIO assigned for VDDC_PCC_GPIO_PINID! \n");
-			break;
-		}
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,
-			ixCNB_PWRMGT_CNTL, temp_reg);
-	}
-
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_EnableSMU7ThermalManagement);
-	phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-		PHM_PlatformCaps_SMU7);
-
-	data->vddc_phase_shed_control = false;
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		      PHM_PlatformCaps_UVDPowerGating);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-		      PHM_PlatformCaps_VCEPowerGating);
-	sys_info.size = sizeof(struct cgs_system_info);
-	sys_info.info_id = CGS_SYSTEM_INFO_PG_FLAGS;
-	result = cgs_query_system_info(hwmgr->device, &sys_info);
-	if (!result) {
-		if (sys_info.value & AMD_PG_SUPPORT_UVD)
-			phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				      PHM_PlatformCaps_UVDPowerGating);
-		if (sys_info.value & AMD_PG_SUPPORT_VCE)
-			phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				      PHM_PlatformCaps_VCEPowerGating);
-	}
-
-	if (0 == result) {
-		data->is_tlu_enabled = false;
-		hwmgr->platform_descriptor.hardwareActivityPerformanceLevels =
-			TONGA_MAX_HARDWARE_POWERLEVELS;
-		hwmgr->platform_descriptor.hardwarePerformanceLevels = 2;
-		hwmgr->platform_descriptor.minimumClocksReductionPercentage  = 50;
-
-		sys_info.size = sizeof(struct cgs_system_info);
-		sys_info.info_id = CGS_SYSTEM_INFO_PCIE_GEN_INFO;
-		result = cgs_query_system_info(hwmgr->device, &sys_info);
-		if (result)
-			data->pcie_gen_cap = AMDGPU_DEFAULT_PCIE_GEN_MASK;
-		else
-			data->pcie_gen_cap = (uint32_t)sys_info.value;
-		if (data->pcie_gen_cap & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)
-			data->pcie_spc_cap = 20;
-		sys_info.size = sizeof(struct cgs_system_info);
-		sys_info.info_id = CGS_SYSTEM_INFO_PCIE_MLW;
-		result = cgs_query_system_info(hwmgr->device, &sys_info);
-		if (result)
-			data->pcie_lane_cap = AMDGPU_DEFAULT_PCIE_MLW_MASK;
-		else
-			data->pcie_lane_cap = (uint32_t)sys_info.value;
-	} else {
-		/* Ignore return value in here, we are cleaning up a mess. */
-		tonga_hwmgr_backend_fini(hwmgr);
-	}
-
-	return result;
-}
-
-static int tonga_force_dpm_level(struct pp_hwmgr *hwmgr,
-		enum amd_dpm_forced_level level)
-{
-	int ret = 0;
-
-	switch (level) {
-	case AMD_DPM_FORCED_LEVEL_HIGH:
-		ret = tonga_force_dpm_highest(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	case AMD_DPM_FORCED_LEVEL_LOW:
-		ret = tonga_force_dpm_lowest(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	case AMD_DPM_FORCED_LEVEL_AUTO:
-		ret = tonga_unforce_dpm_levels(hwmgr);
-		if (ret)
-			return ret;
-		break;
-	default:
-		break;
-	}
-
-	hwmgr->dpm_level = level;
-	return ret;
-}
-
-static int tonga_apply_state_adjust_rules(struct pp_hwmgr *hwmgr,
-				struct pp_power_state  *prequest_ps,
-			const struct pp_power_state *pcurrent_ps)
-{
-	struct tonga_power_state *tonga_ps =
-				cast_phw_tonga_power_state(&prequest_ps->hardware);
-
-	uint32_t sclk;
-	uint32_t mclk;
-	struct PP_Clocks minimum_clocks = {0};
-	bool disable_mclk_switching;
-	bool disable_mclk_switching_for_frame_lock;
-	struct cgs_display_info info = {0};
-	const struct phm_clock_and_voltage_limits *max_limits;
-	uint32_t i;
-	tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	int32_t count;
-	int32_t stable_pstate_sclk = 0, stable_pstate_mclk = 0;
-
-	data->battery_state = (PP_StateUILabel_Battery == prequest_ps->classification.ui_label);
-
-	PP_ASSERT_WITH_CODE(tonga_ps->performance_level_count == 2,
-				 "VI should always have 2 performance levels",
-				 );
-
-	max_limits = (PP_PowerSource_AC == hwmgr->power_source) ?
-			&(hwmgr->dyn_state.max_clock_voltage_on_ac) :
-			&(hwmgr->dyn_state.max_clock_voltage_on_dc);
-
-	if (PP_PowerSource_DC == hwmgr->power_source) {
-		for (i = 0; i < tonga_ps->performance_level_count; i++) {
-			if (tonga_ps->performance_levels[i].memory_clock > max_limits->mclk)
-				tonga_ps->performance_levels[i].memory_clock = max_limits->mclk;
-			if (tonga_ps->performance_levels[i].engine_clock > max_limits->sclk)
-				tonga_ps->performance_levels[i].engine_clock = max_limits->sclk;
-		}
-	}
-
-	tonga_ps->vce_clocks.EVCLK = hwmgr->vce_arbiter.evclk;
-	tonga_ps->vce_clocks.ECCLK = hwmgr->vce_arbiter.ecclk;
-
-	tonga_ps->acp_clk = hwmgr->acp_arbiter.acpclk;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	/*TO DO result = PHM_CheckVBlankTime(hwmgr, &vblankTooShort);*/
-
-	/* TO DO GetMinClockSettings(hwmgr->pPECI, &minimum_clocks); */
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) {
-
-		max_limits = &(hwmgr->dyn_state.max_clock_voltage_on_ac);
-		stable_pstate_sclk = (max_limits->sclk * 75) / 100;
-
-		for (count = pptable_info->vdd_dep_on_sclk->count-1; count >= 0; count--) {
-			if (stable_pstate_sclk >= pptable_info->vdd_dep_on_sclk->entries[count].clk) {
-				stable_pstate_sclk = pptable_info->vdd_dep_on_sclk->entries[count].clk;
-				break;
-			}
-		}
-
-		if (count < 0)
-			stable_pstate_sclk = pptable_info->vdd_dep_on_sclk->entries[0].clk;
-
-		stable_pstate_mclk = max_limits->mclk;
-
-		minimum_clocks.engineClock = stable_pstate_sclk;
-		minimum_clocks.memoryClock = stable_pstate_mclk;
-	}
-
-	if (minimum_clocks.engineClock < hwmgr->gfx_arbiter.sclk)
-		minimum_clocks.engineClock = hwmgr->gfx_arbiter.sclk;
-
-	if (minimum_clocks.memoryClock < hwmgr->gfx_arbiter.mclk)
-		minimum_clocks.memoryClock = hwmgr->gfx_arbiter.mclk;
-
-	tonga_ps->sclk_threshold = hwmgr->gfx_arbiter.sclk_threshold;
-
-	if (0 != hwmgr->gfx_arbiter.sclk_over_drive) {
-		PP_ASSERT_WITH_CODE((hwmgr->gfx_arbiter.sclk_over_drive <= hwmgr->platform_descriptor.overdriveLimit.engineClock),
-					"Overdrive sclk exceeds limit",
-					hwmgr->gfx_arbiter.sclk_over_drive = hwmgr->platform_descriptor.overdriveLimit.engineClock);
-
-		if (hwmgr->gfx_arbiter.sclk_over_drive >= hwmgr->gfx_arbiter.sclk)
-			tonga_ps->performance_levels[1].engine_clock = hwmgr->gfx_arbiter.sclk_over_drive;
-	}
-
-	if (0 != hwmgr->gfx_arbiter.mclk_over_drive) {
-		PP_ASSERT_WITH_CODE((hwmgr->gfx_arbiter.mclk_over_drive <= hwmgr->platform_descriptor.overdriveLimit.memoryClock),
-			"Overdrive mclk exceeds limit",
-			hwmgr->gfx_arbiter.mclk_over_drive = hwmgr->platform_descriptor.overdriveLimit.memoryClock);
-
-		if (hwmgr->gfx_arbiter.mclk_over_drive >= hwmgr->gfx_arbiter.mclk)
-			tonga_ps->performance_levels[1].memory_clock = hwmgr->gfx_arbiter.mclk_over_drive;
-	}
-
-	disable_mclk_switching_for_frame_lock = phm_cap_enabled(
-				    hwmgr->platform_descriptor.platformCaps,
-				    PHM_PlatformCaps_DisableMclkSwitchingForFrameLock);
-
-	disable_mclk_switching = (1 < info.display_count) ||
-				    disable_mclk_switching_for_frame_lock;
-
-	sclk  = tonga_ps->performance_levels[0].engine_clock;
-	mclk  = tonga_ps->performance_levels[0].memory_clock;
-
-	if (disable_mclk_switching)
-		mclk  = tonga_ps->performance_levels[tonga_ps->performance_level_count - 1].memory_clock;
-
-	if (sclk < minimum_clocks.engineClock)
-		sclk = (minimum_clocks.engineClock > max_limits->sclk) ? max_limits->sclk : minimum_clocks.engineClock;
-
-	if (mclk < minimum_clocks.memoryClock)
-		mclk = (minimum_clocks.memoryClock > max_limits->mclk) ? max_limits->mclk : minimum_clocks.memoryClock;
-
-	tonga_ps->performance_levels[0].engine_clock = sclk;
-	tonga_ps->performance_levels[0].memory_clock = mclk;
-
-	tonga_ps->performance_levels[1].engine_clock =
-		(tonga_ps->performance_levels[1].engine_clock >= tonga_ps->performance_levels[0].engine_clock) ?
-			      tonga_ps->performance_levels[1].engine_clock :
-			      tonga_ps->performance_levels[0].engine_clock;
-
-	if (disable_mclk_switching) {
-		if (mclk < tonga_ps->performance_levels[1].memory_clock)
-			mclk = tonga_ps->performance_levels[1].memory_clock;
-
-		tonga_ps->performance_levels[0].memory_clock = mclk;
-		tonga_ps->performance_levels[1].memory_clock = mclk;
-	} else {
-		if (tonga_ps->performance_levels[1].memory_clock < tonga_ps->performance_levels[0].memory_clock)
-			tonga_ps->performance_levels[1].memory_clock = tonga_ps->performance_levels[0].memory_clock;
-	}
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) {
-		for (i=0; i < tonga_ps->performance_level_count; i++) {
-			tonga_ps->performance_levels[i].engine_clock = stable_pstate_sclk;
-			tonga_ps->performance_levels[i].memory_clock = stable_pstate_mclk;
-			tonga_ps->performance_levels[i].pcie_gen = data->pcie_gen_performance.max;
-			tonga_ps->performance_levels[i].pcie_lane = data->pcie_gen_performance.max;
-		}
-	}
-
-	return 0;
-}
-
-int tonga_get_power_state_size(struct pp_hwmgr *hwmgr)
-{
-	return sizeof(struct tonga_power_state);
-}
-
-static int tonga_dpm_get_mclk(struct pp_hwmgr *hwmgr, bool low)
-{
-	struct pp_power_state  *ps;
-	struct tonga_power_state  *tonga_ps;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	tonga_ps = cast_phw_tonga_power_state(&ps->hardware);
-
-	if (low)
-		return tonga_ps->performance_levels[0].memory_clock;
-	else
-		return tonga_ps->performance_levels[tonga_ps->performance_level_count-1].memory_clock;
-}
-
-static int tonga_dpm_get_sclk(struct pp_hwmgr *hwmgr, bool low)
-{
-	struct pp_power_state  *ps;
-	struct tonga_power_state  *tonga_ps;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	tonga_ps = cast_phw_tonga_power_state(&ps->hardware);
-
-	if (low)
-		return tonga_ps->performance_levels[0].engine_clock;
-	else
-		return tonga_ps->performance_levels[tonga_ps->performance_level_count-1].engine_clock;
-}
-
-static uint16_t tonga_get_current_pcie_speed(
-						   struct pp_hwmgr *hwmgr)
-{
-	uint32_t speed_cntl = 0;
-
-	speed_cntl = cgs_read_ind_register(hwmgr->device,
-						   CGS_IND_REG__PCIE,
-						   ixPCIE_LC_SPEED_CNTL);
-	return((uint16_t)PHM_GET_FIELD(speed_cntl,
-			PCIE_LC_SPEED_CNTL, LC_CURRENT_DATA_RATE));
-}
-
-static int tonga_get_current_pcie_lane_number(
-						   struct pp_hwmgr *hwmgr)
-{
-	uint32_t link_width;
-
-	link_width = PHM_READ_INDIRECT_FIELD(hwmgr->device,
-							CGS_IND_REG__PCIE,
-						  PCIE_LC_LINK_WIDTH_CNTL,
-							LC_LINK_WIDTH_RD);
-
-	PP_ASSERT_WITH_CODE((7 >= link_width),
-			"Invalid PCIe lane width!", return 0);
-
-	return decode_pcie_lane_width(link_width);
-}
-
-static int tonga_dpm_patch_boot_state(struct pp_hwmgr *hwmgr,
-					struct pp_hw_power_state *hw_ps)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_power_state *ps = (struct tonga_power_state *)hw_ps;
-	ATOM_FIRMWARE_INFO_V2_2 *fw_info;
-	uint16_t size;
-	uint8_t frev, crev;
-	int index = GetIndexIntoMasterTable(DATA, FirmwareInfo);
-
-	/* First retrieve the Boot clocks and VDDC from the firmware info table.
-	 * We assume here that fw_info is unchanged if this call fails.
-	 */
-	fw_info = (ATOM_FIRMWARE_INFO_V2_2 *)cgs_atom_get_data_table(
-			hwmgr->device, index,
-			&size, &frev, &crev);
-	if (!fw_info)
-		/* During a test, there is no firmware info table. */
-		return 0;
-
-	/* Patch the state. */
-	data->vbios_boot_state.sclk_bootup_value  = le32_to_cpu(fw_info->ulDefaultEngineClock);
-	data->vbios_boot_state.mclk_bootup_value  = le32_to_cpu(fw_info->ulDefaultMemoryClock);
-	data->vbios_boot_state.mvdd_bootup_value  = le16_to_cpu(fw_info->usBootUpMVDDCVoltage);
-	data->vbios_boot_state.vddc_bootup_value  = le16_to_cpu(fw_info->usBootUpVDDCVoltage);
-	data->vbios_boot_state.vddci_bootup_value = le16_to_cpu(fw_info->usBootUpVDDCIVoltage);
-	data->vbios_boot_state.pcie_gen_bootup_value = tonga_get_current_pcie_speed(hwmgr);
-	data->vbios_boot_state.pcie_lane_bootup_value =
-			(uint16_t)tonga_get_current_pcie_lane_number(hwmgr);
-
-	/* set boot power state */
-	ps->performance_levels[0].memory_clock = data->vbios_boot_state.mclk_bootup_value;
-	ps->performance_levels[0].engine_clock = data->vbios_boot_state.sclk_bootup_value;
-	ps->performance_levels[0].pcie_gen = data->vbios_boot_state.pcie_gen_bootup_value;
-	ps->performance_levels[0].pcie_lane = data->vbios_boot_state.pcie_lane_bootup_value;
-
-	return 0;
-}
-
-static int tonga_get_pp_table_entry_callback_func(struct pp_hwmgr *hwmgr,
-		void *state, struct pp_power_state *power_state,
-		void *pp_table, uint32_t classification_flag)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	struct tonga_power_state  *tonga_ps =
-			(struct tonga_power_state *)(&(power_state->hardware));
-
-	struct tonga_performance_level *performance_level;
-
-	ATOM_Tonga_State *state_entry = (ATOM_Tonga_State *)state;
-
-	ATOM_Tonga_POWERPLAYTABLE *powerplay_table =
-			(ATOM_Tonga_POWERPLAYTABLE *)pp_table;
-
-	ATOM_Tonga_SCLK_Dependency_Table *sclk_dep_table =
-			(ATOM_Tonga_SCLK_Dependency_Table *)
-			(((unsigned long)powerplay_table) +
-			le16_to_cpu(powerplay_table->usSclkDependencyTableOffset));
-
-	ATOM_Tonga_MCLK_Dependency_Table *mclk_dep_table =
-			(ATOM_Tonga_MCLK_Dependency_Table *)
-			(((unsigned long)powerplay_table) +
-			le16_to_cpu(powerplay_table->usMclkDependencyTableOffset));
-
-	/* The following fields are not initialized here: id orderedList allStatesList */
-	power_state->classification.ui_label =
-			(le16_to_cpu(state_entry->usClassification) &
-			ATOM_PPLIB_CLASSIFICATION_UI_MASK) >>
-			ATOM_PPLIB_CLASSIFICATION_UI_SHIFT;
-	power_state->classification.flags = classification_flag;
-	/* NOTE: There is a classification2 flag in BIOS that is not being used right now */
-
-	power_state->classification.temporary_state = false;
-	power_state->classification.to_be_deleted = false;
-
-	power_state->validation.disallowOnDC =
-			(0 != (le32_to_cpu(state_entry->ulCapsAndSettings) & ATOM_Tonga_DISALLOW_ON_DC));
-
-	power_state->pcie.lanes = 0;
-
-	power_state->display.disableFrameModulation = false;
-	power_state->display.limitRefreshrate = false;
-	power_state->display.enableVariBright =
-			(0 != (le32_to_cpu(state_entry->ulCapsAndSettings) & ATOM_Tonga_ENABLE_VARIBRIGHT));
-
-	power_state->validation.supportedPowerLevels = 0;
-	power_state->uvd_clocks.VCLK = 0;
-	power_state->uvd_clocks.DCLK = 0;
-	power_state->temperatures.min = 0;
-	power_state->temperatures.max = 0;
-
-	performance_level = &(tonga_ps->performance_levels
-			[tonga_ps->performance_level_count++]);
-
-	PP_ASSERT_WITH_CODE(
-			(tonga_ps->performance_level_count < SMU72_MAX_LEVELS_GRAPHICS),
-			"Performance levels exceeds SMC limit!",
-			return -1);
-
-	PP_ASSERT_WITH_CODE(
-			(tonga_ps->performance_level_count <=
-					hwmgr->platform_descriptor.hardwareActivityPerformanceLevels),
-			"Performance levels exceeds Driver limit!",
-			return -1);
-
-	/* Performance levels are arranged from low to high. */
-	performance_level->memory_clock =
-				le32_to_cpu(mclk_dep_table->entries[state_entry->ucMemoryClockIndexLow].ulMclk);
-
-	performance_level->engine_clock =
-				le32_to_cpu(sclk_dep_table->entries[state_entry->ucEngineClockIndexLow].ulSclk);
-
-	performance_level->pcie_gen = get_pcie_gen_support(
-							data->pcie_gen_cap,
-					     state_entry->ucPCIEGenLow);
-
-	performance_level->pcie_lane = get_pcie_lane_support(
-						    data->pcie_lane_cap,
-					   state_entry->ucPCIELaneHigh);
-
-	performance_level =
-			&(tonga_ps->performance_levels[tonga_ps->performance_level_count++]);
-
-	performance_level->memory_clock =
-				le32_to_cpu(mclk_dep_table->entries[state_entry->ucMemoryClockIndexHigh].ulMclk);
-
-	performance_level->engine_clock =
-				le32_to_cpu(sclk_dep_table->entries[state_entry->ucEngineClockIndexHigh].ulSclk);
-
-	performance_level->pcie_gen = get_pcie_gen_support(
-							data->pcie_gen_cap,
-					    state_entry->ucPCIEGenHigh);
-
-	performance_level->pcie_lane = get_pcie_lane_support(
-						    data->pcie_lane_cap,
-					   state_entry->ucPCIELaneHigh);
-
-	return 0;
-}
-
-static int tonga_get_pp_table_entry(struct pp_hwmgr *hwmgr,
-		    unsigned long entry_index, struct pp_power_state *ps)
-{
-	int result;
-	struct tonga_power_state *tonga_ps;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	struct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table =
-					   table_info->vdd_dep_on_mclk;
-
-	ps->hardware.magic = PhwTonga_Magic;
-
-	tonga_ps = cast_phw_tonga_power_state(&(ps->hardware));
-
-	result = get_powerplay_table_entry_v1_0(hwmgr, entry_index, ps,
-			tonga_get_pp_table_entry_callback_func);
-
-	/* This is the earliest time we have all the dependency table and the VBIOS boot state
-	 * as PP_Tables_GetPowerPlayTableEntry retrieves the VBIOS boot state
-	 * if there is only one VDDCI/MCLK level, check if it's the same as VBIOS boot state
-	 */
-	if (dep_mclk_table != NULL && dep_mclk_table->count == 1) {
-		if (dep_mclk_table->entries[0].clk !=
-				data->vbios_boot_state.mclk_bootup_value)
-			printk(KERN_ERR "Single MCLK entry VDDCI/MCLK dependency table "
-					"does not match VBIOS boot MCLK level");
-		if (dep_mclk_table->entries[0].vddci !=
-				data->vbios_boot_state.vddci_bootup_value)
-			printk(KERN_ERR "Single VDDCI entry VDDCI/MCLK dependency table "
-					"does not match VBIOS boot VDDCI level");
-	}
-
-	/* set DC compatible flag if this state supports DC */
-	if (!ps->validation.disallowOnDC)
-		tonga_ps->dc_compatible = true;
-
-	if (ps->classification.flags & PP_StateClassificationFlag_ACPI)
-		data->acpi_pcie_gen = tonga_ps->performance_levels[0].pcie_gen;
-	else if (ps->classification.flags & PP_StateClassificationFlag_Boot) {
-		if (data->bacos.best_match == 0xffff) {
-			/* For V.I. use boot state as base BACO state */
-			data->bacos.best_match = PP_StateClassificationFlag_Boot;
-			data->bacos.performance_level = tonga_ps->performance_levels[0];
-		}
-	}
-
-	tonga_ps->uvd_clocks.VCLK = ps->uvd_clocks.VCLK;
-	tonga_ps->uvd_clocks.DCLK = ps->uvd_clocks.DCLK;
-
-	if (!result) {
-		uint32_t i;
-
-		switch (ps->classification.ui_label) {
-		case PP_StateUILabel_Performance:
-			data->use_pcie_performance_levels = true;
-
-			for (i = 0; i < tonga_ps->performance_level_count; i++) {
-				if (data->pcie_gen_performance.max <
-						tonga_ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_performance.max =
-							tonga_ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_gen_performance.min >
-						tonga_ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_performance.min =
-							tonga_ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_lane_performance.max <
-						tonga_ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_performance.max =
-							tonga_ps->performance_levels[i].pcie_lane;
-
-				if (data->pcie_lane_performance.min >
-						tonga_ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_performance.min =
-							tonga_ps->performance_levels[i].pcie_lane;
-			}
-			break;
-		case PP_StateUILabel_Battery:
-			data->use_pcie_power_saving_levels = true;
-
-			for (i = 0; i < tonga_ps->performance_level_count; i++) {
-				if (data->pcie_gen_power_saving.max <
-						tonga_ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_power_saving.max =
-							tonga_ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_gen_power_saving.min >
-						tonga_ps->performance_levels[i].pcie_gen)
-					data->pcie_gen_power_saving.min =
-							tonga_ps->performance_levels[i].pcie_gen;
-
-				if (data->pcie_lane_power_saving.max <
-						tonga_ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_power_saving.max =
-							tonga_ps->performance_levels[i].pcie_lane;
-
-				if (data->pcie_lane_power_saving.min >
-						tonga_ps->performance_levels[i].pcie_lane)
-					data->pcie_lane_power_saving.min =
-							tonga_ps->performance_levels[i].pcie_lane;
-			}
-			break;
-		default:
-			break;
-		}
-	}
-	return 0;
-}
-
-static void
-tonga_print_current_perforce_level(struct pp_hwmgr *hwmgr, struct seq_file *m)
-{
-	uint32_t sclk, mclk, activity_percent;
-	uint32_t offset;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	smum_send_msg_to_smc(hwmgr->smumgr, (PPSMC_Msg)(PPSMC_MSG_API_GetSclkFrequency));
-
-	sclk = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-	smum_send_msg_to_smc(hwmgr->smumgr, (PPSMC_Msg)(PPSMC_MSG_API_GetMclkFrequency));
-
-	mclk = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-	seq_printf(m, "\n [  mclk  ]: %u MHz\n\n [  sclk  ]: %u MHz\n", mclk/100, sclk/100);
-
-	offset = data->soft_regs_start + offsetof(SMU72_SoftRegisters, AverageGraphicsActivity);
-	activity_percent = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, offset);
-	activity_percent += 0x80;
-	activity_percent >>= 8;
-
-	seq_printf(m, "\n [GPU load]: %u%%\n\n", activity_percent > 100 ? 100 : activity_percent);
-
-	seq_printf(m, "uvd    %sabled\n", data->uvd_power_gated ? "dis" : "en");
-
-	seq_printf(m, "vce    %sabled\n", data->vce_power_gated ? "dis" : "en");
-}
-
-static int tonga_find_dpm_states_clocks_in_dpm_table(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	const struct tonga_power_state *tonga_ps = cast_const_phw_tonga_power_state(states->pnew_state);
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_single_dpm_table *psclk_table = &(data->dpm_table.sclk_table);
-	uint32_t sclk = tonga_ps->performance_levels[tonga_ps->performance_level_count-1].engine_clock;
-	struct tonga_single_dpm_table *pmclk_table = &(data->dpm_table.mclk_table);
-	uint32_t mclk = tonga_ps->performance_levels[tonga_ps->performance_level_count-1].memory_clock;
-	struct PP_Clocks min_clocks = {0};
-	uint32_t i;
-	struct cgs_display_info info = {0};
-
-	data->need_update_smu7_dpm_table = 0;
-
-	for (i = 0; i < psclk_table->count; i++) {
-		if (sclk == psclk_table->dpm_levels[i].value)
-			break;
-	}
-
-	if (i >= psclk_table->count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_SCLK;
-	else {
-	/* TODO: Check SCLK in DAL's minimum clocks in case DeepSleep divider update is required.*/
-		if(data->display_timing.min_clock_insr != min_clocks.engineClockInSR)
-			data->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_SCLK;
-	}
-
-	for (i=0; i < pmclk_table->count; i++) {
-		if (mclk == pmclk_table->dpm_levels[i].value)
-			break;
-	}
-
-	if (i >= pmclk_table->count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_MCLK;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (data->display_timing.num_existing_displays != info.display_count)
-		data->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_MCLK;
-
-	return 0;
-}
-
-static uint16_t tonga_get_maximum_link_speed(struct pp_hwmgr *hwmgr, const struct tonga_power_state *hw_ps)
-{
-	uint32_t i;
-	uint32_t sclk, max_sclk = 0;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_dpm_table *pdpm_table = &data->dpm_table;
-
-	for (i = 0; i < hw_ps->performance_level_count; i++) {
-		sclk = hw_ps->performance_levels[i].engine_clock;
-		if (max_sclk < sclk)
-			max_sclk = sclk;
-	}
-
-	for (i = 0; i < pdpm_table->sclk_table.count; i++) {
-		if (pdpm_table->sclk_table.dpm_levels[i].value == max_sclk)
-			return (uint16_t) ((i >= pdpm_table->pcie_speed_table.count) ?
-					pdpm_table->pcie_speed_table.dpm_levels[pdpm_table->pcie_speed_table.count-1].value :
-					pdpm_table->pcie_speed_table.dpm_levels[i].value);
-	}
-
-	return 0;
-}
-
-static int tonga_request_link_speed_change_before_state_change(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	const struct tonga_power_state *tonga_nps = cast_const_phw_tonga_power_state(states->pnew_state);
-	const struct tonga_power_state *tonga_cps = cast_const_phw_tonga_power_state(states->pcurrent_state);
-
-	uint16_t target_link_speed = tonga_get_maximum_link_speed(hwmgr, tonga_nps);
-	uint16_t current_link_speed;
-
-	if (data->force_pcie_gen == PP_PCIEGenInvalid)
-		current_link_speed = tonga_get_maximum_link_speed(hwmgr, tonga_cps);
-	else
-		current_link_speed = data->force_pcie_gen;
-
-	data->force_pcie_gen = PP_PCIEGenInvalid;
-	data->pspp_notify_required = false;
-	if (target_link_speed > current_link_speed) {
-		switch(target_link_speed) {
-		case PP_PCIEGen3:
-			if (0 == acpi_pcie_perf_request(hwmgr->device, PCIE_PERF_REQ_GEN3, false))
-				break;
-			data->force_pcie_gen = PP_PCIEGen2;
-			if (current_link_speed == PP_PCIEGen2)
-				break;
-		case PP_PCIEGen2:
-			if (0 == acpi_pcie_perf_request(hwmgr->device, PCIE_PERF_REQ_GEN2, false))
-				break;
-		default:
-			data->force_pcie_gen = tonga_get_current_pcie_speed(hwmgr);
-			break;
-		}
-	} else {
-		if (target_link_speed < current_link_speed)
-			data->pspp_notify_required = true;
-	}
-
-	return 0;
-}
-
-static int tonga_freeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if ((0 == data->sclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK))) {
-		PP_ASSERT_WITH_CODE(!tonga_is_dpm_running(hwmgr),
-				    "Trying to freeze SCLK DPM when DPM is disabled",
-			);
-		PP_ASSERT_WITH_CODE(
-			0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					  PPSMC_MSG_SCLKDPM_FreezeLevel),
-			"Failed to freeze SCLK DPM during FreezeSclkMclkDPM Function!",
-			return -1);
-	}
-
-	if ((0 == data->mclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		 DPMTABLE_OD_UPDATE_MCLK)) {
-		PP_ASSERT_WITH_CODE(!tonga_is_dpm_running(hwmgr),
-				    "Trying to freeze MCLK DPM when DPM is disabled",
-			);
-		PP_ASSERT_WITH_CODE(
-			0 == smum_send_msg_to_smc(hwmgr->smumgr,
-							PPSMC_MSG_MCLKDPM_FreezeLevel),
-			"Failed to freeze MCLK DPM during FreezeSclkMclkDPM Function!",
-			return -1);
-	}
-
-	return 0;
-}
-
-static int tonga_populate_and_upload_sclk_mclk_dpm_levels(struct pp_hwmgr *hwmgr, const void *input)
-{
-	int result = 0;
-
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	const struct tonga_power_state *tonga_ps = cast_const_phw_tonga_power_state(states->pnew_state);
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	uint32_t sclk = tonga_ps->performance_levels[tonga_ps->performance_level_count-1].engine_clock;
-	uint32_t mclk = tonga_ps->performance_levels[tonga_ps->performance_level_count-1].memory_clock;
-	struct tonga_dpm_table *pdpm_table = &data->dpm_table;
-
-	struct tonga_dpm_table *pgolden_dpm_table = &data->golden_dpm_table;
-	uint32_t dpm_count, clock_percent;
-	uint32_t i;
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_SCLK) {
-		pdpm_table->sclk_table.dpm_levels[pdpm_table->sclk_table.count-1].value = sclk;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinACSupport) ||
-		    phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinDCSupport)) {
-		/* Need to do calculation based on the golden DPM table
-		 * as the Heatmap GPU Clock axis is also based on the default values
-		 */
-			PP_ASSERT_WITH_CODE(
-				(pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value != 0),
-				"Divide by 0!",
-				return -1);
-			dpm_count = pdpm_table->sclk_table.count < 2 ? 0 : pdpm_table->sclk_table.count-2;
-			for (i = dpm_count; i > 1; i--) {
-				if (sclk > pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value) {
-					clock_percent = ((sclk - pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value)*100) /
-							pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value;
-
-					pdpm_table->sclk_table.dpm_levels[i].value =
-							pgolden_dpm_table->sclk_table.dpm_levels[i].value +
-							(pgolden_dpm_table->sclk_table.dpm_levels[i].value * clock_percent)/100;
-
-				} else if (pgolden_dpm_table->sclk_table.dpm_levels[pdpm_table->sclk_table.count-1].value > sclk) {
-					clock_percent = ((pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value - sclk)*100) /
-								pgolden_dpm_table->sclk_table.dpm_levels[pgolden_dpm_table->sclk_table.count-1].value;
-
-					pdpm_table->sclk_table.dpm_levels[i].value =
-							pgolden_dpm_table->sclk_table.dpm_levels[i].value -
-							(pgolden_dpm_table->sclk_table.dpm_levels[i].value * clock_percent)/100;
-				} else
-					pdpm_table->sclk_table.dpm_levels[i].value =
-							pgolden_dpm_table->sclk_table.dpm_levels[i].value;
-			}
-		}
-	}
-
-	if (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK) {
-		pdpm_table->mclk_table.dpm_levels[pdpm_table->mclk_table.count-1].value = mclk;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinACSupport) ||
-			phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_OD6PlusinDCSupport)) {
-
-			PP_ASSERT_WITH_CODE(
-					(pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value != 0),
-					"Divide by 0!",
-					return -1);
-			dpm_count = pdpm_table->mclk_table.count < 2? 0 : pdpm_table->mclk_table.count-2;
-			for (i = dpm_count; i > 1; i--) {
-				if (mclk > pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value) {
-						clock_percent = ((mclk - pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value)*100) /
-								    pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value;
-
-						pdpm_table->mclk_table.dpm_levels[i].value =
-										pgolden_dpm_table->mclk_table.dpm_levels[i].value +
-										(pgolden_dpm_table->mclk_table.dpm_levels[i].value * clock_percent)/100;
-
-				} else if (pgolden_dpm_table->mclk_table.dpm_levels[pdpm_table->mclk_table.count-1].value > mclk) {
-						clock_percent = ((pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value - mclk)*100) /
-								    pgolden_dpm_table->mclk_table.dpm_levels[pgolden_dpm_table->mclk_table.count-1].value;
-
-						pdpm_table->mclk_table.dpm_levels[i].value =
-									pgolden_dpm_table->mclk_table.dpm_levels[i].value -
-									(pgolden_dpm_table->mclk_table.dpm_levels[i].value * clock_percent)/100;
-				} else
-					pdpm_table->mclk_table.dpm_levels[i].value = pgolden_dpm_table->mclk_table.dpm_levels[i].value;
-			}
-		}
-	}
-
-	if (data->need_update_smu7_dpm_table & (DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK)) {
-		result = tonga_populate_all_graphic_levels(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == result),
-			"Failed to populate SCLK during PopulateNewDPMClocksStates Function!",
-			return result);
-	}
-
-	if (data->need_update_smu7_dpm_table & (DPMTABLE_OD_UPDATE_MCLK + DPMTABLE_UPDATE_MCLK)) {
-		/*populate MCLK dpm table to SMU7 */
-		result = tonga_populate_all_memory_levels(hwmgr);
-		PP_ASSERT_WITH_CODE((0 == result),
-				"Failed to populate MCLK during PopulateNewDPMClocksStates Function!",
-				return result);
-	}
-
-	return result;
-}
-
-static	int tonga_trim_single_dpm_states(struct pp_hwmgr *hwmgr,
-			  struct tonga_single_dpm_table * pdpm_table,
-			     uint32_t low_limit, uint32_t high_limit)
-{
-	uint32_t i;
-
-	for (i = 0; i < pdpm_table->count; i++) {
-		if ((pdpm_table->dpm_levels[i].value < low_limit) ||
-		    (pdpm_table->dpm_levels[i].value > high_limit))
-			pdpm_table->dpm_levels[i].enabled = false;
-		else
-			pdpm_table->dpm_levels[i].enabled = true;
-	}
-	return 0;
-}
-
-static int tonga_trim_dpm_states(struct pp_hwmgr *hwmgr, const struct tonga_power_state *hw_state)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	uint32_t high_limit_count;
-
-	PP_ASSERT_WITH_CODE((hw_state->performance_level_count >= 1),
-				"power state did not have any performance level",
-				 return -1);
-
-	high_limit_count = (1 == hw_state->performance_level_count) ? 0: 1;
-
-	tonga_trim_single_dpm_states(hwmgr,
-					&(data->dpm_table.sclk_table),
-					hw_state->performance_levels[0].engine_clock,
-					hw_state->performance_levels[high_limit_count].engine_clock);
-
-	tonga_trim_single_dpm_states(hwmgr,
-						&(data->dpm_table.mclk_table),
-						hw_state->performance_levels[0].memory_clock,
-						hw_state->performance_levels[high_limit_count].memory_clock);
-
-	return 0;
-}
-
-static int tonga_generate_dpm_level_enable_mask(struct pp_hwmgr *hwmgr, const void *input)
-{
-	int result;
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	const struct tonga_power_state *tonga_ps = cast_const_phw_tonga_power_state(states->pnew_state);
-
-	result = tonga_trim_dpm_states(hwmgr, tonga_ps);
-	if (0 != result)
-		return result;
-
-	data->dpm_level_enable_mask.sclk_dpm_enable_mask = tonga_get_dpm_level_enable_mask_value(&data->dpm_table.sclk_table);
-	data->dpm_level_enable_mask.mclk_dpm_enable_mask = tonga_get_dpm_level_enable_mask_value(&data->dpm_table.mclk_table);
-	data->last_mclk_dpm_enable_mask = data->dpm_level_enable_mask.mclk_dpm_enable_mask;
-	if (data->uvd_enabled)
-		data->dpm_level_enable_mask.mclk_dpm_enable_mask &= 0xFFFFFFFE;
-
-	data->dpm_level_enable_mask.pcie_dpm_enable_mask = tonga_get_dpm_level_enable_mask_value(&data->dpm_table.pcie_speed_table);
-
-	return 0;
-}
-
-int tonga_enable_disable_vce_dpm(struct pp_hwmgr *hwmgr, bool enable)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, enable ?
-				  (PPSMC_Msg)PPSMC_MSG_VCEDPM_Enable :
-				  (PPSMC_Msg)PPSMC_MSG_VCEDPM_Disable);
-}
-
-int tonga_enable_disable_uvd_dpm(struct pp_hwmgr *hwmgr, bool enable)
-{
-	return smum_send_msg_to_smc(hwmgr->smumgr, enable ?
-				  (PPSMC_Msg)PPSMC_MSG_UVDDPM_Enable :
-				  (PPSMC_Msg)PPSMC_MSG_UVDDPM_Disable);
-}
-
-int tonga_update_uvd_dpm(struct pp_hwmgr *hwmgr, bool bgate)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	uint32_t mm_boot_level_offset, mm_boot_level_value;
-	struct phm_ppt_v1_information *ptable_information = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (!bgate) {
-		data->smc_state_table.UvdBootLevel = (uint8_t) (ptable_information->mm_dep_table->count - 1);
-		mm_boot_level_offset = data->dpm_table_start + offsetof(SMU72_Discrete_DpmTable, UvdBootLevel);
-		mm_boot_level_offset /= 4;
-		mm_boot_level_offset *= 4;
-		mm_boot_level_value = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, mm_boot_level_offset);
-		mm_boot_level_value &= 0x00FFFFFF;
-		mm_boot_level_value |= data->smc_state_table.UvdBootLevel << 24;
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, mm_boot_level_offset, mm_boot_level_value);
-
-		if (!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_UVDDPM) ||
-		    phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState))
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-						PPSMC_MSG_UVDDPM_SetEnabledMask,
-						(uint32_t)(1 << data->smc_state_table.UvdBootLevel));
-	}
-
-	return tonga_enable_disable_uvd_dpm(hwmgr, !bgate);
-}
-
-int tonga_update_vce_dpm(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	const struct tonga_power_state *tonga_nps = cast_const_phw_tonga_power_state(states->pnew_state);
-	const struct tonga_power_state *tonga_cps = cast_const_phw_tonga_power_state(states->pcurrent_state);
-
-	uint32_t mm_boot_level_offset, mm_boot_level_value;
-	struct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);
-
-	if (tonga_nps->vce_clocks.EVCLK > 0 && (tonga_cps == NULL || tonga_cps->vce_clocks.EVCLK == 0)) {
-		data->smc_state_table.VceBootLevel = (uint8_t) (pptable_info->mm_dep_table->count - 1);
-
-		mm_boot_level_offset = data->dpm_table_start + offsetof(SMU72_Discrete_DpmTable, VceBootLevel);
-		mm_boot_level_offset /= 4;
-		mm_boot_level_offset *= 4;
-		mm_boot_level_value = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, mm_boot_level_offset);
-		mm_boot_level_value &= 0xFF00FFFF;
-		mm_boot_level_value |= data->smc_state_table.VceBootLevel << 16;
-		cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, mm_boot_level_offset, mm_boot_level_value);
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState))
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-					PPSMC_MSG_VCEDPM_SetEnabledMask,
-				(uint32_t)(1 << data->smc_state_table.VceBootLevel));
-
-		tonga_enable_disable_vce_dpm(hwmgr, true);
-	} else if (tonga_nps->vce_clocks.EVCLK == 0 && tonga_cps != NULL && tonga_cps->vce_clocks.EVCLK > 0)
-		tonga_enable_disable_vce_dpm(hwmgr, false);
-
-	return 0;
-}
-
-static int tonga_update_and_upload_mc_reg_table(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	uint32_t address;
-	int32_t result;
-
-	if (0 == (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK))
-		return 0;
-
-
-	memset(&data->mc_reg_table, 0, sizeof(SMU72_Discrete_MCRegisters));
-
-	result = tonga_convert_mc_reg_table_to_smc(hwmgr, &(data->mc_reg_table));
-
-	if(result != 0)
-		return result;
-
-
-	address = data->mc_reg_table_start + (uint32_t)offsetof(SMU72_Discrete_MCRegisters, data[0]);
-
-	return  tonga_copy_bytes_to_smc(hwmgr->smumgr, address,
-				 (uint8_t *)&data->mc_reg_table.data[0],
-				sizeof(SMU72_Discrete_MCRegisterSet) * data->dpm_table.mclk_table.count,
-				data->sram_end);
-}
-
-static int tonga_program_memory_timing_parameters_conditionally(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	if (data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_OD_UPDATE_MCLK))
-		return tonga_program_memory_timing_parameters(hwmgr);
-
-	return 0;
-}
-
-static int tonga_unfreeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	if (0 == data->need_update_smu7_dpm_table)
-		return 0;
-
-	if ((0 == data->sclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table &
-		(DPMTABLE_OD_UPDATE_SCLK + DPMTABLE_UPDATE_SCLK))) {
-
-		PP_ASSERT_WITH_CODE(!tonga_is_dpm_running(hwmgr),
-				    "Trying to Unfreeze SCLK DPM when DPM is disabled",
-			);
-		PP_ASSERT_WITH_CODE(
-			 0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					 PPSMC_MSG_SCLKDPM_UnfreezeLevel),
-			"Failed to unfreeze SCLK DPM during UnFreezeSclkMclkDPM Function!",
-			return -1);
-	}
-
-	if ((0 == data->mclk_dpm_key_disabled) &&
-		(data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK)) {
-
-		PP_ASSERT_WITH_CODE(!tonga_is_dpm_running(hwmgr),
-				    "Trying to Unfreeze MCLK DPM when DPM is disabled",
-				);
-		PP_ASSERT_WITH_CODE(
-			 0 == smum_send_msg_to_smc(hwmgr->smumgr,
-					 PPSMC_MSG_SCLKDPM_UnfreezeLevel),
-		    "Failed to unfreeze MCLK DPM during UnFreezeSclkMclkDPM Function!",
-		    return -1);
-	}
-
-	data->need_update_smu7_dpm_table = 0;
-
-	return 0;
-}
-
-static int tonga_notify_link_speed_change_after_state_change(struct pp_hwmgr *hwmgr, const void *input)
-{
-	const struct phm_set_power_state_input *states = (const struct phm_set_power_state_input *)input;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	const struct tonga_power_state *tonga_ps = cast_const_phw_tonga_power_state(states->pnew_state);
-	uint16_t target_link_speed = tonga_get_maximum_link_speed(hwmgr, tonga_ps);
-	uint8_t  request;
-
-	if (data->pspp_notify_required  ||
-	    data->pcie_performance_request) {
-		if (target_link_speed == PP_PCIEGen3)
-			request = PCIE_PERF_REQ_GEN3;
-		else if (target_link_speed == PP_PCIEGen2)
-			request = PCIE_PERF_REQ_GEN2;
-		else
-			request = PCIE_PERF_REQ_GEN1;
-
-		if(request == PCIE_PERF_REQ_GEN1 && tonga_get_current_pcie_speed(hwmgr) > 0) {
-			data->pcie_performance_request = false;
-			return 0;
-		}
-
-		if (0 != acpi_pcie_perf_request(hwmgr->device, request, false)) {
-			if (PP_PCIEGen2 == target_link_speed)
-				printk("PSPP request to switch to Gen2 from Gen3 Failed!");
-			else
-				printk("PSPP request to switch to Gen1 from Gen2 Failed!");
-		}
-	}
-
-	data->pcie_performance_request = false;
-	return 0;
-}
-
-static int tonga_set_power_state_tasks(struct pp_hwmgr *hwmgr, const void *input)
-{
-	int tmp_result, result = 0;
-
-	tmp_result = tonga_find_dpm_states_clocks_in_dpm_table(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to find DPM states clocks in DPM table!", result = tmp_result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_PCIEPerformanceRequest)) {
-		tmp_result = tonga_request_link_speed_change_before_state_change(hwmgr, input);
-		PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to request link speed change before state change!", result = tmp_result);
-	}
-
-	tmp_result = tonga_freeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to freeze SCLK MCLK DPM!", result = tmp_result);
-
-	tmp_result = tonga_populate_and_upload_sclk_mclk_dpm_levels(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to populate and upload SCLK MCLK DPM levels!", result = tmp_result);
-
-	tmp_result = tonga_generate_dpm_level_enable_mask(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to generate DPM level enabled mask!", result = tmp_result);
-
-	tmp_result = tonga_update_vce_dpm(hwmgr, input);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to update VCE DPM!", result = tmp_result);
-
-	tmp_result = tonga_update_sclk_threshold(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to update SCLK threshold!", result = tmp_result);
-
-	tmp_result = tonga_update_and_upload_mc_reg_table(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to upload MC reg table!", result = tmp_result);
-
-	tmp_result = tonga_program_memory_timing_parameters_conditionally(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to program memory timing parameters!", result = tmp_result);
-
-	tmp_result = tonga_unfreeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to unfreeze SCLK MCLK DPM!", result = tmp_result);
-
-	tmp_result = tonga_upload_dpm_level_enable_mask(hwmgr);
-	PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to upload DPM level enabled mask!", result = tmp_result);
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_PCIEPerformanceRequest)) {
-		tmp_result = tonga_notify_link_speed_change_after_state_change(hwmgr, input);
-		PP_ASSERT_WITH_CODE((0 == tmp_result), "Failed to notify link speed change after state change!", result = tmp_result);
-	}
-
-	return result;
-}
-
-/**
-*  Set maximum target operating fan output PWM
-*
-* @param    pHwMgr:  the address of the powerplay hardware manager.
-* @param    usMaxFanPwm:  max operating fan PWM in percents
-* @return   The response that came from the SMC.
-*/
-static int tonga_set_max_fan_pwm_output(struct pp_hwmgr *hwmgr, uint16_t us_max_fan_pwm)
-{
-	hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanPWM = us_max_fan_pwm;
-
-	if (phm_is_hw_access_blocked(hwmgr))
-		return 0;
-
-	return (0 == smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_SetFanPwmMax, us_max_fan_pwm) ? 0 : -1);
-}
-
-int tonga_notify_smc_display_config_after_ps_adjustment(struct pp_hwmgr *hwmgr)
-{
-	uint32_t num_active_displays = 0;
-	struct cgs_display_info info = {0};
-	info.mode_info = NULL;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	num_active_displays = info.display_count;
-
-	if (num_active_displays > 1)  /* to do && (pHwMgr->pPECI->displayConfiguration.bMultiMonitorInSync != TRUE)) */
-		tonga_notify_smc_display_change(hwmgr, false);
-	else
-		tonga_notify_smc_display_change(hwmgr, true);
-
-	return 0;
-}
-
-/**
-* Programs the display gap
-*
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @return   always OK
-*/
-int tonga_program_display_gap(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	uint32_t num_active_displays = 0;
-	uint32_t display_gap = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL);
-	uint32_t display_gap2;
-	uint32_t pre_vbi_time_in_us;
-	uint32_t frame_time_in_us;
-	uint32_t ref_clock;
-	uint32_t refresh_rate = 0;
-	struct cgs_display_info info = {0};
-	struct cgs_mode_info mode_info;
-
-	info.mode_info = &mode_info;
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-	num_active_displays = info.display_count;
-
-	display_gap = PHM_SET_FIELD(display_gap, CG_DISPLAY_GAP_CNTL, DISP_GAP, (num_active_displays > 0)? DISPLAY_GAP_VBLANK_OR_WM : DISPLAY_GAP_IGNORE);
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL, display_gap);
-
-	ref_clock = mode_info.ref_clock;
-	refresh_rate = mode_info.refresh_rate;
-
-	if(0 == refresh_rate)
-		refresh_rate = 60;
-
-	frame_time_in_us = 1000000 / refresh_rate;
-
-	pre_vbi_time_in_us = frame_time_in_us - 200 - mode_info.vblank_time_us;
-	display_gap2 = pre_vbi_time_in_us * (ref_clock / 100);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL2, display_gap2);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, data->soft_regs_start + offsetof(SMU72_SoftRegisters, PreVBlankGap), 0x64);
-
-	cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, data->soft_regs_start + offsetof(SMU72_SoftRegisters, VBlankTimeout), (frame_time_in_us - pre_vbi_time_in_us));
-
-	if (num_active_displays == 1)
-		tonga_notify_smc_display_change(hwmgr, true);
-
-	return 0;
-}
-
-int tonga_display_configuration_changed_task(struct pp_hwmgr *hwmgr)
-{
-
-	tonga_program_display_gap(hwmgr);
-
-	/* to do PhwTonga_CacUpdateDisplayConfiguration(pHwMgr); */
-	return 0;
-}
-
-/**
-*  Set maximum target operating fan output RPM
-*
-* @param    pHwMgr:  the address of the powerplay hardware manager.
-* @param    usMaxFanRpm:  max operating fan RPM value.
-* @return   The response that came from the SMC.
-*/
-static int tonga_set_max_fan_rpm_output(struct pp_hwmgr *hwmgr, uint16_t us_max_fan_pwm)
-{
-	hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanRPM = us_max_fan_pwm;
-
-	if (phm_is_hw_access_blocked(hwmgr))
-		return 0;
-
-	return (0 == smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_SetFanRpmMax, us_max_fan_pwm) ? 0 : -1);
-}
-
-uint32_t tonga_get_xclk(struct pp_hwmgr *hwmgr)
-{
-	uint32_t reference_clock;
-	uint32_t tc;
-	uint32_t divide;
-
-	ATOM_FIRMWARE_INFO *fw_info;
-	uint16_t size;
-	uint8_t frev, crev;
-	int index = GetIndexIntoMasterTable(DATA, FirmwareInfo);
-
-	tc = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_CLKPIN_CNTL_2, MUX_TCLK_TO_XCLK);
-
-	if (tc)
-		return TCLK;
-
-	fw_info = (ATOM_FIRMWARE_INFO *)cgs_atom_get_data_table(hwmgr->device, index,
-						  &size, &frev, &crev);
-
-	if (!fw_info)
-		return 0;
-
-	reference_clock = le16_to_cpu(fw_info->usReferenceClock);
-
-	divide = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_CLKPIN_CNTL, XTALIN_DIVIDE);
-
-	if (0 != divide)
-		return reference_clock / 4;
-
-	return reference_clock;
-}
-
-int tonga_dpm_set_interrupt_state(void *private_data,
-					 unsigned src_id, unsigned type,
-					 int enabled)
-{
-	uint32_t cg_thermal_int;
-	struct pp_hwmgr *hwmgr = ((struct pp_eventmgr *)private_data)->hwmgr;
-
-	if (hwmgr == NULL)
-		return -EINVAL;
-
-	switch (type) {
-	case AMD_THERMAL_IRQ_LOW_TO_HIGH:
-		if (enabled) {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int |= CG_THERMAL_INT_CTRL__THERM_INTH_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		} else {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int &= ~CG_THERMAL_INT_CTRL__THERM_INTH_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		}
-		break;
-
-	case AMD_THERMAL_IRQ_HIGH_TO_LOW:
-		if (enabled) {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int |= CG_THERMAL_INT_CTRL__THERM_INTL_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		} else {
-			cg_thermal_int = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT);
-			cg_thermal_int &= ~CG_THERMAL_INT_CTRL__THERM_INTL_MASK_MASK;
-			cgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_THERMAL_INT, cg_thermal_int);
-		}
-		break;
-	default:
-		break;
-	}
-	return 0;
-}
-
-int tonga_register_internal_thermal_interrupt(struct pp_hwmgr *hwmgr,
-					const void *thermal_interrupt_info)
-{
-	int result;
-	const struct pp_interrupt_registration_info *info =
-			(const struct pp_interrupt_registration_info *)thermal_interrupt_info;
-
-	if (info == NULL)
-		return -EINVAL;
-
-	result = cgs_add_irq_source(hwmgr->device, 230, AMD_THERMAL_IRQ_LAST,
-				tonga_dpm_set_interrupt_state,
-				info->call_back, info->context);
-
-	if (result)
-		return -EINVAL;
-
-	result = cgs_add_irq_source(hwmgr->device, 231, AMD_THERMAL_IRQ_LAST,
-				tonga_dpm_set_interrupt_state,
-				info->call_back, info->context);
-
-	if (result)
-		return -EINVAL;
-
-	return 0;
-}
-
-bool tonga_check_smc_update_required_for_display_configuration(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	bool is_update_required = false;
-	struct cgs_display_info info = {0,0,NULL};
-
-	cgs_get_active_displays_info(hwmgr->device, &info);
-
-	if (data->display_timing.num_existing_displays != info.display_count)
-		is_update_required = true;
-/* TO DO NEED TO GET DEEP SLEEP CLOCK FROM DAL
-	if (phm_cap_enabled(hwmgr->hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_SclkDeepSleep)) {
-		cgs_get_min_clock_settings(hwmgr->device, &min_clocks);
-		if(min_clocks.engineClockInSR != data->display_timing.minClockInSR)
-			is_update_required = true;
-*/
-	return is_update_required;
-}
-
-static inline bool tonga_are_power_levels_equal(const struct tonga_performance_level *pl1,
-							   const struct tonga_performance_level *pl2)
-{
-	return ((pl1->memory_clock == pl2->memory_clock) &&
-		  (pl1->engine_clock == pl2->engine_clock) &&
-		  (pl1->pcie_gen == pl2->pcie_gen) &&
-		  (pl1->pcie_lane == pl2->pcie_lane));
-}
-
-int tonga_check_states_equal(struct pp_hwmgr *hwmgr, const struct pp_hw_power_state *pstate1, const struct pp_hw_power_state *pstate2, bool *equal)
-{
-	const struct tonga_power_state *psa = cast_const_phw_tonga_power_state(pstate1);
-	const struct tonga_power_state *psb = cast_const_phw_tonga_power_state(pstate2);
-	int i;
-
-	if (equal == NULL || psa == NULL || psb == NULL)
-		return -EINVAL;
-
-	/* If the two states don't even have the same number of performance levels they cannot be the same state. */
-	if (psa->performance_level_count != psb->performance_level_count) {
-		*equal = false;
-		return 0;
-	}
-
-	for (i = 0; i < psa->performance_level_count; i++) {
-		if (!tonga_are_power_levels_equal(&(psa->performance_levels[i]), &(psb->performance_levels[i]))) {
-			/* If we have found even one performance level pair that is different the states are different. */
-			*equal = false;
-			return 0;
-		}
-	}
-
-	/* If all performance levels are the same try to use the UVD clocks to break the tie.*/
-	*equal = ((psa->uvd_clocks.VCLK == psb->uvd_clocks.VCLK) && (psa->uvd_clocks.DCLK == psb->uvd_clocks.DCLK));
-	*equal &= ((psa->vce_clocks.EVCLK == psb->vce_clocks.EVCLK) && (psa->vce_clocks.ECCLK == psb->vce_clocks.ECCLK));
-	*equal &= (psa->sclk_threshold == psb->sclk_threshold);
-	*equal &= (psa->acp_clk == psb->acp_clk);
-
-	return 0;
-}
-
-static int tonga_set_fan_control_mode(struct pp_hwmgr *hwmgr, uint32_t mode)
-{
-	if (mode) {
-		/* stop auto-manage */
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_MicrocodeFanControl))
-			tonga_fan_ctrl_stop_smc_fan_control(hwmgr);
-		tonga_fan_ctrl_set_static_mode(hwmgr, mode);
-	} else
-		/* restart auto-manage */
-		tonga_fan_ctrl_reset_fan_speed_to_default(hwmgr);
-
-	return 0;
-}
-
-static int tonga_get_fan_control_mode(struct pp_hwmgr *hwmgr)
-{
-	if (hwmgr->fan_ctrl_is_in_default_mode)
-		return hwmgr->fan_ctrl_default_mode;
-	else
-		return PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-				CG_FDO_CTRL2, FDO_PWM_MODE);
-}
-
-static int tonga_force_clock_level(struct pp_hwmgr *hwmgr,
-                enum pp_clock_type type, int level)
-{
-        struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
- 
-        if (hwmgr->dpm_level != AMD_DPM_FORCED_LEVEL_MANUAL)
-                return -EINVAL;
- 
-        switch (type) {
-        case PP_SCLK:
-                if (!data->sclk_dpm_key_disabled)
-                        smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-                                        PPSMC_MSG_SCLKDPM_SetEnabledMask,
-                                        (1 << level));
-                break;
-        case PP_MCLK:
-                if (!data->mclk_dpm_key_disabled)
-                        smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-                                        PPSMC_MSG_MCLKDPM_SetEnabledMask,
-                                        (1 << level));
-                break;
-        case PP_PCIE:
-                if (!data->pcie_dpm_key_disabled)
-                        smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-                                        PPSMC_MSG_PCIeDPM_ForceLevel,
-                                        (1 << level));
-                break;
-        default:
-                break;
-        }
- 
-        return 0;
-}
-
-static int tonga_print_clock_levels(struct pp_hwmgr *hwmgr,
-		enum pp_clock_type type, char *buf)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	struct tonga_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	struct tonga_single_dpm_table *pcie_table = &(data->dpm_table.pcie_speed_table);
-	int i, now, size = 0;
-	uint32_t clock, pcie_speed;
-
-	switch (type) {
-	case PP_SCLK:
-		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetSclkFrequency);
-		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-		for (i = 0; i < sclk_table->count; i++) {
-			if (clock > sclk_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < sclk_table->count; i++)
-			size += sprintf(buf + size, "%d: %uMhz %s\n",
-					i, sclk_table->dpm_levels[i].value / 100,
-					(i == now) ? "*" : "");
-		break;
-	case PP_MCLK:
-		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetMclkFrequency);
-		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
-
-		for (i = 0; i < mclk_table->count; i++) {
-			if (clock > mclk_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < mclk_table->count; i++)
-			size += sprintf(buf + size, "%d: %uMhz %s\n",
-					i, mclk_table->dpm_levels[i].value / 100,
-					(i == now) ? "*" : "");
-		break;
-	case PP_PCIE:
-		pcie_speed = tonga_get_current_pcie_speed(hwmgr);
-		for (i = 0; i < pcie_table->count; i++) {
-			if (pcie_speed != pcie_table->dpm_levels[i].value)
-				continue;
-			break;
-		}
-		now = i;
-
-		for (i = 0; i < pcie_table->count; i++)
-			size += sprintf(buf + size, "%d: %s %s\n", i,
-					(pcie_table->dpm_levels[i].value == 0) ? "2.5GB, x8" :
-					(pcie_table->dpm_levels[i].value == 1) ? "5.0GB, x16" :
-					(pcie_table->dpm_levels[i].value == 2) ? "8.0GB, x16" : "",
-					(i == now) ? "*" : "");
-		break;
-	default:
-		break;
-	}
-	return size;
-}
-
-static int tonga_get_sclk_od(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-	struct tonga_single_dpm_table *golden_sclk_table =
-			&(data->golden_dpm_table.sclk_table);
-	int value;
-
-	value = (sclk_table->dpm_levels[sclk_table->count - 1].value -
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value) *
-			100 /
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
-
-	return value;
-}
-
-static int tonga_set_sclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_single_dpm_table *golden_sclk_table =
-			&(data->golden_dpm_table.sclk_table);
-	struct pp_power_state  *ps;
-	struct tonga_power_state  *tonga_ps;
-
-	if (value > 20)
-		value = 20;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	tonga_ps = cast_phw_tonga_power_state(&ps->hardware);
-
-	tonga_ps->performance_levels[tonga_ps->performance_level_count - 1].engine_clock =
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value *
-			value / 100 +
-			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
-
-	return 0;
-}
-
-static int tonga_get_mclk_od(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-	struct tonga_single_dpm_table *golden_mclk_table =
-			&(data->golden_dpm_table.mclk_table);
-	int value;
-
-	value = (mclk_table->dpm_levels[mclk_table->count - 1].value -
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value) *
-			100 /
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
-
-	return value;
-}
-
-static int tonga_set_mclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_single_dpm_table *golden_mclk_table =
-			&(data->golden_dpm_table.mclk_table);
-	struct pp_power_state  *ps;
-	struct tonga_power_state  *tonga_ps;
-
-	if (value > 20)
-		value = 20;
-
-	ps = hwmgr->request_ps;
-
-	if (ps == NULL)
-		return -EINVAL;
-
-	tonga_ps = cast_phw_tonga_power_state(&ps->hardware);
-
-	tonga_ps->performance_levels[tonga_ps->performance_level_count - 1].memory_clock =
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value *
-			value / 100 +
-			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
-
-	return 0;
-}
-
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-static int tonga_populate_requested_graphic_levels(struct pp_hwmgr *hwmgr,
-		struct pp_profile *request)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_dpm_table *dpm_table = &(data->dpm_table);
-	struct SMU72_Discrete_GraphicsLevel *levels =
-			data->smc_state_table.GraphicsLevel;
-	uint32_t array = data->dpm_table_start +
-			offsetof(SMU72_Discrete_DpmTable, GraphicsLevel);
-	uint32_t array_size = sizeof(struct SMU72_Discrete_GraphicsLevel) *
-			SMU72_MAX_LEVELS_GRAPHICS;
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		levels[i].ActivityLevel =
-				cpu_to_be16(request->activity_threshold);
-		levels[i].EnabledForActivity = 1;
-		levels[i].UpHyst = request->up_hyst;
-		levels[i].DownHyst = request->down_hyst;
-	}
-
-	return tonga_copy_bytes_to_smc(hwmgr->smumgr, array, (uint8_t *)levels,
-				array_size, data->sram_end);
-}
-
-static void tonga_find_min_clock_masks(struct pp_hwmgr *hwmgr,
-		uint32_t *sclk_mask, uint32_t *mclk_mask,
-		uint32_t min_sclk, uint32_t min_mclk)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_dpm_table *dpm_table = &(data->dpm_table);
-	uint32_t i;
-
-	for (i = 0; i < dpm_table->sclk_table.count; i++) {
-		if (dpm_table->sclk_table.dpm_levels[i].enabled &&
-			dpm_table->sclk_table.dpm_levels[i].value >= min_sclk)
-			*sclk_mask |= 1 << i;
-	}
-
-	for (i = 0; i < dpm_table->mclk_table.count; i++) {
-		if (dpm_table->mclk_table.dpm_levels[i].enabled &&
-			dpm_table->mclk_table.dpm_levels[i].value >= min_mclk)
-			*mclk_mask |= 1 << i;
-	}
-}
-
-static int tonga_set_power_profile_state(struct pp_hwmgr *hwmgr,
-		struct pp_profile *request)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	int tmp_result, result = 0;
-	uint32_t sclk_mask = 0, mclk_mask = 0;
-
-	if (hwmgr->dpm_level != AMD_DPM_FORCED_LEVEL_AUTO)
-		return -EINVAL;
-
-	tmp_result = tonga_freeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE(!tmp_result,
-			"Failed to freeze SCLK MCLK DPM!",
-			result = tmp_result);
-
-	tmp_result = tonga_populate_requested_graphic_levels(hwmgr, request);
-	PP_ASSERT_WITH_CODE(!tmp_result,
-			"Failed to populate requested graphic levels!",
-			result = tmp_result);
-
-	tmp_result = tonga_unfreeze_sclk_mclk_dpm(hwmgr);
-	PP_ASSERT_WITH_CODE(!tmp_result,
-			"Failed to unfreeze SCLK MCLK DPM!",
-			result = tmp_result);
-
-	tonga_find_min_clock_masks(hwmgr, &sclk_mask, &mclk_mask,
-			request->min_sclk, request->min_mclk);
-
-	if (sclk_mask) {
-		if (!data->sclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_SCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.
-				sclk_dpm_enable_mask &
-				sclk_mask);
-	}
-
-	if (mclk_mask) {
-		if (!data->mclk_dpm_key_disabled)
-			smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_MCLKDPM_SetEnabledMask,
-				data->dpm_level_enable_mask.
-				mclk_dpm_enable_mask &
-				mclk_mask);
-	}
-
-	return result;
-}
-#endif
-
-static const struct pp_hwmgr_func tonga_hwmgr_funcs = {
-	.backend_init = &tonga_hwmgr_backend_init,
-	.backend_fini = &tonga_hwmgr_backend_fini,
-	.asic_setup = &tonga_setup_asic_task,
-	.dynamic_state_management_enable = &tonga_enable_dpm_tasks,
-	.dynamic_state_management_disable = &tonga_disable_dpm_tasks,
-	.apply_state_adjust_rules = tonga_apply_state_adjust_rules,
-	.force_dpm_level = &tonga_force_dpm_level,
-	.power_state_set = tonga_set_power_state_tasks,
-	.get_power_state_size = tonga_get_power_state_size,
-	.get_mclk = tonga_dpm_get_mclk,
-	.get_sclk = tonga_dpm_get_sclk,
-	.patch_boot_state = tonga_dpm_patch_boot_state,
-	.get_pp_table_entry = tonga_get_pp_table_entry,
-	.get_num_of_pp_table_entries = get_number_of_powerplay_table_entries_v1_0,
-	.print_current_perforce_level = tonga_print_current_perforce_level,
-	.powerdown_uvd = tonga_phm_powerdown_uvd,
-	.powergate_uvd = tonga_phm_powergate_uvd,
-	.powergate_vce = tonga_phm_powergate_vce,
-	.disable_clock_power_gating = tonga_phm_disable_clock_power_gating,
-	.update_clock_gatings = tonga_phm_update_clock_gatings,
-	.notify_smc_display_config_after_ps_adjustment = tonga_notify_smc_display_config_after_ps_adjustment,
-	.display_config_changed = tonga_display_configuration_changed_task,
-	.set_max_fan_pwm_output = tonga_set_max_fan_pwm_output,
-	.set_max_fan_rpm_output = tonga_set_max_fan_rpm_output,
-	.get_temperature = tonga_thermal_get_temperature,
-	.stop_thermal_controller = tonga_thermal_stop_thermal_controller,
-	.get_fan_speed_info = tonga_fan_ctrl_get_fan_speed_info,
-	.get_fan_speed_percent = tonga_fan_ctrl_get_fan_speed_percent,
-	.set_fan_speed_percent = tonga_fan_ctrl_set_fan_speed_percent,
-	.reset_fan_speed_to_default = tonga_fan_ctrl_reset_fan_speed_to_default,
-	.get_fan_speed_rpm = tonga_fan_ctrl_get_fan_speed_rpm,
-	.set_fan_speed_rpm = tonga_fan_ctrl_set_fan_speed_rpm,
-	.uninitialize_thermal_controller = tonga_thermal_ctrl_uninitialize_thermal_controller,
-	.register_internal_thermal_interrupt = tonga_register_internal_thermal_interrupt,
-	.check_smc_update_required_for_display_configuration = tonga_check_smc_update_required_for_display_configuration,
-	.check_states_equal = tonga_check_states_equal,
-	.set_fan_control_mode = tonga_set_fan_control_mode,
-	.get_fan_control_mode = tonga_get_fan_control_mode,
-	.force_clock_level = tonga_force_clock_level,
-	.print_clock_levels = tonga_print_clock_levels,
-	.get_sclk_od = tonga_get_sclk_od,
-	.set_sclk_od = tonga_set_sclk_od,
-	.get_mclk_od = tonga_get_mclk_od,
-	.set_mclk_od = tonga_set_mclk_od,
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
-	.set_power_profile_state = tonga_set_power_profile_state,
-#endif	
-};
-
-int tonga_hwmgr_init(struct pp_hwmgr *hwmgr)
-{
-	hwmgr->hwmgr_func = &tonga_hwmgr_funcs;
-	hwmgr->pptable_func = &pptable_v1_0_funcs;
-	pp_tonga_thermal_initialize(hwmgr);
-	return 0;
-}
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.h b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.h
deleted file mode 100644
index fcad942..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.h
+++ /dev/null
@@ -1,402 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-#ifndef TONGA_HWMGR_H
-#define TONGA_HWMGR_H
-
-#include "hwmgr.h"
-#include "smu72_discrete.h"
-#include "ppatomctrl.h"
-#include "ppinterrupt.h"
-#include "tonga_powertune.h"
-#include "pp_endian.h"
-
-#define TONGA_MAX_HARDWARE_POWERLEVELS 2
-#define TONGA_DYNCLK_NUMBER_OF_TREND_COEFFICIENTS 15
-
-struct tonga_performance_level {
-	uint32_t	memory_clock;
-	uint32_t	engine_clock;
-	uint16_t    pcie_gen;
-	uint16_t    pcie_lane;
-};
-
-struct _phw_tonga_bacos {
-	uint32_t                          best_match;
-	uint32_t                          baco_flags;
-	struct tonga_performance_level		  performance_level;
-};
-typedef struct _phw_tonga_bacos phw_tonga_bacos;
-
-struct _phw_tonga_uvd_clocks {
-	uint32_t   VCLK;
-	uint32_t   DCLK;
-};
-
-typedef struct _phw_tonga_uvd_clocks phw_tonga_uvd_clocks;
-
-struct _phw_tonga_vce_clocks {
-	uint32_t   EVCLK;
-	uint32_t   ECCLK;
-};
-
-typedef struct _phw_tonga_vce_clocks phw_tonga_vce_clocks;
-
-struct tonga_power_state {
-	uint32_t                    magic;
-	phw_tonga_uvd_clocks        uvd_clocks;
-	phw_tonga_vce_clocks        vce_clocks;
-	uint32_t                    sam_clk;
-	uint32_t                    acp_clk;
-	uint16_t                    performance_level_count;
-	bool                        dc_compatible;
-	uint32_t                    sclk_threshold;
-	struct tonga_performance_level performance_levels[TONGA_MAX_HARDWARE_POWERLEVELS];
-};
-
-struct _phw_tonga_dpm_level {
-	bool		enabled;
-	uint32_t    value;
-	uint32_t    param1;
-};
-typedef struct _phw_tonga_dpm_level phw_tonga_dpm_level;
-
-#define TONGA_MAX_DEEPSLEEP_DIVIDER_ID 5
-#define MAX_REGULAR_DPM_NUMBER 8
-#define TONGA_MINIMUM_ENGINE_CLOCK 2500
-
-struct tonga_single_dpm_table {
-	uint32_t count;
-	phw_tonga_dpm_level dpm_levels[MAX_REGULAR_DPM_NUMBER];
-};
-
-struct tonga_dpm_table {
-	struct tonga_single_dpm_table  sclk_table;
-	struct tonga_single_dpm_table  mclk_table;
-	struct tonga_single_dpm_table  pcie_speed_table;
-	struct tonga_single_dpm_table  vddc_table;
-	struct tonga_single_dpm_table  vdd_gfx_table;
-	struct tonga_single_dpm_table  vdd_ci_table;
-	struct tonga_single_dpm_table  mvdd_table;
-};
-typedef struct _phw_tonga_dpm_table phw_tonga_dpm_table;
-
-
-struct _phw_tonga_clock_regisiters {
-	uint32_t  vCG_SPLL_FUNC_CNTL;
-	uint32_t  vCG_SPLL_FUNC_CNTL_2;
-	uint32_t  vCG_SPLL_FUNC_CNTL_3;
-	uint32_t  vCG_SPLL_FUNC_CNTL_4;
-	uint32_t  vCG_SPLL_SPREAD_SPECTRUM;
-	uint32_t  vCG_SPLL_SPREAD_SPECTRUM_2;
-	uint32_t  vDLL_CNTL;
-	uint32_t  vMCLK_PWRMGT_CNTL;
-	uint32_t  vMPLL_AD_FUNC_CNTL;
-	uint32_t  vMPLL_DQ_FUNC_CNTL;
-	uint32_t  vMPLL_FUNC_CNTL;
-	uint32_t  vMPLL_FUNC_CNTL_1;
-	uint32_t  vMPLL_FUNC_CNTL_2;
-	uint32_t  vMPLL_SS1;
-	uint32_t  vMPLL_SS2;
-};
-typedef struct _phw_tonga_clock_regisiters phw_tonga_clock_registers;
-
-struct _phw_tonga_voltage_smio_registers {
-	uint32_t vs0_vid_lower_smio_cntl;
-};
-typedef struct _phw_tonga_voltage_smio_registers phw_tonga_voltage_smio_registers;
-
-
-struct _phw_tonga_mc_reg_entry {
-	uint32_t mclk_max;
-	uint32_t mc_data[SMU72_DISCRETE_MC_REGISTER_ARRAY_SIZE];
-};
-typedef struct _phw_tonga_mc_reg_entry phw_tonga_mc_reg_entry;
-
-struct _phw_tonga_mc_reg_table {
-	uint8_t   last;               /* number of registers*/
-	uint8_t   num_entries;        /* number of entries in mc_reg_table_entry used*/
-	uint16_t  validflag;          /* indicate the corresponding register is valid or not. 1: valid, 0: invalid. bit0->address[0], bit1->address[1], etc.*/
-	phw_tonga_mc_reg_entry    mc_reg_table_entry[MAX_AC_TIMING_ENTRIES];
-	SMU72_Discrete_MCRegisterAddress mc_reg_address[SMU72_DISCRETE_MC_REGISTER_ARRAY_SIZE];
-};
-typedef struct _phw_tonga_mc_reg_table phw_tonga_mc_reg_table;
-
-#define DISABLE_MC_LOADMICROCODE   1
-#define DISABLE_MC_CFGPROGRAMMING  2
-
-/*Ultra Low Voltage parameter structure */
-struct _phw_tonga_ulv_parm{
-	bool	ulv_supported;
-	uint32_t   ch_ulv_parameter;
-	uint32_t   ulv_volt_change_delay;
-	struct tonga_performance_level   ulv_power_level;
-};
-typedef struct _phw_tonga_ulv_parm phw_tonga_ulv_parm;
-
-#define TONGA_MAX_LEAKAGE_COUNT  8
-
-struct _phw_tonga_leakage_voltage {
-	uint16_t  count;
-	uint16_t  leakage_id[TONGA_MAX_LEAKAGE_COUNT];
-	uint16_t  actual_voltage[TONGA_MAX_LEAKAGE_COUNT];
-};
-typedef struct _phw_tonga_leakage_voltage phw_tonga_leakage_voltage;
-
-struct _phw_tonga_display_timing {
-	uint32_t min_clock_insr;
-	uint32_t num_existing_displays;
-};
-typedef struct _phw_tonga_display_timing phw_tonga_display_timing;
-
-struct _phw_tonga_dpmlevel_enable_mask {
-	uint32_t uvd_dpm_enable_mask;
-	uint32_t vce_dpm_enable_mask;
-	uint32_t acp_dpm_enable_mask;
-	uint32_t samu_dpm_enable_mask;
-	uint32_t sclk_dpm_enable_mask;
-	uint32_t mclk_dpm_enable_mask;
-	uint32_t pcie_dpm_enable_mask;
-};
-typedef struct _phw_tonga_dpmlevel_enable_mask phw_tonga_dpmlevel_enable_mask;
-
-struct _phw_tonga_pcie_perf_range {
-	uint16_t max;
-	uint16_t min;
-};
-typedef struct _phw_tonga_pcie_perf_range phw_tonga_pcie_perf_range;
-
-struct _phw_tonga_vbios_boot_state {
-	uint16_t					mvdd_bootup_value;
-	uint16_t					vddc_bootup_value;
-	uint16_t					vddci_bootup_value;
-	uint16_t					vddgfx_bootup_value;
-	uint32_t					sclk_bootup_value;
-	uint32_t					mclk_bootup_value;
-	uint16_t					pcie_gen_bootup_value;
-	uint16_t					pcie_lane_bootup_value;
-};
-typedef struct _phw_tonga_vbios_boot_state phw_tonga_vbios_boot_state;
-
-#define DPMTABLE_OD_UPDATE_SCLK     0x00000001
-#define DPMTABLE_OD_UPDATE_MCLK     0x00000002
-#define DPMTABLE_UPDATE_SCLK        0x00000004
-#define DPMTABLE_UPDATE_MCLK        0x00000008
-
-/* We need to review which fields are needed. */
-/* This is mostly a copy of the RV7xx/Evergreen structure which is close, but not identical to the N.Islands one. */
-struct tonga_hwmgr {
-	struct tonga_dpm_table               dpm_table;
-	struct tonga_dpm_table               golden_dpm_table;
-
-	uint32_t                           voting_rights_clients0;
-	uint32_t                           voting_rights_clients1;
-	uint32_t                           voting_rights_clients2;
-	uint32_t                           voting_rights_clients3;
-	uint32_t                           voting_rights_clients4;
-	uint32_t                           voting_rights_clients5;
-	uint32_t                           voting_rights_clients6;
-	uint32_t                           voting_rights_clients7;
-	uint32_t                           static_screen_threshold_unit;
-	uint32_t                           static_screen_threshold;
-	uint32_t                           voltage_control;
-	uint32_t                           vdd_gfx_control;
-
-	uint32_t                           vddc_vddci_delta;
-	uint32_t                           vddc_vddgfx_delta;
-
-	struct pp_interrupt_registration_info    internal_high_thermal_interrupt_info;
-	struct pp_interrupt_registration_info    internal_low_thermal_interrupt_info;
-	struct pp_interrupt_registration_info    smc_to_host_interrupt_info;
-	uint32_t                          active_auto_throttle_sources;
-
-	struct pp_interrupt_registration_info    external_throttle_interrupt;
-	irq_handler_func_t             external_throttle_callback;
-	void                             *external_throttle_context;
-
-	struct pp_interrupt_registration_info    ctf_interrupt_info;
-	irq_handler_func_t             ctf_callback;
-	void                             *ctf_context;
-
-	phw_tonga_clock_registers	  clock_registers;
-	phw_tonga_voltage_smio_registers  voltage_smio_registers;
-
-	bool	is_memory_GDDR5;
-	uint16_t                          acpi_vddc;
-	bool	pspp_notify_required;        /* Flag to indicate if PSPP notification to SBIOS is required */
-	uint16_t                          force_pcie_gen;            /* The forced PCI-E speed if not 0xffff */
-	uint16_t                          acpi_pcie_gen;             /* The PCI-E speed at ACPI time */
-	uint32_t                           pcie_gen_cap;             /* The PCI-E speed capabilities bitmap from CAIL */
-	uint32_t                           pcie_lane_cap;            /* The PCI-E lane capabilities bitmap from CAIL */
-	uint32_t                           pcie_spc_cap;             /* Symbol Per Clock Capabilities from registry */
-	phw_tonga_leakage_voltage	vddc_leakage;            /* The Leakage VDDC supported (based on leakage ID).*/
-	phw_tonga_leakage_voltage	vddcgfx_leakage;         /* The Leakage VDDC supported (based on leakage ID). */
-	phw_tonga_leakage_voltage	vddci_leakage;           /* The Leakage VDDCI supported (based on leakage ID). */
-
-	uint32_t                           mvdd_control;
-	uint32_t                           vddc_mask_low;
-	uint32_t                           mvdd_mask_low;
-	uint16_t                          max_vddc_in_pp_table;        /* the maximum VDDC value in the powerplay table*/
-	uint16_t                          min_vddc_in_pp_table;
-	uint16_t                          max_vddci_in_pp_table;       /* the maximum VDDCI value in the powerplay table */
-	uint16_t                          min_vddci_in_pp_table;
-	uint32_t                           mclk_strobe_mode_threshold;
-	uint32_t                           mclk_stutter_mode_threshold;
-	uint32_t                           mclk_edc_enable_threshold;
-	uint32_t                           mclk_edc_wr_enable_threshold;
-	bool	is_uvd_enabled;
-	bool	is_xdma_enabled;
-	phw_tonga_vbios_boot_state      vbios_boot_state;
-
-	bool                         battery_state;
-	bool                         is_tlu_enabled;
-	bool                         pcie_performance_request;
-
-	/* -------------- SMC SRAM Address of firmware header tables ----------------*/
-	uint32_t                           sram_end;                 /* The first address after the SMC SRAM. */
-	uint32_t                           dpm_table_start;          /* The start of the dpm table in the SMC SRAM. */
-	uint32_t                           soft_regs_start;          /* The start of the soft registers in the SMC SRAM. */
-	uint32_t                           mc_reg_table_start;       /* The start of the mc register table in the SMC SRAM. */
-	uint32_t                           fan_table_start;          /* The start of the fan table in the SMC SRAM. */
-	uint32_t                           arb_table_start;          /* The start of the ARB setting table in the SMC SRAM. */
-	SMU72_Discrete_DpmTable         smc_state_table;             /* The carbon copy of the SMC state table. */
-	SMU72_Discrete_MCRegisters      mc_reg_table;
-	SMU72_Discrete_Ulv              ulv_setting;                 /* The carbon copy of ULV setting. */
-	/* -------------- Stuff originally coming from Evergreen --------------------*/
-	phw_tonga_mc_reg_table			tonga_mc_reg_table;
-	uint32_t                         vdd_ci_control;
-	pp_atomctrl_voltage_table        vddc_voltage_table;
-	pp_atomctrl_voltage_table        vddci_voltage_table;
-	pp_atomctrl_voltage_table        vddgfx_voltage_table;
-	pp_atomctrl_voltage_table        mvdd_voltage_table;
-
-	uint32_t                           mgcg_cgtt_local2;
-	uint32_t                           mgcg_cgtt_local3;
-	uint32_t                           gpio_debug;
-	uint32_t							mc_micro_code_feature;
-	uint32_t							highest_mclk;
-	uint16_t                          acpi_vdd_ci;
-	uint8_t                           mvdd_high_index;
-	uint8_t                           mvdd_low_index;
-	bool                         dll_defaule_on;
-	bool                         performance_request_registered;
-
-
-	/* ----------------- Low Power Features ---------------------*/
-	phw_tonga_bacos					bacos;
-	phw_tonga_ulv_parm              ulv;
-	/* ----------------- CAC Stuff ---------------------*/
-	uint32_t					cac_table_start;
-	bool                         cac_configuration_required;    /* TRUE if PP_CACConfigurationRequired == 1 */
-	bool                         driver_calculate_cac_leakage;  /* TRUE if PP_DriverCalculateCACLeakage == 1 */
-	bool                         cac_enabled;
-	/* ----------------- DPM2 Parameters ---------------------*/
-	uint32_t					power_containment_features;
-	bool                         enable_bapm_feature;
-	bool                         enable_tdc_limit_feature;
-	bool                         enable_pkg_pwr_tracking_feature;
-	bool                         disable_uvd_power_tune_feature;
-	struct tonga_pt_defaults           *power_tune_defaults;
-	SMU72_Discrete_PmFuses           power_tune_table;
-	uint32_t                           dte_tj_offset;             /* Fudge factor in DPM table to correct HW DTE errors */
-	uint32_t                           fast_watermark_threshold;      /* use fast watermark if clock is equal or above this. In percentage of the target high sclk. */
-
-
-	bool                           enable_dte_feature;
-
-
-	/* ----------------- Phase Shedding ---------------------*/
-	bool                         vddc_phase_shed_control;
-	/* --------------------- DI/DT --------------------------*/
-	phw_tonga_display_timing       display_timing;
-	/* --------- ReadRegistry data for memory and engine clock margins ---- */
-	uint32_t                           engine_clock_data;
-	uint32_t                           memory_clock_data;
-	/* -------- Thermal Temperature Setting --------------*/
-	phw_tonga_dpmlevel_enable_mask     dpm_level_enable_mask;
-	uint32_t                           need_update_smu7_dpm_table;
-	uint32_t                           sclk_dpm_key_disabled;
-	uint32_t                           mclk_dpm_key_disabled;
-	uint32_t                           pcie_dpm_key_disabled;
-	uint32_t                           min_engine_clocks; /* used to store the previous dal min sclock */
-	phw_tonga_pcie_perf_range       pcie_gen_performance;
-	phw_tonga_pcie_perf_range       pcie_lane_performance;
-	phw_tonga_pcie_perf_range       pcie_gen_power_saving;
-	phw_tonga_pcie_perf_range       pcie_lane_power_saving;
-	bool                            use_pcie_performance_levels;
-	bool                            use_pcie_power_saving_levels;
-	uint32_t                           activity_target[SMU72_MAX_LEVELS_GRAPHICS]; /* percentage value from 0-100, default 50 */
-	uint32_t                           mclk_activity_target;
-	uint32_t                           low_sclk_interrupt_threshold;
-	uint32_t                           last_mclk_dpm_enable_mask;
-	bool								uvd_enabled;
-	uint32_t                           pcc_monitor_enabled;
-
-	/* --------- Power Gating States ------------*/
-	bool                           uvd_power_gated;  /* 1: gated, 0:not gated */
-	bool                           vce_power_gated;  /* 1: gated, 0:not gated */
-	bool                           samu_power_gated; /* 1: gated, 0:not gated */
-	bool                           acp_power_gated;  /* 1: gated, 0:not gated */
-	bool                           pg_acp_init;
-};
-
-typedef struct tonga_hwmgr tonga_hwmgr;
-
-#define TONGA_DPM2_NEAR_TDP_DEC          10
-#define TONGA_DPM2_ABOVE_SAFE_INC        5
-#define TONGA_DPM2_BELOW_SAFE_INC        20
-
-#define TONGA_DPM2_LTA_WINDOW_SIZE       7  /* Log2 of the LTA window size (l2numWin_TDP). Eg. If LTA windows size is 128, then this value should be Log2(128) = 7. */
-
-#define TONGA_DPM2_LTS_TRUNCATE          0
-
-#define TONGA_DPM2_TDP_SAFE_LIMIT_PERCENT            80  /* Maximum 100 */
-
-#define TONGA_DPM2_MAXPS_PERCENT_H                   90  /* Maximum 0xFF */
-#define TONGA_DPM2_MAXPS_PERCENT_M                   90  /* Maximum 0xFF */
-
-#define TONGA_DPM2_PWREFFICIENCYRATIO_MARGIN         50
-
-#define TONGA_DPM2_SQ_RAMP_MAX_POWER                 0x3FFF
-#define TONGA_DPM2_SQ_RAMP_MIN_POWER                 0x12
-#define TONGA_DPM2_SQ_RAMP_MAX_POWER_DELTA           0x15
-#define TONGA_DPM2_SQ_RAMP_SHORT_TERM_INTERVAL_SIZE  0x1E
-#define TONGA_DPM2_SQ_RAMP_LONG_TERM_INTERVAL_RATIO  0xF
-
-#define TONGA_VOLTAGE_CONTROL_NONE                   0x0
-#define TONGA_VOLTAGE_CONTROL_BY_GPIO                0x1
-#define TONGA_VOLTAGE_CONTROL_BY_SVID2               0x2
-#define TONGA_VOLTAGE_CONTROL_MERGED                 0x3
-
-#define TONGA_Q88_FORMAT_CONVERSION_UNIT             256 /*To convert to Q8.8 format for firmware */
-
-#define TONGA_UNUSED_GPIO_PIN                        0x7F
-
-int tonga_hwmgr_init(struct pp_hwmgr *hwmgr);
-int tonga_update_vce_dpm(struct pp_hwmgr *hwmgr, const void *input);
-int tonga_update_uvd_dpm(struct pp_hwmgr *hwmgr, bool bgate);
-int tonga_enable_disable_uvd_dpm(struct pp_hwmgr *hwmgr, bool enable);
-int tonga_enable_disable_vce_dpm(struct pp_hwmgr *hwmgr, bool enable);
-uint32_t tonga_get_xclk(struct pp_hwmgr *hwmgr);
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_powertune.c b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_powertune.c
deleted file mode 100644
index 24d9a05..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_powertune.c
+++ /dev/null
@@ -1,495 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#include "hwmgr.h"
-#include "smumgr.h"
-#include "tonga_hwmgr.h"
-#include "tonga_powertune.h"
-#include "tonga_smumgr.h"
-#include "smu72_discrete.h"
-#include "pp_debug.h"
-#include "tonga_ppsmc.h"
-
-#define VOLTAGE_SCALE  4
-#define POWERTUNE_DEFAULT_SET_MAX    1
-
-struct tonga_pt_defaults tonga_power_tune_data_set_array[POWERTUNE_DEFAULT_SET_MAX] = {
-/*    sviLoadLIneEn, SviLoadLineVddC, TDC_VDDC_ThrottleReleaseLimitPerc, TDC_MAWt, TdcWaterfallCtl, DTEAmbientTempBase, DisplayCac, BAPM_TEMP_GRADIENT */
-	{1,               0xF,             0xFD,                               0x19,     5,               45,                  0,          0xB0000,
-	{0x79,  0x253, 0x25D, 0xAE,  0x72,  0x80,    0x83,  0x86,  0x6F,  0xC8,    0xC9,  0xC9,  0x2F,  0x4D, 0x61},
-	{0x17C, 0x172, 0x180, 0x1BC, 0x1B3, 0x1BD, 0x206, 0x200, 0x203, 0x25D, 0x25A, 0x255, 0x2C3, 0x2C5, 0x2B4 } },
-};
-
-void tonga_initialize_power_tune_defaults(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *tonga_hwmgr = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct  phm_ppt_v1_information *table_info =
-			(struct  phm_ppt_v1_information *)(hwmgr->pptable);
-	uint32_t tmp = 0;
-
-	if (table_info &&
-			table_info->cac_dtp_table->usPowerTuneDataSetID <= POWERTUNE_DEFAULT_SET_MAX &&
-			table_info->cac_dtp_table->usPowerTuneDataSetID)
-		tonga_hwmgr->power_tune_defaults =
-				&tonga_power_tune_data_set_array
-				[table_info->cac_dtp_table->usPowerTuneDataSetID - 1];
-	else
-		tonga_hwmgr->power_tune_defaults = &tonga_power_tune_data_set_array[0];
-
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_SQRamping);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_DBRamping);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_TDRamping);
-	phm_cap_unset(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_TCPRamping);
-
-	tonga_hwmgr->dte_tj_offset = tmp;
-
-	if (!tmp) {
-		phm_cap_set(hwmgr->platform_descriptor.platformCaps,
-				PHM_PlatformCaps_CAC);
-
-		tonga_hwmgr->fast_watermark_threshold = 100;
-
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-					PHM_PlatformCaps_PowerContainment)) {
-			tmp = 1;
-			tonga_hwmgr->enable_dte_feature = tmp ? false : true;
-			tonga_hwmgr->enable_tdc_limit_feature = tmp ? true : false;
-			tonga_hwmgr->enable_pkg_pwr_tracking_feature = tmp ? true : false;
-		}
-	}
-}
-
-
-int tonga_populate_bapm_parameters_in_dpm_table(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct tonga_pt_defaults *defaults = data->power_tune_defaults;
-	SMU72_Discrete_DpmTable  *dpm_table = &(data->smc_state_table);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_cac_tdp_table *cac_dtp_table = table_info->cac_dtp_table;
-	int  i, j, k;
-	uint16_t *pdef1;
-	uint16_t *pdef2;
-
-
-	/* TDP number of fraction bits are changed from 8 to 7 for Fiji
-	 * as requested by SMC team
-	 */
-	dpm_table->DefaultTdp = PP_HOST_TO_SMC_US(
-			(uint16_t)(cac_dtp_table->usTDP * 256));
-	dpm_table->TargetTdp = PP_HOST_TO_SMC_US(
-			(uint16_t)(cac_dtp_table->usConfigurableTDP * 256));
-
-	PP_ASSERT_WITH_CODE(cac_dtp_table->usTargetOperatingTemp <= 255,
-			"Target Operating Temp is out of Range!",
-			);
-
-	dpm_table->GpuTjMax = (uint8_t)(cac_dtp_table->usTargetOperatingTemp);
-	dpm_table->GpuTjHyst = 8;
-
-	dpm_table->DTEAmbientTempBase = defaults->dte_ambient_temp_base;
-
-	dpm_table->BAPM_TEMP_GRADIENT = PP_HOST_TO_SMC_UL(defaults->bamp_temp_gradient);
-	pdef1 = defaults->bapmti_r;
-	pdef2 = defaults->bapmti_rc;
-
-	for (i = 0; i < SMU72_DTE_ITERATIONS; i++) {
-		for (j = 0; j < SMU72_DTE_SOURCES; j++) {
-			for (k = 0; k < SMU72_DTE_SINKS; k++) {
-				dpm_table->BAPMTI_R[i][j][k] = PP_HOST_TO_SMC_US(*pdef1);
-				dpm_table->BAPMTI_RC[i][j][k] = PP_HOST_TO_SMC_US(*pdef2);
-				pdef1++;
-				pdef2++;
-			}
-		}
-	}
-
-	return 0;
-}
-
-static int tonga_populate_svi_load_line(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	const struct tonga_pt_defaults *defaults = data->power_tune_defaults;
-
-	data->power_tune_table.SviLoadLineEn = defaults->svi_load_line_en;
-	data->power_tune_table.SviLoadLineVddC = defaults->svi_load_line_vddC;
-	data->power_tune_table.SviLoadLineTrimVddC = 3;
-	data->power_tune_table.SviLoadLineOffsetVddC = 0;
-
-	return 0;
-}
-
-static int tonga_populate_tdc_limit(struct pp_hwmgr *hwmgr)
-{
-	uint16_t tdc_limit;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	const struct tonga_pt_defaults *defaults = data->power_tune_defaults;
-
-	/* TDC number of fraction bits are changed from 8 to 7
-	 * for Fiji as requested by SMC team
-	 */
-	tdc_limit = (uint16_t)(table_info->cac_dtp_table->usTDC * 256);
-	data->power_tune_table.TDC_VDDC_PkgLimit =
-			CONVERT_FROM_HOST_TO_SMC_US(tdc_limit);
-	data->power_tune_table.TDC_VDDC_ThrottleReleaseLimitPerc =
-			defaults->tdc_vddc_throttle_release_limit_perc;
-	data->power_tune_table.TDC_MAWt = defaults->tdc_mawt;
-
-	return 0;
-}
-
-static int tonga_populate_dw8(struct pp_hwmgr *hwmgr, uint32_t fuse_table_offset)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	const struct tonga_pt_defaults *defaults = data->power_tune_defaults;
-	uint32_t temp;
-
-	if (tonga_read_smc_sram_dword(hwmgr->smumgr,
-			fuse_table_offset +
-			offsetof(SMU72_Discrete_PmFuses, TdcWaterfallCtl),
-			(uint32_t *)&temp, data->sram_end))
-		PP_ASSERT_WITH_CODE(false,
-				"Attempt to read PmFuses.DW6 (SviLoadLineEn) from SMC Failed!",
-				return -EINVAL);
-	else
-		data->power_tune_table.TdcWaterfallCtl = defaults->tdc_waterfall_ctl;
-
-	return 0;
-}
-
-static int tonga_populate_temperature_scaler(struct pp_hwmgr *hwmgr)
-{
-	int i;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	/* Currently not used. Set all to zero. */
-	for (i = 0; i < 16; i++)
-		data->power_tune_table.LPMLTemperatureScaler[i] = 0;
-
-	return 0;
-}
-
-static int tonga_populate_fuzzy_fan(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	if ((hwmgr->thermal_controller.advanceFanControlParameters.
-			usFanOutputSensitivity & (1 << 15)) ||
-		(hwmgr->thermal_controller.advanceFanControlParameters.usFanOutputSensitivity == 0))
-		hwmgr->thermal_controller.advanceFanControlParameters.
-		usFanOutputSensitivity = hwmgr->thermal_controller.
-			advanceFanControlParameters.usDefaultFanOutputSensitivity;
-
-	data->power_tune_table.FuzzyFan_PwmSetDelta =
-			PP_HOST_TO_SMC_US(hwmgr->thermal_controller.
-					advanceFanControlParameters.usFanOutputSensitivity);
-	return 0;
-}
-
-static int tonga_populate_gnb_lpml(struct pp_hwmgr *hwmgr)
-{
-	int i;
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	/* Currently not used. Set all to zero. */
-	for (i = 0; i < 16; i++)
-		data->power_tune_table.GnbLPML[i] = 0;
-
-	return 0;
-}
-
-static int tonga_min_max_vgnb_lpml_id_from_bapm_vddc(struct pp_hwmgr *hwmgr)
-{
-	return 0;
-}
-
-static int tonga_populate_bapm_vddc_base_leakage_sidd(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	uint16_t hi_sidd = data->power_tune_table.BapmVddCBaseLeakageHiSidd;
-	uint16_t lo_sidd = data->power_tune_table.BapmVddCBaseLeakageLoSidd;
-	struct phm_cac_tdp_table *cac_table = table_info->cac_dtp_table;
-
-	hi_sidd = (uint16_t)(cac_table->usHighCACLeakage / 100 * 256);
-	lo_sidd = (uint16_t)(cac_table->usLowCACLeakage / 100 * 256);
-
-	data->power_tune_table.BapmVddCBaseLeakageHiSidd =
-			CONVERT_FROM_HOST_TO_SMC_US(hi_sidd);
-	data->power_tune_table.BapmVddCBaseLeakageLoSidd =
-			CONVERT_FROM_HOST_TO_SMC_US(lo_sidd);
-
-	return 0;
-}
-
-int tonga_populate_pm_fuses(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	uint32_t pm_fuse_table_offset;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		if (tonga_read_smc_sram_dword(hwmgr->smumgr,
-				SMU72_FIRMWARE_HEADER_LOCATION +
-				offsetof(SMU72_Firmware_Header, PmFuseTable),
-				&pm_fuse_table_offset, data->sram_end))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to get pm_fuse_table_offset Failed!",
-					return -EINVAL);
-
-		/* DW6 */
-		if (tonga_populate_svi_load_line(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate SviLoadLine Failed!",
-					return -EINVAL);
-		/* DW7 */
-		if (tonga_populate_tdc_limit(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate TDCLimit Failed!", return -EINVAL);
-		/* DW8 */
-		if (tonga_populate_dw8(hwmgr, pm_fuse_table_offset))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate TdcWaterfallCtl Failed !",
-					return -EINVAL);
-
-		/* DW9-DW12 */
-		if (tonga_populate_temperature_scaler(hwmgr) != 0)
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate LPMLTemperatureScaler Failed!",
-					return -EINVAL);
-
-		/* DW13-DW14 */
-		if (tonga_populate_fuzzy_fan(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate Fuzzy Fan Control parameters Failed!",
-					return -EINVAL);
-
-		/* DW15-DW18 */
-		if (tonga_populate_gnb_lpml(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate GnbLPML Failed!",
-					return -EINVAL);
-
-		/* DW19 */
-		if (tonga_min_max_vgnb_lpml_id_from_bapm_vddc(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate GnbLPML Min and Max Vid Failed!",
-					return -EINVAL);
-
-		/* DW20 */
-		if (tonga_populate_bapm_vddc_base_leakage_sidd(hwmgr))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to populate BapmVddCBaseLeakage Hi and Lo Sidd Failed!",
-					return -EINVAL);
-
-		if (tonga_copy_bytes_to_smc(hwmgr->smumgr, pm_fuse_table_offset,
-				(uint8_t *)&data->power_tune_table,
-				sizeof(struct SMU72_Discrete_PmFuses), data->sram_end))
-			PP_ASSERT_WITH_CODE(false,
-					"Attempt to download PmFuseTable Failed!",
-					return -EINVAL);
-	}
-	return 0;
-}
-
-int tonga_enable_smc_cac(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC)) {
-		int smc_result;
-
-		smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-				(uint16_t)(PPSMC_MSG_EnableCac));
-		PP_ASSERT_WITH_CODE((smc_result == 0),
-				"Failed to enable CAC in SMC.", result = -1);
-
-		data->cac_enabled = (smc_result == 0) ? true : false;
-	}
-	return result;
-}
-
-int tonga_disable_smc_cac(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_CAC) && data->cac_enabled) {
-		int smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-				(uint16_t)(PPSMC_MSG_DisableCac));
-		PP_ASSERT_WITH_CODE((smc_result == 0),
-				"Failed to disable CAC in SMC.", result = -1);
-
-		data->cac_enabled = false;
-	}
-	return result;
-}
-
-int tonga_set_power_limit(struct pp_hwmgr *hwmgr, uint32_t n)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-
-	if (data->power_containment_features &
-			POWERCONTAINMENT_FEATURE_PkgPwrLimit)
-		return smum_send_msg_to_smc_with_parameter(hwmgr->smumgr,
-				PPSMC_MSG_PkgPwrSetLimit, n);
-	return 0;
-}
-
-static int tonga_set_overdriver_target_tdp(struct pp_hwmgr *pHwMgr, uint32_t target_tdp)
-{
-	return smum_send_msg_to_smc_with_parameter(pHwMgr->smumgr,
-			PPSMC_MSG_OverDriveSetTargetTdp, target_tdp);
-}
-
-int tonga_enable_power_containment(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	int smc_result;
-	int result = 0;
-
-	data->power_containment_features = 0;
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		if (data->enable_dte_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_EnableDTE));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to enable DTE in SMC.", result = -1;);
-			if (smc_result == 0)
-				data->power_containment_features |= POWERCONTAINMENT_FEATURE_DTE;
-		}
-
-		if (data->enable_tdc_limit_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_TDCLimitEnable));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to enable TDCLimit in SMC.", result = -1;);
-			if (smc_result == 0)
-				data->power_containment_features |=
-						POWERCONTAINMENT_FEATURE_TDCLimit;
-		}
-
-		if (data->enable_pkg_pwr_tracking_feature) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_PkgPwrLimitEnable));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to enable PkgPwrTracking in SMC.", result = -1;);
-			if (smc_result == 0) {
-				struct phm_cac_tdp_table *cac_table =
-						table_info->cac_dtp_table;
-				uint32_t default_limit =
-					(uint32_t)(cac_table->usMaximumPowerDeliveryLimit * 256);
-
-				data->power_containment_features |=
-						POWERCONTAINMENT_FEATURE_PkgPwrLimit;
-
-				if (tonga_set_power_limit(hwmgr, default_limit))
-					printk(KERN_ERR "Failed to set Default Power Limit in SMC!");
-			}
-		}
-	}
-	return result;
-}
-
-int tonga_disable_power_containment(struct pp_hwmgr *hwmgr)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment) &&
-			data->power_containment_features) {
-		int smc_result;
-
-		if (data->power_containment_features &
-				POWERCONTAINMENT_FEATURE_TDCLimit) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_TDCLimitDisable));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to disable TDCLimit in SMC.",
-					result = smc_result);
-		}
-
-		if (data->power_containment_features &
-				POWERCONTAINMENT_FEATURE_DTE) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_DisableDTE));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to disable DTE in SMC.",
-					result = smc_result);
-		}
-
-		if (data->power_containment_features &
-				POWERCONTAINMENT_FEATURE_PkgPwrLimit) {
-			smc_result = smum_send_msg_to_smc(hwmgr->smumgr,
-					(uint16_t)(PPSMC_MSG_PkgPwrLimitDisable));
-			PP_ASSERT_WITH_CODE((smc_result == 0),
-					"Failed to disable PkgPwrTracking in SMC.",
-					result = smc_result);
-		}
-		data->power_containment_features = 0;
-	}
-
-	return result;
-}
-
-int tonga_power_control_set_level(struct pp_hwmgr *hwmgr)
-{
-	struct phm_ppt_v1_information *table_info =
-			(struct phm_ppt_v1_information *)(hwmgr->pptable);
-	struct phm_cac_tdp_table *cac_table = table_info->cac_dtp_table;
-	int adjust_percent, target_tdp;
-	int result = 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,
-			PHM_PlatformCaps_PowerContainment)) {
-		/* adjustment percentage has already been validated */
-		adjust_percent = hwmgr->platform_descriptor.TDPAdjustmentPolarity ?
-				hwmgr->platform_descriptor.TDPAdjustment :
-				(-1 * hwmgr->platform_descriptor.TDPAdjustment);
-		/* SMC requested that target_tdp to be 7 bit fraction in DPM table
-		 * but message to be 8 bit fraction for messages
-		 */
-		target_tdp = ((100 + adjust_percent) * (int)(cac_table->usTDP * 256)) / 100;
-		result = tonga_set_overdriver_target_tdp(hwmgr, (uint32_t)target_tdp);
-	}
-
-	return result;
-}
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_powertune.h b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_powertune.h
deleted file mode 100644
index c8bdb92..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_powertune.h
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef TONGA_POWERTUNE_H
-#define TONGA_POWERTUNE_H
-
-enum _phw_tonga_ptc_config_reg_type {
-	TONGA_CONFIGREG_MMR = 0,
-	TONGA_CONFIGREG_SMC_IND,
-	TONGA_CONFIGREG_DIDT_IND,
-	TONGA_CONFIGREG_CACHE,
-
-	TONGA_CONFIGREG_MAX
-};
-typedef enum _phw_tonga_ptc_config_reg_type phw_tonga_ptc_config_reg_type;
-
-/* PowerContainment Features */
-#define POWERCONTAINMENT_FEATURE_DTE             0x00000001
-
-
-/* PowerContainment Features */
-#define POWERCONTAINMENT_FEATURE_BAPM            0x00000001
-#define POWERCONTAINMENT_FEATURE_TDCLimit        0x00000002
-#define POWERCONTAINMENT_FEATURE_PkgPwrLimit     0x00000004
-
-struct tonga_pt_config_reg {
-	uint32_t                           Offset;
-	uint32_t                           Mask;
-	uint32_t                           Shift;
-	uint32_t                           Value;
-	phw_tonga_ptc_config_reg_type     Type;
-};
-
-struct tonga_pt_defaults {
-	uint8_t   svi_load_line_en;
-	uint8_t   svi_load_line_vddC;
-	uint8_t   tdc_vddc_throttle_release_limit_perc;
-	uint8_t   tdc_mawt;
-	uint8_t   tdc_waterfall_ctl;
-	uint8_t   dte_ambient_temp_base;
-	uint32_t  display_cac;
-	uint32_t  bamp_temp_gradient;
-	uint16_t  bapmti_r[SMU72_DTE_ITERATIONS * SMU72_DTE_SOURCES * SMU72_DTE_SINKS];
-	uint16_t  bapmti_rc[SMU72_DTE_ITERATIONS * SMU72_DTE_SOURCES * SMU72_DTE_SINKS];
-};
-
-
-
-void tonga_initialize_power_tune_defaults(struct pp_hwmgr *hwmgr);
-int tonga_populate_bapm_parameters_in_dpm_table(struct pp_hwmgr *hwmgr);
-int tonga_populate_pm_fuses(struct pp_hwmgr *hwmgr);
-int tonga_enable_smc_cac(struct pp_hwmgr *hwmgr);
-int tonga_disable_smc_cac(struct pp_hwmgr *hwmgr);
-int tonga_enable_power_containment(struct pp_hwmgr *hwmgr);
-int tonga_disable_power_containment(struct pp_hwmgr *hwmgr);
-int tonga_set_power_limit(struct pp_hwmgr *hwmgr, uint32_t n);
-int tonga_power_control_set_level(struct pp_hwmgr *hwmgr);
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_thermal.c b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_thermal.c
deleted file mode 100644
index 47ef1ca..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_thermal.c
+++ /dev/null
@@ -1,590 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-#include <asm/div64.h>
-#include "tonga_thermal.h"
-#include "tonga_hwmgr.h"
-#include "tonga_smumgr.h"
-#include "tonga_ppsmc.h"
-#include "smu/smu_7_1_2_d.h"
-#include "smu/smu_7_1_2_sh_mask.h"
-
-/**
-* Get Fan Speed Control Parameters.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pSpeed is the address of the structure where the result is to be placed.
-* @exception Always succeeds except if we cannot zero out the output structure.
-*/
-int tonga_fan_ctrl_get_fan_speed_info(struct pp_hwmgr *hwmgr, struct phm_fan_speed_info *fan_speed_info)
-{
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	fan_speed_info->supports_percent_read = true;
-	fan_speed_info->supports_percent_write = true;
-	fan_speed_info->min_percent = 0;
-	fan_speed_info->max_percent = 100;
-
-	if (0 != hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution) {
-		fan_speed_info->supports_rpm_read = true;
-		fan_speed_info->supports_rpm_write = true;
-		fan_speed_info->min_rpm = hwmgr->thermal_controller.fanInfo.ulMinRPM;
-		fan_speed_info->max_rpm = hwmgr->thermal_controller.fanInfo.ulMaxRPM;
-	} else {
-		fan_speed_info->min_rpm = 0;
-		fan_speed_info->max_rpm = 0;
-	}
-
-	return 0;
-}
-
-/**
-* Get Fan Speed in percent.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pSpeed is the address of the structure where the result is to be placed.
-* @exception Fails is the 100% setting appears to be 0.
-*/
-int tonga_fan_ctrl_get_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t *speed)
-{
-	uint32_t duty100;
-	uint32_t duty;
-	uint64_t tmp64;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL1, FMAX_DUTY100);
-	duty = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_STATUS, FDO_PWM_DUTY);
-
-	if (0 == duty100)
-		return -EINVAL;
-
-
-	tmp64 = (uint64_t)duty * 100;
-	do_div(tmp64, duty100);
-	*speed = (uint32_t)tmp64;
-
-	if (*speed > 100)
-		*speed = 100;
-
-	return 0;
-}
-
-/**
-* Get Fan Speed in RPM.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the address of the structure where the result is to be placed.
-* @exception Returns not supported if no fan is found or if pulses per revolution are not set
-*/
-int tonga_fan_ctrl_get_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t *speed)
-{
-	return 0;
-}
-
-/**
-* Set Fan Speed Control to static mode, so that the user can decide what speed to use.
-* @param    hwmgr  the address of the powerplay hardware manager.
-*           mode    the fan control mode, 0 default, 1 by percent, 5, by RPM
-* @exception Should always succeed.
-*/
-int tonga_fan_ctrl_set_static_mode(struct pp_hwmgr *hwmgr, uint32_t mode)
-{
-
-	if (hwmgr->fan_ctrl_is_in_default_mode) {
-		hwmgr->fan_ctrl_default_mode = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, FDO_PWM_MODE);
-		hwmgr->tmin = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, TMIN);
-		hwmgr->fan_ctrl_is_in_default_mode = false;
-	}
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, TMIN, 0);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, FDO_PWM_MODE, mode);
-
-	return 0;
-}
-
-/**
-* Reset Fan Speed Control to default mode.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @exception Should always succeed.
-*/
-int tonga_fan_ctrl_set_default_mode(struct pp_hwmgr *hwmgr)
-{
-	if (!hwmgr->fan_ctrl_is_in_default_mode) {
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, FDO_PWM_MODE, hwmgr->fan_ctrl_default_mode);
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, TMIN, hwmgr->tmin);
-		hwmgr->fan_ctrl_is_in_default_mode = true;
-	}
-
-	return 0;
-}
-
-int tonga_fan_ctrl_start_smc_fan_control(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_ODFuzzyFanControlSupport)) {
-		cgs_write_register(hwmgr->device, mmSMC_MSG_ARG_0, FAN_CONTROL_FUZZY);
-		result = (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StartFanControl) == 0) ?  0 : -EINVAL;
-/*
-		if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_FanSpeedInTableIsRPM))
-			hwmgr->set_max_fan_rpm_output(hwmgr, hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanRPM);
-		else
-			hwmgr->set_max_fan_pwm_output(hwmgr, hwmgr->thermal_controller.advanceFanControlParameters.usMaxFanPWM);
-*/
-	} else {
-		cgs_write_register(hwmgr->device, mmSMC_MSG_ARG_0, FAN_CONTROL_TABLE);
-		result = (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StartFanControl) == 0) ?  0 : -EINVAL;
-	}
-/* TO DO FOR SOME DEVICE ID 0X692b, send this msg return invalid command.
-	if (result == 0 && hwmgr->thermal_controller.advanceFanControlParameters.ucTargetTemperature != 0)
-		result = (0 == smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_SetFanTemperatureTarget, \
-								hwmgr->thermal_controller.advanceFanControlParameters.ucTargetTemperature) ? 0 : -EINVAL);
-*/
-	return result;
-}
-
-
-int tonga_fan_ctrl_stop_smc_fan_control(struct pp_hwmgr *hwmgr)
-{
-	return (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_StopFanControl) == 0) ?  0 : -EINVAL;
-}
-
-/**
-* Set Fan Speed in percent.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the percentage value (0% - 100%) to be set.
-* @exception Fails is the 100% setting appears to be 0.
-*/
-int tonga_fan_ctrl_set_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t speed)
-{
-	uint32_t duty100;
-	uint32_t duty;
-	uint64_t tmp64;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return -EINVAL;
-
-	if (speed > 100)
-		speed = 100;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl))
-		tonga_fan_ctrl_stop_smc_fan_control(hwmgr);
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL1, FMAX_DUTY100);
-
-	if (0 == duty100)
-		return -EINVAL;
-
-	tmp64 = (uint64_t)speed * duty100;
-	do_div(tmp64, 100);
-	duty = (uint32_t)tmp64;
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL0, FDO_STATIC_DUTY, duty);
-
-	return tonga_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-}
-
-/**
-* Reset Fan Speed to default.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @exception Always succeeds.
-*/
-int tonga_fan_ctrl_reset_fan_speed_to_default(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		return 0;
-
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl)) {
-		result = tonga_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-		if (0 == result)
-			result = tonga_fan_ctrl_start_smc_fan_control(hwmgr);
-	} else
-		result = tonga_fan_ctrl_set_default_mode(hwmgr);
-
-	return result;
-}
-
-/**
-* Set Fan Speed in RPM.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    speed is the percentage value (min - max) to be set.
-* @exception Fails is the speed not lie between min and max.
-*/
-int tonga_fan_ctrl_set_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t speed)
-{
-	return 0;
-}
-
-/**
-* Reads the remote temperature from the SIslands thermal controller.
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-int tonga_thermal_get_temperature(struct pp_hwmgr *hwmgr)
-{
-	int temp;
-
-	temp = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_MULT_THERMAL_STATUS, CTF_TEMP);
-
-/* Bit 9 means the reading is lower than the lowest usable value. */
-	if (0 != (0x200 & temp))
-		temp = TONGA_THERMAL_MAXIMUM_TEMP_READING;
-	else
-		temp = (temp & 0x1ff);
-
-	temp = temp * PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-
-	return temp;
-}
-
-/**
-* Set the requested temperature range for high and low alert signals
-*
-* @param    hwmgr The address of the hardware manager.
-* @param    range Temperature range to be programmed for high and low alert signals
-* @exception PP_Result_BadInput if the input data is not valid.
-*/
-static int tonga_thermal_set_temperature_range(struct pp_hwmgr *hwmgr, uint32_t low_temp, uint32_t high_temp)
-{
-	uint32_t low = TONGA_THERMAL_MINIMUM_ALERT_TEMP * PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-	uint32_t high = TONGA_THERMAL_MAXIMUM_ALERT_TEMP * PP_TEMPERATURE_UNITS_PER_CENTIGRADES;
-
-	if (low < low_temp)
-		low = low_temp;
-	if (high > high_temp)
-		high = high_temp;
-
-	if (low > high)
-		return -EINVAL;
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, DIG_THERM_INTH, (high / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, DIG_THERM_INTL, (low / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_CTRL, DIG_THERM_DPM, (high / PP_TEMPERATURE_UNITS_PER_CENTIGRADES));
-
-	return 0;
-}
-
-/**
-* Programs thermal controller one-time setting registers
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-static int tonga_thermal_initialize(struct pp_hwmgr *hwmgr)
-{
-	if (0 != hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution)
-		PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,
-						CG_TACH_CTRL, EDGE_PER_REV,
-						hwmgr->thermal_controller.fanInfo.ucTachometerPulsesPerRevolution - 1);
-
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL2, TACH_PWM_RESP_RATE, 0x28);
-
-	return 0;
-}
-
-/**
-* Enable thermal alerts on the RV770 thermal controller.
-*
-* @param    hwmgr The address of the hardware manager.
-*/
-static int tonga_thermal_enable_alert(struct pp_hwmgr *hwmgr)
-{
-	uint32_t alert;
-
-	alert = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, THERM_INT_MASK);
-	alert &= ~(TONGA_THERMAL_HIGH_ALERT_MASK | TONGA_THERMAL_LOW_ALERT_MASK);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, THERM_INT_MASK, alert);
-
-	/* send message to SMU to enable internal thermal interrupts */
-	return (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_Thermal_Cntl_Enable) == 0) ? 0 : -1;
-}
-
-/**
-* Disable thermal alerts on the RV770 thermal controller.
-* @param    hwmgr The address of the hardware manager.
-*/
-static int tonga_thermal_disable_alert(struct pp_hwmgr *hwmgr)
-{
-	uint32_t alert;
-
-	alert = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, THERM_INT_MASK);
-	alert |= (TONGA_THERMAL_HIGH_ALERT_MASK | TONGA_THERMAL_LOW_ALERT_MASK);
-	PHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_INT, THERM_INT_MASK, alert);
-
-	/* send message to SMU to disable internal thermal interrupts */
-	return (smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_Thermal_Cntl_Disable) == 0) ? 0 : -1;
-}
-
-/**
-* Uninitialize the thermal controller.
-* Currently just disables alerts.
-* @param    hwmgr The address of the hardware manager.
-*/
-int tonga_thermal_stop_thermal_controller(struct pp_hwmgr *hwmgr)
-{
-	int result = tonga_thermal_disable_alert(hwmgr);
-
-	if (hwmgr->thermal_controller.fanInfo.bNoFan)
-		tonga_fan_ctrl_set_default_mode(hwmgr);
-
-	return result;
-}
-
-/**
-* Set up the fan table to control the fan using the SMC.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-int tf_tonga_thermal_setup_fan_table(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result)
-{
-	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-	SMU72_Discrete_FanTable fan_table = { FDO_MODE_HARDWARE };
-	uint32_t duty100;
-	uint32_t t_diff1, t_diff2, pwm_diff1, pwm_diff2;
-	uint16_t fdo_min, slope1, slope2;
-	uint32_t reference_clock;
-	int res;
-	uint64_t tmp64;
-
-	if (!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl))
-		return 0;
-
-	if (0 == data->fan_table_start) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl);
-		return 0;
-	}
-
-	duty100 = PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_FDO_CTRL1, FMAX_DUTY100);
-
-	if (0 == duty100) {
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl);
-		return 0;
-	}
-
-	tmp64 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMMin * duty100;
-	do_div(tmp64, 10000);
-	fdo_min = (uint16_t)tmp64;
-
-	t_diff1 = hwmgr->thermal_controller.advanceFanControlParameters.usTMed - hwmgr->thermal_controller.advanceFanControlParameters.usTMin;
-	t_diff2 = hwmgr->thermal_controller.advanceFanControlParameters.usTHigh - hwmgr->thermal_controller.advanceFanControlParameters.usTMed;
-
-	pwm_diff1 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMMed - hwmgr->thermal_controller.advanceFanControlParameters.usPWMMin;
-	pwm_diff2 = hwmgr->thermal_controller.advanceFanControlParameters.usPWMHigh - hwmgr->thermal_controller.advanceFanControlParameters.usPWMMed;
-
-	slope1 = (uint16_t)((50 + ((16 * duty100 * pwm_diff1) / t_diff1)) / 100);
-	slope2 = (uint16_t)((50 + ((16 * duty100 * pwm_diff2) / t_diff2)) / 100);
-
-	fan_table.TempMin = cpu_to_be16((50 + hwmgr->thermal_controller.advanceFanControlParameters.usTMin) / 100);
-	fan_table.TempMed = cpu_to_be16((50 + hwmgr->thermal_controller.advanceFanControlParameters.usTMed) / 100);
-	fan_table.TempMax = cpu_to_be16((50 + hwmgr->thermal_controller.advanceFanControlParameters.usTMax) / 100);
-
-	fan_table.Slope1 = cpu_to_be16(slope1);
-	fan_table.Slope2 = cpu_to_be16(slope2);
-
-	fan_table.FdoMin = cpu_to_be16(fdo_min);
-
-	fan_table.HystDown = cpu_to_be16(hwmgr->thermal_controller.advanceFanControlParameters.ucTHyst);
-
-	fan_table.HystUp = cpu_to_be16(1);
-
-	fan_table.HystSlope = cpu_to_be16(1);
-
-	fan_table.TempRespLim = cpu_to_be16(5);
-
-	reference_clock = tonga_get_xclk(hwmgr);
-
-	fan_table.RefreshPeriod = cpu_to_be32((hwmgr->thermal_controller.advanceFanControlParameters.ulCycleDelay * reference_clock) / 1600);
-
-	fan_table.FdoMax = cpu_to_be16((uint16_t)duty100);
-
-	fan_table.TempSrc = (uint8_t)PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_MULT_THERMAL_CTRL, TEMP_SEL);
-
-	fan_table.FanControl_GL_Flag = 1;
-
-	res = tonga_copy_bytes_to_smc(hwmgr->smumgr, data->fan_table_start, (uint8_t *)&fan_table, (uint32_t)sizeof(fan_table), data->sram_end);
-/* TO DO FOR SOME DEVICE ID 0X692b, send this msg return invalid command.
-	if (res == 0 && hwmgr->thermal_controller.advanceFanControlParameters.ucMinimumPWMLimit != 0)
-		res = (0 == smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_SetFanMinPwm, \
-						hwmgr->thermal_controller.advanceFanControlParameters.ucMinimumPWMLimit) ? 0 : -1);
-
-	if (res == 0 && hwmgr->thermal_controller.advanceFanControlParameters.ulMinFanSCLKAcousticLimit != 0)
-		res = (0 == smum_send_msg_to_smc_with_parameter(hwmgr->smumgr, PPSMC_MSG_SetFanSclkTarget, \
-					hwmgr->thermal_controller.advanceFanControlParameters.ulMinFanSCLKAcousticLimit) ? 0 : -1);
-
-	if (0 != res)
-		phm_cap_unset(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl);
-*/
-	return 0;
-}
-
-/**
-* Start the fan control on the SMC.
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-int tf_tonga_thermal_start_smc_fan_control(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result)
-{
-/* If the fantable setup has failed we could have disabled PHM_PlatformCaps_MicrocodeFanControl even after this function was included in the table.
- * Make sure that we still think controlling the fan is OK.
-*/
-	if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_MicrocodeFanControl)) {
-		tonga_fan_ctrl_start_smc_fan_control(hwmgr);
-		tonga_fan_ctrl_set_static_mode(hwmgr, FDO_PWM_MODE_STATIC);
-	}
-
-	return 0;
-}
-
-/**
-* Set temperature range for high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from set temperature range routine
-*/
-int tf_tonga_thermal_set_temperature_range(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result)
-{
-	struct PP_TemperatureRange *range = (struct PP_TemperatureRange *)input;
-
-	if (range == NULL)
-		return -EINVAL;
-
-	return tonga_thermal_set_temperature_range(hwmgr, range->min, range->max);
-}
-
-/**
-* Programs one-time setting registers
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from initialize thermal controller routine
-*/
-int tf_tonga_thermal_initialize(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result)
-{
-    return tonga_thermal_initialize(hwmgr);
-}
-
-/**
-* Enable high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from enable alert routine
-*/
-int tf_tonga_thermal_enable_alert(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result)
-{
-	return tonga_thermal_enable_alert(hwmgr);
-}
-
-/**
-* Disable high and low alerts
-* @param    hwmgr  the address of the powerplay hardware manager.
-* @param    pInput the pointer to input data
-* @param    pOutput the pointer to output data
-* @param    pStorage the pointer to temporary storage
-* @param    Result the last failure code
-* @return   result from disable alert routine
-*/
-static int tf_tonga_thermal_disable_alert(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result)
-{
-	return tonga_thermal_disable_alert(hwmgr);
-}
-
-static const struct phm_master_table_item tonga_thermal_start_thermal_controller_master_list[] = {
-	{ NULL, tf_tonga_thermal_initialize },
-	{ NULL, tf_tonga_thermal_set_temperature_range },
-	{ NULL, tf_tonga_thermal_enable_alert },
-/* We should restrict performance levels to low before we halt the SMC.
- * On the other hand we are still in boot state when we do this so it would be pointless.
- * If this assumption changes we have to revisit this table.
- */
-	{ NULL, tf_tonga_thermal_setup_fan_table},
-	{ NULL, tf_tonga_thermal_start_smc_fan_control},
-	{ NULL, NULL }
-};
-
-static const struct phm_master_table_header tonga_thermal_start_thermal_controller_master = {
-	0,
-	PHM_MasterTableFlag_None,
-	tonga_thermal_start_thermal_controller_master_list
-};
-
-static const struct phm_master_table_item tonga_thermal_set_temperature_range_master_list[] = {
-	{ NULL, tf_tonga_thermal_disable_alert},
-	{ NULL, tf_tonga_thermal_set_temperature_range},
-	{ NULL, tf_tonga_thermal_enable_alert},
-	{ NULL, NULL }
-};
-
-static const struct phm_master_table_header tonga_thermal_set_temperature_range_master = {
-	0,
-	PHM_MasterTableFlag_None,
-	tonga_thermal_set_temperature_range_master_list
-};
-
-int tonga_thermal_ctrl_uninitialize_thermal_controller(struct pp_hwmgr *hwmgr)
-{
-	if (!hwmgr->thermal_controller.fanInfo.bNoFan)
-		tonga_fan_ctrl_set_default_mode(hwmgr);
-	return 0;
-}
-
-/**
-* Initializes the thermal controller related functions in the Hardware Manager structure.
-* @param    hwmgr The address of the hardware manager.
-* @exception Any error code from the low-level communication.
-*/
-int pp_tonga_thermal_initialize(struct pp_hwmgr *hwmgr)
-{
-	int result;
-
-	result = phm_construct_table(hwmgr, &tonga_thermal_set_temperature_range_master, &(hwmgr->set_temperature_range));
-
-	if (0 == result) {
-		result = phm_construct_table(hwmgr,
-						&tonga_thermal_start_thermal_controller_master,
-						&(hwmgr->start_thermal_controller));
-		if (0 != result)
-			phm_destroy_table(hwmgr, &(hwmgr->set_temperature_range));
-	}
-
-	if (0 == result)
-		hwmgr->fan_ctrl_is_in_default_mode = true;
-	return result;
-}
-
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_thermal.h b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_thermal.h
deleted file mode 100644
index aa335f2..0000000
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_thermal.h
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Copyright 2015 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- */
-
-#ifndef TONGA_THERMAL_H
-#define TONGA_THERMAL_H
-
-#include "hwmgr.h"
-
-#define TONGA_THERMAL_HIGH_ALERT_MASK         0x1
-#define TONGA_THERMAL_LOW_ALERT_MASK          0x2
-
-#define TONGA_THERMAL_MINIMUM_TEMP_READING    -256
-#define TONGA_THERMAL_MAXIMUM_TEMP_READING    255
-
-#define TONGA_THERMAL_MINIMUM_ALERT_TEMP      0
-#define TONGA_THERMAL_MAXIMUM_ALERT_TEMP      255
-
-#define FDO_PWM_MODE_STATIC  1
-#define FDO_PWM_MODE_STATIC_RPM 5
-
-
-extern int tf_tonga_thermal_initialize(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result);
-extern int tf_tonga_thermal_set_temperature_range(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result);
-extern int tf_tonga_thermal_enable_alert(struct pp_hwmgr *hwmgr, void *input, void *output, void *storage, int result);
-
-extern int tonga_thermal_get_temperature(struct pp_hwmgr *hwmgr);
-extern int tonga_thermal_stop_thermal_controller(struct pp_hwmgr *hwmgr);
-extern int tonga_fan_ctrl_get_fan_speed_info(struct pp_hwmgr *hwmgr, struct phm_fan_speed_info *fan_speed_info);
-extern int tonga_fan_ctrl_get_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t *speed);
-extern int tonga_fan_ctrl_set_default_mode(struct pp_hwmgr *hwmgr);
-extern int tonga_fan_ctrl_set_static_mode(struct pp_hwmgr *hwmgr, uint32_t mode);
-extern int tonga_fan_ctrl_set_fan_speed_percent(struct pp_hwmgr *hwmgr, uint32_t speed);
-extern int tonga_fan_ctrl_reset_fan_speed_to_default(struct pp_hwmgr *hwmgr);
-extern int pp_tonga_thermal_initialize(struct pp_hwmgr *hwmgr);
-extern int tonga_thermal_ctrl_uninitialize_thermal_controller(struct pp_hwmgr *hwmgr);
-extern int tonga_fan_ctrl_set_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t speed);
-extern int tonga_fan_ctrl_get_fan_speed_rpm(struct pp_hwmgr *hwmgr, uint32_t *speed);
-extern int tonga_fan_ctrl_stop_smc_fan_control(struct pp_hwmgr *hwmgr);
-
-#endif
-
diff --git a/drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h b/drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h
index be4c574..6d52a39 100644
--- a/drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h
+++ b/drivers/gpu/drm/amd/powerplay/inc/amd_powerplay.h
@@ -25,6 +25,7 @@
 
 #include <linux/seq_file.h>
 #include <linux/types.h>
+#include <linux/errno.h>
 #include "amd_shared.h"
 #include "cgs_common.h"
 
@@ -288,21 +289,7 @@ struct pp_states_info {
 	uint32_t nums;
 	uint32_t states[16];
 };
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)
-enum pp_profile_type {
-	PP_GFX_PROFILE = 0,
-	PP_COMPUTE_PROFILE,
-};
 
-struct pp_profile {
-	enum pp_profile_type type;
-	uint32_t min_sclk;
-	uint32_t min_mclk;
-	uint16_t activity_threshold;
-	uint8_t up_hyst;
-	uint8_t down_hyst;
-};
-#endif
 #define PP_GROUP_MASK        0xF0000000
 #define PP_GROUP_SHIFT       28
 
@@ -367,16 +354,6 @@ struct amd_powerplay_funcs {
 	int (*set_sclk_od)(void *handle, uint32_t value);
 	int (*get_mclk_od)(void *handle);
 	int (*set_mclk_od)(void *handle, uint32_t value);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
-	int (*reset_power_profile_state)(void *handle,
-			struct pp_profile *request);
-	int (*get_power_profile_state)(void *handle,
-			struct pp_profile *query);
-	int (*set_power_profile_state)(void *handle,
-			struct pp_profile *request);
-	int (*switch_power_profile)(void *handle,
-			enum pp_profile_type type);	
-#endif			
 	int (*read_sensor)(void *handle, int idx, int32_t *value);
 	struct amd_vce_state* (*get_vce_clock_state)(void *handle, unsigned idx);
 };
diff --git a/drivers/gpu/drm/amd/powerplay/inc/hwmgr.h b/drivers/gpu/drm/amd/powerplay/inc/hwmgr.h
index 303e01d..3b7450e 100644
--- a/drivers/gpu/drm/amd/powerplay/inc/hwmgr.h
+++ b/drivers/gpu/drm/amd/powerplay/inc/hwmgr.h
@@ -357,13 +357,9 @@ struct pp_hwmgr_func {
 	int (*set_sclk_od)(struct pp_hwmgr *hwmgr, uint32_t value);
 	int (*get_mclk_od)(struct pp_hwmgr *hwmgr);
 	int (*set_mclk_od)(struct pp_hwmgr *hwmgr, uint32_t value);
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
-	int (*set_power_profile_state)(struct pp_hwmgr *hwmgr,
-			struct pp_profile *request);
-#endif 				
 	int (*read_sensor)(struct pp_hwmgr *hwmgr, int idx, int32_t *value);
-        int (*request_firmware)(struct pp_hwmgr *hwmgr);
-        int (*release_firmware)(struct pp_hwmgr *hwmgr);
+	int (*request_firmware)(struct pp_hwmgr *hwmgr);
+	int (*release_firmware)(struct pp_hwmgr *hwmgr);
 };
 
 struct pp_table_func {
@@ -655,14 +651,6 @@ struct pp_hwmgr {
 	struct pp_power_state    *uvd_ps;
 	struct amd_pp_display_configuration display_config;
 	uint32_t feature_mask;
-#if defined(CONFIG_HSA_AMD) || defined(CONFIG_HSA_AMD_MODULE)	
-	/* power profile */
-	struct pp_profile gfx_power_profile;
-	struct pp_profile compute_power_profile;
-	struct pp_profile default_gfx_power_profile;
-	struct pp_profile default_compute_power_profile;
-	enum pp_profile_type current_power_profile;
-#endif	
 };
 
 
diff --git a/drivers/gpu/drm/amd/powerplay/smumgr/fiji_smumgr.c b/drivers/gpu/drm/amd/powerplay/smumgr/fiji_smumgr.c
old mode 100644
new mode 100755
-- 
2.7.4

