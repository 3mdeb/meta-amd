From 26cc438fb45b97db8911a105d7374999003facb0 Mon Sep 17 00:00:00 2001
From: Andres Rodriguez <andresx7@gmail.com>
Date: Fri, 3 Feb 2017 16:28:48 -0500
Subject: [PATCH 5178/5855] drm/amdkfd: allow split HQD on per-queue
 granularity v5
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Update the KGD to KFD interface to allow sharing pipes with queue
granularity instead of pipe granularity.

This allows for more interesting pipe/queue splits.

v2: fix overflow check for res.queue_mask
v3: fix shift overflow when setting res.queue_mask
v4: fix comment in is_pipeline_enabled()
v5: clamp res.queue_mask to the first MEC only

Reviewed-by: Edward O'Callaghan <funfunctor@folklore1984.net>
Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
Acked-by: Christian KÃ¶nig <christian.koenig@amd.com>
Signed-off-by: Andres Rodriguez <andresx7@gmail.com>
Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
---
 drivers/gpu/drm/amd/amdkfd/kfd_device.c            |  4 +++
 .../gpu/drm/amd/amdkfd/kfd_device_queue_manager.c  | 34 +++++++++-------------
 drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c    |  4 +--
 .../gpu/drm/amd/amdkfd/kfd_process_queue_manager.c |  2 +-
 4 files changed, 20 insertions(+), 24 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device.c b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
index a9d2538..28a0da5 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
@@ -228,6 +228,10 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
 
 	kfd->shared_resources = *gpu_resources;
 
+	/* We only use the first MEC */
+	if (kfd->shared_resources.num_mec > 1)
+		kfd->shared_resources.num_mec = 1;
+
 	/* calculate max size of mqds needed for queues */
 	size = max_num_of_queues_per_device *
 			kfd->device_info->mqd_size_aligned;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
index 0517b29..23b9219 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
@@ -220,8 +220,8 @@ static int allocate_hqd(struct device_queue_manager *dqm, struct queue *q)
 
 	set = false;
 
-	for (pipe = dqm->next_pipe_to_allocate, i = 0; i < get_pipes_num(dqm);
-			pipe = ((pipe + 1) % get_pipes_per_mec(dqm)), ++i) {
+        for (pipe = dqm->next_pipe_to_allocate, i = 0; i < get_pipes_per_mec(dqm);
+                        pipe = ((pipe + 1) % get_pipes_per_mec(dqm)), ++i) {
 
 		if (!is_pipe_enabled(dqm, 0, pipe))
 			continue;
@@ -229,8 +229,7 @@ static int allocate_hqd(struct device_queue_manager *dqm, struct queue *q)
 		if (dqm->allocated_queues[pipe] != 0) {
                        bit = find_first_bit(
                                (unsigned long *)&dqm->allocated_queues[pipe],
-                               QUEUES_PER_PIPE);
-
+				get_queues_per_pipe(dqm));
                        clear_bit(bit,
                                (unsigned long *)&dqm->allocated_queues[pipe]);
 			q->pipe = pipe;
@@ -497,22 +496,18 @@ static void init_interrupts(struct device_queue_manager *dqm)
 {
 	unsigned int i;
 	BUG_ON(dqm == NULL);
-
-	for (i = 0 ; i < get_pipes_per_mec(dqm) ; i++)
-		if (is_pipe_enabled(dqm, 0, i))
-			dqm->dev->kfd2kgd->init_interrupts(dqm->dev->kgd,
-					i + get_first_pipe(dqm));
+        for (i = 0 ; i < get_pipes_per_mec(dqm) ; i++)
+                if (is_pipe_enabled(dqm, 0, i))
+                        dqm->dev->kfd2kgd->init_interrupts(dqm->dev->kgd, i);
 }
 
 static int init_scheduler(struct device_queue_manager *dqm)
 {
-       int retval;
-
+	int retval = 0;
        BUG_ON(!dqm);
 
        pr_debug("kfd: In %s\n", __func__);
 
-       retval = init_pipelines(dqm, get_pipes_num(dqm), get_first_pipe(dqm));
        return retval;
 
 }
@@ -524,13 +519,12 @@ static int initialize_nocpsch(struct device_queue_manager *dqm)
 	BUG_ON(!dqm);
 
        pr_debug("kfd: In func %s num of pipes: %d\n",
-                       __func__, get_pipes_num(dqm));
+		__func__, get_pipes_per_mec(dqm));
 	mutex_init(&dqm->lock);
 	INIT_LIST_HEAD(&dqm->queues);
 	dqm->queue_count = dqm->next_pipe_to_allocate = 0;
 	dqm->sdma_queue_count = 0;
-
-        dqm->allocated_queues = kcalloc(get_pipes_num(dqm),
+	dqm->allocated_queues = kcalloc(get_pipes_per_mec(dqm),
                                         sizeof(unsigned int), GFP_KERNEL);
         if (!dqm->allocated_queues) {
                 mutex_destroy(&dqm->lock);
@@ -538,9 +532,8 @@ static int initialize_nocpsch(struct device_queue_manager *dqm)
         }
 
 
-	for (i = 0; i < get_pipes_per_mec(dqm); i++)
-		dqm->allocated_queues[i] = (1 << get_queues_per_pipe(dqm)) - 1;
-
+        for (i = 0; i < get_pipes_per_mec(dqm); i++)
+               dqm->allocated_queues[i] = (1 << get_queues_per_pipe(dqm)) - 1;
 	dqm->vmid_bitmap = (1 << VMID_PER_DEVICE) - 1
 	dqm->sdma_bitmap = (1 << CIK_SDMA_QUEUES) - 1;
 
@@ -653,7 +646,8 @@ static int set_sched_resources(struct device_queue_manager *dqm)
 
         pr_debug("kfd: In func %s\n", __func__);
 
-	res.vmid_mask = dqm->dev->shared_resources.compute_vmid_bitmap;
+	res.vmid_mask = = (1 << VMID_PER_DEVICE) - 1;
+	res.vmid_mask <<= KFD_VMID_START_OFFSET;
 
 	res.queue_mask = 0;
 	for (i = 0; i < KGD_MAX_QUEUES; ++i) {
@@ -695,7 +689,7 @@ static int initialize_cpsch(struct device_queue_manager *dqm)
         BUG_ON(!dqm);
 
         pr_debug("kfd: In func %s num of pipes: %d\n",
-                        __func__, get_pipes_num_cpsch());
+			__func__, get_pipes_per_mec(dqm));
 	mutex_init(&dqm->lock);
 	INIT_LIST_HEAD(&dqm->queues);
 	dqm->queue_count = dqm->processes_count = 0;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
index 9713ef1..192fd69 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
@@ -67,9 +67,7 @@ static void pm_calc_rlib_size(struct packet_manager *pm,
 	/* check if there is over subscription*/
 
 	*over_subscription = false;
-
-	if ((process_count > 1) ||
-		queue_count > PIPE_PER_ME_CP_SCHEDULING * QUEUES_PER_PIPE) {
+	if ((process_count > 1) || queue_count > get_queues_num(pm->dqm)) {
 		*over_subscription = true;
 		pr_debug("kfd: over subscribed runlist\n");
 	}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
index ad47d3d..e388f12 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
@@ -210,7 +210,7 @@ int pqm_create_queue(struct process_queue_manager *pqm,
 		/* check if there is over subscription */
                 if ((sched_policy == KFD_SCHED_POLICY_HWS_NO_OVERSUBSCRIPTION) &&
                 ((dev->dqm->processes_count >= VMID_PER_DEVICE) ||
-                (dev->dqm->queue_count >= PIPE_PER_ME_CP_SCHEDULING * QUEUES_PER_PIPE))) {
+			(dev->dqm->queue_count >= get_queues_num(dev->dqm)))) {
                         pr_err("kfd: over-subscription is not allowed in radeon_kfd.sched_policy == 1\n");
 			retval = -EPERM;
 			goto err_create_queue;
-- 
2.7.4

