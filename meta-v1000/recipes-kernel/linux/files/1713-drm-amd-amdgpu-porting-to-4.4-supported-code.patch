From 4031b7a7313b988d93c984fc623a75b25c713b56 Mon Sep 17 00:00:00 2001
From: Ayyappa Ch <ayyappa.chandolu@amd.com>
Date: Fri, 27 Jan 2017 19:14:10 +0530
Subject: [PATCH 1713/1722] drm/amd/amdgpu: porting to 4.4 supported code

Signed-off-by: Ayyappa Ch <ayyappa.chandolu@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c        |   2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c  |  44 ++++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c            |   9 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c            | 202 ++++++++++----------
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h            |   1 +
 drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c | 214 +++++++++++-----------
 6 files changed, 255 insertions(+), 217 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
index edca508..1d07bf3 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
@@ -194,7 +194,7 @@ static void amdgdu_amdkfd_restore_mem_worker(struct work_struct *work)
 
 	mutex_lock(&mem->lock);
 
-	adev = mem->bo->adev;
+	adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);
 	mm = mem->mm;
 
 	/* Restoration may have been canceled by another eviction or
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
index 03dfa01..9d44637 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
@@ -381,7 +381,7 @@ static int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,
 			if (ret)
 				goto validate_fail;
 
-			ttm_bo_wait(&bo->tbo, false, false);
+			ttm_bo_wait(&bo->tbo, false, false, false);
 			amdgpu_amdkfd_add_eviction_fence(bo, ef_list,
 							 ef_count);
 		}
@@ -396,7 +396,7 @@ static int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,
 			if (ret)
 				goto validate_fail;
 			if (wait)
-				ttm_bo_wait(&bo->tbo, false, false);
+				ttm_bo_wait(&bo->tbo, false, false, false);
 		}
 		bo->pin_count++;
 	}
@@ -409,7 +409,7 @@ static int amdgpu_amdkfd_validate(void *param, struct amdgpu_bo *bo)
 {
 	struct amdgpu_vm_parser *p = param;
 
-	return amdgpu_amdkfd_bo_validate(bo, p->domain, p->wait);
+	return amdgpu_amdkfd_bo_validate(bo, bo->prefered_domains, p->wait);
 }
 
 static int amdgpu_amdkfd_bo_invalidate(struct amdgpu_bo *bo)
@@ -438,7 +438,7 @@ static int validate_pt_pd_bos(struct amdgpu_vm *vm)
 
 	/* PTs share same reservation object as PD. So only fence PD */
 	for (i = 0; i <= vm->max_pde_used; ++i) {
-		bo = vm->page_tables[i].entry.robj;
+		bo = vm->page_tables[i].bo;
 
 		if (!bo)
 			continue;
@@ -726,6 +726,7 @@ static int reserve_bo_and_vm(struct kgd_mem *mem,
 			      struct bo_vm_reservation_context *ctx)
 {
 	struct amdgpu_bo *bo = mem->bo;
+	struct amdgpu_vm_parser param;
 	int ret;
 
 	WARN_ON(!vm);
@@ -750,7 +751,10 @@ static int reserve_bo_and_vm(struct kgd_mem *mem,
 	list_add(&ctx->kfd_bo.tv.head, &ctx->list);
 
 	amdgpu_vm_get_pd_bo(vm, &ctx->list, &ctx->vm_pd[0]);
-	amdgpu_vm_get_pt_bos(bo->adev, vm, &ctx->duplicates);
+	param.domain = bo->prefered_domains;
+	param.wait = false;
+	amdgpu_vm_validate_pt_bos(amdgpu_ttm_adev(bo->tbo.bdev), vm,
+			amdgpu_amdkfd_validate, &param);
 
 	ret = ttm_eu_reserve_buffers(&ctx->ticket, &ctx->list,
 				     false, &ctx->duplicates);
@@ -788,6 +792,7 @@ static int reserve_bo_and_cond_vms(struct kgd_mem *mem,
 {
 	struct amdgpu_bo *bo = mem->bo;
 	struct kfd_bo_va_list *entry;
+	struct amdgpu_vm_parser param;
 	unsigned i;
 	int ret;
 
@@ -831,11 +836,16 @@ static int reserve_bo_and_cond_vms(struct kgd_mem *mem,
 
 		amdgpu_vm_get_pd_bo(entry->bo_va->vm, &ctx->list,
 				&ctx->vm_pd[i]);
-		amdgpu_vm_get_pt_bos(bo->adev, entry->bo_va->vm,
-				&ctx->duplicates);
 		i++;
 	}
 
+	if (vm) {
+		param.domain = bo->prefered_domains;
+		param.wait = false;
+		amdgpu_vm_validate_pt_bos(amdgpu_ttm_adev(bo->tbo.bdev), vm,
+				amdgpu_amdkfd_validate, &param);
+	}
+
 	ret = ttm_eu_reserve_buffers(&ctx->ticket, &ctx->list,
 				     false, &ctx->duplicates);
 	if (!ret)
@@ -2089,6 +2099,7 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *m_vm)
 	struct bo_vm_reservation_context ctx;
 	struct amdgpu_amdkfd_fence *old_fence;
 	int ret = 0, i;
+	struct amdgpu_vm_parser param;
 
 	if (WARN_ON(master_vm == NULL || master_vm->master != master_vm))
 		return -EINVAL;
@@ -2118,15 +2129,30 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *m_vm)
 	/* Get PD BO list and PT BO list from all the VMs */
 	amdgpu_vm_get_pd_bo(&master_vm->base, &ctx.list,
 			    &pd_bo_list[0]);
+#if 0				
 	amdgpu_vm_get_pt_bos(master_vm->adev, &master_vm->base,
 			     &ctx.duplicates);
+#else
+	//param.domain = bo->prefered_domains;
+	param.wait = false;
+	amdgpu_vm_validate_pt_bos(master_vm->adev, &master_vm->base,
+			amdgpu_amdkfd_validate, &param);
+
+#endif			 
 
 	i = 1;
 	list_for_each_entry(peer_vm, &master_vm->kfd_vm_list, kfd_vm_list) {
 		amdgpu_vm_get_pd_bo(&peer_vm->base, &ctx.list,
 				    &pd_bo_list[i]);
+#if 0					
 		amdgpu_vm_get_pt_bos(peer_vm->adev, &peer_vm->base,
 				     &ctx.duplicates);
+#else
+	//param.domain = bo->prefered_domains;
+	param.wait = false;
+	amdgpu_vm_validate_pt_bos(peer_vm->adev, &peer_vm->base,
+			amdgpu_amdkfd_validate, &param);
+#endif				 
 		i++;
 	}
 
@@ -2179,7 +2205,7 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *m_vm)
 	list_for_each_entry(entry, &ctx.list, tv.head) {
 		struct amdgpu_bo *bo = entry->robj;
 
-		ttm_bo_wait(&bo->tbo, false, false);
+		ttm_bo_wait(&bo->tbo, false, false, false);
 	}
 
 
@@ -2217,7 +2243,7 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *m_vm)
 		bo_list_entry.tv.head) {
 		struct amdgpu_bo *bo = mem->bo;
 
-		ttm_bo_wait(&bo->tbo, false, false);
+		ttm_bo_wait(&bo->tbo, false, false, false);
 		amdgpu_bo_fence(bo, &master_vm->eviction_fence->base, true);
 	}
 	list_for_each_entry(entry, &ctx.list, tv.head) {
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c
index 641bb44..5c9a829 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c
@@ -258,7 +258,9 @@ static void amdgpu_mn_invalidate_range_start_hsa(struct mmu_notifier *mn,
 
 			if (amdgpu_ttm_tt_affect_userptr(bo->tbo.ttm,
 							 start, end))
-				amdgpu_amdkfd_evict_mem(bo->adev, mem, mm);
+				amdgpu_amdkfd_evict_mem(
+						amdgpu_ttm_adev(bo->tbo.bdev),
+						mem, mm);
 		}
 	}
 }
@@ -301,8 +303,9 @@ static void amdgpu_mn_invalidate_range_end_hsa(struct mmu_notifier *mn,
 
 			if (amdgpu_ttm_tt_affect_userptr(bo->tbo.ttm,
 							 start, end))
-				amdgpu_amdkfd_schedule_restore_mem(bo->adev,
-								   mem, mm, 1);
+				amdgpu_amdkfd_schedule_restore_mem(
+						amdgpu_ttm_adev(bo->tbo.bdev),
+						mem, mm, 1);
 		}
 	}
 }
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index 9559f20..c6f6d34 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -59,6 +59,8 @@ struct amdgpu_pte_update_params {
 	struct amdgpu_device *adev;
 	/* address where to copy page table entries from */
 	uint64_t src;
+	/* DMA addresses to use for mapping */
+	dma_addr_t *pages_addr;
 	/* indirect buffer to fill with commands */
 	struct amdgpu_ib *ib;
 	/* Function which actually does the update */
@@ -75,7 +77,7 @@ struct amdgpu_pte_update_params {
  * write_pte_using_cpu - update the page tables using CPU
  *
  * @adev: amdgpu_device pointer
- * @vm_update_params: see structure definition
+ * @pte_update_params: see structure definition
  * @pe_start: offset from start of the page table
  * @addr: dst addr to write into pe
  * @count: number of page entries to update
@@ -578,15 +580,21 @@ static void amdgpu_vm_do_copy_ptes(struct amdgpu_pte_update_params *params,
  * Look up the physical address of the page that the pte resolves
  * to and return the pointer for the page table entry.
  */
-static uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
+uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
 {
 	uint64_t result;
 
-	/* page table offset */
-	result = pages_addr[addr >> PAGE_SHIFT];
+	if (pages_addr) {
+		/* page table offset */
+		result = pages_addr[addr >> PAGE_SHIFT];
 
-	/* in case cpu page size != gpu page size*/
-	result |= addr & (~PAGE_MASK);
+		/* in case cpu page size != gpu page size*/
+		result |= addr & (~PAGE_MASK);
+
+	} else {
+		/* No mapping required */
+		result = addr;
+	}
 
 	result &= 0xFFFFFFFFFFFFF000ULL;
 
@@ -752,7 +760,7 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 		vm->page_directory_fence = fence_get(fence);
 		fence_put(fence);
 
-	} else if (vm_update_params.kptr)
+	} else if (params.kptr)
 		amdgpu_bo_kunmap(vm->page_directory);
 	return 0;
 
@@ -761,95 +769,6 @@ error_free:
 	return r;
 }
 
-/*
- * amdgpu_vm_frag_ptes - add fragment information to PTEs
- *
- * @params: see amdgpu_pte_update_params definition
- * @vm: requested vm
- * @start: first PTE to handle
- * @end: last PTE to handle
- * @dst: addr those PTEs should point to
- * @flags: hw mapping flags
- */
-static void amdgpu_vm_frag_ptes(struct amdgpu_pte_update_params	*params,
-				struct amdgpu_vm *vm,
-				uint64_t start, uint64_t end,
-				uint64_t dst, uint32_t flags)
-{
-	/**
-	 * The MC L1 TLB supports variable sized pages, based on a fragment
-	 * field in the PTE. When this field is set to a non-zero value, page
-	 * granularity is increased from 4KB to (1 << (12 + frag)). The PTE
-	 * flags are considered valid for all PTEs within the fragment range
-	 * and corresponding mappings are assumed to be physically contiguous.
-	 *
-	 * The L1 TLB can store a single PTE for the whole fragment,
-	 * significantly increasing the space available for translation
-	 * caching. This leads to large improvements in throughput when the
-	 * TLB is under pressure.
-	 *
-	 * The L2 TLB distributes small and large fragments into two
-	 * asymmetric partitions. The large fragment cache is significantly
-	 * larger. Thus, we try to use large fragments wherever possible.
-	 * Userspace can support this by aligning virtual base address and
-	 * allocation size to the fragment size.
-	 */
-
-	/* SI and newer are optimized for 64KB */
-	uint64_t frag_flags = AMDGPU_PTE_FRAG(AMDGPU_LOG2_PAGES_PER_FRAG);
-	uint64_t frag_align = 1 << AMDGPU_LOG2_PAGES_PER_FRAG;
-
-	uint64_t frag_start = ALIGN(start, frag_align);
-	uint64_t frag_end = end & ~(frag_align - 1);
-
-	unsigned count;
-	uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
-	/* system pages are non continuously */
-	if (params->src || !(flags & AMDGPU_PTE_VALID) ||
-	    (frag_start >= frag_end)) {
-
-		count = end - start;
-				
-		if (params->kptr)
-			write_pte_using_cpu(params->adev, params, start, dst, count, incr, flags);
-		else
-			amdgpu_vm_update_ptes_sdma(params, vm, start, end, dst, flags);
-		return;
-	}
-
-	/* handle the 4K area at the beginning */
-	if (start != frag_start) {
-	
-		count = frag_start - start;
-				
-		if (params->kptr)
-			write_pte_using_cpu(params->adev, params, start, dst, count, incr, flags);
-		else
-			amdgpu_vm_update_ptes_sdma(params, vm, start, frag_start,
-				      	dst, flags);
-		dst += (frag_start - start) * AMDGPU_GPU_PAGE_SIZE;
-	}
-
-	/* handle the area in the middle */
-	count = frag_end - frag_start;
-				
-	if (params->kptr)
-		write_pte_using_cpu(params->adev, params, frag_start, dst, count, incr, flags);
-	else	
-		amdgpu_vm_update_ptes_sdma(params, vm, frag_start, frag_end, dst,
-		      flags | frag_flags);
-
-	/* handle the 4K area at the end */
-	if (frag_end != end) {
-		dst += (frag_end - frag_start) * AMDGPU_GPU_PAGE_SIZE;
-		count = end - frag_end;
-		if (params->kptr)
-			write_pte_using_cpu(params->adev, params, frag_end, dst, count, incr, flags);
-		else		
-			amdgpu_vm_update_ptes_sdma(params, vm, frag_end, end, dst, flags);
-	}
-}
-
 /**
  * amdgpu_vm_update_ptes_sdma - make sure that page tables are valid
  *
@@ -942,6 +861,95 @@ static void amdgpu_vm_update_ptes_sdma(struct amdgpu_pte_update_params *params,
 		     AMDGPU_GPU_PAGE_SIZE, flags);
 }
 
+/*
+ * amdgpu_vm_frag_ptes - add fragment information to PTEs
+ *
+ * @params: see amdgpu_pte_update_params definition
+ * @vm: requested vm
+ * @start: first PTE to handle
+ * @end: last PTE to handle
+ * @dst: addr those PTEs should point to
+ * @flags: hw mapping flags
+ */
+static void amdgpu_vm_frag_ptes(struct amdgpu_pte_update_params	*params,
+				struct amdgpu_vm *vm,
+				uint64_t start, uint64_t end,
+				uint64_t dst, uint32_t flags)
+{
+	/**
+	 * The MC L1 TLB supports variable sized pages, based on a fragment
+	 * field in the PTE. When this field is set to a non-zero value, page
+	 * granularity is increased from 4KB to (1 << (12 + frag)). The PTE
+	 * flags are considered valid for all PTEs within the fragment range
+	 * and corresponding mappings are assumed to be physically contiguous.
+	 *
+	 * The L1 TLB can store a single PTE for the whole fragment,
+	 * significantly increasing the space available for translation
+	 * caching. This leads to large improvements in throughput when the
+	 * TLB is under pressure.
+	 *
+	 * The L2 TLB distributes small and large fragments into two
+	 * asymmetric partitions. The large fragment cache is significantly
+	 * larger. Thus, we try to use large fragments wherever possible.
+	 * Userspace can support this by aligning virtual base address and
+	 * allocation size to the fragment size.
+	 */
+
+	/* SI and newer are optimized for 64KB */
+	uint64_t frag_flags = AMDGPU_PTE_FRAG(AMDGPU_LOG2_PAGES_PER_FRAG);
+	uint64_t frag_align = 1 << AMDGPU_LOG2_PAGES_PER_FRAG;
+
+	uint64_t frag_start = ALIGN(start, frag_align);
+	uint64_t frag_end = end & ~(frag_align - 1);
+
+	unsigned count;
+	uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
+	/* system pages are non continuously */
+	if (params->src || !(flags & AMDGPU_PTE_VALID) ||
+	    (frag_start >= frag_end)) {
+
+		count = end - start;
+				
+		if (params->kptr)
+			write_pte_using_cpu(params->adev, params, start, dst, count, incr, flags);
+		else
+			amdgpu_vm_update_ptes_sdma(params, vm, start, end, dst, flags);
+		return;
+	}
+
+	/* handle the 4K area at the beginning */
+	if (start != frag_start) {
+	
+		count = frag_start - start;
+				
+		if (params->kptr)
+			write_pte_using_cpu(params->adev, params, start, dst, count, incr, flags);
+		else
+			amdgpu_vm_update_ptes_sdma(params, vm, start, frag_start,
+				      	dst, flags);
+		dst += (frag_start - start) * AMDGPU_GPU_PAGE_SIZE;
+	}
+
+	/* handle the area in the middle */
+	count = frag_end - frag_start;
+				
+	if (params->kptr)
+		write_pte_using_cpu(params->adev, params, frag_start, dst, count, incr, flags);
+	else	
+		amdgpu_vm_update_ptes_sdma(params, vm, frag_start, frag_end, dst,
+		      flags | frag_flags);
+
+	/* handle the 4K area at the end */
+	if (frag_end != end) {
+		dst += (frag_end - frag_start) * AMDGPU_GPU_PAGE_SIZE;
+		count = end - frag_end;
+		if (params->kptr)
+			write_pte_using_cpu(params->adev, params, frag_end, dst, count, incr, flags);
+		else		
+			amdgpu_vm_update_ptes_sdma(params, vm, frag_end, end, dst, flags);
+	}
+}
+
 
 /**
  * amdgpu_vm_update_ptes_cpu - make sure that page tables are valid
@@ -1057,7 +1065,7 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 		amdgpu_sync_wait(&sync);
 		amdgpu_sync_free(&sync);
 
-		r = amdgpu_vm_update_ptes_cpu(adev, &vm_update_params,
+		r = amdgpu_vm_update_ptes_cpu(adev, &params,
 			vm, start, last + 1, addr, flags);
 		if (r) {
 			dev_info(adev->dev, "CPU pte update failed. Fallback to sdma\n");
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
index 7a9f6bd..ba2c1d6 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
@@ -195,6 +195,7 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 		      struct amdgpu_job *job);
 int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job);
 void amdgpu_vm_reset_id(struct amdgpu_device *adev, unsigned vm_id);
+uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr);
 int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 				    struct amdgpu_vm *vm);
 int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c
index 9eeeab2..c825e1a 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/tonga_hwmgr.c
@@ -6196,126 +6196,126 @@ static int tonga_force_clock_level(struct pp_hwmgr *hwmgr,
 }
 
 static int tonga_print_clock_levels(struct pp_hwmgr *hwmgr,
-                enum pp_clock_type type, char *buf)
+		enum pp_clock_type type, char *buf)
 {
-        struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-        struct tonga_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-        struct tonga_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-        struct tonga_single_dpm_table *pcie_table = &(data->dpm_table.pcie_speed_table);
-        int i, now, size = 0;
-        uint32_t clock, pcie_speed;
- 
-        switch (type) {
-        case PP_SCLK:
-                smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetSclkFrequency);
-                clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
- 
-                for (i = 0; i < sclk_table->count; i++) {
-                        if (clock > sclk_table->dpm_levels[i].value)
-                                continue;
-                        break;
-                }
-                now = i;
- 
-                for (i = 0; i < sclk_table->count; i++)
-                        size += sprintf(buf + size, "%d: %uMhz %s\n",
-                                        i, sclk_table->dpm_levels[i].value / 100,
-                                        (i == now) ? "*" : "");
-                break;
-        case PP_MCLK:
-                smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetMclkFrequency);
-                clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
- 
-                for (i = 0; i < mclk_table->count; i++) {
-                        if (clock > mclk_table->dpm_levels[i].value)
-                                continue;
-                        break;
-                }
-                now = i;
- 
-                for (i = 0; i < mclk_table->count; i++)
-                        size += sprintf(buf + size, "%d: %uMhz %s\n",
-                                        i, mclk_table->dpm_levels[i].value / 100,
-                                        (i == now) ? "*" : "");
-                break;
-        case PP_PCIE:
-                pcie_speed = tonga_get_current_pcie_speed(hwmgr);
-                for (i = 0; i < pcie_table->count; i++) {
-                        if (pcie_speed != pcie_table->dpm_levels[i].value)
-                                continue;
-                        break;
-                }
-                now = i;
- 
-                for (i = 0; i < pcie_table->count; i++)
-                        size += sprintf(buf + size, "%d: %s %s\n", i,
-                                        (pcie_table->dpm_levels[i].value == 0) ? "2.5GB, x8" :
-                                        (pcie_table->dpm_levels[i].value == 1) ? "5.0GB, x16" :
-                                        (pcie_table->dpm_levels[i].value == 2) ? "8.0GB, x16" : "",
-                                        (i == now) ? "*" : "");
-                break;
-        default:
-                break;
-        }
-        return size;
+	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
+	struct tonga_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
+	struct tonga_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
+	struct tonga_single_dpm_table *pcie_table = &(data->dpm_table.pcie_speed_table);
+	int i, now, size = 0;
+	uint32_t clock, pcie_speed;
+
+	switch (type) {
+	case PP_SCLK:
+		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetSclkFrequency);
+		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
+
+		for (i = 0; i < sclk_table->count; i++) {
+			if (clock > sclk_table->dpm_levels[i].value)
+				continue;
+			break;
+		}
+		now = i;
+
+		for (i = 0; i < sclk_table->count; i++)
+			size += sprintf(buf + size, "%d: %uMhz %s\n",
+					i, sclk_table->dpm_levels[i].value / 100,
+					(i == now) ? "*" : "");
+		break;
+	case PP_MCLK:
+		smum_send_msg_to_smc(hwmgr->smumgr, PPSMC_MSG_API_GetMclkFrequency);
+		clock = cgs_read_register(hwmgr->device, mmSMC_MSG_ARG_0);
+
+		for (i = 0; i < mclk_table->count; i++) {
+			if (clock > mclk_table->dpm_levels[i].value)
+				continue;
+			break;
+		}
+		now = i;
+
+		for (i = 0; i < mclk_table->count; i++)
+			size += sprintf(buf + size, "%d: %uMhz %s\n",
+					i, mclk_table->dpm_levels[i].value / 100,
+					(i == now) ? "*" : "");
+		break;
+	case PP_PCIE:
+		pcie_speed = tonga_get_current_pcie_speed(hwmgr);
+		for (i = 0; i < pcie_table->count; i++) {
+			if (pcie_speed != pcie_table->dpm_levels[i].value)
+				continue;
+			break;
+		}
+		now = i;
+
+		for (i = 0; i < pcie_table->count; i++)
+			size += sprintf(buf + size, "%d: %s %s\n", i,
+					(pcie_table->dpm_levels[i].value == 0) ? "2.5GB, x8" :
+					(pcie_table->dpm_levels[i].value == 1) ? "5.0GB, x16" :
+					(pcie_table->dpm_levels[i].value == 2) ? "8.0GB, x16" : "",
+					(i == now) ? "*" : "");
+		break;
+	default:
+		break;
+	}
+	return size;
 }
 
 static int tonga_get_sclk_od(struct pp_hwmgr *hwmgr)
 {
-        struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-        struct tonga_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
-        struct tonga_single_dpm_table *golden_sclk_table =
-                        &(data->golden_dpm_table.sclk_table);
-        int value;
- 
-        value = (sclk_table->dpm_levels[sclk_table->count - 1].value -
-                        golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value) *
-                        100 /
-                        golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
- 
-        return value;
+	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
+	struct tonga_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);
+	struct tonga_single_dpm_table *golden_sclk_table =
+			&(data->golden_dpm_table.sclk_table);
+	int value;
+
+	value = (sclk_table->dpm_levels[sclk_table->count - 1].value -
+			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value) *
+			100 /
+			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
+
+	return value;
 }
 
 static int tonga_set_sclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
 {
-        struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-        struct tonga_single_dpm_table *golden_sclk_table =
-                        &(data->golden_dpm_table.sclk_table);
-        struct pp_power_state  *ps;
-        struct tonga_power_state  *tonga_ps;
- 
-        if (value > 20)
-                value = 20;
- 
-        ps = hwmgr->request_ps;
- 
-        if (ps == NULL)
-                return -EINVAL;
- 
-        tonga_ps = cast_phw_tonga_power_state(&ps->hardware);
- 
-        tonga_ps->performance_levels[tonga_ps->performance_level_count - 1].engine_clock =
-                        golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value *
-                        value / 100 +
-                        golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
- 
-        return 0;
+	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
+	struct tonga_single_dpm_table *golden_sclk_table =
+			&(data->golden_dpm_table.sclk_table);
+	struct pp_power_state  *ps;
+	struct tonga_power_state  *tonga_ps;
+
+	if (value > 20)
+		value = 20;
+
+	ps = hwmgr->request_ps;
+
+	if (ps == NULL)
+		return -EINVAL;
+
+	tonga_ps = cast_phw_tonga_power_state(&ps->hardware);
+
+	tonga_ps->performance_levels[tonga_ps->performance_level_count - 1].engine_clock =
+			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value *
+			value / 100 +
+			golden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;
+
+	return 0;
 }
 
 static int tonga_get_mclk_od(struct pp_hwmgr *hwmgr)
 {
-       struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
-        struct tonga_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
-        struct tonga_single_dpm_table *golden_mclk_table =
-                        &(data->golden_dpm_table.mclk_table);
-        int value;
- 
-        value = (mclk_table->dpm_levels[mclk_table->count - 1].value -
-                        golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value) *
-                        100 /
-                        golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
- 
-        return value;
+	struct tonga_hwmgr *data = (struct tonga_hwmgr *)(hwmgr->backend);
+	struct tonga_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);
+	struct tonga_single_dpm_table *golden_mclk_table =
+			&(data->golden_dpm_table.mclk_table);
+	int value;
+
+	value = (mclk_table->dpm_levels[mclk_table->count - 1].value -
+			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value) *
+			100 /
+			golden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;
+
+	return value;
 }
 
 static int tonga_set_mclk_od(struct pp_hwmgr *hwmgr, uint32_t value)
-- 
2.7.4

