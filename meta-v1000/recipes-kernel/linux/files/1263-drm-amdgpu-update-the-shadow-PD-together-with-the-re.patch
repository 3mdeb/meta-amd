From 0f1b137459b729d3ddf95448a91b9ac8ddea300d Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Christian=20K=C3=B6nig?= <christian.koenig@amd.com>
Date: Fri, 16 Sep 2016 15:36:49 +0200
Subject: [PATCH 1263/1722] drm/amdgpu: update the shadow PD together with the
 real one v2
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Far less CPU cycles needed for this approach.

v2: fix typo

Signed-off-by: Christian KÃ¶nig <christian.koenig@amd.com>
Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
Signed-off-by: Kalyan Alle <kalyan.alle@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c | 155 ++-------------------------------
 1 file changed, 9 insertions(+), 146 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index 66ce464..cf769e6 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -616,16 +616,14 @@ static uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)
 	return result;
 }
 
-static int amdgpu_vm_update_pd_or_shadow(struct amdgpu_device *adev,
-                                         struct amdgpu_vm *vm,
-                                         bool shadow)
+int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
+				    struct amdgpu_vm *vm)
 {
+	struct amdgpu_bo *shadow;
         struct amdgpu_ring *ring;
-        struct amdgpu_bo *pd = shadow ? vm->page_directory->shadow :
-                vm->page_directory;
-        uint64_t pd_addr;
+        uint64_t pd_addr, shadow_addr;
         uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
-        uint64_t last_pde = ~0, last_pt = ~0;
+        uint64_t last_pde = ~0, last_pt = ~0 ,last_shadow = ~0;
         unsigned count = 0, pt_idx, ndw;
         struct amdgpu_job *job;
         struct amdgpu_pte_update_params params;
@@ -663,7 +661,7 @@ static int amdgpu_vm_update_pd_or_shadow(struct amdgpu_device *adev,
 
         /* walk over the address space and update the page directory */
         for (pt_idx = 0; pt_idx <= vm->max_pde_used; ++pt_idx) {
-                struct amdgpu_bo *bo = vm->page_tables[pt_idx].entry.robj;
+                struct amdgpu_bo *bo = vm->page_tables[pt_idx].bo;
                 uint64_t pde, pt;
 
                 if (bo == NULL)
@@ -677,13 +675,13 @@ static int amdgpu_vm_update_pd_or_shadow(struct amdgpu_device *adev,
                         if (r)
                                 return r;
                 }
+	
+		pt = amdgpu_bo_gpu_offset(bo);
                 if (vm->page_tables[pt_idx].addr == pt)
                         continue;
  
                 vm->page_tables[pt_idx].addr = pt;
 
-                pt = amdgpu_bo_gpu_offset(bo);
-
                 pde = pd_addr + pt_idx * 8;
                 if (((last_pde + 8 * count) != pde) ||
                     ((last_pt + incr * count) != pt) ||
@@ -748,131 +746,6 @@ error_free:
         return r;
 }
 
- /*
-  * amdgpu_vm_update_pdes - make sure that page directory is valid
-  *
-  * @adev: amdgpu_device pointer
-  * @vm: requested vm
-  * @start: start of GPU address range
-  * @end: end of GPU address range
-  *
-  * Allocates new page tables if necessary
-  * and updates the page directory.
-  * Returns 0 for success, error for failure.
- */
-int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
-                                    struct amdgpu_vm *vm)
-
-{
-        struct amdgpu_bo *shadow;
-	struct amdgpu_ring *ring;
-        uint64_t pd_addr, shadow_addr;
-	uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
-        uint64_t last_pde = ~0, last_pt = ~0, last_shadow = ~0;
-	unsigned count = 0, pt_idx, ndw;
-	struct amdgpu_job *job;
-        struct amdgpu_pte_update_params params;
-	struct fence *fence = NULL;
-
-	int r;
- 
-        if (!pd)
-                return 0;
- 
-        r = amdgpu_ttm_bind(&pd->tbo, &pd->tbo.mem);
-        if (r)
-                return r;
-
-        pd_addr = amdgpu_bo_gpu_offset(pd);
-	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
-
-	/* padding, etc. */
-	ndw = 64;
-
-	/* assume the worst case */
-	ndw += vm->max_pde_used * 6;
-	r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
-	if (r)
-		return r;
- 
-        memset(&params, 0, sizeof(params));
-        params.adev = adev;
-        params.ib = &job->ibs[0];
-	/* walk over the address space and update the page directory */
-	for (pt_idx = 0; pt_idx <= vm->max_pde_used; ++pt_idx) {
-                struct amdgpu_bo *bo = vm->page_tables[pt_idx].bo;
-		uint64_t pde, pt;
-
-		if (bo == NULL)
-			continue;
- 
-                if (bo->shadow) {
-                        struct amdgpu_bo *shadow = bo->shadow;
- 
-                        r = amdgpu_ttm_bind(&shadow->tbo, &shadow->tbo.mem);
-                        if (r)
-                                return r;
-                }
- 
-		pt = amdgpu_bo_gpu_offset(bo);
-                if (!shadow) {
-                        if (vm->page_tables[pt_idx].addr == pt)
-                                continue;
-                        vm->page_tables[pt_idx].addr = pt;
-                } else {
-                        if (vm->page_tables[pt_idx].shadow_addr == pt)
-                                continue;
-                        vm->page_tables[pt_idx].shadow_addr = pt;
-                }
-
-		pde = pd_addr + pt_idx * 8;
-		if (((last_pde + 8 * count) != pde) ||
-                    ((last_pt + incr * count) != pt) ||
-                    (count == AMDGPU_VM_MAX_UPDATE_SIZE)) {
-
-			if (count) {
-                                amdgpu_vm_do_set_ptes(&params, last_pde,
-                                                      last_pt, count, incr,
-                                                      AMDGPU_PTE_VALID);
-			}
-
-			count = 1;
-			last_pde = pde;
-			last_pt = pt;
-		} else {
-			++count;
-		}
-	}
-
-	if (count)
-                amdgpu_vm_do_set_ptes(&params, last_pde, last_pt,
-                                      count, incr, AMDGPU_PTE_VALID);
-
-	if (params.ib->length_dw != 0) {
-		amdgpu_ring_pad_ib(ring, params.ib);
-		amdgpu_sync_resv(adev, &job->sync, pd->tbo.resv,
-				 AMDGPU_FENCE_OWNER_VM);
-		WARN_ON(params.ib->length_dw > ndw);
-		r = amdgpu_job_submit(job, ring, &vm->entity,
-				      AMDGPU_FENCE_OWNER_VM, &fence);
-		if (r)
-			goto error_free;
-
-		amdgpu_bo_fence(pd, fence, true);
-		fence_put(vm->page_directory_fence);
-		vm->page_directory_fence = fence_get(fence);
-		fence_put(fence);
-	} else {
-		amdgpu_job_free(job);
-	}
-
-	return 0;
-
-error_free:
-	amdgpu_job_free(job);
-	return r;
-}
-
 /**
  * amdgpu_vm_update_ptes - make sure that page tables are valid
  *
@@ -1361,21 +1234,17 @@ int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
 	struct amdgpu_bo_va_mapping *mapping;
 	int r;
  
-        spin_lock(&vm->freed_lock);
 	while (!list_empty(&vm->freed)) {
 		mapping = list_first_entry(&vm->freed,
 			struct amdgpu_bo_va_mapping, list);
 		list_del(&mapping->list);
-                spin_unlock(&vm->freed_lock);
                 r = amdgpu_vm_bo_split_mapping(adev, NULL, 0, NULL, vm, mapping,
 					       0, 0, NULL);
 		kfree(mapping);
 		if (r)
 			return r;
  
-                spin_lock(&vm->freed_lock);
 	}
-        spin_unlock(&vm->freed_lock);
 
 	return 0;
 }
@@ -1633,12 +1502,9 @@ int amdgpu_vm_bo_unmap(struct amdgpu_device *adev,
 	trace_amdgpu_vm_bo_unmap(bo_va, mapping);
 
 	if (valid) {
-                spin_lock(&vm->freed_lock);
 		list_add(&mapping->list, &vm->freed);
-                spin_unlock(&vm->freed_lock);
-        } else {
+        else 
 		kfree(mapping);
-	}
 	
 	return 0;
 }
@@ -1669,9 +1535,7 @@ void amdgpu_vm_bo_rmv(struct amdgpu_device *adev,
 		list_del(&mapping->list);
 		interval_tree_remove(&mapping->it, &vm->va);
 		trace_amdgpu_vm_bo_unmap(bo_va, mapping);
-                spin_lock(&vm->freed_lock);
 		list_add(&mapping->list, &vm->freed);
-                spin_unlock(&vm->freed_lock);
 	}
 	list_for_each_entry_safe(mapping, next, &bo_va->invalids, list) {
 		list_del(&mapping->list);
@@ -1731,7 +1595,6 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 	INIT_LIST_HEAD(&vm->invalidated);
 	INIT_LIST_HEAD(&vm->cleared);
 	INIT_LIST_HEAD(&vm->freed);
-        spin_lock_init(&vm->freed_lock);
 
 	pd_size = amdgpu_vm_directory_size(adev);
 	pd_entries = amdgpu_vm_num_pdes(adev);
-- 
2.7.4

