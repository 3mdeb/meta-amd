From 39fef29611c963a7202b2b6512edae042ffc0133 Mon Sep 17 00:00:00 2001
From: Sanjay R Mehta <Sanju.Mehta@amd.com>
Date: Fri, 19 May 2017 03:36:44 +0530
Subject: [PATCH] drm/amdgpu: remove blank lines

Signed-off-by: Indrajit Das <indrajit-kumar.das@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_device.c | 210 ++++++++++++++---------------
 1 file changed, 104 insertions(+), 106 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
index 70b7c4e..a175a70 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
@@ -22,8 +22,8 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  *
  * Authors: Dave Airlie
- *	    Alex Deucher
- *	    Jerome Glisse
+ *          Alex Deucher
+ *          Jerome Glisse
  */
 #include <linux/kthread.h>
 #include <linux/console.h>
@@ -395,7 +395,7 @@ static void amdgpu_doorbell_fini(struct amdgpu_device *adev)
 
 /**
  * amdgpu_doorbell_get_kfd_info - Report doorbell configuration required to
- *				  setup amdkfd
+ *                                setup amdkfd
  *
  * @adev: amdgpu_device pointer
  * @aperture_base: output returning doorbell aperture base physical address
@@ -470,7 +470,7 @@ static int amdgpu_wb_init(struct amdgpu_device *adev)
 
 	if (adev->wb.wb_obj == NULL) {
 		r = amdgpu_bo_create(adev, AMDGPU_MAX_WB * 4, PAGE_SIZE, true,
-				     AMDGPU_GEM_DOMAIN_GTT, 0,	NULL, NULL,
+				     AMDGPU_GEM_DOMAIN_GTT, 0,  NULL, NULL,
 				     &adev->wb.wb_obj);
 		if (r) {
 			dev_warn(adev->dev, "(%d) create WB bo failed\n", r);
@@ -1068,7 +1068,7 @@ static void amdgpu_check_arguments(struct amdgpu_device *adev)
  * @pdev: pci dev pointer
  * @state: vga_switcheroo state
  *
- * Callback for the switcheroo driver.	Suspends or resumes the
+ * Callback for the switcheroo driver.  Suspends or resumes the
  * the asics before or after it is powered up using ACPI methods.
  */
 static void amdgpu_switcheroo_set_state(struct pci_dev *pdev, enum vga_switcheroo_state state)
@@ -1105,7 +1105,7 @@ static void amdgpu_switcheroo_set_state(struct pci_dev *pdev, enum vga_switchero
  *
  * @pdev: pci dev pointer
  *
- * Callback for the switcheroo driver.	Check of the switcheroo
+ * Callback for the switcheroo driver.  Check of the switcheroo
  * state can be changed.
  * Returns true if the state can be changed, false if not.
  */
@@ -1775,7 +1775,7 @@ int amdgpu_device_init(struct amdgpu_device *adev,
 
 	if (amdgpu_runtime_pm == 1)
 		runtime = true;
-        if (amdgpu_device_is_px(ddev))
+	if (amdgpu_device_is_px(ddev))
 		runtime = true;
 	vga_switcheroo_register_client(adev->pdev, &amdgpu_switcheroo_ops, runtime);
 	if (runtime)
@@ -1822,7 +1822,7 @@ int amdgpu_device_init(struct amdgpu_device *adev,
 	r = amdgpu_atombios_get_clock_info(adev);
 	if (r) {
 		dev_err(adev->dev, "amdgpu_atombios_get_clock_info failed\n");
-                goto failed;
+		goto failed;
 	}
 
 	/* init i2c buses */
@@ -2338,115 +2338,113 @@ err:
 int amdgpu_gpu_reset(struct amdgpu_device *adev)
 {
 	struct drm_atomic_state *state = NULL;
-
 	int i, r;
 	int resched;
-        bool need_full_reset; 
+	bool need_full_reset;
 
-        if (!amdgpu_check_soft_reset(adev)) {
-                DRM_INFO("No hardware hang detected. Did some blocks stall?\n");
-                return 0;
-        }
+	if (!amdgpu_check_soft_reset(adev)) {
+		DRM_INFO("No hardware hang detected. Did some blocks stall?\n");
+		return 0;
+	}
 
 	atomic_inc(&adev->gpu_reset_counter);
- 
-        /* block TTM */
-        resched = ttm_bo_lock_delayed_workqueue(&adev->mman.bdev);
-	
-	 /* store modesetting */
+
+	/* block TTM */
+	resched = ttm_bo_lock_delayed_workqueue(&adev->mman.bdev);
+	/* store modesetting */
 	if (amdgpu_device_has_dal_support(adev))
-		state = drm_atomic_helper_suspend(adev->ddev); 
- 	
+		state = drm_atomic_helper_suspend(adev->ddev);
+
 	/* block scheduler */
-        for (i = 0; i < AMDGPU_MAX_RINGS; ++i) {
-                struct amdgpu_ring *ring = adev->rings[i];
- 
-                if (!ring)
-                        continue;
-                kthread_park(ring->sched.thread);
-                amd_sched_hw_job_reset(&ring->sched);
-        }
-        /* after all hw jobs are reset, hw fence is meaningless, so force_completion */
-        amdgpu_fence_driver_force_completion(adev);
- 
-        need_full_reset = amdgpu_need_full_reset(adev);
- 
-        if (!need_full_reset) {
-                amdgpu_pre_soft_reset(adev);
-                r = amdgpu_soft_reset(adev);
-                amdgpu_post_soft_reset(adev);
-                if (r || amdgpu_check_soft_reset(adev)) {
-                        DRM_INFO("soft reset failed, will fallback to full reset!\n");
-                        need_full_reset = true;
-                }
-	}
- 
-        if (need_full_reset) {
-                r = amdgpu_suspend(adev);
+	for (i = 0; i < AMDGPU_MAX_RINGS; ++i) {
+		struct amdgpu_ring *ring = adev->rings[i];
+
+		if (!ring)
+			continue;
+		kthread_park(ring->sched.thread);
+		amd_sched_hw_job_reset(&ring->sched);
+	}
+	/* after all hw jobs are reset, hw fence is meaningless, so force_completion */
+	amdgpu_fence_driver_force_completion(adev);
+
+	need_full_reset = amdgpu_need_full_reset(adev);
+
+	if (!need_full_reset) {
+		amdgpu_pre_soft_reset(adev);
+		r = amdgpu_soft_reset(adev);
+		amdgpu_post_soft_reset(adev);
+		if (r || amdgpu_check_soft_reset(adev)) {
+			DRM_INFO("soft reset failed, will fallback to full reset!\n");
+			need_full_reset = true;
+		}
+	}
+
+	if (need_full_reset) {
+		r = amdgpu_suspend(adev);
 
 retry:
-                /* Disable fb access */
-                if (adev->mode_info.num_crtc) {
-                        struct amdgpu_mode_mc_save save;
-                        amdgpu_display_stop_mc_access(adev, &save);
-                        amdgpu_wait_for_idle(adev, AMD_IP_BLOCK_TYPE_GMC);
-                }
- 
-                r = amdgpu_asic_reset(adev);
-                /* post card */
-                amdgpu_atom_asic_init(adev->mode_info.atom_context);
- 
-                if (!r) {
-                        dev_info(adev->dev, "GPU reset succeeded, trying to resume\n");
-                        r = amdgpu_resume(adev);
-                }
+		/* Disable fb access */
+		if (adev->mode_info.num_crtc) {
+			struct amdgpu_mode_mc_save save;
+			amdgpu_display_stop_mc_access(adev, &save);
+			amdgpu_wait_for_idle(adev, AMD_IP_BLOCK_TYPE_GMC);
+		}
+
+		r = amdgpu_asic_reset(adev);
+		/* post card */
+		amdgpu_atom_asic_init(adev->mode_info.atom_context);
+
+		if (!r) {
+			dev_info(adev->dev, "GPU reset succeeded, trying to resume\n");
+			r = amdgpu_resume(adev);
+		}
 	}
 	if (!r) {
-                amdgpu_irq_gpu_reset_resume_helper(adev);
-                if (need_full_reset && amdgpu_need_backup(adev)) {
-                        r = amdgpu_ttm_recover_gart(adev);
-                        if (r)
-                               DRM_ERROR("gart recovery failed!!!\n");
-                }
-                r = amdgpu_ib_ring_tests(adev);
-                if (r) {
-                        dev_err(adev->dev, "ib ring test failed (%d).\n", r);
-                        r = amdgpu_suspend(adev);
-                        need_full_reset = true;
-                        goto retry;
-                }
-                /**
-                 * recovery vm page tables, since we cannot depend on VRAM is
-                 * consistent after gpu full reset.
-                 */
-                if (need_full_reset && amdgpu_need_backup(adev)) {
-                        struct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;
-                        struct amdgpu_bo *bo, *tmp;
-                        struct fence *fence = NULL, *next = NULL;
- 
-                        DRM_INFO("recover vram bo from shadow\n");
-                        mutex_lock(&adev->shadow_list_lock);
-                        list_for_each_entry_safe(bo, tmp, &adev->shadow_list, shadow_list) {
-                                amdgpu_recover_vram_from_shadow(adev, ring, bo, &next);
-                                if (fence) {
-                                        r = fence_wait(fence, false);
-                                        if (r) {
-                                                WARN(r, "recovery from shadow isn't comleted\n");
-                                                break;
-                                        }
-                                }
-
-                                fence_put(fence);
-                                fence = next;
-                        }
-                        mutex_unlock(&adev->shadow_list_lock);
-                        if (fence) {
-                                r = fence_wait(fence, false);
-                                if (r)
-                                        WARN(r, "recovery from shadow isn't comleted\n");
-                        }
-                        fence_put(fence);
-                } 
+		amdgpu_irq_gpu_reset_resume_helper(adev);
+		if (need_full_reset && amdgpu_need_backup(adev)) {
+			r = amdgpu_ttm_recover_gart(adev);
+			if (r)
+				DRM_ERROR("gart recovery failed!!!\n");
+		}
+		r = amdgpu_ib_ring_tests(adev);
+		if (r) {
+			dev_err(adev->dev, "ib ring test failed (%d).\n", r);
+			r = amdgpu_suspend(adev);
+			need_full_reset = true;
+			goto retry;
+		}
+		/**
+		 * recovery vm page tables, since we cannot depend on VRAM is
+		 * consistent after gpu full reset.
+		 */
+		if (need_full_reset && amdgpu_need_backup(adev)) {
+			struct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;
+			struct amdgpu_bo *bo, *tmp;
+			struct fence *fence = NULL, *next = NULL;
+
+			DRM_INFO("recover vram bo from shadow\n");
+			mutex_lock(&adev->shadow_list_lock);
+			list_for_each_entry_safe(bo, tmp, &adev->shadow_list, shadow_list) {
+				amdgpu_recover_vram_from_shadow(adev, ring, bo, &next);
+				if (fence) {
+					r = fence_wait(fence, false);
+					if (r) {
+						WARN(r, "recovery from shadow isn't comleted\n");
+						break;
+					}
+				}
+
+				fence_put(fence);
+				fence = next;
+			}
+			mutex_unlock(&adev->shadow_list_lock);
+			if (fence) {
+				r = fence_wait(fence, false);
+				if (r)
+					WARN(r, "recovery from shadow isn't comleted\n");
+			}
+			fence_put(fence);
+		}
 		for (i = 0; i < AMDGPU_MAX_RINGS; ++i) {
 			struct amdgpu_ring *ring = adev->rings[i];
 			if (!ring)
-- 
2.7.4

