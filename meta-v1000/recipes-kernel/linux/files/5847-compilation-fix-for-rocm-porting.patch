From 73504c2421a97b18907afdaa52b98775534a4a9b Mon Sep 17 00:00:00 2001
From: Chaudhary Amit Kumar <chaudharyamit.kumar@amd.com>
Date: Tue, 10 Jul 2018 19:20:32 +0530
Subject: [PATCH 5847/5855] compilation fix for rocm porting

Signed-off-by: Sanjay R Mehta <sanju.mehta@amd.com>

Signed-off-by: Chaudhary Amit Kumar <chaudharyamit.kumar@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu.h                |    1 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c         |  168 ++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h         |   69 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c   |   24 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c  |  110 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c  |  125 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c  |  107 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c   | 1432 ++++++++++----------
 drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c            |   26 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c             |  121 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h             |   13 +-
 drivers/gpu/drm/amd/amdgpu/soc15d.h                |    1 +
 drivers/gpu/drm/amd/amdkfd/Kconfig                 |    2 +-
 .../gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h | 1384 +++++++++++++++++++
 drivers/gpu/drm/amd/amdkfd/kfd_chardev.c           |   14 +-
 drivers/gpu/drm/amd/amdkfd/kfd_device.c            |   15 +-
 drivers/gpu/drm/amd/amdkfd/kfd_events.c            |   10 +-
 drivers/gpu/drm/amd/amdkfd/kfd_priv.h              |    6 +-
 drivers/gpu/drm/amd/amdkfd/kfd_process.c           |   15 +-
 drivers/gpu/drm/amd/include/kgd_kfd_interface.h    |   42 +-
 .../drm/amd/powerplay/hwmgr/cz_clockpowergating.c  |    8 +-
 include/uapi/drm/amdgpu_drm.h                      |    2 +-
 include/uapi/linux/kfd_ioctl.h                     |   35 +-
 23 files changed, 2566 insertions(+), 1164 deletions(-)
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdgpu/amdgpu.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
 create mode 100644 drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu.h b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
old mode 100644
new mode 100755
index 073069f..394f1e4
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
@@ -132,6 +132,7 @@ extern int amdgpu_job_hang_limit;
 extern int amdgpu_lbpw;
 extern int amdgpu_compute_multipipe;
 extern int amdgpu_gpu_recovery;
+extern int amdgpu_emu_mode;
 
 #ifdef CONFIG_DRM_AMDGPU_SI
 extern int amdgpu_si_support;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
index 63ac46a..38b5639 100755
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
@@ -20,7 +20,6 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  */
 
-#undef pr_fmt
 #define pr_fmt(fmt) "kfd2kgd: " fmt
 
 #include "amdgpu_amdkfd.h"
@@ -30,12 +29,11 @@
 #include "amdgpu_gfx.h"
 #include <linux/module.h>
 
-#define AMDKFD_SKIP_UNCOMPILED_CODE 1
 
 const struct kgd2kfd_calls *kgd2kfd;
 bool (*kgd2kfd_init_p)(unsigned int, const struct kgd2kfd_calls**);
 
-unsigned int global_compute_vmid_bitmap = 0xFF00;
+static unsigned int compute_vmid_bitmap = 0xFF00;
 
 int amdgpu_amdkfd_init(void)
 {
@@ -98,12 +96,6 @@ void amdgpu_amdkfd_device_probe(struct amdgpu_device *adev)
 		break;
 	case CHIP_VEGA10:
 	case CHIP_RAVEN:
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 16, 0)
-		if (rdev->asic_type == CHIP_RAVEN) {
-			dev_dbg(rdev->dev, "DKMS installed kfd does not support Raven for kernel < 4.16\n");
-			return;
-		}
-#endif
 		kfd2kgd = amdgpu_amdkfd_gfx_9_0_get_functions();
 		break;
 	default:
@@ -274,61 +266,6 @@ void amdgpu_amdkfd_gpu_reset(struct kgd_dev *kgd)
 	amdgpu_device_gpu_recover(adev, NULL, false);
 }
 
-int amdgpu_amdkfd_submit_ib(struct kgd_dev *kgd, enum kgd_engine_type engine,
-				uint32_t vmid, uint64_t gpu_addr,
-				uint32_t *ib_cmd, uint32_t ib_len)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
-	struct amdgpu_job *job;
-	struct amdgpu_ib *ib;
-	struct amdgpu_ring *ring;
-	struct fence *f = NULL;
-	int ret;
-
-	switch (engine) {
-	case KGD_ENGINE_MEC1:
-		ring = &adev->gfx.compute_ring[0];
-		break;
-	case KGD_ENGINE_SDMA1:
-		ring = &adev->sdma.instance[0].ring;
-		break;
-	case KGD_ENGINE_SDMA2:
-		ring = &adev->sdma.instance[1].ring;
-		break;
-	default:
-		pr_err("Invalid engine in IB submission: %d\n", engine);
-		ret = -EINVAL;
-		goto err;
-	}
-
-	ret = amdgpu_job_alloc(adev, 1, &job, NULL);
-	if (ret)
-		goto err;
-
-	ib = &job->ibs[0];
-	memset(ib, 0, sizeof(struct amdgpu_ib));
-
-	ib->gpu_addr = gpu_addr;
-	ib->ptr = ib_cmd;
-	ib->length_dw = ib_len;
-	/* This works for NO_HWS. TODO: need to handle without knowing VMID */
-	job->vmid = vmid;
-
-	ret = amdgpu_ib_schedule(ring, 1, ib, job, &f);
-	if (ret) {
-		DRM_ERROR("amdgpu: failed to schedule IB.\n");
-		goto err_ib_sched;
-	}
-
-	ret = fence_wait(f, false);
-
-err_ib_sched:
-	fence_put(f);
-	amdgpu_job_free(job);
-err:
-	return ret;
-}
-
 u32 pool_to_domain(enum kgd_memory_pool p)
 {
 	switch (p) {
@@ -414,7 +351,7 @@ void get_local_mem_info(struct kgd_dev *kgd,
 
 	address_mask = adev->dev->dma_mask ? ~*adev->dev->dma_mask :
 					     ~((1ULL << 32) - 1);
-	aper_limit = rdev->gmc.aper_base + rdev->gmc.aper_size;
+	aper_limit = adev->gmc.aper_base + adev->gmc.aper_size;
 	memset(mem_info, 0, sizeof(*mem_info));
 	if (!(adev->gmc.aper_base & address_mask ||
 			aper_limit & address_mask)) {
@@ -432,8 +369,13 @@ void get_local_mem_info(struct kgd_dev *kgd,
 			mem_info->local_mem_size_public,
 			mem_info->local_mem_size_private);
 
-	if (amdgpu_sriov_vf(adev))
-		mem_info->mem_clk_max = adev->clock.default_mclk / 100;
+       if (amdgpu_emu_mode == 1) {
+               mem_info->mem_clk_max = 100;
+               return;
+       }
+	
+        if (amdgpu_sriov_vf(adev))
+                mem_info->mem_clk_max = adev->clock.default_mclk / 100;
 	else
 		mem_info->mem_clk_max = amdgpu_dpm_get_mclk(adev, false) / 100;
 }
@@ -449,13 +391,17 @@ uint64_t get_gpu_clock_counter(struct kgd_dev *kgd)
 
 uint32_t get_max_engine_clock_in_mhz(struct kgd_dev *kgd)
 {
-	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
+       struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
 
-	/* the sclk is in quantas of 10kHz */
-	if (amdgpu_sriov_vf(adev))
-		return adev->clock.default_sclk / 100;
+       /* the sclk is in quantas of 10kHz */
+       if (amdgpu_emu_mode == 1)
+               return 100;
+
+       if (amdgpu_sriov_vf(adev))
+               return adev->clock.default_sclk / 100;
+
+       return amdgpu_dpm_get_sclk(adev, false) / 100;
 
-	return amdgpu_dpm_get_sclk(adev, false) / 100;
 }
 
 void get_cu_info(struct kgd_dev *kgd, struct kfd_cu_info *cu_info)
@@ -510,9 +456,8 @@ int amdgpu_amdkfd_get_dmabuf_info(struct kgd_dev *kgd, int dma_buf_fd,
 	adev = obj->dev->dev_private;
 	bo = gem_to_amdgpu_bo(obj);
 	if (!(bo->preferred_domains & (AMDGPU_GEM_DOMAIN_VRAM |
-				    AMDGPU_GEM_DOMAIN_GTT |
-				    AMDGPU_GEM_DOMAIN_DGMA)))
-		/* Only VRAM, GTT and DGMA BOs are supported */
+				    AMDGPU_GEM_DOMAIN_GTT)))
+		/* Only VRAM and GTT BOs are supported */
 		goto out_put;
 
 	r = 0;
@@ -526,12 +471,8 @@ int amdgpu_amdkfd_get_dmabuf_info(struct kgd_dev *kgd, int dma_buf_fd,
 		r = amdgpu_bo_get_metadata(bo, metadata_buffer, buffer_size,
 					   metadata_size, &metadata_flags);
 	if (flags) {
-		/* If the preferred domain is DGMA, set flags to VRAM because
-		 * KFD doesn't support allocating DGMA memory
-		 */
-		*flags = (bo->preferred_domains & (AMDGPU_GEM_DOMAIN_VRAM |
-				AMDGPU_GEM_DOMAIN_DGMA)) ?
-				ALLOC_MEM_FLAGS_VRAM : ALLOC_MEM_FLAGS_GTT;
+		*flags = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?
+			ALLOC_MEM_FLAGS_VRAM : ALLOC_MEM_FLAGS_GTT;
 
 		if (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)
 			*flags |= ALLOC_MEM_FLAGS_PUBLIC;
@@ -544,17 +485,72 @@ int amdgpu_amdkfd_get_dmabuf_info(struct kgd_dev *kgd, int dma_buf_fd,
 
 uint64_t amdgpu_amdkfd_get_vram_usage(struct kgd_dev *kgd)
 {
-        struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
-        uint64_t usage =
-                amdgpu_vram_mgr_usage(&adev->mman.bdev.man[TTM_PL_VRAM]);
-        return usage;
+	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
+	uint64_t usage =
+		amdgpu_vram_mgr_usage(&adev->mman.bdev.man[TTM_PL_VRAM]);
+	return usage;
+}
+
+int amdgpu_amdkfd_submit_ib(struct kgd_dev *kgd, enum kgd_engine_type engine,
+				uint32_t vmid, uint64_t gpu_addr,
+				uint32_t *ib_cmd, uint32_t ib_len)
+{
+	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
+	struct amdgpu_job *job;
+	struct amdgpu_ib *ib;
+	struct amdgpu_ring *ring;
+	struct fence *f = NULL;
+	int ret;
+
+	switch (engine) {
+	case KGD_ENGINE_MEC1:
+		ring = &adev->gfx.compute_ring[0];
+		break;
+	case KGD_ENGINE_SDMA1:
+		ring = &adev->sdma.instance[0].ring;
+		break;
+	case KGD_ENGINE_SDMA2:
+		ring = &adev->sdma.instance[1].ring;
+		break;
+	default:
+		pr_err("Invalid engine in IB submission: %d\n", engine);
+		ret = -EINVAL;
+		goto err;
+	}
+
+	ret = amdgpu_job_alloc(adev, 1, &job, NULL);
+	if (ret)
+		goto err;
+
+	ib = &job->ibs[0];
+	memset(ib, 0, sizeof(struct amdgpu_ib));
+
+	ib->gpu_addr = gpu_addr;
+	ib->ptr = ib_cmd;
+	ib->length_dw = ib_len;
+	/* This works for NO_HWS. TODO: need to handle without knowing VMID */
+	job->vmid = vmid;
+
+	ret = amdgpu_ib_schedule(ring, 1, ib, job, &f);
+	if (ret) {
+		DRM_ERROR("amdgpu: failed to schedule IB.\n");
+		goto err_ib_sched;
+	}
+
+	ret = fence_wait(f, false);
+
+err_ib_sched:
+	fence_put(f);
+	amdgpu_job_free(job);
+err:
+	return ret;
 }
 
 bool amdgpu_amdkfd_is_kfd_vmid(struct amdgpu_device *adev,
 			u32 vmid)
 {
 	if (adev->kfd) {
-		if ((1 << vmid) & global_compute_vmid_bitmap)
+		if ((1 << vmid) & compute_vmid_bitmap)
 			return true;
 	}
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
old mode 100644
new mode 100755
index 2ca271b..b2ab890
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
@@ -54,7 +54,6 @@ struct kgd_mem {
 	struct ttm_validate_buffer resv_list;
 	uint32_t domain;
 	unsigned int mapped_to_gpu_memory;
-	void *kptr;
 	uint64_t va;
 
 	uint32_t mapping_flags;
@@ -65,23 +64,20 @@ struct kgd_mem {
 
 	struct amdgpu_sync sync;
 
-	/* flags bitfield */
-	bool coherent      : 1;
-	bool no_substitute : 1;
-	bool aql_queue     : 1;
+	bool aql_queue;
 };
 
 /* KFD Memory Eviction */
 struct amdgpu_amdkfd_fence {
 	struct fence base;
-	void *mm;
+	struct mm_struct *mm;
 	spinlock_t lock;
 	char timeline_name[TASK_COMM_LEN];
 };
 
 struct amdgpu_amdkfd_fence *amdgpu_amdkfd_fence_create(u64 context,
-						       void *mm);
-bool amd_kfd_fence_check_mm(struct fence *f, void *mm);
+						       struct mm_struct *mm);
+bool amd_kfd_fence_check_mm(struct fence *f, struct mm_struct *mm);
 struct amdgpu_amdkfd_fence *to_amdgpu_amdkfd_fence(struct fence *f);
 
 struct amdkfd_process_info {
@@ -106,26 +102,6 @@ struct amdkfd_process_info {
 	struct pid *pid;
 };
 
-/* struct amdkfd_vm -
- * For Memory Eviction KGD requires a mechanism to keep track of all KFD BOs
- * belonging to a KFD process. All the VMs belonging to the same process point
- * to the same amdkfd_process_info.
- */
-struct amdkfd_vm {
-	/* Keep base as the first parameter for pointer compatibility between
-	 * amdkfd_vm and amdgpu_vm.
-	 */
-	struct amdgpu_vm base;
-
-	/* List node in amdkfd_process_info.vm_list_head*/
-	struct list_head vm_list_node;
-
-	struct amdgpu_device *adev;
-	/* Points to the KFD process VM info*/
-	struct amdkfd_process_info *process_info;
-};
-
-int amdgpu_amdkfd_init(void);
 void amdgpu_amdkfd_fini(void);
 
 void amdgpu_amdkfd_suspend(struct amdgpu_device *adev);
@@ -140,7 +116,6 @@ int amdgpu_amdkfd_evict_userptr(struct kgd_mem *mem, struct mm_struct *mm);
 int amdgpu_amdkfd_submit_ib(struct kgd_dev *kgd, enum kgd_engine_type engine,
 				uint32_t vmid, uint64_t gpu_addr,
 				uint32_t *ib_cmd, uint32_t ib_len);
-int amdgpu_amdkfd_gpuvm_restore_process_bos(void *process_info);
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_7_get_functions(void);
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_8_0_get_functions(void);
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_9_0_get_functions(void);
@@ -159,8 +134,6 @@ int amdgpu_amdkfd_post_reset(struct amdgpu_device *adev);
 void amdgpu_amdkfd_gpu_reset(struct kgd_dev *kgd);
 
 /* Shared API */
-int map_bo(struct amdgpu_device *rdev, uint64_t va, void *vm,
-		struct amdgpu_bo *bo, struct amdgpu_bo_va **bo_va);
 int alloc_gtt_mem(struct kgd_dev *kgd, size_t size,
 			void **mem_obj, uint64_t *gpu_addr,
 			void **cpu_ptr);
@@ -194,33 +167,38 @@ uint64_t amdgpu_amdkfd_get_vram_usage(struct kgd_dev *kgd);
 	})
 
 /* GPUVM API */
-int amdgpu_amdkfd_gpuvm_sync_memory(
-		struct kgd_dev *kgd, struct kgd_mem *mem, bool intr);
+int amdgpu_amdkfd_gpuvm_create_process_vm(struct kgd_dev *kgd, void **vm,
+					  void **process_info,
+					  struct fence **ef);
+int amdgpu_amdkfd_gpuvm_acquire_process_vm(struct kgd_dev *kgd,
+					   struct file *filp,
+					   void **vm, void **process_info,
+					   struct fence **ef);
+void amdgpu_amdkfd_gpuvm_destroy_cb(struct amdgpu_device *adev,
+				    struct amdgpu_vm *vm);
+void amdgpu_amdkfd_gpuvm_destroy_process_vm(struct kgd_dev *kgd, void *vm);
+uint32_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *vm);
 int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
 		struct kgd_dev *kgd, uint64_t va, uint64_t size,
 		void *vm, struct kgd_mem **mem,
 		uint64_t *offset, uint32_t flags);
 int amdgpu_amdkfd_gpuvm_free_memory_of_gpu(
-		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm);
+		struct kgd_dev *kgd, struct kgd_mem *mem);
 int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
 		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm);
 int amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(
 		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm);
+int amdgpu_amdkfd_gpuvm_sync_memory(
+		struct kgd_dev *kgd, struct kgd_mem *mem, bool intr);
+int amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_dev *kgd,
+		struct kgd_mem *mem, void **kptr, uint64_t *size);
+int amdgpu_amdkfd_gpuvm_restore_process_bos(void *process_info,
+					    struct fence **ef);
 
-int amdgpu_amdkfd_gpuvm_create_process_vm(struct kgd_dev *kgd, void **vm,
-					  void **process_info);
-void amdgpu_amdkfd_gpuvm_destroy_process_vm(struct kgd_dev *kgd, void *vm);
-
-uint32_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *vm);
 
 int amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct kgd_dev *kgd,
 					      struct kfd_vm_fault_info *info);
 
-int amdgpu_amdkfd_gpuvm_mmap_bo(
-		struct kgd_dev *kgd, struct vm_area_struct *vma);
-
-int amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_dev *kgd,
-		struct kgd_mem *mem, void **kptr);
 
 int amdgpu_amdkfd_gpuvm_pin_get_sg_table(struct kgd_dev *kgd,
 		struct kgd_mem *mem, uint64_t offset,
@@ -235,10 +213,9 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
 int amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_dev *kgd, void *vm,
 				      struct kgd_mem *mem,
 				      struct dma_buf **dmabuf);
-int amdgpu_amdkfd_gpuvm_evict_mem(struct kgd_mem *mem, struct mm_struct *mm);
-int amdgpu_amdkfd_gpuvm_restore_mem(struct kgd_mem *mem, struct mm_struct *mm);
 
 void amdgpu_amdkfd_gpuvm_init_mem_limits(void);
 void amdgpu_amdkfd_unreserve_system_memory_limit(struct amdgpu_bo *bo);
+
 #endif /* AMDGPU_AMDKFD_H_INCLUDED */
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c
index 811c315..43bd167 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_fence.c
@@ -1,5 +1,5 @@
 /*
- * Copyright 2016 Advanced Micro Devices, Inc.
+ * Copyright 2016-2018 Advanced Micro Devices, Inc.
  *
  * Permission is hereby granted, free of charge, to any person obtaining a
  * copy of this software and associated documentation files (the "Software"),
@@ -26,6 +26,7 @@
 #include <linux/stacktrace.h>
 #include <linux/sched.h>
 #include <linux/slab.h>
+#include <linux/mm.h>
 #include "amdgpu_amdkfd.h"
 
 const struct fence_ops amd_kfd_fence_ops;
@@ -61,7 +62,7 @@ static int amd_kfd_fence_signal(struct fence *f);
  */
 
 struct amdgpu_amdkfd_fence *amdgpu_amdkfd_fence_create(u64 context,
-						       void *mm)
+						       struct mm_struct *mm)
 {
 	struct amdgpu_amdkfd_fence *fence = NULL;
 
@@ -69,10 +70,8 @@ struct amdgpu_amdkfd_fence *amdgpu_amdkfd_fence_create(u64 context,
 	if (fence == NULL)
 		return NULL;
 
-	/* mm_struct mm is used as void pointer to identify the parent
-	 * KFD process. Don't dereference it. Fence and any threads using
-	 * mm is guranteed to be released before process termination.
-	 */
+	/* This reference gets released in amd_kfd_fence_release */
+//	mmgrab(mm);
 	fence->mm = mm;
 	get_task_comm(fence->timeline_name, current);
 	spin_lock_init(&fence->lock);
@@ -125,8 +124,7 @@ static bool amd_kfd_fence_enable_signaling(struct fence *f)
 	if (fence_is_signaled(f))
 		return true;
 
-	if (!kgd2kfd->schedule_evict_and_restore_process(
-				(struct mm_struct *)fence->mm, f))
+	if (!kgd2kfd->schedule_evict_and_restore_process(fence->mm, f))
 		return true;
 
 	return false;
@@ -146,24 +144,27 @@ static int amd_kfd_fence_signal(struct fence *f)
 	return ret;
 }
 
+
 /**
  * amd_kfd_fence_release - callback that fence can be freed
  *
  * @fence: fence
  *
  * This function is called when the reference count becomes zero.
- * It just RCU schedules freeing up the fence.
+ * Drops the mm_struct reference and RCU schedules freeing up the fence.
  */
 static void amd_kfd_fence_release(struct fence *f)
 {
 	struct amdgpu_amdkfd_fence *fence = to_amdgpu_amdkfd_fence(f);
+
 	/* Unconditionally signal the fence. The process is getting
 	 * terminated.
 	 */
 	if (WARN_ON(!fence))
 		return; /* Not an amdgpu_amdkfd_fence */
 
-	amd_kfd_fence_signal(f);
+//	amd_kfd_fence_signal(f);
+//	mmdrop(fence->mm);
 	kfree_rcu(f, rcu);
 }
 
@@ -174,7 +175,7 @@ static void amd_kfd_fence_release(struct fence *f)
  * @f: [IN] fence
  * @mm: [IN] mm that needs to be verified
  */
-bool amd_kfd_fence_check_mm(struct fence *f, void *mm)
+bool amd_kfd_fence_check_mm(struct fence *f, struct mm_struct *mm)
 {
 	struct amdgpu_amdkfd_fence *fence = to_amdgpu_amdkfd_fence(f);
 
@@ -194,4 +195,3 @@ const struct fence_ops amd_kfd_fence_ops = {
 	.wait = fence_default_wait,
 	.release = amd_kfd_fence_release,
 };
-
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
index 51b267f..0fe10a3 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
@@ -20,7 +20,6 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  */
 
-#undef pr_fmt
 #define pr_fmt(fmt) "kfd2kgd: " fmt
 
 #include <linux/fdtable.h>
@@ -42,7 +41,6 @@
 #include "gmc/gmc_7_1_sh_mask.h"
 #include "cik_structs.h"
 
-#define AMDKFD_SKIP_UNCOMPILED_CODE 1
 
 enum hqd_dequeue_request_type {
 	NO_ACTION = 0,
@@ -92,12 +90,7 @@ union TCP_WATCH_CNTL_BITS {
 	float f32All;
 };
 
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm,
-			int fd, uint32_t handle, struct kgd_mem **mem);
-
 static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type);
-static void set_scratch_backing_va(struct kgd_dev *kgd,
-                                        uint64_t va, uint32_t vmid);
 
 /*
  * Register access functions
@@ -108,8 +101,6 @@ static void kgd_program_sh_mem_settings(struct kgd_dev *kgd, uint32_t vmid,
 			uint32_t sh_mem_ape1_limit, uint32_t sh_mem_bases);
 static int kgd_set_pasid_vmid_mapping(struct kgd_dev *kgd, unsigned int pasid,
 			unsigned int vmid);
-static int kgd_init_pipeline(struct kgd_dev *kgd, uint32_t pipe_id,
-			uint32_t hpd_size, uint64_t hpd_gpu_addr);
 static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id);
 static int kgd_hqd_load(struct kgd_dev *kgd, void *mqd, uint32_t pipe_id,
 			uint32_t queue_id, uint32_t __user *wptr,
@@ -148,9 +139,8 @@ static uint32_t kgd_address_watch_get_offset(struct kgd_dev *kgd,
 static bool get_atc_vmid_pasid_mapping_valid(struct kgd_dev *kgd, uint8_t vmid);
 static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 							uint8_t vmid);
-static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid);
 static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid);
-static void set_num_of_requests(struct kgd_dev *dev, uint8_t num_of_req);
+static int invalidate_tlbs_vmid(struct kgd_dev *kgd, uint16_t vmid);
 static int alloc_memory_of_scratch(struct kgd_dev *kgd,
 					 uint64_t va, uint32_t vmid);
 static int write_config_static_mem(struct kgd_dev *kgd, bool swizzle_enable,
@@ -181,7 +171,6 @@ static int amdgpu_amdkfd_get_tile_config(struct kgd_dev *kgd,
 	config->num_macro_tile_configs =
 			ARRAY_SIZE(adev->gfx.config.macrotile_mode_array);
 
-
 	return 0;
 }
 
@@ -192,14 +181,13 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.get_gpu_clock_counter = get_gpu_clock_counter,
 	.get_max_engine_clock_in_mhz = get_max_engine_clock_in_mhz,
 	.create_process_vm = amdgpu_amdkfd_gpuvm_create_process_vm,
+	.acquire_process_vm = amdgpu_amdkfd_gpuvm_acquire_process_vm,
 	.destroy_process_vm = amdgpu_amdkfd_gpuvm_destroy_process_vm,
 	.get_process_page_dir = amdgpu_amdkfd_gpuvm_get_process_page_dir,
-	.open_graphic_handle = open_graphic_handle,
 	.alloc_pasid = amdgpu_pasid_alloc,
 	.free_pasid = amdgpu_pasid_free,
 	.program_sh_mem_settings = kgd_program_sh_mem_settings,
 	.set_pasid_vmid_mapping = kgd_set_pasid_vmid_mapping,
-	.init_pipeline = kgd_init_pipeline,
 	.init_interrupts = kgd_init_interrupts,
 	.hqd_load = kgd_hqd_load,
 	.hqd_sdma_load = kgd_hqd_sdma_load,
@@ -218,20 +206,17 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.get_atc_vmid_pasid_mapping_valid =
 			get_atc_vmid_pasid_mapping_valid,
 	.read_vmid_from_vmfault_reg = read_vmid_from_vmfault_reg,
-	.write_vmid_invalidate_request = write_vmid_invalidate_request,
 	.invalidate_tlbs = invalidate_tlbs,
+	.invalidate_tlbs_vmid = invalidate_tlbs_vmid,
 	.sync_memory = amdgpu_amdkfd_gpuvm_sync_memory,
 	.alloc_memory_of_gpu = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu,
 	.free_memory_of_gpu = amdgpu_amdkfd_gpuvm_free_memory_of_gpu,
 	.map_memory_to_gpu = amdgpu_amdkfd_gpuvm_map_memory_to_gpu,
 	.unmap_memory_to_gpu = amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu,
 	.get_fw_version = get_fw_version,
-	.set_scratch_backing_va = set_scratch_backing_va,
-	.set_num_of_requests = set_num_of_requests,
 	.get_cu_info = get_cu_info,
 	.alloc_memory_of_scratch = alloc_memory_of_scratch,
 	.write_config_static_mem = write_config_static_mem,
-	.mmap_bo = amdgpu_amdkfd_gpuvm_mmap_bo,
 	.map_gtt_bo_to_kernel = amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel,
 	.set_vm_context_page_table_base = set_vm_context_page_table_base,
 	.pin_get_sg_table_bo = amdgpu_amdkfd_gpuvm_pin_get_sg_table,
@@ -243,8 +228,8 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.submit_ib = amdgpu_amdkfd_submit_ib,
 	.get_tile_config = amdgpu_amdkfd_get_tile_config,
 	.restore_process_bos = amdgpu_amdkfd_gpuvm_restore_process_bos,
-        .copy_mem_to_mem = amdgpu_amdkfd_copy_mem_to_mem,
-        .get_vram_usage = amdgpu_amdkfd_get_vram_usage
+	.copy_mem_to_mem = amdgpu_amdkfd_copy_mem_to_mem,
+	.get_vram_usage = amdgpu_amdkfd_get_vram_usage
 };
 
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_7_get_functions()
@@ -252,12 +237,6 @@ struct kfd2kgd_calls *amdgpu_amdkfd_gfx_7_get_functions()
 	return (struct kfd2kgd_calls *)&kfd2kgd;
 }
 
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm,
-				int fd, uint32_t handle, struct kgd_mem **mem)
-{
-	return 0;
-}
-
 static inline struct amdgpu_device *get_amdgpu_device(struct kgd_dev *kgd)
 {
 	return (struct amdgpu_device *)kgd;
@@ -341,12 +320,6 @@ static int kgd_set_pasid_vmid_mapping(struct kgd_dev *kgd, unsigned int pasid,
 	return 0;
 }
 
-static int kgd_init_pipeline(struct kgd_dev *kgd, uint32_t pipe_id,
-				uint32_t hpd_size, uint64_t hpd_gpu_addr)
-{
-	/* amdgpu owns the per-pipe state */
-	return 0;
-}
 
 static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id)
 {
@@ -356,7 +329,6 @@ static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id)
 
 	mec = (pipe_id / adev->gfx.mec.num_pipe_per_mec) + 1;
 	pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
-
 	lock_srbm(kgd, mec, pipe, 0, 0);
 
 	WREG32(mmCPC_INT_CNTL, CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
@@ -475,8 +447,8 @@ static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 	struct cik_sdma_rlc_registers *m;
+	unsigned long end_jiffies;
 	uint32_t sdma_base_addr;
-	uint32_t temp, timeout = 2000;
 	uint32_t data;
 
 	m = get_sdma_mqd(mqd);
@@ -485,14 +457,14 @@ static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
 	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL,
 		m->sdma_rlc_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
 
+	end_jiffies = msecs_to_jiffies(2000) + jiffies;
 	while (true) {
-		temp = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
-		if (temp & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
+		data = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
+		if (data & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
 			break;
-		if (timeout == 0)
+		if (time_after(jiffies, end_jiffies))
 			return -ETIME;
-		msleep(10);
-		timeout -= 10;
+		usleep_range(500, 1000);
 	}
 	if (m->sdma_engine_id) {
 		data = RREG32(mmSDMA1_GFX_CONTEXT_CNTL);
@@ -843,41 +815,39 @@ static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 	return reg & ATC_VMID0_PASID_MAPPING__PASID_MASK;
 }
 
-static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
+static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
+	int vmid;
+	unsigned int tmp;
 
-	WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
-}
+	for (vmid = 0; vmid < 16; vmid++) {
+		if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid))
+			continue;
 
-static void set_scratch_backing_va(struct kgd_dev *kgd,
-                                       uint64_t va, uint32_t vmid)
-{
-        struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
+		tmp = RREG32(mmATC_VMID0_PASID_MAPPING + vmid);
+		if ((tmp & ATC_VMID0_PASID_MAPPING__VALID_MASK) &&
+			(tmp & ATC_VMID0_PASID_MAPPING__PASID_MASK) == pasid) {
+			WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
+			RREG32(mmVM_INVALIDATE_RESPONSE);
+			break;
+		}
+	}
 
-        lock_srbm(kgd, 0, 0, 0, vmid);
-        WREG32(mmSH_HIDDEN_PRIVATE_BASE_VMID, va);
-        unlock_srbm(kgd);
+	return 0;
 }
 
-static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid)
+static int invalidate_tlbs_vmid(struct kgd_dev *kgd, uint16_t vmid)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
-	int vmid;
 
-	for (vmid = 0; vmid < 16; vmid++) {
-		if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid))
-			continue;
-		if (RREG32(mmATC_VMID0_PASID_MAPPING + vmid) &
-			ATC_VMID0_PASID_MAPPING__VALID_MASK) {
-			if ((RREG32(mmATC_VMID0_PASID_MAPPING + vmid) &
-				ATC_VMID0_PASID_MAPPING__PASID_MASK) == pasid) {
-				WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
-				break;
-			}
-		}
+	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
+		pr_err("non kfd vmid\n");
+		return 0;
 	}
 
+	WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
+	RREG32(mmVM_INVALIDATE_RESPONSE);
 	return 0;
 }
 
@@ -957,24 +927,12 @@ static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type)
 	return hdr->common.ucode_version;
 }
 
-static void set_num_of_requests(struct kgd_dev *dev, uint8_t num_of_req)
-{
-	uint32_t value;
-	struct amdgpu_device *adev = get_amdgpu_device(dev);
-
-	value = RREG32(mmATC_ATS_DEBUG);
-	value &= ~ATC_ATS_DEBUG__NUM_REQUESTS_AT_ERR_MASK;
-	value |= (num_of_req << ATC_ATS_DEBUG__NUM_REQUESTS_AT_ERR__SHIFT);
-
-	WREG32(mmATC_ATS_DEBUG, value);
-}
-
 static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
 			uint32_t page_table_base)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	/* TODO: Don't use hardcoded VMIDs */
-	if (vmid < 8 || vmid > 15) {
+
+	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
 		pr_err("trying to set page table base for wrong VMID\n");
 		return;
 	}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
index d7d422e..85cef99 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
@@ -20,7 +20,6 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  */
 
-#undef pr_fmt
 #define pr_fmt(fmt) "kfd2kgd: " fmt
 
 #include <linux/module.h>
@@ -57,18 +56,12 @@ static const uint32_t watchRegs[MAX_WATCH_ADDRESSES * ADDRESS_WATCH_REG_MAX] = {
 };
 
 
-struct vi_sdma_mqd;
-
 static int create_process_gpumem(struct kgd_dev *kgd, uint64_t va, size_t size,
 		void *vm, struct kgd_mem **mem);
 static void destroy_process_gpumem(struct kgd_dev *kgd, struct kgd_mem *mem);
 
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm,
-				int fd, uint32_t handle, struct kgd_mem **mem);
-
 static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type);
-static void set_scratch_backing_va(struct kgd_dev *kgd,
-                                        uint64_t va, uint32_t vmid);
+
 /*
  * Register access functions
  */
@@ -79,8 +72,6 @@ static void kgd_program_sh_mem_settings(struct kgd_dev *kgd, uint32_t vmid,
 		uint32_t sh_mem_bases);
 static int kgd_set_pasid_vmid_mapping(struct kgd_dev *kgd, unsigned int pasid,
 		unsigned int vmid);
-static int kgd_init_pipeline(struct kgd_dev *kgd, uint32_t pipe_id,
-		uint32_t hpd_size, uint64_t hpd_gpu_addr);
 static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id);
 static int kgd_hqd_load(struct kgd_dev *kgd, void *mqd, uint32_t pipe_id,
 			uint32_t queue_id, uint32_t __user *wptr,
@@ -120,9 +111,6 @@ static bool get_atc_vmid_pasid_mapping_valid(struct kgd_dev *kgd,
 		uint8_t vmid);
 static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 		uint8_t vmid);
-static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid);
-static void set_num_of_requests(struct kgd_dev *kgd,
-			uint8_t num_of_requests);
 static int alloc_memory_of_scratch(struct kgd_dev *kgd,
 				 uint64_t va, uint32_t vmid);
 static int write_config_static_mem(struct kgd_dev *kgd, bool swizzle_enable,
@@ -130,6 +118,7 @@ static int write_config_static_mem(struct kgd_dev *kgd, bool swizzle_enable,
 static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
 		uint32_t page_table_base);
 static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid);
+static int invalidate_tlbs_vmid(struct kgd_dev *kgd, uint16_t vmid);
 
 /* Because of REG_GET_FIELD() being used, we put this function in the
  * asic specific file.
@@ -163,16 +152,15 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.get_gpu_clock_counter = get_gpu_clock_counter,
 	.get_max_engine_clock_in_mhz = get_max_engine_clock_in_mhz,
 	.create_process_vm = amdgpu_amdkfd_gpuvm_create_process_vm,
+	.acquire_process_vm = amdgpu_amdkfd_gpuvm_acquire_process_vm,
 	.destroy_process_vm = amdgpu_amdkfd_gpuvm_destroy_process_vm,
 	.create_process_gpumem = create_process_gpumem,
 	.destroy_process_gpumem = destroy_process_gpumem,
 	.get_process_page_dir = amdgpu_amdkfd_gpuvm_get_process_page_dir,
-	.open_graphic_handle = open_graphic_handle,
 	.alloc_pasid = amdgpu_pasid_alloc,
 	.free_pasid = amdgpu_pasid_free,
 	.program_sh_mem_settings = kgd_program_sh_mem_settings,
 	.set_pasid_vmid_mapping = kgd_set_pasid_vmid_mapping,
-	.init_pipeline = kgd_init_pipeline,
 	.init_interrupts = kgd_init_interrupts,
 	.hqd_load = kgd_hqd_load,
 	.hqd_sdma_load = kgd_hqd_sdma_load,
@@ -190,20 +178,17 @@ static const struct kfd2kgd_calls kfd2kgd = {
 			get_atc_vmid_pasid_mapping_pasid,
 	.get_atc_vmid_pasid_mapping_valid =
 			get_atc_vmid_pasid_mapping_valid,
-	.write_vmid_invalidate_request = write_vmid_invalidate_request,
 	.invalidate_tlbs = invalidate_tlbs,
+	.invalidate_tlbs_vmid = invalidate_tlbs_vmid,
 	.sync_memory = amdgpu_amdkfd_gpuvm_sync_memory,
 	.alloc_memory_of_gpu = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu,
 	.free_memory_of_gpu = amdgpu_amdkfd_gpuvm_free_memory_of_gpu,
 	.map_memory_to_gpu = amdgpu_amdkfd_gpuvm_map_memory_to_gpu,
 	.unmap_memory_to_gpu = amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu,
 	.get_fw_version = get_fw_version,
-	.set_scratch_backing_va = set_scratch_backing_va,
-	.set_num_of_requests = set_num_of_requests,
 	.get_cu_info = get_cu_info,
 	.alloc_memory_of_scratch = alloc_memory_of_scratch,
 	.write_config_static_mem = write_config_static_mem,
-	.mmap_bo = amdgpu_amdkfd_gpuvm_mmap_bo,
 	.map_gtt_bo_to_kernel = amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel,
 	.set_vm_context_page_table_base = set_vm_context_page_table_base,
 	.pin_get_sg_table_bo = amdgpu_amdkfd_gpuvm_pin_get_sg_table,
@@ -215,8 +200,8 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.submit_ib = amdgpu_amdkfd_submit_ib,
 	.get_tile_config = amdgpu_amdkfd_get_tile_config,
 	.restore_process_bos = amdgpu_amdkfd_gpuvm_restore_process_bos,
-        .copy_mem_to_mem = amdgpu_amdkfd_copy_mem_to_mem,
-        .get_vram_usage = amdgpu_amdkfd_get_vram_usage
+	.copy_mem_to_mem = amdgpu_amdkfd_copy_mem_to_mem,
+	.get_vram_usage = amdgpu_amdkfd_get_vram_usage
 };
 
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_8_0_get_functions()
@@ -236,12 +221,6 @@ static void destroy_process_gpumem(struct kgd_dev *kgd, struct kgd_mem *mem)
 
 }
 
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm,
-				int fd, uint32_t handle, struct kgd_mem **mem)
-{
-	return 0;
-}
-
 static inline struct amdgpu_device *get_amdgpu_device(struct kgd_dev *kgd)
 {
 	return (struct amdgpu_device *)kgd;
@@ -326,12 +305,6 @@ static int kgd_set_pasid_vmid_mapping(struct kgd_dev *kgd, unsigned int pasid,
 	return 0;
 }
 
-static int kgd_init_pipeline(struct kgd_dev *kgd, uint32_t pipe_id,
-				uint32_t hpd_size, uint64_t hpd_gpu_addr)
-{
-	/* amdgpu owns the per-pipe state */
-	return 0;
-}
 
 static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id)
 {
@@ -489,8 +462,8 @@ static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 	struct vi_sdma_mqd *m;
+	unsigned long end_jiffies;
 	uint32_t sdma_base_addr;
-	uint32_t temp, timeout = 2000;
 	uint32_t data;
 
 	m = get_sdma_mqd(mqd);
@@ -498,14 +471,14 @@ static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
 	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL,
 		m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
 
+	end_jiffies = msecs_to_jiffies(2000) + jiffies;
 	while (true) {
-		temp = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
-		if (temp & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
+		data = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
+		if (data & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
 			break;
-		if (timeout == 0)
+		if (time_after(jiffies, end_jiffies))
 			return -ETIME;
-		msleep(10);
-		timeout -= 10;
+		usleep_range(500, 1000);
 	}
 	if (m->sdma_engine_id) {
 		data = RREG32(mmSDMA1_GFX_CONTEXT_CNTL);
@@ -633,9 +606,13 @@ static int kgd_hqd_destroy(struct kgd_dev *kgd, void *mqd,
 	enum hqd_dequeue_request_type type;
 	unsigned long flags, end_jiffies;
 	int retry;
+	struct vi_mqd *m = get_mqd(mqd);
 
 	acquire_queue(kgd, pipe_id, queue_id);
 
+	if (m->cp_hqd_vmid == 0)
+		WREG32_FIELD(RLC_CP_SCHEDULERS, scheduler1, 0);
+
 	switch (reset_type) {
 	case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
 		type = DRAIN_PIPE;
@@ -776,13 +753,6 @@ static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 	return reg & ATC_VMID0_PASID_MAPPING__PASID_MASK;
 }
 
-static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
-
-	WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
-}
-
 /*
  * FIXME: Poliars test failed with this package, FIJI works fine
  * From the CP spec it does not official support the invalidation
@@ -793,7 +763,7 @@ static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
 static int invalidate_tlbs_with_kiq(struct amdgpu_device *adev, uint16_t pasid)
 {
 	signed long r;
-	struct fence *f;
+	uint32_t seq;
 	struct amdgpu_ring *ring = &adev->gfx.kiq.ring;
 
 	spin_lock(&adev->gfx.kiq.ring_lock);
@@ -802,22 +772,24 @@ static int invalidate_tlbs_with_kiq(struct amdgpu_device *adev, uint16_t pasid)
 	amdgpu_ring_write(ring,
 			PACKET3_INVALIDATE_TLBS_DST_SEL(1) |
 			PACKET3_INVALIDATE_TLBS_PASID(pasid));
-	amdgpu_fence_emit(ring, &f);
+	amdgpu_fence_emit_polling(ring, &seq);
 	amdgpu_ring_commit(ring);
 	spin_unlock(&adev->gfx.kiq.ring_lock);
 
-	r = fence_wait(f, false);
-	if (r)
+	r = amdgpu_fence_wait_polling(ring, seq, adev->usec_timeout);
+	if (r < 1) {
 		DRM_ERROR("wait for kiq fence error: %ld.\n", r);
-	fence_put(f);
+		return -ETIME;
+	}
 
-	return r;
+	return 0;
 }
 #endif
 static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
 	int vmid;
+	unsigned int tmp;
 
 #ifdef V8_SUPPORT_IT_OFFICIAL
 	struct amdgpu_ring *ring = &adev->gfx.kiq.ring;
@@ -829,19 +801,33 @@ static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid)
 	for (vmid = 0; vmid < 16; vmid++) {
 		if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid))
 			continue;
-		if (RREG32(mmATC_VMID0_PASID_MAPPING + vmid) &
-			ATC_VMID0_PASID_MAPPING__VALID_MASK) {
-			if ((RREG32(mmATC_VMID0_PASID_MAPPING + vmid) &
-				ATC_VMID0_PASID_MAPPING__PASID_MASK) == pasid) {
-				WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
-				break;
-			}
+
+		tmp = RREG32(mmATC_VMID0_PASID_MAPPING + vmid);
+		if ((tmp & ATC_VMID0_PASID_MAPPING__VALID_MASK) &&
+			(tmp & ATC_VMID0_PASID_MAPPING__PASID_MASK) == pasid) {
+			WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
+			RREG32(mmVM_INVALIDATE_RESPONSE);
+			break;
 		}
 	}
 
 	return 0;
 }
 
+static int invalidate_tlbs_vmid(struct kgd_dev *kgd, uint16_t vmid)
+{
+	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
+
+	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
+		pr_err("non kfd vmid %d\n", vmid);
+		return 0;
+	}
+
+	WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
+	RREG32(mmVM_INVALIDATE_RESPONSE);
+	return 0;
+}
+
 static int kgd_address_watch_disable(struct kgd_dev *kgd)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
@@ -956,17 +942,6 @@ static int alloc_memory_of_scratch(struct kgd_dev *kgd,
 	return 0;
 }
 
-static void set_scratch_backing_va(struct kgd_dev *kgd,
-                                        uint64_t va, uint32_t vmid)
-{
-        struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
- 
-        lock_srbm(kgd, 0, 0, 0, vmid);
-        WREG32(mmSH_HIDDEN_PRIVATE_BASE_VMID, va);
-        unlock_srbm(kgd);
-}
-
-
 static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
@@ -1024,18 +999,12 @@ static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type)
 	return hdr->common.ucode_version;
 }
 
-static void set_num_of_requests(struct kgd_dev *kgd,
-			uint8_t num_of_requests)
-{
-	pr_debug("This is a stub\n");
-}
-
 static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
 		uint32_t page_table_base)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-	/* TODO: Don't use hardcoded VMIDs */
-	if (vmid < 8 || vmid > 15) {
+
+	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
 		pr_err("trying to set page table base for wrong VMID\n");
 		return;
 	}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
index 28fd7ee..2784181 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
@@ -19,7 +19,7 @@
  * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
  * OTHER DEALINGS IN THE SOFTWARE.
  */
-#undef pr_fmt
+
 #define pr_fmt(fmt) "kfd2kgd: " fmt
 
 #include <linux/module.h>
@@ -99,9 +99,6 @@ static int create_process_gpumem(struct kgd_dev *kgd, uint64_t va, size_t size,
 		void *vm, struct kgd_mem **mem);
 static void destroy_process_gpumem(struct kgd_dev *kgd, struct kgd_mem *mem);
 
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm,
-				int fd, uint32_t handle, struct kgd_mem **mem);
-
 static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type);
 
 /*
@@ -114,8 +111,6 @@ static void kgd_program_sh_mem_settings(struct kgd_dev *kgd, uint32_t vmid,
 		uint32_t sh_mem_bases);
 static int kgd_set_pasid_vmid_mapping(struct kgd_dev *kgd, unsigned int pasid,
 		unsigned int vmid);
-static int kgd_init_pipeline(struct kgd_dev *kgd, uint32_t pipe_id,
-		uint32_t hpd_size, uint64_t hpd_gpu_addr);
 static int kgd_init_interrupts(struct kgd_dev *kgd, uint32_t pipe_id);
 static int kgd_hqd_load(struct kgd_dev *kgd, void *mqd, uint32_t pipe_id,
 			uint32_t queue_id, uint32_t __user *wptr,
@@ -138,7 +133,6 @@ static int kgd_hqd_destroy(struct kgd_dev *kgd, void *mqd,
 				uint32_t queue_id);
 static int kgd_hqd_sdma_destroy(struct kgd_dev *kgd, void *mqd,
 				unsigned int utimeout);
-static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid);
 static uint32_t get_watch_base_addr(struct amdgpu_device *adev);
 static int kgd_address_watch_disable(struct kgd_dev *kgd);
 static int kgd_address_watch_execute(struct kgd_dev *kgd,
@@ -157,9 +151,6 @@ static bool get_atc_vmid_pasid_mapping_valid(struct kgd_dev *kgd,
 		uint8_t vmid);
 static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 		uint8_t vmid);
-static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid);
-static void set_num_of_requests(struct kgd_dev *kgd,
-			uint8_t num_of_requests);
 static int alloc_memory_of_scratch(struct kgd_dev *kgd,
 				 uint64_t va, uint32_t vmid);
 static int write_config_static_mem(struct kgd_dev *kgd, bool swizzle_enable,
@@ -167,6 +158,7 @@ static int write_config_static_mem(struct kgd_dev *kgd, bool swizzle_enable,
 static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
 		uint32_t page_table_base);
 static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid);
+static int invalidate_tlbs_vmid(struct kgd_dev *kgd, uint16_t vmid);
 
 /* Because of REG_GET_FIELD() being used, we put this function in the
  * asic specific file.
@@ -207,16 +199,15 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.get_gpu_clock_counter = get_gpu_clock_counter,
 	.get_max_engine_clock_in_mhz = get_max_engine_clock_in_mhz,
 	.create_process_vm = amdgpu_amdkfd_gpuvm_create_process_vm,
+	.acquire_process_vm = amdgpu_amdkfd_gpuvm_acquire_process_vm,
 	.destroy_process_vm = amdgpu_amdkfd_gpuvm_destroy_process_vm,
 	.create_process_gpumem = create_process_gpumem,
 	.destroy_process_gpumem = destroy_process_gpumem,
 	.get_process_page_dir = amdgpu_amdkfd_gpuvm_get_process_page_dir,
-	.open_graphic_handle = open_graphic_handle,
 	.program_sh_mem_settings = kgd_program_sh_mem_settings,
 	.alloc_pasid = amdgpu_pasid_alloc,
 	.free_pasid = amdgpu_pasid_free,
 	.set_pasid_vmid_mapping = kgd_set_pasid_vmid_mapping,
-	.init_pipeline = kgd_init_pipeline,
 	.init_interrupts = kgd_init_interrupts,
 	.hqd_load = kgd_hqd_load,
 	.hqd_sdma_load = kgd_hqd_sdma_load,
@@ -234,19 +225,17 @@ static const struct kfd2kgd_calls kfd2kgd = {
 			get_atc_vmid_pasid_mapping_pasid,
 	.get_atc_vmid_pasid_mapping_valid =
 			get_atc_vmid_pasid_mapping_valid,
-	.write_vmid_invalidate_request = write_vmid_invalidate_request,
 	.invalidate_tlbs = invalidate_tlbs,
+	.invalidate_tlbs_vmid = invalidate_tlbs_vmid,
 	.sync_memory = amdgpu_amdkfd_gpuvm_sync_memory,
 	.alloc_memory_of_gpu = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu,
 	.free_memory_of_gpu = amdgpu_amdkfd_gpuvm_free_memory_of_gpu,
 	.map_memory_to_gpu = amdgpu_amdkfd_gpuvm_map_memory_to_gpu,
 	.unmap_memory_to_gpu = amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu,
 	.get_fw_version = get_fw_version,
-	.set_num_of_requests = set_num_of_requests,
 	.get_cu_info = get_cu_info,
 	.alloc_memory_of_scratch = alloc_memory_of_scratch,
 	.write_config_static_mem = write_config_static_mem,
-	.mmap_bo = amdgpu_amdkfd_gpuvm_mmap_bo,
 	.map_gtt_bo_to_kernel = amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel,
 	.set_vm_context_page_table_base = set_vm_context_page_table_base,
 	.pin_get_sg_table_bo = amdgpu_amdkfd_gpuvm_pin_get_sg_table,
@@ -258,8 +247,8 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.submit_ib = amdgpu_amdkfd_submit_ib,
 	.get_tile_config = amdgpu_amdkfd_get_tile_config,
 	.restore_process_bos = amdgpu_amdkfd_gpuvm_restore_process_bos,
-        .copy_mem_to_mem = amdgpu_amdkfd_copy_mem_to_mem,
-        .get_vram_usage = amdgpu_amdkfd_get_vram_usage
+	.copy_mem_to_mem = amdgpu_amdkfd_copy_mem_to_mem,
+	.get_vram_usage = amdgpu_amdkfd_get_vram_usage
 };
 
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_9_0_get_functions()
@@ -279,12 +268,6 @@ static void destroy_process_gpumem(struct kgd_dev *kgd, struct kgd_mem *mem)
 
 }
 
-static int open_graphic_handle(struct kgd_dev *kgd, uint64_t va, void *vm,
-				int fd, uint32_t handle, struct kgd_mem **mem)
-{
-	return 0;
-}
-
 static inline struct amdgpu_device *get_amdgpu_device(struct kgd_dev *kgd)
 {
 	return (struct amdgpu_device *)kgd;
@@ -406,12 +389,6 @@ static int kgd_set_pasid_vmid_mapping(struct kgd_dev *kgd, unsigned int pasid,
 	return 0;
 }
 
-static int kgd_init_pipeline(struct kgd_dev *kgd, uint32_t pipe_id,
-				uint32_t hpd_size, uint64_t hpd_gpu_addr)
-{
-	/* amdgpu owns the per-pipe state */
-	return 0;
-}
 
 /* TODO - RING0 form of field is obsolete, seems to date back to SI
  * but still works
@@ -610,7 +587,7 @@ static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_base_addr, sdmax_gfx_context_cntl;
-	uint32_t temp, timeout = 2000;
+	unsigned long end_jiffies;
 	uint32_t data;
 	uint64_t data64;
 	uint64_t __user *wptr64 = (uint64_t __user *)wptr;
@@ -625,14 +602,14 @@ static int kgd_hqd_sdma_load(struct kgd_dev *kgd, void *mqd,
 	WREG32(sdma_base_addr + mmSDMA0_RLC0_RB_CNTL,
 		m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
 
+	end_jiffies = msecs_to_jiffies(2000) + jiffies;
 	while (true) {
-		temp = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
-		if (temp & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
+		data = RREG32(sdma_base_addr + mmSDMA0_RLC0_CONTEXT_STATUS);
+		if (data & SDMA0_RLC0_CONTEXT_STATUS__IDLE_MASK)
 			break;
-		if (timeout == 0)
+		if (time_after(jiffies, end_jiffies))
 			return -ETIME;
-		msleep(10);
-		timeout -= 10;
+		usleep_range(500, 1000);
 	}
 	data = RREG32(sdmax_gfx_context_cntl);
 	data = REG_SET_FIELD(data, SDMA0_GFX_CONTEXT_CNTL,
@@ -759,6 +736,8 @@ static int kgd_hqd_destroy(struct kgd_dev *kgd, void *mqd,
 	enum hqd_dequeue_request_type type;
 	unsigned long end_jiffies;
 	uint32_t temp;
+	struct v9_mqd *m = get_mqd(mqd);
+
 #if 0
 	unsigned long flags;
 	int retry;
@@ -766,6 +745,9 @@ static int kgd_hqd_destroy(struct kgd_dev *kgd, void *mqd,
 
 	acquire_queue(kgd, pipe_id, queue_id);
 
+	if (m->cp_hqd_vmid == 0)
+		WREG32_FIELD15(GC, 0, RLC_CP_SCHEDULERS, scheduler1, 0);
+
 	switch (reset_type) {
 	case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
 		type = DRAIN_PIPE;
@@ -917,16 +899,23 @@ static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
 	uint32_t req = (1 << vmid) |
-		(1 << VM_INVALIDATE_ENG16_REQ__FLUSH_TYPE__SHIFT) | /* light */
+		(0 << VM_INVALIDATE_ENG16_REQ__FLUSH_TYPE__SHIFT) | /* legacy */
 		VM_INVALIDATE_ENG16_REQ__INVALIDATE_L2_PTES_MASK |
 		VM_INVALIDATE_ENG16_REQ__INVALIDATE_L2_PDE0_MASK |
 		VM_INVALIDATE_ENG16_REQ__INVALIDATE_L2_PDE1_MASK |
 		VM_INVALIDATE_ENG16_REQ__INVALIDATE_L2_PDE2_MASK |
 		VM_INVALIDATE_ENG16_REQ__INVALIDATE_L1_PTES_MASK;
 
-	spin_lock(&adev->tlb_invalidation_lock);
 
-	/* Use light weight invalidation.
+	 mutex_lock(&adev->srbm_mutex);
+        /* Use legacy mode tlb invalidation.
+         *
+         * Currently on Raven the code below is broken for anything but
+         * legacy mode due to a MMHUB power gating problem. A workaround
+         * is for MMHUB to wait until the condition PER_VMID_INVALIDATE_REQ
+         * == PER_VMID_INVALIDATE_ACK instead of simply waiting for the ack
+         * bit.
+
 	 *
 	 * TODO 1: agree on the right set of invalidation registers for
 	 * KFD use. Use the last one for now. Invalidate both GC and
@@ -960,15 +949,13 @@ static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
 					mmMMHUB_VM_INVALIDATE_ENG16_ACK)) &
 					(1 << vmid)))
 		cpu_relax();
-
-	spin_unlock(&adev->tlb_invalidation_lock);
+	mutex_unlock(&adev->srbm_mutex);
 
 }
 
 static int invalidate_tlbs_with_kiq(struct amdgpu_device *adev, uint16_t pasid)
 {
 	signed long r;
-	struct fence *f;
 	struct amdgpu_ring *ring = &adev->gfx.kiq.ring;
 
 	spin_lock(&adev->gfx.kiq.ring_lock);
@@ -977,17 +964,19 @@ static int invalidate_tlbs_with_kiq(struct amdgpu_device *adev, uint16_t pasid)
 	amdgpu_ring_write(ring,
 			PACKET3_INVALIDATE_TLBS_DST_SEL(1) |
 			PACKET3_INVALIDATE_TLBS_ALL_HUB(1) |
-			PACKET3_INVALIDATE_TLBS_PASID(pasid));
-	amdgpu_fence_emit(ring, &f);
+			PACKET3_INVALIDATE_TLBS_PASID(pasid) |
+			PACKET3_INVALIDATE_TLBS_FLUSH_TYPE(0)); /* legacy */
+
+	amdgpu_fence_emit_polling(ring, &seq);
 	amdgpu_ring_commit(ring);
 	spin_unlock(&adev->gfx.kiq.ring_lock);
 
-	r = fence_wait(f, false);
-	if (r)
+	r = amdgpu_fence_wait_polling(ring, seq, adev->usec_timeout);
+	if (r < 1) {
 		DRM_ERROR("wait for kiq fence error: %ld.\n", r);
-	fence_put(f);
-
-	return r;
+		return -ETIME;
+	}
+	return 0;
 }
 
 static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid)
@@ -1014,6 +1003,19 @@ static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid)
 	return 0;
 }
 
+static int invalidate_tlbs_vmid(struct kgd_dev *kgd, uint16_t vmid)
+{
+	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
+
+	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
+		pr_err("non kfd vmid %d\n", vmid);
+		return 0;
+	}
+
+	write_vmid_invalidate_request(kgd, vmid);
+	return 0;
+}
+
 static int kgd_address_watch_disable(struct kgd_dev *kgd)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
@@ -1176,12 +1178,6 @@ static uint16_t get_fw_version(struct kgd_dev *kgd, enum kgd_engine_type type)
 	return hdr->common.ucode_version;
 }
 
-static void set_num_of_requests(struct kgd_dev *kgd,
-			uint8_t num_of_requests)
-{
-	pr_debug("This is a stub\n");
-}
-
 static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
 		uint32_t page_table_base)
 {
@@ -1189,8 +1185,7 @@ static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
 	uint64_t base = (uint64_t)page_table_base << PAGE_SHIFT |
 		AMDGPU_PTE_VALID;
 
-	/* TODO: Don't use hardcoded VMIDs */
-	if (vmid < 8 || vmid > 15) {
+	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
 		pr_err("trying to set page table base for wrong VMID %u\n",
 		       vmid);
 		return;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
index dbec057..cfaa6dd3 100755
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
@@ -20,26 +20,14 @@
  * OTHER DEALINGS IN THE SOFTWARE.
  */
 
-#undef pr_fmt
 #define pr_fmt(fmt) "kfd2kgd: " fmt
 
-#include <linux/module.h>
-#include <linux/fdtable.h>
-#include <linux/uaccess.h>
-#include <linux/firmware.h>
 #include <linux/list.h>
+#include <linux/mm.h>
 #include <drm/drmP.h>
-#include <linux/dma-buf.h>
-#include <linux/pagemap.h>
+#include "amdgpu_object.h"
+#include "amdgpu_vm.h"
 #include "amdgpu_amdkfd.h"
-#include "amdgpu_ucode.h"
-#include "gca/gfx_8_0_sh_mask.h"
-#include "gca/gfx_8_0_d.h"
-#include "gca/gfx_8_0_enum.h"
-#include "oss/oss_3_0_sh_mask.h"
-#include "oss/oss_3_0_d.h"
-#include "gmc/gmc_8_1_sh_mask.h"
-#include "gmc/gmc_8_1_d.h"
 
 /* Special VM and GART address alignment needed for VI pre-Fiji due to
  * a HW bug.
@@ -50,15 +38,13 @@
 #define AMDGPU_AMDKFD_USERPTR_BO (1ULL << 63)
 
 /* Impose limit on how much memory KFD can use */
-struct kfd_mem_usage_limit {
+static struct {
 	uint64_t max_system_mem_limit;
 	uint64_t max_userptr_mem_limit;
 	int64_t system_mem_used;
 	int64_t userptr_mem_used;
 	spinlock_t mem_limit_lock;
-};
-
-static struct kfd_mem_usage_limit kfd_mem_limit;
+} kfd_mem_limit;
 
 /* Struct used for amdgpu_amdkfd_bo_validate */
 struct amdgpu_vm_parser {
@@ -181,7 +167,8 @@ void amdgpu_amdkfd_unreserve_system_memory_limit(struct amdgpu_bo *bo)
 	if (bo->flags & AMDGPU_AMDKFD_USERPTR_BO) {
 		kfd_mem_limit.system_mem_used -= bo->tbo.acc_size;
 		kfd_mem_limit.userptr_mem_used -= amdgpu_bo_size(bo);
-	} else if (bo->preferred_domains == AMDGPU_GEM_DOMAIN_GTT) {
+	} else if (bo->preferred_domains == AMDGPU_GEM_DOMAIN_GTT &&
+		   !bo->tbo.sg) {
 		kfd_mem_limit.system_mem_used -=
 			(bo->tbo.acc_size + amdgpu_bo_size(bo));
 	}
@@ -268,7 +255,6 @@ static int amdgpu_amdkfd_remove_eviction_fence(struct amdgpu_bo *bo,
 	/* Alloc memory for count number of eviction fence pointers. Fill the
 	 * ef_list array and ef_count
 	 */
-
 	fence_list = kcalloc(count, sizeof(struct amdgpu_amdkfd_fence *),
 			     GFP_KERNEL);
 	if (!fence_list)
@@ -332,68 +318,6 @@ static void amdgpu_amdkfd_add_eviction_fence(struct amdgpu_bo *bo,
 	kfree(ef_list);
 }
 
-static int add_bo_to_vm(struct amdgpu_device *adev, struct kgd_mem *mem,
-		struct amdgpu_vm *avm, bool is_aql,
-		struct kfd_bo_va_list **p_bo_va_entry)
-{
-	int ret;
-	struct kfd_bo_va_list *bo_va_entry;
-	struct amdgpu_bo *bo = mem->bo;
-	uint64_t va = mem->va;
-	struct list_head *list_bo_va = &mem->bo_va_list;
-	unsigned long bo_size = bo->tbo.mem.size;
-
-	if (is_aql)
-		va += bo_size;
-
-	bo_va_entry = kzalloc(sizeof(*bo_va_entry), GFP_KERNEL);
-	if (!bo_va_entry)
-		return -ENOMEM;
-
-	if (!va) {
-		pr_err("Invalid VA when adding BO to VM\n");
-		return -EINVAL;
-	}
-
-	pr_debug("\t add VA 0x%llx - 0x%llx to vm %p\n", va,
-			va + bo_size, avm);
-
-	/* Add BO to VM internal data structures*/
-	bo_va_entry->bo_va = amdgpu_vm_bo_add(adev, avm, bo);
-	if (bo_va_entry->bo_va == NULL) {
-		ret = -EINVAL;
-		pr_err("Failed to add BO object to VM. ret == %d\n",
-				ret);
-		goto err_vmadd;
-	}
-
-	bo_va_entry->va = va;
-	bo_va_entry->pte_flags = amdgpu_gmc_get_pte_flags(adev,
-							 mem->mapping_flags);
-	bo_va_entry->kgd_dev = (void *)adev;
-	list_add(&bo_va_entry->bo_list, list_bo_va);
-
-	if (p_bo_va_entry)
-		*p_bo_va_entry = bo_va_entry;
-
-	return 0;
-
-err_vmadd:
-	kfree(bo_va_entry);
-	return ret;
-}
-
-static void remove_bo_from_vm(struct amdgpu_device *adev,
-		struct kfd_bo_va_list *entry, unsigned long size)
-{
-	pr_debug("\t remove VA 0x%llx - 0x%llx in entry %p\n",
-			entry->va,
-			entry->va + size, entry);
-	amdgpu_vm_bo_rmv(adev, entry->bo_va);
-	list_del(&entry->bo_list);
-	kfree(entry);
-}
-
 static int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,
 				     bool wait)
 {
@@ -433,6 +357,23 @@ static int amdgpu_amdkfd_validate(void *param, struct amdgpu_bo *bo)
 	return amdgpu_amdkfd_bo_validate(bo, p->domain, p->wait);
 }
 
+static u64 get_vm_pd_gpu_offset(struct amdgpu_vm *vm)
+{
+	struct amdgpu_device *adev =
+		amdgpu_ttm_adev(vm->root.base.bo->tbo.bdev);
+	u64 offset;
+	uint64_t flags = AMDGPU_PTE_VALID;
+
+	offset = amdgpu_bo_gpu_offset(vm->root.base.bo);
+
+	/* On some ASICs the FB doesn't start at 0. Adjust FB offset
+	 * to an actual MC address.
+	 */
+	adev->gmc.gmc_funcs->get_vm_pde(adev, -1, &offset, &flags);
+
+	return offset;
+}
+
 /* vm_validate_pt_pd_bos - Validate page table and directory BOs
  *
  * Page directories are not updated here because huge page handling
@@ -440,18 +381,17 @@ static int amdgpu_amdkfd_validate(void *param, struct amdgpu_bo *bo)
  * again. Page directories are only updated after updating page
  * tables.
  */
-static int vm_validate_pt_pd_bos(struct amdkfd_vm *vm)
+static int vm_validate_pt_pd_bos(struct amdgpu_vm *vm)
 {
-	struct amdgpu_bo *pd = vm->base.root.base.bo;
+	struct amdgpu_bo *pd = vm->root.base.bo;
 	struct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);
 	struct amdgpu_vm_parser param;
-	uint64_t addr, flags = AMDGPU_PTE_VALID;
 	int ret;
 
 	param.domain = AMDGPU_GEM_DOMAIN_VRAM;
 	param.wait = false;
 
-	ret = amdgpu_vm_validate_pt_bos(adev, &vm->base, amdgpu_amdkfd_validate,
+	ret = amdgpu_vm_validate_pt_bos(adev, vm, amdgpu_amdkfd_validate,
 					&param);
 	if (ret) {
 		pr_err("amdgpu: failed to validate PT BOs\n");
@@ -464,7 +404,133 @@ static int vm_validate_pt_pd_bos(struct amdkfd_vm *vm)
 		return ret;
 	}
 
+	vm->pd_phys_addr = get_vm_pd_gpu_offset(vm);
+
+	if (vm->use_cpu_for_update) {
+		ret = amdgpu_bo_kmap(pd, NULL);
+		if (ret) {
+			pr_err("amdgpu: failed to kmap PD, ret=%d\n", ret);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int vm_update_pds(struct amdgpu_vm *vm, struct amdgpu_sync *sync)
+{
+	struct amdgpu_bo *pd = vm->root.base.bo;
+	struct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);
+	int ret;
+
+	ret = amdgpu_vm_update_directories(adev, vm);
+	if (ret)
+		return ret;
+
+	return amdgpu_sync_fence(NULL, sync, vm->last_update, false);
+}
+
+/* add_bo_to_vm - Add a BO to a VM
+ *
+ * Everything that needs to bo done only once when a BO is first added
+ * to a VM. It can later be mapped and unmapped many times without
+ * repeating these steps.
+ *
+ * 1. Allocate and initialize BO VA entry data structure
+ * 2. Add BO to the VM
+ * 3. Determine ASIC-specific PTE flags
+ * 4. Alloc page tables and directories if needed
+ * 4a.  Validate new page tables and directories
+ */
+static int add_bo_to_vm(struct amdgpu_device *adev, struct kgd_mem *mem,
+		struct amdgpu_vm *vm, bool is_aql,
+		struct kfd_bo_va_list **p_bo_va_entry)
+{
+	int ret;
+	struct kfd_bo_va_list *bo_va_entry;
+	struct amdgpu_bo *pd = vm->root.base.bo;
+	struct amdgpu_bo *bo = mem->bo;
+	uint64_t va = mem->va;
+	struct list_head *list_bo_va = &mem->bo_va_list;
+	unsigned long bo_size = bo->tbo.mem.size;
+
+	if (!va) {
+		pr_err("Invalid VA when adding BO to VM\n");
+		return -EINVAL;
+	}
+
+	if (is_aql)
+		va += bo_size;
+
+	bo_va_entry = kzalloc(sizeof(*bo_va_entry), GFP_KERNEL);
+	if (!bo_va_entry)
+		return -ENOMEM;
+
+	pr_debug("\t add VA 0x%llx - 0x%llx to vm %p\n", va,
+			va + bo_size, vm);
+
+	/* Add BO to VM internal data structures*/
+	bo_va_entry->bo_va = amdgpu_vm_bo_add(adev, vm, bo);
+	if (!bo_va_entry->bo_va) {
+		ret = -EINVAL;
+		pr_err("Failed to add BO object to VM. ret == %d\n",
+				ret);
+		goto err_vmadd;
+	}
+
+	bo_va_entry->va = va;
+	bo_va_entry->pte_flags = amdgpu_gmc_get_pte_flags(adev,
+							 mem->mapping_flags);
+	bo_va_entry->kgd_dev = (void *)adev;
+	list_add(&bo_va_entry->bo_list, list_bo_va);
+
+	if (p_bo_va_entry)
+		*p_bo_va_entry = bo_va_entry;
+
+	/* Allocate new page tables if neeeded and validate
+	 * them. Clearing of new page tables and validate need to wait
+	 * on move fences. We don't want that to trigger the eviction
+	 * fence, so remove it temporarily.
+	 */
+	amdgpu_amdkfd_remove_eviction_fence(pd,
+					vm->process_info->eviction_fence,
+					NULL, NULL);
+
+	ret = amdgpu_vm_alloc_pts(adev, vm, va, amdgpu_bo_size(bo));
+	if (ret) {
+		pr_err("Failed to allocate pts, err=%d\n", ret);
+		goto err_alloc_pts;
+	}
+
+	ret = vm_validate_pt_pd_bos(vm);
+	if (ret) {
+		pr_err("validate_pt_pd_bos() failed\n");
+		goto err_alloc_pts;
+	}
+
+	/* Add the eviction fence back */
+	amdgpu_bo_fence(pd, &vm->process_info->eviction_fence->base, true);
+
 	return 0;
+
+err_alloc_pts:
+	amdgpu_bo_fence(pd, &vm->process_info->eviction_fence->base, true);
+	amdgpu_vm_bo_rmv(adev, bo_va_entry->bo_va);
+	list_del(&bo_va_entry->bo_list);
+err_vmadd:
+	kfree(bo_va_entry);
+	return ret;
+}
+
+static void remove_bo_from_vm(struct amdgpu_device *adev,
+		struct kfd_bo_va_list *entry, unsigned long size)
+{
+	pr_debug("\t remove VA 0x%llx - 0x%llx in entry %p\n",
+			entry->va,
+			entry->va + size, entry);
+	amdgpu_vm_bo_rmv(adev, entry->bo_va);
+	list_del(&entry->bo_list);
+	kfree(entry);
 }
 
 static void add_kgd_mem_to_kfd_bo_list(struct kgd_mem *mem,
@@ -556,7 +622,7 @@ static int init_user_pages(struct kgd_mem *mem, struct mm_struct *mm,
 	if (ret)
 		release_pages(mem->user_pages, bo->tbo.ttm->num_pages, 0);
 free_out:
-	drm_free_large(mem->user_pages);
+	kvfree(mem->user_pages);
 	mem->user_pages = NULL;
 unregister_out:
 	if (ret)
@@ -566,134 +632,25 @@ static int init_user_pages(struct kgd_mem *mem, struct mm_struct *mm,
 	return ret;
 }
 
-static int __alloc_memory_of_gpu(struct kgd_dev *kgd, uint64_t va,
-		uint64_t size, void *vm, struct kgd_mem **mem,
-		uint64_t *offset, u32 domain, u64 flags,
-		struct sg_table *sg, bool aql_queue,
-		bool readonly, bool execute, bool coherent, bool no_sub,
-		bool userptr)
-{
-	struct amdgpu_device *adev;
-	int ret;
-	struct amdgpu_bo *bo;
-	uint64_t user_addr = 0;
-	int byte_align;
-	u32 alloc_domain;
-	uint32_t mapping_flags;
-	struct amdkfd_vm *kfd_vm = (struct amdkfd_vm *)vm;
-
-	if (aql_queue)
-		size = size >> 1;
-	if (userptr) {
-		if (!offset || !*offset)
-			return -EINVAL;
-		user_addr = *offset;
-	}
-
-	adev = get_amdgpu_device(kgd);
-	byte_align = (adev->family == AMDGPU_FAMILY_VI &&
-			adev->asic_type != CHIP_FIJI &&
-			adev->asic_type != CHIP_POLARIS10 &&
-			adev->asic_type != CHIP_POLARIS11) ?
-			VI_BO_SIZE_ALIGN : 1;
-
-	*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);
-	if (*mem == NULL) {
-		ret = -ENOMEM;
-		goto err;
-	}
-	INIT_LIST_HEAD(&(*mem)->bo_va_list);
-	mutex_init(&(*mem)->lock);
-	(*mem)->coherent = coherent;
-	(*mem)->no_substitute = no_sub;
-	(*mem)->aql_queue = aql_queue;
-
-	mapping_flags = AMDGPU_VM_PAGE_READABLE;
-	if (!readonly)
-		mapping_flags |= AMDGPU_VM_PAGE_WRITEABLE;
-	if (execute)
-		mapping_flags |= AMDGPU_VM_PAGE_EXECUTABLE;
-	if (coherent)
-		mapping_flags |= AMDGPU_VM_MTYPE_UC;
-	else
-		mapping_flags |= AMDGPU_VM_MTYPE_NC;
-
-	(*mem)->mapping_flags = mapping_flags;
-
-	alloc_domain = userptr ? AMDGPU_GEM_DOMAIN_CPU : domain;
-
-	amdgpu_sync_create(&(*mem)->sync);
-
-	ret = amdgpu_amdkfd_reserve_system_mem_limit(adev, size, alloc_domain);
-	if (ret) {
-		pr_debug("Insufficient system memory\n");
-		goto err_bo_create;
-	}
-
-	pr_debug("\t create BO VA 0x%llx size 0x%llx domain %s\n",
-			va, size, domain_string(alloc_domain));
-
-	/* Allocate buffer object. Userptr objects need to start out
-	 * in the CPU domain, get moved to GTT when pinned.
-	 */
-	ret = amdgpu_bo_create(adev, size, byte_align, false,
-				alloc_domain,
-			       flags, sg, NULL, &bo);
-	if (ret != 0) {
-		pr_debug("Failed to create BO on domain %s. ret %d\n",
-				domain_string(alloc_domain), ret);
-		unreserve_system_mem_limit(adev, size, alloc_domain);
-		goto err_bo_create;
-	}
-	bo->kfd_bo = *mem;
-	(*mem)->bo = bo;
-	if (userptr)
-		bo->flags |= AMDGPU_AMDKFD_USERPTR_BO;
-
-	(*mem)->va = va;
-	(*mem)->domain = domain;
-	(*mem)->mapped_to_gpu_memory = 0;
-	(*mem)->process_info = kfd_vm->process_info;
-	add_kgd_mem_to_kfd_bo_list(*mem, kfd_vm->process_info, userptr);
-
-	if (userptr) {
-		ret = init_user_pages(*mem, current->mm, user_addr);
-		if (ret) {
-			mutex_lock(&kfd_vm->process_info->lock);
-			list_del(&(*mem)->validate_list.head);
-			mutex_unlock(&kfd_vm->process_info->lock);
-			goto allocate_init_user_pages_failed;
-		}
-	}
-
-	if (offset)
-		*offset = amdgpu_bo_mmap_offset(bo);
-
-	return 0;
-
-allocate_init_user_pages_failed:
-	amdgpu_bo_unref(&bo);
-err_bo_create:
-	kfree(*mem);
-err:
-	return ret;
-}
-
 /* Reserving a BO and its page table BOs must happen atomically to
- * avoid deadlocks. When updating userptrs we need to temporarily
- * back-off the reservation and then reacquire it. Track all the
- * reservation info in a context structure. Buffers can be mapped to
- * multiple VMs simultaneously (buffers being restored on multiple
- * GPUs).
+ * avoid deadlocks. Some operations update multiple VMs at once. Track
+ * all the reservation info in a context structure. Optionally a sync
+ * object can track VM updates.
  */
 struct bo_vm_reservation_context {
-	struct amdgpu_bo_list_entry kfd_bo;
-	unsigned int n_vms;
-	struct amdgpu_bo_list_entry *vm_pd;
-	struct ww_acquire_ctx ticket;
-	struct list_head list, duplicates;
-	struct amdgpu_sync *sync;
-	bool reserved;
+	struct amdgpu_bo_list_entry kfd_bo; /* BO list entry for the KFD BO */
+	unsigned int n_vms;		    /* Number of VMs reserved	    */
+	struct amdgpu_bo_list_entry *vm_pd; /* Array of VM BO list entries  */
+	struct ww_acquire_ctx ticket;	    /* Reservation ticket	    */
+	struct list_head list, duplicates;  /* BO lists			    */
+	struct amdgpu_sync *sync;	    /* Pointer to sync object	    */
+	bool reserved;			    /* Whether BOs are reserved	    */
+};
+
+enum bo_vm_match {
+	BO_VM_NOT_MAPPED = 0,	/* Match VMs where a BO is not mapped */
+	BO_VM_MAPPED,		/* Match VMs where a BO is mapped     */
+	BO_VM_ALL,		/* Match all VMs a BO was added to    */
 };
 
 /**
@@ -718,9 +675,8 @@ static int reserve_bo_and_vm(struct kgd_mem *mem,
 	INIT_LIST_HEAD(&ctx->list);
 	INIT_LIST_HEAD(&ctx->duplicates);
 
-	ctx->vm_pd = kzalloc(sizeof(struct amdgpu_bo_list_entry)
-			      * ctx->n_vms, GFP_KERNEL);
-	if (ctx->vm_pd == NULL)
+	ctx->vm_pd = kcalloc(ctx->n_vms, sizeof(*ctx->vm_pd), GFP_KERNEL);
+	if (!ctx->vm_pd)
 		return -ENOMEM;
 
 	ctx->kfd_bo.robj = bo;
@@ -736,10 +692,8 @@ static int reserve_bo_and_vm(struct kgd_mem *mem,
 				     false, &ctx->duplicates);
 	if (!ret)
 		ctx->reserved = true;
-	else
+	else {
 		pr_err("Failed to reserve buffers in ttm\n");
-
-	if (ret) {
 		kfree(ctx->vm_pd);
 		ctx->vm_pd = NULL;
 	}
@@ -747,24 +701,19 @@ static int reserve_bo_and_vm(struct kgd_mem *mem,
 	return ret;
 }
 
-enum VA_TYPE {
-	VA_NOT_MAPPED = 0,
-	VA_MAPPED,
-	VA_DO_NOT_CARE,
-};
-
 /**
- * reserve_bo_and_vm - reserve a BO and some VMs that the BO has been added
- * to, conditionally based on map_type.
+ * reserve_bo_and_cond_vms - reserve a BO and some VMs conditionally
  * @mem: KFD BO structure.
  * @vm: the VM to reserve. If NULL, then all VMs associated with the BO
  * is used. Otherwise, a single VM associated with the BO.
  * @map_type: the mapping status that will be used to filter the VMs.
  * @ctx: the struct that will be used in unreserve_bo_and_vms().
+ *
+ * Returns 0 for success, negative for failure.
  */
 static int reserve_bo_and_cond_vms(struct kgd_mem *mem,
-			      struct amdgpu_vm *vm, enum VA_TYPE map_type,
-			      struct bo_vm_reservation_context *ctx)
+				struct amdgpu_vm *vm, enum bo_vm_match map_type,
+				struct bo_vm_reservation_context *ctx)
 {
 	struct amdgpu_bo *bo = mem->bo;
 	struct kfd_bo_va_list *entry;
@@ -782,16 +731,16 @@ static int reserve_bo_and_cond_vms(struct kgd_mem *mem,
 	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
 		if ((vm && vm != entry->bo_va->base.vm) ||
 			(entry->is_mapped != map_type
-			&& map_type != VA_DO_NOT_CARE))
+			&& map_type != BO_VM_ALL))
 			continue;
 
 		ctx->n_vms++;
 	}
 
 	if (ctx->n_vms != 0) {
-		ctx->vm_pd = kzalloc(sizeof(struct amdgpu_bo_list_entry)
-			      * ctx->n_vms, GFP_KERNEL);
-		if (ctx->vm_pd == NULL)
+		ctx->vm_pd = kcalloc(ctx->n_vms, sizeof(*ctx->vm_pd),
+				     GFP_KERNEL);
+		if (!ctx->vm_pd)
 			return -ENOMEM;
 	}
 
@@ -806,7 +755,7 @@ static int reserve_bo_and_cond_vms(struct kgd_mem *mem,
 	list_for_each_entry(entry, &mem->bo_va_list, bo_list) {
 		if ((vm && vm != entry->bo_va->base.vm) ||
 			(entry->is_mapped != map_type
-			&& map_type != VA_DO_NOT_CARE))
+			&& map_type != BO_VM_ALL))
 			continue;
 
 		amdgpu_vm_get_pd_bo(entry->bo_va->base.vm, &ctx->list,
@@ -829,16 +778,24 @@ static int reserve_bo_and_cond_vms(struct kgd_mem *mem,
 	return ret;
 }
 
+/**
+ * unreserve_bo_and_vms - Unreserve BO and VMs from a reservation context
+ * @ctx: Reservation context to unreserve
+ * @wait: Optionally wait for a sync object representing pending VM updates
+ * @intr: Whether the wait is interruptible
+ *
+ * Also frees any resources allocated in
+ * reserve_bo_and_(cond_)vm(s). Returns the status from
+ * amdgpu_sync_wait.
+ */
+
 static int unreserve_bo_and_vms(struct bo_vm_reservation_context *ctx,
 				 bool wait, bool intr)
 {
 	int ret = 0;
 
-	if (wait) {
+	if (wait)
 		ret = amdgpu_sync_wait(ctx->sync, intr);
-                if (ret)
-                    return ret;
-        }
 
 	if (ctx->reserved)
 		ttm_eu_backoff_reservation(&ctx->ticket, &ctx->list);
@@ -858,34 +815,25 @@ static int unmap_bo_from_gpuvm(struct amdgpu_device *adev,
 {
 	struct amdgpu_bo_va *bo_va = entry->bo_va;
 	struct amdgpu_vm *vm = bo_va->base.vm;
-	struct amdkfd_vm *kvm = container_of(vm, struct amdkfd_vm, base);
 	struct amdgpu_bo *pd = vm->root.base.bo;
 
-	/* Remove eviction fence from PD (and thereby from PTs too as they
-	 * share the resv. object. Otherwise during PT update job (see
-	 * amdgpu_vm_bo_update_mapping), eviction fence will get added to
-	 * job->sync object
+	/* Remove eviction fence from PD (and thereby from PTs too as
+	 * they share the resv. object). Otherwise during PT update
+	 * job (see amdgpu_vm_bo_update_mapping), eviction fence would
+	 * get added to job->sync object and job execution would
+	 * trigger the eviction fence.
 	 */
 	amdgpu_amdkfd_remove_eviction_fence(pd,
-					    kvm->process_info->eviction_fence,
+					    vm->process_info->eviction_fence,
 					    NULL, NULL);
 	amdgpu_vm_bo_unmap(adev, bo_va, entry->va);
 
 	amdgpu_vm_clear_freed(adev, vm, &bo_va->last_pt_update);
 
 	/* Add the eviction fence back */
-	amdgpu_bo_fence(pd, &kvm->process_info->eviction_fence->base, true);
-
-	amdgpu_sync_fence(adev, sync, bo_va->last_pt_update, false);
+	amdgpu_bo_fence(pd, &vm->process_info->eviction_fence->base, true);
 
-	/* Sync objects can't handle multiple GPUs (contexts) updating
-	 * sync->last_vm_update. Fortunately we don't need it for
-	 * KFD's purposes, so we can just drop that fence.
-	 */
-	if (sync->last_vm_update) {
-		fence_put(sync->last_vm_update);
-		sync->last_vm_update = NULL;
-	}
+	amdgpu_sync_fence(NULL, sync, bo_va->last_pt_update, false);
 
 	return 0;
 }
@@ -903,35 +851,15 @@ static int update_gpuvm_pte(struct amdgpu_device *adev,
 	vm = bo_va->base.vm;
 	bo = bo_va->base.bo;
 
-	/* Update the page directory */
-	ret = amdgpu_vm_update_directories(adev, vm);
-	if (ret != 0) {
-		pr_err("amdgpu_vm_update_directories failed\n");
-		return ret;
-	}
-
-	amdgpu_sync_fence(adev, sync, vm->last_update, false);
 
 	/* Update the page tables  */
 	ret = amdgpu_vm_bo_update(adev, bo_va, false);
-	if (ret != 0) {
+	if (ret) {
 		pr_err("amdgpu_vm_bo_update failed\n");
 		return ret;
 	}
 
-	amdgpu_sync_fence(adev, sync, bo_va->last_pt_update, false);
-
-
-	/* Sync objects can't handle multiple GPUs (contexts) updating
-	 * sync->last_vm_update. Fortunately we don't need it for
-	 * KFD's purposes, so we can just drop that fence.
-	 */
-	if (sync->last_vm_update) {
-		fence_put(sync->last_vm_update);
-		sync->last_vm_update = NULL;
-	}
-
-	return 0;
+	return amdgpu_sync_fence(NULL, sync, bo_va->last_pt_update, false);
 }
 
 static int map_bo_to_gpuvm(struct amdgpu_device *adev,
@@ -939,57 +867,22 @@ static int map_bo_to_gpuvm(struct amdgpu_device *adev,
 		bool no_update_pte)
 {
 	int ret;
-	struct amdgpu_bo *bo = entry->bo_va->base.bo;
-	struct amdkfd_vm *kvm = container_of(entry->bo_va->base.vm,
-					     struct amdkfd_vm, base);
-	struct amdgpu_bo *pd = entry->bo_va->base.vm->root.base.bo;
-
-	/* Remove eviction fence from PD (and thereby from PTs too as they
-	 * share the resv. object. This is necessary because new PTs are
-	 * cleared and validate needs to wait on move fences. The eviction
-	 * fence shouldn't interfere in both these activities
-	 */
-	amdgpu_amdkfd_remove_eviction_fence(pd,
-					kvm->process_info->eviction_fence,
-					NULL, NULL);
-
-	ret = amdgpu_vm_alloc_pts(adev, entry->bo_va->base.vm, entry->va,
-				  amdgpu_bo_size(bo));
 
+	/* Set virtual address for the allocation */
+	ret = amdgpu_vm_bo_map(adev, entry->bo_va, entry->va, 0,
+			       amdgpu_bo_size(entry->bo_va->base.bo),
+			       entry->pte_flags);
 	if (ret) {
-		pr_err("Failed to allocate pts, err=%d\n", ret);
-		return ret;
-	}
-
-	/* Set virtual address for the allocation, allocate PTs,
-	 * if needed, and zero them.
-	 */
-	ret = amdgpu_vm_bo_map(adev, entry->bo_va,
-			entry->va, 0, amdgpu_bo_size(bo),
-			entry->pte_flags);
-	if (ret != 0) {
 		pr_err("Failed to map VA 0x%llx in vm. ret %d\n",
 				entry->va, ret);
 		return ret;
 	}
 
-	/* PT BOs may be created during amdgpu_vm_bo_map() call,
-	 * so we have to validate the newly created PT BOs.
-	 */
-	ret = vm_validate_pt_pd_bos(kvm);
-	if (ret != 0) {
-		pr_err("validate_pt_pd_bos() failed\n");
-		return ret;
-	}
-
-	/* Add the eviction fence back */
-	amdgpu_bo_fence(pd, &kvm->process_info->eviction_fence->base, true);
-
 	if (no_update_pte)
 		return 0;
 
 	ret = update_gpuvm_pte(adev, entry, sync);
-	if (ret != 0) {
+	if (ret) {
 		pr_err("update_gpuvm_pte() failed\n");
 		goto update_gpuvm_pte_failed;
 	}
@@ -1019,116 +912,424 @@ static struct sg_table *create_doorbell_sg(uint64_t addr, uint32_t size)
 	return sg;
 }
 
-int amdgpu_amdkfd_gpuvm_sync_memory(
-		struct kgd_dev *kgd, struct kgd_mem *mem, bool intr)
+static int process_validate_vms(struct amdkfd_process_info *process_info)
 {
-	int ret = 0;
-	struct amdgpu_sync sync;
-	struct amdgpu_device *adev;
+	struct amdgpu_vm *peer_vm;
+	int ret;
 
-	adev = get_amdgpu_device(kgd);
-	amdgpu_sync_create(&sync);
+	list_for_each_entry(peer_vm, &process_info->vm_list_head,
+			    vm_list_node) {
+		ret = vm_validate_pt_pd_bos(peer_vm);
+		if (ret)
+			return ret;
+	}
 
-	mutex_lock(&mem->lock);
-	amdgpu_sync_clone(adev, &mem->sync, &sync);
-	mutex_unlock(&mem->lock);
+	return 0;
+}
 
-	ret = amdgpu_sync_wait(&sync, intr);
-	amdgpu_sync_free(&sync);
+static int process_sync_pds_resv(struct amdkfd_process_info *process_info,
+				 struct amdgpu_sync *sync)
+{
+	struct amdgpu_vm *peer_vm;
+	int ret;
+
+	list_for_each_entry(peer_vm, &process_info->vm_list_head,
+			    vm_list_node) {
+		struct amdgpu_bo *pd = peer_vm->root.base.bo;
+
+		ret = amdgpu_sync_resv(NULL,
+					sync, pd->tbo.resv,
+					AMDGPU_FENCE_OWNER_UNDEFINED, false);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int process_update_pds(struct amdkfd_process_info *process_info,
+			      struct amdgpu_sync *sync)
+{
+	struct amdgpu_vm *peer_vm;
+	int ret;
+
+	list_for_each_entry(peer_vm, &process_info->vm_list_head,
+			    vm_list_node) {
+		ret = vm_update_pds(peer_vm, sync);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int init_kfd_vm(struct amdgpu_vm *vm, void **process_info,
+		       struct fence **ef)
+{
+	struct amdkfd_process_info *info = NULL;
+	int ret;
+
+	if (!*process_info) {
+		info = kzalloc(sizeof(*info), GFP_KERNEL);
+		if (!info)
+			return -ENOMEM;
+
+		mutex_init(&info->lock);
+		INIT_LIST_HEAD(&info->vm_list_head);
+		INIT_LIST_HEAD(&info->kfd_bo_list);
+		INIT_LIST_HEAD(&info->userptr_valid_list);
+		INIT_LIST_HEAD(&info->userptr_inval_list);
+
+		info->eviction_fence =
+			amdgpu_amdkfd_fence_create(fence_context_alloc(1),
+						   current->mm);
+		if (!info->eviction_fence) {
+			pr_err("Failed to create eviction fence\n");
+			ret = -ENOMEM;
+			goto create_evict_fence_fail;
+		}
+
+		info->pid = get_task_pid(current->group_leader, PIDTYPE_PID);
+		atomic_set(&info->evicted_bos, 0);
+		INIT_DELAYED_WORK(&info->work,
+				  amdgpu_amdkfd_restore_userptr_worker);
+
+		*process_info = info;
+		*ef = fence_get(&info->eviction_fence->base);
+	}
+
+	vm->process_info = *process_info;
+
+	/* Validate page directory and attach eviction fence */
+	ret = amdgpu_bo_reserve(vm->root.base.bo, true);
+	if (ret)
+		goto reserve_pd_fail;
+	ret = vm_validate_pt_pd_bos(vm);
+	if (ret) {
+		pr_err("validate_pt_pd_bos() failed\n");
+		goto validate_pd_fail;
+	}
+	amdgpu_bo_fence(vm->root.base.bo,
+			&vm->process_info->eviction_fence->base, true);
+	amdgpu_bo_unreserve(vm->root.base.bo);
+
+	/* Update process info */
+	mutex_lock(&vm->process_info->lock);
+	list_add_tail(&vm->vm_list_node,
+			&(vm->process_info->vm_list_head));
+	vm->process_info->n_vms++;
+	mutex_unlock(&vm->process_info->lock);
+
+	return 0;
+
+validate_pd_fail:
+	amdgpu_bo_unreserve(vm->root.base.bo);
+reserve_pd_fail:
+	vm->process_info = NULL;
+	if (info) {
+		/* Two fence references: one in info and one in *ef */
+		fence_put(&info->eviction_fence->base);
+		fence_put(*ef);
+		*ef = NULL;
+		*process_info = NULL;
+create_evict_fence_fail:
+		kfree(info);
+	}
 	return ret;
 }
 
-#define BOOL_TO_STR(b)	(b == true) ? "true" : "false"
+int amdgpu_amdkfd_gpuvm_create_process_vm(struct kgd_dev *kgd, void **vm,
+					  void **process_info,
+					  struct fence **ef)
+{
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	struct amdgpu_vm *new_vm;
+	int ret;
+
+	new_vm = kzalloc(sizeof(*new_vm), GFP_KERNEL);
+	if (!new_vm)
+		return -ENOMEM;
+
+	/* Initialize AMDGPU part of the VM */
+	ret = amdgpu_vm_init(adev, new_vm, AMDGPU_VM_CONTEXT_COMPUTE, 0);
+	if (ret) {
+		pr_err("Failed init vm ret %d\n", ret);
+		goto amdgpu_vm_init_fail;
+	}
+
+	/* Initialize KFD part of the VM and process info */
+	ret = init_kfd_vm(new_vm, process_info, ef);
+	if (ret)
+		goto init_kfd_vm_fail;
+
+	*vm = (void *) new_vm;
+
+	return 0;
+
+init_kfd_vm_fail:
+	amdgpu_vm_fini(adev, new_vm);
+amdgpu_vm_init_fail:
+	kfree(new_vm);
+	return ret;
+}
+
+int amdgpu_amdkfd_gpuvm_acquire_process_vm(struct kgd_dev *kgd,
+					   struct file *filp,
+					   void **vm, void **process_info,
+					   struct fence **ef)
+{
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	struct drm_file *drm_priv = filp->private_data;
+	struct amdgpu_fpriv *drv_priv = drm_priv->driver_priv;
+	struct amdgpu_vm *avm = &drv_priv->vm;
+	int ret;
+
+	/* Convert VM into a compute VM */
+	ret = amdgpu_vm_make_compute(adev, avm);
+	if (ret)
+		return ret;
+
+	/* Initialize KFD part of the VM and process info */
+	ret = init_kfd_vm(avm, process_info, ef);
+	if (ret)
+		return ret;
+
+	*vm = (void *)avm;
+
+	return 0;
+}
+
+void amdgpu_amdkfd_gpuvm_destroy_cb(struct amdgpu_device *adev,
+				    struct amdgpu_vm *vm)
+{
+	struct amdkfd_process_info *process_info = vm->process_info;
+	struct amdgpu_bo *pd = vm->root.base.bo;
+
+	if (vm->vm_context != AMDGPU_VM_CONTEXT_COMPUTE)
+		return;
+
+	/* Release eviction fence from PD */
+	amdgpu_bo_reserve(pd, false);
+	amdgpu_bo_fence(pd, NULL, false);
+	amdgpu_bo_unreserve(pd);
+
+	if (!process_info)
+		return;
+
+	/* Update process info */
+	mutex_lock(&process_info->lock);
+	process_info->n_vms--;
+	list_del(&vm->vm_list_node);
+	mutex_unlock(&process_info->lock);
+
+	/* Release per-process resources when last compute VM is destroyed */
+	if (!process_info->n_vms) {
+		WARN_ON(!list_empty(&process_info->kfd_bo_list));
+		WARN_ON(!list_empty(&process_info->userptr_valid_list));
+		WARN_ON(!list_empty(&process_info->userptr_inval_list));
+
+		fence_put(&process_info->eviction_fence->base);
+		cancel_delayed_work_sync(&process_info->work);
+		put_pid(process_info->pid);
+		kfree(process_info);
+	}
+}
+
+void amdgpu_amdkfd_gpuvm_destroy_process_vm(struct kgd_dev *kgd, void *vm)
+{
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	struct amdgpu_vm *avm = (struct amdgpu_vm *)vm;
+
+	if (WARN_ON(!kgd || !vm))
+		return;
+
+	pr_debug("Destroying process vm %p\n", vm);
+
+	/* Release the VM context */
+	amdgpu_vm_fini(adev, avm);
+	kfree(vm);
+}
+
+uint32_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *vm)
+{
+	struct amdgpu_vm *avm = (struct amdgpu_vm *)vm;
+
+	return avm->pd_phys_addr >> AMDGPU_GPU_PAGE_SHIFT;
+}
 
 int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
 		struct kgd_dev *kgd, uint64_t va, uint64_t size,
 		void *vm, struct kgd_mem **mem,
 		uint64_t *offset, uint32_t flags)
 {
-	bool aql_queue, public, readonly, execute, coherent, no_sub, userptr;
-	u64 alloc_flag;
-	uint32_t domain;
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	struct amdgpu_vm *avm = (struct amdgpu_vm *)vm;
+	uint64_t user_addr = 0;
 	struct sg_table *sg = NULL;
-
-	if (!(flags & ALLOC_MEM_FLAGS_NONPAGED)) {
-		pr_debug("current hw doesn't support paged memory\n");
-		return -EINVAL;
-	}
-
-	domain = 0;
-	alloc_flag = 0;
-
-	aql_queue = (flags & ALLOC_MEM_FLAGS_AQL_QUEUE_MEM) ? true : false;
-	public    = (flags & ALLOC_MEM_FLAGS_PUBLIC) ? true : false;
-	readonly  = (flags & ALLOC_MEM_FLAGS_READONLY) ? true : false;
-	execute   = (flags & ALLOC_MEM_FLAGS_EXECUTE_ACCESS) ? true : false;
-	coherent  = (flags & ALLOC_MEM_FLAGS_COHERENT) ? true : false;
-	no_sub    = (flags & ALLOC_MEM_FLAGS_NO_SUBSTITUTE) ? true : false;
-	userptr   = (flags & ALLOC_MEM_FLAGS_USERPTR) ? true : false;
+	enum ttm_bo_type bo_type = ttm_bo_type_device;
+	struct amdgpu_bo *bo;
+	int byte_align;
+	u32 domain, alloc_domain;
+	u64 alloc_flags;
+	uint32_t mapping_flags;
+	int ret;
 
 	/*
 	 * Check on which domain to allocate BO
 	 */
 	if (flags & ALLOC_MEM_FLAGS_VRAM) {
-		domain = AMDGPU_GEM_DOMAIN_VRAM;
-		alloc_flag = AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
-		if (public) {
-			alloc_flag = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
-		}
-		alloc_flag |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
-	} else if (flags & (ALLOC_MEM_FLAGS_GTT | ALLOC_MEM_FLAGS_USERPTR)) {
+		domain = alloc_domain = AMDGPU_GEM_DOMAIN_VRAM;
+		alloc_flags = AMDGPU_GEM_CREATE_VRAM_CLEARED;
+		alloc_flags |= (flags & ALLOC_MEM_FLAGS_PUBLIC) ?
+			AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED :
+			AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
+	} else if (flags & ALLOC_MEM_FLAGS_GTT) {
+		domain = alloc_domain = AMDGPU_GEM_DOMAIN_GTT;
+		alloc_flags = 0;
+	} else if (flags & ALLOC_MEM_FLAGS_USERPTR) {
 		domain = AMDGPU_GEM_DOMAIN_GTT;
-		alloc_flag = 0;
+		alloc_domain = AMDGPU_GEM_DOMAIN_CPU;
+		alloc_flags = 0;
+		if (!offset || !*offset)
+			return -EINVAL;
+		user_addr = *offset;
 	} else if (flags & ALLOC_MEM_FLAGS_DOORBELL) {
 		domain = AMDGPU_GEM_DOMAIN_GTT;
-		alloc_flag = 0;
+		alloc_domain = AMDGPU_GEM_DOMAIN_CPU;
+		alloc_flags = 0;
 		if (size > UINT_MAX)
 			return -EINVAL;
 		sg = create_doorbell_sg(*offset, size);
 		if (!sg)
 			return -ENOMEM;
+		bo_type = ttm_bo_type_sg;
+	} else {
+		return -EINVAL;
+	}
+
+	*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);
+	if (!*mem) {
+		ret = -ENOMEM;
+		goto err;
+	}
+	INIT_LIST_HEAD(&(*mem)->bo_va_list);
+	mutex_init(&(*mem)->lock);
+	(*mem)->aql_queue     = !!(flags & ALLOC_MEM_FLAGS_AQL_QUEUE_MEM);
+
+	/* Workaround for AQL queue wraparound bug. Map the same
+	 * memory twice. That means we only actually allocate half
+	 * the memory.
+	 */
+	if ((*mem)->aql_queue)
+		size = size >> 1;
+
+	/* Workaround for TLB bug on older VI chips */
+	byte_align = (adev->family == AMDGPU_FAMILY_VI &&
+			adev->asic_type != CHIP_FIJI &&
+			adev->asic_type != CHIP_POLARIS10 &&
+			adev->asic_type != CHIP_POLARIS11) ?
+			VI_BO_SIZE_ALIGN : 1;
+
+	mapping_flags = AMDGPU_VM_PAGE_READABLE;
+	if (!(flags & ALLOC_MEM_FLAGS_READONLY))
+		mapping_flags |= AMDGPU_VM_PAGE_WRITEABLE;
+	if (flags & ALLOC_MEM_FLAGS_EXECUTE_ACCESS)
+		mapping_flags |= AMDGPU_VM_PAGE_EXECUTABLE;
+	if (flags & ALLOC_MEM_FLAGS_COHERENT)
+		mapping_flags |= AMDGPU_VM_MTYPE_UC;
+	else
+		mapping_flags |= AMDGPU_VM_MTYPE_NC;
+	(*mem)->mapping_flags = mapping_flags;
+
+	amdgpu_sync_create(&(*mem)->sync);
+
+	if (!sg) {
+		ret = amdgpu_amdkfd_reserve_system_mem_limit(adev, size,
+							     alloc_domain);
+		if (ret) {
+			pr_debug("Insufficient system memory\n");
+			goto err_reserve_limit;
+		}
+	}
+
+	pr_debug("\tcreate BO VA 0x%llx size 0x%llx domain %s\n",
+			va, size, domain_string(alloc_domain));
+
+	/* Allocate buffer object. Userptr objects need to start out
+	 * in the CPU domain, get moved to GTT when pinned.
+	 */
+#if 0	 
+	ret = amdgpu_bo_create(adev, size, byte_align, alloc_domain,
+			       alloc_flags, bo_type, NULL, &bo);
+#else
+	ret = amdgpu_bo_create(adev, size, byte_align, false , alloc_domain,
+			       alloc_flags, sg , NULL, &bo);
+#endif				   
+	if (ret) {
+		pr_debug("Failed to create BO on domain %s. ret %d\n",
+				domain_string(alloc_domain), ret);
+		goto err_bo_create;
 	}
+	if (bo_type == ttm_bo_type_sg) {
+		bo->tbo.sg = sg;
+		bo->tbo.ttm->sg = sg;
+	}
+	bo->kfd_bo = *mem;
+	(*mem)->bo = bo;
+	if (user_addr)
+		bo->flags |= AMDGPU_AMDKFD_USERPTR_BO;
 
-	if (offset && !userptr)
-		*offset = 0;
+	(*mem)->va = va;
+	(*mem)->domain = domain;
+	(*mem)->mapped_to_gpu_memory = 0;
+	(*mem)->process_info = avm->process_info;
+	add_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, user_addr);
 
-	pr_debug("Allocate VA 0x%llx - 0x%llx domain %s aql %s\n",
-			va, va + size, domain_string(domain),
-			BOOL_TO_STR(aql_queue));
+	if (user_addr) {
+		ret = init_user_pages(*mem, current->mm, user_addr);
+		if (ret) {
+			mutex_lock(&avm->process_info->lock);
+			list_del(&(*mem)->validate_list.head);
+			mutex_unlock(&avm->process_info->lock);
+			goto allocate_init_user_pages_failed;
+		}
+	}
 
-	pr_debug("\t alloc_flag 0x%llx public %s readonly %s execute %s coherent %s no_sub %s\n",
-			alloc_flag, BOOL_TO_STR(public),
-			BOOL_TO_STR(readonly), BOOL_TO_STR(execute),
-			BOOL_TO_STR(coherent), BOOL_TO_STR(no_sub));
+	if (offset)
+		*offset = amdgpu_bo_mmap_offset(bo);
 
-	return __alloc_memory_of_gpu(kgd, va, size, vm, mem,
-			offset, domain,
-			alloc_flag, sg,
-			aql_queue, readonly, execute,
-			coherent, no_sub, userptr);
+	return 0;
+
+allocate_init_user_pages_failed:
+	amdgpu_bo_unref(&bo);
+err_bo_create:
+	if (!sg)
+		unreserve_system_mem_limit(adev, size, alloc_domain);
+err_reserve_limit:
+	kfree(*mem);
+err:
+	if (sg) {
+		sg_free_table(sg);
+		kfree(sg);
+	}
+	return ret;
 }
 
 int amdgpu_amdkfd_gpuvm_free_memory_of_gpu(
-		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm)
+		struct kgd_dev *kgd, struct kgd_mem *mem)
 {
-	struct amdgpu_device *adev;
+	struct amdkfd_process_info *process_info = mem->process_info;
+	unsigned long bo_size = mem->bo->tbo.mem.size;
 	struct kfd_bo_va_list *entry, *tmp;
 	struct bo_vm_reservation_context ctx;
-	int ret = 0;
 	struct ttm_validate_buffer *bo_list_entry;
-	struct amdkfd_process_info *process_info;
-	unsigned long bo_size;
-
-	adev = get_amdgpu_device(kgd);
-	process_info = ((struct amdkfd_vm *)vm)->process_info;
-
-	bo_size = mem->bo->tbo.mem.size;
+	int ret;
 
 	mutex_lock(&mem->lock);
 
 	if (mem->mapped_to_gpu_memory > 0) {
-		pr_debug("BO VA 0x%llx size 0x%lx is already mapped to vm %p.\n",
-				mem->va, bo_size, vm);
+		pr_debug("BO VA 0x%llx size 0x%lx is still mapped.\n",
+				mem->va, bo_size);
 		mutex_unlock(&mem->lock);
 		return -EBUSY;
 	}
@@ -1153,11 +1354,11 @@ int amdgpu_amdkfd_gpuvm_free_memory_of_gpu(
 		if (mem->user_pages[0])
 			release_pages(mem->user_pages,
 				      mem->bo->tbo.ttm->num_pages, 0);
-		drm_free_large(mem->user_pages);
+		kvfree(mem->user_pages);
 	}
 
-	ret = reserve_bo_and_cond_vms(mem, NULL, VA_DO_NOT_CARE, &ctx);
-	if (unlikely(ret != 0))
+	ret = reserve_bo_and_cond_vms(mem, NULL, BO_VM_ALL, &ctx);
+	if (unlikely(ret))
 		return ret;
 
 	/* The eviction fence should be removed by the last unmap.
@@ -1171,12 +1372,11 @@ int amdgpu_amdkfd_gpuvm_free_memory_of_gpu(
 		mem->va + bo_size * (1 + mem->aql_queue));
 
 	/* Remove from VM internal data structures */
-	list_for_each_entry_safe(entry, tmp, &mem->bo_va_list, bo_list) {
+	list_for_each_entry_safe(entry, tmp, &mem->bo_va_list, bo_list)
 		remove_bo_from_vm((struct amdgpu_device *)entry->kgd_dev,
 				entry, bo_size);
-	}
 
-	ret = unreserve_bo_and_vms(&ctx, false, true);
+	ret = unreserve_bo_and_vms(&ctx, false, false);
 
 	/* Free the sync object */
 	amdgpu_sync_free(&mem->sync);
@@ -1199,7 +1399,8 @@ int amdgpu_amdkfd_gpuvm_free_memory_of_gpu(
 int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
 		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm)
 {
-	struct amdgpu_device *adev;
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	struct amdgpu_vm *avm = (struct amdgpu_vm *)vm;
 	int ret;
 	struct amdgpu_bo *bo;
 	uint32_t domain;
@@ -1207,11 +1408,14 @@ int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
 	struct bo_vm_reservation_context ctx;
 	struct kfd_bo_va_list *bo_va_entry = NULL;
 	struct kfd_bo_va_list *bo_va_entry_aql = NULL;
-	struct amdkfd_vm *kfd_vm = (struct amdkfd_vm *)vm;
 	unsigned long bo_size;
-	bool is_invalid_userptr;
+	bool is_invalid_userptr = false;
 
-	adev = get_amdgpu_device(kgd);
+	bo = mem->bo;
+	if (!bo) {
+		pr_err("Invalid BO when mapping memory to GPU\n");
+		return -EINVAL;
+	}
 
 	/* Make sure restore is not running concurrently. Since we
 	 * don't map invalid userptr BOs, we rely on the next restore
@@ -1223,19 +1427,14 @@ int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
 	 * sure that the MMU notifier is no longer running
 	 * concurrently and the queues are actually stopped
 	 */
-	down_read(&current->mm->mmap_sem);
-	is_invalid_userptr = atomic_read(&mem->invalid);
-	up_read(&current->mm->mmap_sem);
+	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {
+		down_write(&current->mm->mmap_sem);
+		is_invalid_userptr = atomic_read(&mem->invalid);
+		up_write(&current->mm->mmap_sem);
+	}
 
 	mutex_lock(&mem->lock);
 
-	bo = mem->bo;
-
-	if (!bo) {
-		pr_err("Invalid BO when mapping memory to GPU\n");
-		return -EINVAL;
-	}
-
 	domain = mem->domain;
 	bo_size = bo->tbo.mem.size;
 
@@ -1245,8 +1444,8 @@ int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
 			vm, domain_string(domain));
 
 	ret = reserve_bo_and_vm(mem, vm, &ctx);
-	if (unlikely(ret != 0))
-		goto bo_reserve_failed;
+	if (unlikely(ret))
+		goto out;
 
 	/* Userptr can be marked as "not invalid", but not actually be
 	 * validated yet (still in the system domain). In that case
@@ -1256,20 +1455,20 @@ int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
 	if (bo->tbo.mem.mem_type == TTM_PL_SYSTEM)
 		is_invalid_userptr = true;
 
-	if (check_if_add_bo_to_vm((struct amdgpu_vm *)vm, mem)) {
-		ret = add_bo_to_vm(adev, mem, (struct amdgpu_vm *)vm, false,
+	if (check_if_add_bo_to_vm(avm, mem)) {
+		ret = add_bo_to_vm(adev, mem, avm, false,
 				&bo_va_entry);
-		if (ret != 0)
+		if (ret)
 			goto add_bo_to_vm_failed;
 		if (mem->aql_queue) {
-			ret = add_bo_to_vm(adev, mem, (struct amdgpu_vm *)vm,
+			ret = add_bo_to_vm(adev, mem, avm,
 					true, &bo_va_entry_aql);
-			if (ret != 0)
+			if (ret)
 				goto add_bo_to_vm_failed_aql;
 		}
 	} else {
-		ret = vm_validate_pt_pd_bos((struct amdkfd_vm *)vm);
-		if (unlikely(ret != 0))
+		ret = vm_validate_pt_pd_bos(avm);
+		if (unlikely(ret))
 			goto add_bo_to_vm_failed;
 	}
 
@@ -1294,10 +1493,17 @@ int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
 
 			ret = map_bo_to_gpuvm(adev, entry, ctx.sync,
 					      is_invalid_userptr);
-			if (ret != 0) {
+			if (ret) {
 				pr_err("Failed to map radeon bo to gpuvm\n");
 				goto map_bo_to_gpuvm_failed;
 			}
+
+			ret = vm_update_pds(vm, ctx.sync);
+			if (ret) {
+				pr_err("Failed to update page directories\n");
+				goto map_bo_to_gpuvm_failed;
+			}
+
 			entry->is_mapped = true;
 			mem->mapped_to_gpu_memory++;
 			pr_debug("\t INC mapping count %d\n",
@@ -1305,15 +1511,13 @@ int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
 		}
 	}
 
-	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) == NULL)
+	if (!amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) && !bo->pin_count)
 		amdgpu_bo_fence(bo,
-				&kfd_vm->process_info->eviction_fence->base,
+				&avm->process_info->eviction_fence->base,
 				true);
-	ret = unreserve_bo_and_vms(&ctx, false, true);
+	ret = unreserve_bo_and_vms(&ctx, false, false);
 
-	mutex_unlock(&mem->process_info->lock);
-	mutex_unlock(&mem->lock);
-	return ret;
+	goto out;
 
 map_bo_to_gpuvm_failed:
 	if (bo_va_entry_aql)
@@ -1323,226 +1527,42 @@ int amdgpu_amdkfd_gpuvm_map_memory_to_gpu(
 		remove_bo_from_vm(adev, bo_va_entry, bo_size);
 add_bo_to_vm_failed:
 	unreserve_bo_and_vms(&ctx, false, false);
-bo_reserve_failed:
+out:
 	mutex_unlock(&mem->process_info->lock);
 	mutex_unlock(&mem->lock);
 	return ret;
 }
 
-static u64 get_vm_pd_gpu_offset(void *vm)
-{
-	struct amdgpu_vm *avm = (struct amdgpu_vm *) vm;
-	struct amdgpu_device *adev =
-		amdgpu_ttm_adev(avm->root.base.bo->tbo.bdev);
-	u64 offset;
-	uint64_t flags = AMDGPU_PTE_VALID;
-
-	BUG_ON(avm == NULL);
-
-	amdgpu_bo_reserve(avm->root.base.bo, false);
-
-	offset = amdgpu_bo_gpu_offset(avm->root.base.bo);
-
-	amdgpu_bo_unreserve(avm->root.base.bo);
-
-	/* On some ASICs the FB doesn't start at 0. Adjust FB offset
-	 * to an actual MC address.
-	 */
-       if (adev->gmc.gmc_funcs->get_vm_pde)
-               amdgpu_gmc_get_vm_pde(adev, -1, &offset, &flags);
-	return offset;
-}
-
-int amdgpu_amdkfd_gpuvm_create_process_vm(struct kgd_dev *kgd, void **vm,
-					  void **process_info)
-{
-	int ret;
-	struct amdkfd_vm *new_vm;
-	struct amdkfd_process_info *info;
-	struct amdgpu_device *adev = get_amdgpu_device(kgd);
-
-	new_vm = kzalloc(sizeof(*new_vm), GFP_KERNEL);
-	if (new_vm == NULL)
-		return -ENOMEM;
-
-	/* Initialize the VM context, allocate the page directory and zero it */
-	ret = amdgpu_vm_init(adev, &new_vm->base, AMDGPU_VM_CONTEXT_COMPUTE, 0);
-	if (ret != 0) {
-		pr_err("Failed init vm ret %d\n", ret);
-		/* Undo everything related to the new VM context */
-		goto vm_init_fail;
-	}
-	new_vm->adev = adev;
-
-	if (!*process_info) {
-		info = kzalloc(sizeof(*info), GFP_KERNEL);
-		if (!info) {
-			pr_err("Failed to create amdkfd_process_info");
-			ret = -ENOMEM;
-			goto alloc_process_info_fail;
-		}
-
-		mutex_init(&info->lock);
-		INIT_LIST_HEAD(&info->vm_list_head);
-		INIT_LIST_HEAD(&info->kfd_bo_list);
-		INIT_LIST_HEAD(&info->userptr_valid_list);
-		INIT_LIST_HEAD(&info->userptr_inval_list);
-
-		info->eviction_fence =
-			amdgpu_amdkfd_fence_create(fence_context_alloc(1),
-						   current->mm);
-		if (info->eviction_fence == NULL) {
-			pr_err("Failed to create eviction fence\n");
-			goto create_evict_fence_fail;
-		}
-
-		info->pid = get_task_pid(current->group_leader,
-					 PIDTYPE_PID);
-		atomic_set(&info->evicted_bos, 0);
-		INIT_DELAYED_WORK(&info->work,
-				  amdgpu_amdkfd_restore_userptr_worker);
-
-		*process_info = info;
-	}
-
-	new_vm->process_info = *process_info;
-
-	mutex_lock(&new_vm->process_info->lock);
-	list_add_tail(&new_vm->vm_list_node,
-			&(new_vm->process_info->vm_list_head));
-	new_vm->process_info->n_vms++;
-	mutex_unlock(&new_vm->process_info->lock);
-
-	*vm = (void *) new_vm;
-
-	pr_debug("Created process vm %p\n", *vm);
-
-	return ret;
-
-create_evict_fence_fail:
-	kfree(info);
-alloc_process_info_fail:
-	amdgpu_vm_fini(adev, &new_vm->base);
-vm_init_fail:
-	kfree(new_vm);
-	return ret;
-
-}
-
-void amdgpu_amdkfd_gpuvm_destroy_process_vm(struct kgd_dev *kgd, void *vm)
-{
-	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
-	struct amdkfd_vm *kfd_vm = (struct amdkfd_vm *) vm;
-	struct amdgpu_vm *avm = &kfd_vm->base;
-	struct amdgpu_bo *pd;
-	struct amdkfd_process_info *process_info;
-
-	if (WARN_ON(!kgd || !vm))
-		return;
-
-	pr_debug("Destroying process vm %p\n", vm);
-	/* Release eviction fence from PD */
-	pd = avm->root.base.bo;
-	amdgpu_bo_reserve(pd, false);
-	amdgpu_bo_fence(pd, NULL, false);
-	amdgpu_bo_unreserve(pd);
-
-	process_info = kfd_vm->process_info;
-
-	mutex_lock(&process_info->lock);
-	process_info->n_vms--;
-	list_del(&kfd_vm->vm_list_node);
-	mutex_unlock(&process_info->lock);
-
-	/* Release per-process resources */
-	if (!process_info->n_vms) {
-		WARN_ON(!list_empty(&process_info->kfd_bo_list));
-		WARN_ON(!list_empty(&process_info->userptr_valid_list));
-		WARN_ON(!list_empty(&process_info->userptr_inval_list));
-
-		fence_put(&process_info->eviction_fence->base);
-		cancel_delayed_work_sync(&process_info->work);
-		put_pid(process_info->pid);
-		kfree(process_info);
-	}
-
-	/* Release the VM context */
-	amdgpu_vm_fini(adev, avm);
-	kfree(vm);
-}
-
-uint32_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *vm)
-{
-	return get_vm_pd_gpu_offset(vm) >> AMDGPU_GPU_PAGE_SHIFT;
-}
-
-int amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct kgd_dev *kgd,
-					      struct kfd_vm_fault_info *mem)
-{
-	struct amdgpu_device *adev;
-
-	adev = (struct amdgpu_device *) kgd;
-	if (atomic_read(&adev->gmc.vm_fault_info_updated) == 1) {
-		*mem = *adev->gmc.vm_fault_info;
-		mb();
-		atomic_set(&adev->gmc.vm_fault_info_updated, 0);
-	}
-	return 0;
-}
-
-static bool is_mem_on_local_device(struct kgd_dev *kgd,
-		struct list_head *bo_va_list, void *vm)
-{
-	struct kfd_bo_va_list *entry;
-
-	list_for_each_entry(entry, bo_va_list, bo_list) {
-		if (entry->kgd_dev == kgd && entry->bo_va->base.vm == vm)
-			return true;
-	}
-
-	return false;
-}
 
 int amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(
 		struct kgd_dev *kgd, struct kgd_mem *mem, void *vm)
 {
+        struct amdgpu_device *adev = get_amdgpu_device(kgd);
+        struct amdkfd_process_info *process_info =
+                ((struct amdgpu_vm *)vm)->process_info;
+        unsigned long bo_size = mem->bo->tbo.mem.size;
+
 	struct kfd_bo_va_list *entry;
-	struct amdgpu_device *adev;
-	unsigned int mapped_before;
 	int ret = 0;
 	struct bo_vm_reservation_context ctx;
-	struct amdkfd_process_info *process_info;
-	unsigned long bo_size;
-
-	adev = (struct amdgpu_device *) kgd;
-	process_info = ((struct amdkfd_vm *)vm)->process_info;
-
-	bo_size = mem->bo->tbo.mem.size;
 
+	int ret;
 	mutex_lock(&mem->lock);
 
-	/*
-	 * Make sure that this BO mapped on KGD before unmappping it
-	 */
-	if (!is_mem_on_local_device(kgd, &mem->bo_va_list, vm)) {
-		ret = -EINVAL;
+        ret = reserve_bo_and_cond_vms(mem, vm, BO_VM_MAPPED, &ctx);
+        if (unlikely(ret))
+
 		goto out;
 	}
 
-	if (mem->mapped_to_gpu_memory == 0) {
-		pr_debug("BO VA 0x%llx size 0x%lx is not mapped to vm %p\n",
-				mem->va, bo_size, vm);
+        /* If no VMs were reserved, it means the BO wasn't actually mapped */
+        if (ctx.n_vms == 0) {
 		ret = -EINVAL;
-		goto out;
+		goto unreserve_out;
 	}
-	mapped_before = mem->mapped_to_gpu_memory;
-
-	ret = reserve_bo_and_cond_vms(mem, vm, VA_MAPPED, &ctx);
-	if (unlikely(ret != 0))
-		goto out;
 
-	ret = vm_validate_pt_pd_bos((struct amdkfd_vm *)vm);
-	if (unlikely(ret != 0))
+	ret = vm_validate_pt_pd_bos((struct amdgpu_vm *)vm);
+	if (unlikely(ret))
 		goto unreserve_out;
 
 	pr_debug("Unmap VA 0x%llx - 0x%llx from vm %p\n",
@@ -1576,17 +1596,11 @@ int amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(
 	 * required.
 	 */
 	if (mem->mapped_to_gpu_memory == 0 &&
-	    !amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm))
+	    !amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm) && !mem->bo->pin_count)
 		amdgpu_amdkfd_remove_eviction_fence(mem->bo,
 						process_info->eviction_fence,
 						    NULL, NULL);
 
-	if (mapped_before == mem->mapped_to_gpu_memory) {
-		pr_debug("BO VA 0x%llx size 0x%lx is not mapped to vm %p\n",
-			mem->va, bo_size, vm);
-		ret = -EINVAL;
-	}
-
 unreserve_out:
 	unreserve_bo_and_vms(&ctx, false, false);
 out:
@@ -1594,21 +1608,28 @@ int amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(
 	return ret;
 }
 
-int amdgpu_amdkfd_gpuvm_mmap_bo(struct kgd_dev *kgd, struct vm_area_struct *vma)
+int amdgpu_amdkfd_gpuvm_sync_memory(
+		struct kgd_dev *kgd, struct kgd_mem *mem, bool intr)
 {
+	struct amdgpu_sync sync;
+	int ret;
 	struct amdgpu_device *adev;
 
-	adev = get_amdgpu_device(kgd);
-	if (!adev) {
-		pr_err("Could not get amdgpu device in %s\n", __func__);
-		return -ENODEV;
-	}
+	adev = get_amdgpu_device(kgd);	
+
+	amdgpu_sync_create(&sync);
+
+	mutex_lock(&mem->lock);
+	amdgpu_sync_clone(adev , &mem->sync, &sync);
+	mutex_unlock(&mem->lock);
 
-	return amdgpu_bo_mmap(NULL, vma, &adev->mman.bdev);
+	ret = amdgpu_sync_wait(&sync, intr);
+	amdgpu_sync_free(&sync);
+	return ret;
 }
 
 int amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_dev *kgd,
-		struct kgd_mem *mem, void **kptr)
+		struct kgd_mem *mem, void **kptr, uint64_t *size)
 {
 	int ret;
 	struct amdgpu_bo *bo = mem->bo;
@@ -1645,9 +1666,10 @@ int amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_dev *kgd,
 		bo, mem->process_info->eviction_fence, NULL, NULL);
 	list_del_init(&mem->validate_list.head);
 
-	amdgpu_bo_unreserve(bo);
+	if (size)
+		*size = amdgpu_bo_size(bo);
 
-	mem->kptr = *kptr;
+	amdgpu_bo_unreserve(bo);
 
 	mutex_unlock(&mem->process_info->lock);
 	return 0;
@@ -1662,13 +1684,27 @@ int amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_dev *kgd,
 	return ret;
 }
 
+int amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct kgd_dev *kgd,
+					      struct kfd_vm_fault_info *mem)
+{
+	struct amdgpu_device *adev;
+
+	adev = (struct amdgpu_device *) kgd;
+	if (atomic_read(&adev->gmc.vm_fault_info_updated) == 1) {
+		*mem = *adev->gmc.vm_fault_info;
+		mb();
+		atomic_set(&adev->gmc.vm_fault_info_updated, 0);
+	}
+	return 0;
+}
+
 static int pin_bo_wo_map(struct kgd_mem *mem)
 {
 	struct amdgpu_bo *bo = mem->bo;
 	int ret = 0;
 
 	ret = amdgpu_bo_reserve(bo, false);
-	if (unlikely(ret != 0))
+	if (unlikely(ret))
 		return ret;
 
 	ret = amdgpu_bo_pin(bo, mem->domain, NULL);
@@ -1683,7 +1719,7 @@ static void unpin_bo_wo_map(struct kgd_mem *mem)
 	int ret = 0;
 
 	ret = amdgpu_bo_reserve(bo, false);
-	if (unlikely(ret != 0))
+	if (unlikely(ret))
 		return;
 
 	amdgpu_bo_unpin(bo);
@@ -1728,7 +1764,8 @@ static int get_sg_table(struct amdgpu_device *adev,
 		goto out;
 
 	if (bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM) {
-		bus_addr = bo->tbo.offset + adev->gmc.aper_base + offset;
+		bus_addr = amdgpu_bo_gpu_offset(bo) - adev->gmc.vram_start
+			   + adev->gmc.aper_base + offset;
 
 		for_each_sg(sg->sgl, s, sg->orig_nents, i) {
 			uint64_t chunk_size, length;
@@ -1783,7 +1820,7 @@ int amdgpu_amdkfd_gpuvm_pin_get_sg_table(struct kgd_dev *kgd,
 	struct amdgpu_device *adev;
 
 	ret = pin_bo_wo_map(mem);
-	if (unlikely(ret != 0))
+	if (unlikely(ret))
 		return ret;
 
 	adev = get_amdgpu_device(kgd);
@@ -1813,7 +1850,7 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
 	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
 	struct drm_gem_object *obj;
 	struct amdgpu_bo *bo;
-	struct amdkfd_vm *kfd_vm = (struct amdkfd_vm *)vm;
+	struct amdgpu_vm *avm = (struct amdgpu_vm *)vm;
 
 	if (dma_buf->ops != &drm_gem_prime_dmabuf_ops)
 		/* Can't handle non-graphics buffers */
@@ -1826,13 +1863,12 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
 
 	bo = gem_to_amdgpu_bo(obj);
 	if (!(bo->preferred_domains & (AMDGPU_GEM_DOMAIN_VRAM |
-				    AMDGPU_GEM_DOMAIN_GTT |
-				    AMDGPU_GEM_DOMAIN_DGMA)))
+				    AMDGPU_GEM_DOMAIN_GTT)))
 		/* Only VRAM and GTT BOs are supported */
 		return -EINVAL;
 
 	*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);
-	if (*mem == NULL)
+	if (!*mem)
 		return -ENOMEM;
 
 	if (size)
@@ -1849,16 +1885,12 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
 
 	(*mem)->bo = amdgpu_bo_ref(bo);
 	(*mem)->va = va;
-	if (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM)
-		(*mem)->domain = AMDGPU_GEM_DOMAIN_VRAM;
-	else if (bo->preferred_domains & AMDGPU_GEM_DOMAIN_GTT)
-		(*mem)->domain = AMDGPU_GEM_DOMAIN_GTT;
-	else
-		(*mem)->domain = AMDGPU_GEM_DOMAIN_DGMA;
+        (*mem)->domain = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?
+                AMDGPU_GEM_DOMAIN_VRAM : AMDGPU_GEM_DOMAIN_GTT;
 	(*mem)->mapped_to_gpu_memory = 0;
-	(*mem)->process_info = kfd_vm->process_info;
-	add_kgd_mem_to_kfd_bo_list(*mem, kfd_vm->process_info, false);
-	amdgpu_sync_create(&(*mem)->sync);
+        (*mem)->process_info = avm->process_info;
+        add_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, false);
+        amdgpu_sync_create(&(*mem)->sync);
 
 	return 0;
 }
@@ -1887,21 +1919,6 @@ int amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_dev *kgd, void *vm,
 	return 0;
 }
 
-static int process_validate_vms(struct amdkfd_process_info *process_info)
-{
-	struct amdkfd_vm *peer_vm;
-	int ret;
-
-	list_for_each_entry(peer_vm, &process_info->vm_list_head,
-			    vm_list_node) {
-		ret = vm_validate_pt_pd_bos(peer_vm);
-		if (ret)
-			return ret;
-	}
-
-	return 0;
-}
-
 /* Evict a userptr BO by stopping the queues if necessary
  *
  * Runs in MMU notifier, may be in RECLAIM_FS context. This means it
@@ -1925,7 +1942,7 @@ int amdgpu_amdkfd_evict_userptr(struct kgd_mem *mem,
 	if (evicted_bos == 1) {
 		/* First eviction, stop the queues */
 		r = kgd2kfd->quiesce_mm(NULL, mm);
-		if (r != 0)
+		if (r)
 			pr_err("Failed to quiesce KFD\n");
 		schedule_delayed_work(&process_info->work, 1);
 	}
@@ -2022,6 +2039,7 @@ static int update_invalid_user_pages(struct amdkfd_process_info *process_info,
 		if (atomic_cmpxchg(&mem->invalid, invalid, 0) != invalid)
 			return -EAGAIN;
 	}
+
 	return 0;
 }
 
@@ -2038,7 +2056,7 @@ static int validate_invalid_user_pages(struct amdkfd_process_info *process_info)
 	struct ww_acquire_ctx ticket;
 	struct amdgpu_sync sync;
 
-	struct amdkfd_vm *peer_vm;
+	struct amdgpu_vm *peer_vm;
 	struct kgd_mem *mem, *tmp_mem;
 	struct amdgpu_bo *bo;
 	struct ttm_operation_ctx ctx = { false, false };
@@ -2059,7 +2077,7 @@ static int validate_invalid_user_pages(struct amdkfd_process_info *process_info)
 	i = 0;
 	list_for_each_entry(peer_vm, &process_info->vm_list_head,
 			    vm_list_node)
-		amdgpu_vm_get_pd_bo(&peer_vm->base, &resv_list,
+		amdgpu_vm_get_pd_bo(peer_vm, &resv_list,
 				    &pd_bo_list_entries[i++]);
 	/* Add the userptr_inval_list entries to resv_list */
 	list_for_each_entry(mem, &process_info->userptr_inval_list,
@@ -2083,7 +2101,7 @@ static int validate_invalid_user_pages(struct amdkfd_process_info *process_info)
 	 */
 	list_for_each_entry(peer_vm, &process_info->vm_list_head,
 			    vm_list_node)
-		amdgpu_amdkfd_remove_eviction_fence(peer_vm->base.root.base.bo,
+		amdgpu_amdkfd_remove_eviction_fence(peer_vm->root.base.bo,
 						process_info->eviction_fence,
 						NULL, NULL);
 
@@ -2116,7 +2134,7 @@ static int validate_invalid_user_pages(struct amdkfd_process_info *process_info)
 		 * the userptr_valid_list. If we need to revalidate
 		 * it, we need to start from scratch.
 		 */
-		drm_free_large(mem->user_pages);
+		kvfree(mem->user_pages);
 		mem->user_pages = NULL;
 		list_move_tail(&mem->validate_list.head,
 			       &process_info->userptr_valid_list);
@@ -2142,10 +2160,14 @@ static int validate_invalid_user_pages(struct amdkfd_process_info *process_info)
 			}
 		}
 	}
+
+	/* Update page directories */
+	ret = process_update_pds(process_info, &sync);
+
 unreserve_out:
 	list_for_each_entry(peer_vm, &process_info->vm_list_head,
 			    vm_list_node)
-		amdgpu_bo_fence(peer_vm->base.root.base.bo,
+		amdgpu_bo_fence(peer_vm->root.base.bo,
 				&process_info->eviction_fence->base, true);
 	ttm_eu_backoff_reservation(&ticket, &resv_list);
 	amdgpu_sync_wait(&sync, false);
@@ -2244,11 +2266,11 @@ static void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work)
  * 8.  Unreserve all BOs
  */
 
-int amdgpu_amdkfd_gpuvm_restore_process_bos(void *info)
+int amdgpu_amdkfd_gpuvm_restore_process_bos(void *info, struct fence **ef)
 {
 	struct amdgpu_bo_list_entry *pd_bo_list;
 	struct amdkfd_process_info *process_info = info;
-	struct amdkfd_vm *peer_vm;
+	struct amdgpu_vm *peer_vm;
 	struct kgd_mem *mem;
 	struct bo_vm_reservation_context ctx;
 	struct amdgpu_amdkfd_fence *new_fence;
@@ -2263,15 +2285,14 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *info)
 	pd_bo_list = kcalloc(process_info->n_vms,
 			     sizeof(struct amdgpu_bo_list_entry),
 			     GFP_KERNEL);
-	if (pd_bo_list == NULL)
+	if (!pd_bo_list)
 		return -ENOMEM;
 
 	i = 0;
 	mutex_lock(&process_info->lock);
 	list_for_each_entry(peer_vm, &process_info->vm_list_head,
 			vm_list_node)
-		amdgpu_vm_get_pd_bo(&peer_vm->base, &ctx.list,
-				    &pd_bo_list[i++]);
+		amdgpu_vm_get_pd_bo(peer_vm, &ctx.list, &pd_bo_list[i++]);
 
 	/* Reserve all BOs and page tables/directory. Add all BOs from
 	 * kfd_bo_list to ctx.list
@@ -2292,20 +2313,16 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *info)
 	}
 
 	amdgpu_sync_create(&sync_obj);
-	ctx.sync = &sync_obj;
 
 	/* Validate PDs and PTs */
 	ret = process_validate_vms(process_info);
 	if (ret)
 		goto validate_map_fail;
 
-	/* Wait for PD/PTs validate to finish */
-	/* FIXME: I think this isn't needed */
-	list_for_each_entry(peer_vm, &process_info->vm_list_head,
-			    vm_list_node) {
-		struct amdgpu_bo *bo = peer_vm->base.root.base.bo;
-
-		ttm_bo_wait(&bo->tbo, false, false);
+	ret = process_sync_pds_resv(process_info, &sync_obj);
+	if (ret) {
+		pr_debug("Memory eviction: Failed to sync to PD BO moving fence. Try again\n");
+		goto validate_map_fail;
 	}
 
 	/* Validate BOs and map them to GPUVM (update VM page tables). */
@@ -2321,13 +2338,17 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *info)
 			pr_debug("Memory eviction: Validate BOs failed. Try again\n");
 			goto validate_map_fail;
 		}
-
+		ret = amdgpu_sync_fence(NULL, &sync_obj, bo->tbo.moving, false);
+		if (ret) {
+			pr_debug("Memory eviction: Sync BO fence failed. Try again\n");
+			goto validate_map_fail;
+		}
 		list_for_each_entry(bo_va_entry, &mem->bo_va_list,
 				    bo_list) {
 			ret = update_gpuvm_pte((struct amdgpu_device *)
 					      bo_va_entry->kgd_dev,
 					      bo_va_entry,
-					      ctx.sync);
+					      &sync_obj);
 			if (ret) {
 				pr_debug("Memory eviction: update PTE failed. Try again\n");
 				goto validate_map_fail;
@@ -2335,7 +2356,15 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *info)
 		}
 	}
 
-	amdgpu_sync_wait(ctx.sync, false);
+	/* Update page directories */
+	ret = process_update_pds(process_info, &sync_obj);
+	if (ret) {
+		pr_debug("Memory eviction: update PDs failed. Try again\n");
+		goto validate_map_fail;
+	}
+
+	/* Wait for validate and PT updates to finish */
+	amdgpu_sync_wait(&sync_obj, false);
 
 	/* Release old eviction fence and create new one, because fence only
 	 * goes from unsignaled to signaled, fence cannot be reused.
@@ -2351,20 +2380,18 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *info)
 	}
 	fence_put(&process_info->eviction_fence->base);
 	process_info->eviction_fence = new_fence;
+	*ef = fence_get(&new_fence->base);
 
-	/* Wait for validate to finish and attach new eviction fence */
+	/* Attach new eviction fence to all BOs */
 	list_for_each_entry(mem, &process_info->kfd_bo_list,
-		validate_list.head) {
-		struct amdgpu_bo *bo = mem->bo;
-
-		ttm_bo_wait(&bo->tbo, false, false);
-		amdgpu_bo_fence(bo, &process_info->eviction_fence->base, true);
-	}
+		validate_list.head)
+		amdgpu_bo_fence(mem->bo,
+			&process_info->eviction_fence->base, true);
 
 	/* Attach eviction fence to PD / PT BOs */
 	list_for_each_entry(peer_vm, &process_info->vm_list_head,
 			    vm_list_node) {
-		struct amdgpu_bo *bo = peer_vm->base.root.base.bo;
+		struct amdgpu_bo *bo = peer_vm->root.base.bo;
 
 		amdgpu_bo_fence(bo, &process_info->eviction_fence->base, true);
 	}
@@ -2391,11 +2418,10 @@ int amdgpu_amdkfd_copy_mem_to_mem(struct kgd_dev *kgd, struct kgd_mem *src_mem,
 	struct fence *fence = NULL;
 	int i, r;
 
-	if (!kgd || !src_mem || !dst_mem)
+	if (!kgd || !src_mem || !dst_mem || !actual_size)
 		return -EINVAL;
 
-	if (actual_size)
-		*actual_size = 0;
+	*actual_size = 0;
 
 	adev = get_amdgpu_device(kgd);
 	INIT_LIST_HEAD(&list);
@@ -2423,7 +2449,7 @@ int amdgpu_amdkfd_copy_mem_to_mem(struct kgd_dev *kgd, struct kgd_mem *src_mem,
 	}
 
 	r = amdgpu_ttm_copy_mem_to_mem(adev, &src, &dst, size, NULL,
-					&fence);
+				       &fence);
 	if (r)
 		pr_err("Copy buffer failed %d\n", r);
 	else
@@ -2432,7 +2458,6 @@ int amdgpu_amdkfd_copy_mem_to_mem(struct kgd_dev *kgd, struct kgd_mem *src_mem,
 		amdgpu_bo_fence(src_mem->bo, fence, true);
 		amdgpu_bo_fence(dst_mem->bo, fence, true);
 	}
-
 	if (f)
 		*f = fence_get(fence);
 	fence_put(fence);
@@ -2440,4 +2465,3 @@ int amdgpu_amdkfd_copy_mem_to_mem(struct kgd_dev *kgd, struct kgd_mem *src_mem,
 	ttm_eu_backoff_reservation(&ticket, &list);
 	return r;
 }
-
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
index 731664f..9bc9747 100755
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
@@ -105,7 +105,7 @@ int amdgpu_vm_fragment_size = -1;
 int amdgpu_vm_block_size = -1;
 int amdgpu_vm_fault_stop = 0;
 int amdgpu_vm_debug = 0;
-int amdgpu_vram_page_split = 1024;
+int amdgpu_vram_page_split = 512;
 int amdgpu_vm_update_mode = -1;
 int amdgpu_exp_hw_support = 0;
 int amdgpu_dc = -1;
@@ -132,6 +132,7 @@ int amdgpu_job_hang_limit = 0;
 int amdgpu_lbpw = -1;
 int amdgpu_compute_multipipe = -1;
 int amdgpu_gpu_recovery = -1; /* auto */
+int amdgpu_emu_mode = 0;
 
 MODULE_PARM_DESC(vramlimit, "Restrict VRAM for testing, in megabytes");
 module_param_named(vramlimit, amdgpu_vram_limit, int, 0600);
@@ -139,7 +140,7 @@ module_param_named(vramlimit, amdgpu_vram_limit, int, 0600);
 MODULE_PARM_DESC(vis_vramlimit, "Restrict visible VRAM for testing, in megabytes");
 module_param_named(vis_vramlimit, amdgpu_vis_vram_limit, int, 0444);
 
-MODULE_PARM_DESC(gartsize, "Size of GART to setup in megabytes (32, 64, etc., -1 = auto)");
+MODULE_PARM_DESC(gartsize, "Size of GART to setup in megabytes (32, 64, etc., -1=auto)");
 module_param_named(gartsize, amdgpu_gart_size, uint, 0600);
 
 MODULE_PARM_DESC(gttsize, "Size of the GTT domain in megabytes (-1 = auto)");
@@ -290,16 +291,21 @@ module_param_named(compute_multipipe, amdgpu_compute_multipipe, int, 0444);
 MODULE_PARM_DESC(gpu_recovery, "Enable GPU recovery mechanism, (1 = enable, 0 = disable, -1 = auto");
 module_param_named(gpu_recovery, amdgpu_gpu_recovery, int, 0444);
 
+MODULE_PARM_DESC(emu_mode, "Emulation mode, (1 = enable, 0 = disable)");
+module_param_named(emu_mode, amdgpu_emu_mode, int, 0444);
+
 #ifdef CONFIG_DRM_AMDGPU_SI
 
 int amdgpu_si_support = 1;
 MODULE_PARM_DESC(si_support, "SI support (1 = enabled (default), 0 = disabled)");
+
 module_param_named(si_support, amdgpu_si_support, int, 0444);
 #endif
 
 #ifdef CONFIG_DRM_AMDGPU_CIK
-int amdgpu_cik_support = 0;
-MODULE_PARM_DESC(cik_support, "CIK support (1 = enabled, 0 = disabled (default))");
+
+int amdgpu_cik_support = 1;
+MODULE_PARM_DESC(cik_support, "CIK support (1 = enabled (default), 0 = disabled)");
 module_param_named(cik_support, amdgpu_cik_support, int, 0444);
 #endif
 
@@ -567,7 +573,7 @@ static int amdgpu_pci_probe(struct pci_dev *pdev,
 {
 	struct drm_device *dev;
 	unsigned long flags = ent->driver_data;
-	int ret;
+	int ret, retry = 0;
 	bool supports_atomic = false;
 
 	if (!amdgpu_virtual_display &&
@@ -612,8 +618,14 @@ static int amdgpu_pci_probe(struct pci_dev *pdev,
 
 	pci_set_drvdata(pdev, dev);
 
+retry_init:
 	ret = drm_dev_register(dev, ent->driver_data);
-	if (ret)
+	if (ret == -EAGAIN && ++retry <= 3) {
+		DRM_INFO("retry init %d\n", retry);
+		/* Don't request EX mode too frequently which is attacking */
+		msleep(5000);
+		goto retry_init;
+	} else if (ret)
 		goto err_pci;
 
 	return 0;
@@ -853,8 +865,6 @@ static struct drm_driver kms_driver = {
 	.disable_vblank = amdgpu_disable_vblank_kms,
 	.get_vblank_timestamp = amdgpu_get_vblank_timestamp_kms,
 	.get_scanout_position = amdgpu_display_get_crtc_scanoutpos,
-#if defined(CONFIG_DEBUG_FS)
-#endif
 	.irq_handler = amdgpu_irq_handler,
 	.ioctls = amdgpu_ioctls_kms,
 	.gem_free_object_unlocked = amdgpu_gem_object_free,
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index 40c0243..fb1fbdc 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -32,6 +32,7 @@
 #include <drm/amdgpu_drm.h>
 #include "amdgpu.h"
 #include "amdgpu_trace.h"
+#include "amdgpu_amdkfd.h"
 
 /*
  * GPUVM
@@ -2337,6 +2338,25 @@ void amdgpu_vm_adjust_size(struct amdgpu_device *adev, uint32_t vm_size,
 		 adev->vm_manager.fragment_size);
 }
 
+
+static void amdgpu_inc_compute_vms(struct amdgpu_device *adev)
+{
+	/* Temporary use only the first VM manager */
+	unsigned int vmhub = 0; /*ring->funcs->vmhub;*/
+	struct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];
+
+	mutex_lock(&id_mgr->lock);
+	if ((adev->vm_manager.n_compute_vms++ == 0) &&
+	    (!amdgpu_sriov_vf(adev))) {
+		/* First Compute VM: enable compute power profile */
+		if (adev->powerplay.pp_funcs->switch_power_profile)
+			amdgpu_dpm_switch_power_profile(adev,PP_SMC_POWER_PROFILE_COMPUTE);
+	}
+	mutex_unlock(&id_mgr->lock);
+}
+
+
+
 /**
  * amdgpu_vm_init - initialize a vm instance
  *
@@ -2442,21 +2462,8 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 	vm->fault_credit = 16;
 
 	vm->vm_context = vm_context;
-	if (vm_context == AMDGPU_VM_CONTEXT_COMPUTE) {
-		struct amdgpu_vmid_mgr *id_mgr =
-				&adev->vm_manager.id_mgr[AMDGPU_GFXHUB];
-
-		mutex_lock(&id_mgr->lock);
-
-		if ((adev->vm_manager.n_compute_vms++ == 0) &&
-			(!amdgpu_sriov_vf(adev))) {
-			/* First Compute VM: enable compute power profile */
-			if (adev->powerplay.pp_funcs->switch_power_profile)
-				amdgpu_dpm_switch_power_profile(adev,
-						AMD_PP_COMPUTE_PROFILE);
-		}
-		mutex_unlock(&id_mgr->lock);
-	}
+	if (vm_context == AMDGPU_VM_CONTEXT_COMPUTE)
+		amdgpu_inc_compute_vms(adev);
 
 	return 0;
 
@@ -2475,6 +2482,86 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 }
 
 /**
+ * amdgpu_vm_make_compute - Turn a GFX VM into a compute VM
+ *
+ * This only works on GFX VMs that don't have any BOs added and no
+ * page tables allocated yet.
+ *
+ * Changes the following VM parameters:
+ * - vm_context
+ * - use_cpu_for_update
+ * - pte_supports_ats
+ * - pasid (old PASID is released, because compute manages its own PASIDs)
+ *
+ * Reinitializes the page directory to reflect the changed ATS
+ * setting. May also switch to the compute power profile if this is
+ * the first compute VM. May leave behind an unused shadow BO for the
+ * page directory when switching from SDMA updates to CPU updates.
+ *
+ * Returns 0 for success, -errno for errors.
+ */
+int amdgpu_vm_make_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm)
+{
+	bool pte_support_ats = (adev->asic_type == CHIP_RAVEN);
+	int r;
+
+	r = amdgpu_bo_reserve(vm->root.base.bo, true);
+	if (r)
+		return r;
+
+	/* Sanity checks */
+	if (vm->vm_context == AMDGPU_VM_CONTEXT_COMPUTE) {
+		/* Can happen if ioctl is interrupted by a signal after
+		 * this function already completed. Just return success.
+		*/
+		r = 0;
+		goto error;
+	}
+	if (!RB_EMPTY_ROOT(&vm->va) || vm->root.entries) {
+		r = -EINVAL;
+		goto error;
+	}
+
+	/* Check if PD needs to be reinitialized and do it before
+	 * changing any other state, in case it fails.
+	 */
+	if (pte_support_ats != vm->pte_support_ats) {
+		r = amdgpu_vm_clear_bo(adev, vm, vm->root.base.bo,
+			       adev->vm_manager.root_level,
+			       pte_support_ats);
+		if (r)
+			goto error;
+	}
+
+	/* Update VM state */
+	vm->vm_context = AMDGPU_VM_CONTEXT_COMPUTE;
+	vm->use_cpu_for_update = !!(adev->vm_manager.vm_update_mode &
+				    AMDGPU_VM_USE_CPU_FOR_COMPUTE);
+	vm->pte_support_ats = pte_support_ats;
+	DRM_DEBUG_DRIVER("VM update mode is %s\n",
+			 vm->use_cpu_for_update ? "CPU" : "SDMA");
+	WARN_ONCE((vm->use_cpu_for_update & !amdgpu_vm_is_large_bar(adev)),
+		  "CPU update of VM recommended only for large BAR system\n");
+
+	if (vm->pasid) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&adev->vm_manager.pasid_lock, flags);
+		idr_remove(&adev->vm_manager.pasid_idr, vm->pasid);
+		spin_unlock_irqrestore(&adev->vm_manager.pasid_lock, flags);
+
+		vm->pasid = 0;
+	}
+
+	/* Count the new compute VM */
+	amdgpu_inc_compute_vms(adev);
+
+error:
+	amdgpu_bo_unreserve(vm->root.base.bo);
+	return r;
+}
+
+/**
  * amdgpu_vm_free_levels - free PD/PT levels
  *
  * @adev: amdgpu device structure
@@ -2483,7 +2570,7 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
  *
  * Free the page directory or page table level and all sub levels.
  */
-static void amdgpu_vm_free_levels(struct amdgpu_device *adev,
+void amdgpu_vm_free_levels(struct amdgpu_device *adev,
 				  struct amdgpu_vm_pt *parent,
 				  unsigned level)
 {
@@ -2640,10 +2727,10 @@ void amdgpu_vm_manager_init(struct amdgpu_device *adev)
 	spin_lock_init(&adev->vm_manager.prt_lock);
 	atomic_set(&adev->vm_manager.num_prt_users, 0);
 
-	adev->vm_manager.n_compute_vms = 0;
 
         idr_init(&adev->vm_manager.pasid_idr);
         spin_lock_init(&adev->vm_manager.pasid_lock);
+	adev->vm_manager.n_compute_vms = 0;
 
 	/* If not overridden by the user, by default, only in large BAR systems
 	 * Compute VM tables will be updated by CPU
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
index 0ecce18..c0706a9 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h
@@ -197,6 +197,14 @@ struct amdgpu_vm {
 
 	/* Whether this is a Compute or GFX Context */
 	int			vm_context;
+	/* Points to the KFD process VM info */
+	struct amdkfd_process_info *process_info;
+
+	/* List node in amdkfd_process_info.vm_list_head */
+	struct list_head        vm_list_node;
+
+	/* Valid while the PD is reserved or fenced */
+	uint64_t                pd_phys_addr;
 
 	/* Flag to indicate if VM tables are updated by CPU or GPU (SDMA) */
 	bool                    use_cpu_for_update;
@@ -236,8 +244,6 @@ struct amdgpu_vm_manager {
 	spinlock_t				prt_lock;
 	atomic_t				num_prt_users;
 
-	/* Number of Compute VMs, used for detecting Compute activity */
-	unsigned                                n_compute_vms;
 
 
         /* PASID to VM mapping, will be used in interrupt context to
@@ -246,6 +252,8 @@ struct amdgpu_vm_manager {
         struct idr                              pasid_idr;
         spinlock_t                              pasid_lock;
 
+	/* Number of Compute VMs, used for detecting Compute activity */
+	unsigned                                n_compute_vms;
 	/* controls how VM page tables are updated for Graphics and Compute.
 	 * BIT0[= 0] Graphics updated by SDMA [= 1] by CPU
 	 * BIT1[= 0] Compute updated by SDMA [= 1] by CPU
@@ -257,6 +265,7 @@ void amdgpu_vm_manager_init(struct amdgpu_device *adev);
 void amdgpu_vm_manager_fini(struct amdgpu_device *adev);
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
                    int vm_context, unsigned int pasid);
+int amdgpu_vm_make_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 bool amdgpu_vm_pasid_fault_credit(struct amdgpu_device *adev,
 				  unsigned int pasid);
diff --git a/drivers/gpu/drm/amd/amdgpu/soc15d.h b/drivers/gpu/drm/amd/amdgpu/soc15d.h
index e9583f4..99c7430 100644
--- a/drivers/gpu/drm/amd/amdgpu/soc15d.h
+++ b/drivers/gpu/drm/amd/amdgpu/soc15d.h
@@ -265,6 +265,7 @@
 #define	PACKET3_INVALIDATE_TLBS_DST_SEL(x)		((x) << 0)
 #define PACKET3_INVALIDATE_TLBS_ALL_HUB(x)		((x) << 4)
 #define	PACKET3_INVALIDATE_TLBS_PASID(x)		((x) << 5)
+#define PACKET3_INVALIDATE_TLBS_FLUSH_TYPE(x)  ((x) << 29)
 #define PACKET3_FRAME_CONTROL				0x90
 #			define FRAME_CMD(x) ((x) << 28)
 			/*
diff --git a/drivers/gpu/drm/amd/amdkfd/Kconfig b/drivers/gpu/drm/amd/amdkfd/Kconfig
index 50b8b56..3d46b72 100644
--- a/drivers/gpu/drm/amd/amdkfd/Kconfig
+++ b/drivers/gpu/drm/amd/amdkfd/Kconfig
@@ -6,6 +6,6 @@ config HSA_AMD
 	tristate "HSA kernel driver for AMD GPU devices"
 	depends on (DRM_RADEON || DRM_AMDGPU) && (X86_64 || PPC64 || ARM64)
 	select DRM_AMDGPU_USERPTR
-	imply AMD_IOMMU_V2
+	select AMD_IOMMU_V2
 	help
 	  Enable this if you want to use HSA features on AMD GPU devices.
diff --git a/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h b/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h
new file mode 100644
index 0000000..d5d1331
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_carrizo.h
@@ -0,0 +1,1384 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#if 0
+HW (VI) source code for CWSR trap handler
+#Version 18 + multiple trap handler
+
+// this performance-optimal version was originally from Seven Xu at SRDC
+
+// Revison #18   --...
+/* Rev History
+** #1. Branch from gc dv.   //gfxip/gfx8/main/src/test/suites/block/cs/sr/cs_trap_handler.sp3#1,#50, #51, #52-53(Skip, Already Fixed by PV), #54-56(merged),#57-58(mergerd, skiped-already fixed by PV)
+** #4. SR Memory Layout:
+**             1. VGPR-SGPR-HWREG-{LDS}
+**             2. tba_hi.bits.26 - reconfigured as the first wave in tg bits, for defer Save LDS for a threadgroup.. performance concern..
+** #5. Update: 1. Accurate g8sr_ts_save_d timestamp
+** #6. Update: 1. Fix s_barrier usage; 2. VGPR s/r using swizzle buffer?(NoNeed, already matched the swizzle pattern, more investigation)
+** #7. Update: 1. don't barrier if noLDS
+** #8. Branch: 1. Branch to ver#0, which is very similar to gc dv version
+**             2. Fix SQ issue by s_sleep 2
+** #9. Update: 1. Fix scc restore failed issue, restore wave_status at last
+**             2. optimize s_buffer save by burst 16sgprs...
+** #10. Update 1. Optimize restore sgpr by busrt 16 sgprs.
+** #11. Update 1. Add 2 more timestamp for debug version
+** #12. Update 1. Add VGPR SR using DWx4, some case improve and some case drop performance
+** #13. Integ  1. Always use MUBUF for PV trap shader...
+** #14. Update 1. s_buffer_store soft clause...
+** #15. Update 1. PERF - sclar write with glc:0/mtype0 to allow L2 combine. perf improvement a lot.
+** #16. Update 1. PRRF - UNROLL LDS_DMA got 2500cycle save in IP tree
+** #17. Update 1. FUNC - LDS_DMA has issues while ATC, replace with ds_read/buffer_store for save part[TODO restore part]
+**             2. PERF - Save LDS before save VGPR to cover LDS save long latency...
+** #18. Update 1. FUNC - Implicitly estore STATUS.VCCZ, which is not writable by s_setreg_b32
+**             2. FUNC - Handle non-CWSR traps
+*/
+
+var G8SR_WDMEM_HWREG_OFFSET = 0
+var G8SR_WDMEM_SGPR_OFFSET  = 128  // in bytes
+
+// Keep definition same as the app shader, These 2 time stamps are part of the app shader... Should before any Save and after restore.
+
+var G8SR_DEBUG_TIMESTAMP = 0
+var G8SR_DEBUG_TS_SAVE_D_OFFSET = 40*4  // ts_save_d timestamp offset relative to SGPR_SR_memory_offset
+var s_g8sr_ts_save_s    = s[34:35]   // save start
+var s_g8sr_ts_sq_save_msg  = s[36:37]   // The save shader send SAVEWAVE msg to spi
+var s_g8sr_ts_spi_wrexec   = s[38:39]   // the SPI write the sr address to SQ
+var s_g8sr_ts_save_d    = s[40:41]   // save end
+var s_g8sr_ts_restore_s = s[42:43]   // restore start
+var s_g8sr_ts_restore_d = s[44:45]   // restore end
+
+var G8SR_VGPR_SR_IN_DWX4 = 0
+var G8SR_SAVE_BUF_RSRC_WORD1_STRIDE_DWx4 = 0x00100000    // DWx4 stride is 4*4Bytes
+var G8SR_RESTORE_BUF_RSRC_WORD1_STRIDE_DWx4  = G8SR_SAVE_BUF_RSRC_WORD1_STRIDE_DWx4
+
+
+/*************************************************************************/
+/*					control on how to run the shader					 */
+/*************************************************************************/
+//any hack that needs to be made to run this code in EMU (either becasue various EMU code are not ready or no compute save & restore in EMU run)
+var EMU_RUN_HACK					=	0
+var EMU_RUN_HACK_RESTORE_NORMAL		=	0
+var EMU_RUN_HACK_SAVE_NORMAL_EXIT	=	0
+var	EMU_RUN_HACK_SAVE_SINGLE_WAVE	=	0
+var EMU_RUN_HACK_SAVE_FIRST_TIME	= 	0					//for interrupted restore in which the first save is through EMU_RUN_HACK
+var EMU_RUN_HACK_SAVE_FIRST_TIME_TBA_LO	= 	0					//for interrupted restore in which the first save is through EMU_RUN_HACK
+var EMU_RUN_HACK_SAVE_FIRST_TIME_TBA_HI	= 	0					//for interrupted restore in which the first save is through EMU_RUN_HACK
+var SAVE_LDS						= 	1
+var WG_BASE_ADDR_LO					=   0x9000a000
+var WG_BASE_ADDR_HI					=	0x0
+var WAVE_SPACE						=	0x5000				//memory size that each wave occupies in workgroup state mem
+var CTX_SAVE_CONTROL				=	0x0
+var CTX_RESTORE_CONTROL				=	CTX_SAVE_CONTROL
+var SIM_RUN_HACK					=	0					//any hack that needs to be made to run this code in SIM (either becasue various RTL code are not ready or no compute save & restore in RTL run)
+var	SGPR_SAVE_USE_SQC				=	1					//use SQC D$ to do the write
+var USE_MTBUF_INSTEAD_OF_MUBUF		=	0					//becasue TC EMU curently asserts on 0 of // overload DFMT field to carry 4 more bits of stride for MUBUF opcodes
+var SWIZZLE_EN						=	0					//whether we use swizzled buffer addressing
+
+/**************************************************************************/
+/*                     	variables							              */
+/**************************************************************************/
+var SQ_WAVE_STATUS_INST_ATC_SHIFT  = 23
+var SQ_WAVE_STATUS_INST_ATC_MASK   = 0x00800000
+var SQ_WAVE_STATUS_SPI_PRIO_MASK   = 0x00000006
+
+var SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT	= 12
+var SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE		= 9
+var SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT	= 8
+var SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE	= 6
+var SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT	= 24
+var SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE	= 3						//FIXME	 sq.blk still has 4 bits at this time while SQ programming guide has 3 bits
+
+var	SQ_WAVE_TRAPSTS_SAVECTX_MASK	=	0x400
+var SQ_WAVE_TRAPSTS_EXCE_MASK       =   0x1FF          			// Exception mask					
+var	SQ_WAVE_TRAPSTS_SAVECTX_SHIFT	=	10					
+var	SQ_WAVE_TRAPSTS_MEM_VIOL_MASK	=	0x100					
+var	SQ_WAVE_TRAPSTS_MEM_VIOL_SHIFT	=	8		
+var	SQ_WAVE_TRAPSTS_PRE_SAVECTX_MASK 	=	0x3FF
+var	SQ_WAVE_TRAPSTS_PRE_SAVECTX_SHIFT 	=	0x0
+var	SQ_WAVE_TRAPSTS_PRE_SAVECTX_SIZE 	=	10
+var	SQ_WAVE_TRAPSTS_POST_SAVECTX_MASK 	=	0xFFFFF800	
+var	SQ_WAVE_TRAPSTS_POST_SAVECTX_SHIFT 	=	11
+var	SQ_WAVE_TRAPSTS_POST_SAVECTX_SIZE 	=	21	
+
+var SQ_WAVE_IB_STS_RCNT_SHIFT			=	16					//FIXME
+var SQ_WAVE_IB_STS_RCNT_SIZE			=	4					//FIXME
+var SQ_WAVE_IB_STS_FIRST_REPLAY_SHIFT	=	15					//FIXME
+var SQ_WAVE_IB_STS_FIRST_REPLAY_SIZE	=	1					//FIXME
+var SQ_WAVE_IB_STS_RCNT_FIRST_REPLAY_MASK_NEG	= 0x00007FFF	//FIXME
+ 
+var	SQ_BUF_RSRC_WORD1_ATC_SHIFT		=	24
+var	SQ_BUF_RSRC_WORD3_MTYPE_SHIFT	=	27
+
+
+/*      Save        */
+var	S_SAVE_BUF_RSRC_WORD1_STRIDE		=	0x00040000  		//stride is 4 bytes 
+var	S_SAVE_BUF_RSRC_WORD3_MISC			= 	0x00807FAC			//SQ_SEL_X/Y/Z/W, BUF_NUM_FORMAT_FLOAT, (0 for MUBUF stride[17:14] when ADD_TID_ENABLE and BUF_DATA_FORMAT_32 for MTBUF), ADD_TID_ENABLE			
+
+var	S_SAVE_SPI_INIT_ATC_MASK			=	0x08000000			//bit[27]: ATC bit
+var	S_SAVE_SPI_INIT_ATC_SHIFT			=	27
+var	S_SAVE_SPI_INIT_MTYPE_MASK			=	0x70000000			//bit[30:28]: Mtype
+var	S_SAVE_SPI_INIT_MTYPE_SHIFT			=	28
+var	S_SAVE_SPI_INIT_FIRST_WAVE_MASK		=	0x04000000			//bit[26]: FirstWaveInTG
+var	S_SAVE_SPI_INIT_FIRST_WAVE_SHIFT	=	26
+
+var S_SAVE_PC_HI_RCNT_SHIFT				=	28					//FIXME	 check with Brian to ensure all fields other than PC[47:0] can be used
+var S_SAVE_PC_HI_RCNT_MASK				=   0xF0000000			//FIXME
+var S_SAVE_PC_HI_FIRST_REPLAY_SHIFT		=	27					//FIXME
+var S_SAVE_PC_HI_FIRST_REPLAY_MASK		=	0x08000000			//FIXME
+
+var	s_save_spi_init_lo				=	exec_lo
+var s_save_spi_init_hi				=	exec_hi
+
+												//tba_lo and tba_hi need to be saved/restored
+var	s_save_pc_lo			=	ttmp0			//{TTMP1, TTMP0} = {3h0,pc_rewind[3:0], HT[0],trapID[7:0], PC[47:0]}
+var	s_save_pc_hi			=	ttmp1			
+var s_save_exec_lo			=	ttmp2
+var s_save_exec_hi			= 	ttmp3			
+var	s_save_status			=	ttmp4			
+var	s_save_trapsts			=	ttmp5			//not really used until the end of the SAVE routine
+var s_save_xnack_mask_lo	=	ttmp6
+var s_save_xnack_mask_hi	=	ttmp7
+var	s_save_buf_rsrc0		=	ttmp8
+var	s_save_buf_rsrc1		=	ttmp9
+var	s_save_buf_rsrc2		=	ttmp10
+var	s_save_buf_rsrc3		=	ttmp11
+
+var s_save_mem_offset		= 	tma_lo				
+var s_save_alloc_size		=	s_save_trapsts			//conflict
+var s_save_tmp              =   s_save_buf_rsrc2       	//shared with s_save_buf_rsrc2  (conflict: should not use mem access with s_save_tmp at the same time)
+var s_save_m0				=	tma_hi					
+
+/*      Restore     */
+var	S_RESTORE_BUF_RSRC_WORD1_STRIDE			=	S_SAVE_BUF_RSRC_WORD1_STRIDE 
+var	S_RESTORE_BUF_RSRC_WORD3_MISC			= 	S_SAVE_BUF_RSRC_WORD3_MISC		 
+
+var	S_RESTORE_SPI_INIT_ATC_MASK			    =	0x08000000			//bit[27]: ATC bit
+var	S_RESTORE_SPI_INIT_ATC_SHIFT			=	27
+var	S_RESTORE_SPI_INIT_MTYPE_MASK			=	0x70000000			//bit[30:28]: Mtype
+var	S_RESTORE_SPI_INIT_MTYPE_SHIFT			=	28
+var	S_RESTORE_SPI_INIT_FIRST_WAVE_MASK		=	0x04000000			//bit[26]: FirstWaveInTG
+var	S_RESTORE_SPI_INIT_FIRST_WAVE_SHIFT	    =	26
+
+var S_RESTORE_PC_HI_RCNT_SHIFT				=	S_SAVE_PC_HI_RCNT_SHIFT
+var S_RESTORE_PC_HI_RCNT_MASK				=   S_SAVE_PC_HI_RCNT_MASK
+var S_RESTORE_PC_HI_FIRST_REPLAY_SHIFT		=	S_SAVE_PC_HI_FIRST_REPLAY_SHIFT
+var S_RESTORE_PC_HI_FIRST_REPLAY_MASK		=	S_SAVE_PC_HI_FIRST_REPLAY_MASK
+
+var s_restore_spi_init_lo                   =   exec_lo
+var s_restore_spi_init_hi                   =   exec_hi
+
+var s_restore_mem_offset		= 	ttmp2
+var s_restore_alloc_size		=	ttmp3
+var s_restore_tmp           	=   ttmp6				//tba_lo/hi need to be restored
+var s_restore_mem_offset_save	= 	s_restore_tmp 		//no conflict
+
+var s_restore_m0			=	s_restore_alloc_size	//no conflict			
+
+var s_restore_mode			=  	ttmp7
+
+var	s_restore_pc_lo		    =	ttmp0			
+var	s_restore_pc_hi		    =	ttmp1
+var s_restore_exec_lo		=	tma_lo					//no conflict
+var s_restore_exec_hi		= 	tma_hi					//no conflict
+var	s_restore_status	    =	ttmp4			
+var	s_restore_trapsts	    =	ttmp5
+var s_restore_xnack_mask_lo	=	xnack_mask_lo
+var s_restore_xnack_mask_hi	=	xnack_mask_hi
+var	s_restore_buf_rsrc0		=	ttmp8
+var	s_restore_buf_rsrc1		=	ttmp9
+var	s_restore_buf_rsrc2		=	ttmp10
+var	s_restore_buf_rsrc3		=	ttmp11
+
+/**************************************************************************/
+/*                     	trap handler entry points			              */
+/**************************************************************************/
+/* Shader Main*/
+
+shader main
+  asic(VI)
+  type(CS)
+
+
+    if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_RESTORE_NORMAL)) 					//hack to use trap_id for determining save/restore
+		//FIXME VCCZ un-init assertion s_getreg_b32  	s_save_status, hwreg(HW_REG_STATUS)			//save STATUS since we will change SCC
+		s_and_b32 s_save_tmp, s_save_pc_hi, 0xffff0000 				//change SCC
+    	s_cmp_eq_u32 s_save_tmp, 0x007e0000  						//Save: trap_id = 0x7e. Restore: trap_id = 0x7f.  
+    	s_cbranch_scc0 L_JUMP_TO_RESTORE							//do not need to recover STATUS here  since we are going to RESTORE
+		//FIXME  s_setreg_b32 	hwreg(HW_REG_STATUS), 	s_save_status		//need to recover STATUS since we are going to SAVE	
+		s_branch L_SKIP_RESTORE 									//NOT restore, SAVE actually
+	else	
+		s_branch L_SKIP_RESTORE 									//NOT restore. might be a regular trap or save
+    end
+
+L_JUMP_TO_RESTORE:
+    s_branch L_RESTORE												//restore
+
+L_SKIP_RESTORE:
+	
+	s_getreg_b32  	s_save_status, hwreg(HW_REG_STATUS)								//save STATUS since we will change SCC
+	s_andn2_b32		s_save_status, s_save_status, SQ_WAVE_STATUS_SPI_PRIO_MASK      //check whether this is for save
+	s_getreg_b32  	s_save_trapsts, hwreg(HW_REG_TRAPSTS)    		 				
+	s_and_b32		s_save_trapsts, s_save_trapsts, SQ_WAVE_TRAPSTS_SAVECTX_MASK	//check whether this is for save  
+	s_cbranch_scc1	L_SAVE		     	  						//this is the operation for save
+
+    // *********    Handle non-CWSR traps       *******************
+if (!EMU_RUN_HACK)
+    /* read tba and tma for next level trap handler, ttmp4 is used as s_save_status */
+	s_load_dwordx4  [ttmp8,ttmp9,ttmp10, ttmp11], [tma_lo,tma_hi], 0 
+	s_waitcnt lgkmcnt(0)
+	s_or_b32        ttmp7, ttmp8, ttmp9
+	s_cbranch_scc0  L_NO_NEXT_TRAP //next level trap handler not been set
+	s_setreg_b32    hwreg(HW_REG_STATUS), s_save_status //restore HW status(SCC)
+	s_setpc_b64     [ttmp8,ttmp9] //jump to next level trap handler 
+
+L_NO_NEXT_TRAP:
+	s_getreg_b32    s_save_trapsts, hwreg(HW_REG_TRAPSTS)
+	s_and_b32       s_save_trapsts, s_save_trapsts, SQ_WAVE_TRAPSTS_EXCE_MASK // Check whether it is an exception
+	s_cbranch_scc1  L_EXCP_CASE   // Exception, jump back to the shader program directly.
+	s_add_u32       ttmp0, ttmp0, 4   // S_TRAP case, add 4 to ttmp0
+	s_addc_u32	ttmp1, ttmp1, 0
+L_EXCP_CASE:
+	s_and_b32	ttmp1, ttmp1, 0xFFFF
+	s_setreg_b32    hwreg(HW_REG_STATUS), s_save_status //restore HW status(SCC)   
+	s_rfe_b64    	[ttmp0, ttmp1]
+end
+    // *********        End handling of non-CWSR traps   *******************
+
+/**************************************************************************/
+/*                     	save routine						              */
+/**************************************************************************/
+
+L_SAVE:	
+
+if G8SR_DEBUG_TIMESTAMP
+        s_memrealtime	s_g8sr_ts_save_s
+        s_waitcnt lgkmcnt(0)         //FIXME, will cause xnack??
+end
+
+	//check whether there is mem_viol
+	s_getreg_b32  	s_save_trapsts, hwreg(HW_REG_TRAPSTS)    		 				
+	s_and_b32	s_save_trapsts, s_save_trapsts, SQ_WAVE_TRAPSTS_MEM_VIOL_MASK			
+	s_cbranch_scc0	L_NO_PC_REWIND
+    
+	//if so, need rewind PC assuming GDS operation gets NACKed
+	s_mov_b32       s_save_tmp, 0															//clear mem_viol bit
+	s_setreg_b32	hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_MEM_VIOL_SHIFT, 1), s_save_tmp	//clear mem_viol bit 
+	s_and_b32 		s_save_pc_hi, s_save_pc_hi, 0x0000ffff    //pc[47:32]
+	s_sub_u32 		s_save_pc_lo, s_save_pc_lo, 8             //pc[31:0]-8
+	s_subb_u32 		s_save_pc_hi, s_save_pc_hi, 0x0			  // -scc
+
+L_NO_PC_REWIND:
+    s_mov_b32       s_save_tmp, 0															//clear saveCtx bit
+	s_setreg_b32	hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_SAVECTX_SHIFT, 1), s_save_tmp		//clear saveCtx bit   
+
+	s_mov_b32		s_save_xnack_mask_lo,	xnack_mask_lo									//save XNACK_MASK  
+	s_mov_b32		s_save_xnack_mask_hi,	xnack_mask_hi    //save XNACK must before any memory operation
+	s_getreg_b32	s_save_tmp, hwreg(HW_REG_IB_STS, SQ_WAVE_IB_STS_RCNT_SHIFT, SQ_WAVE_IB_STS_RCNT_SIZE)					//save RCNT
+	s_lshl_b32		s_save_tmp, s_save_tmp, S_SAVE_PC_HI_RCNT_SHIFT
+	s_or_b32		s_save_pc_hi, s_save_pc_hi, s_save_tmp
+	s_getreg_b32	s_save_tmp, hwreg(HW_REG_IB_STS, SQ_WAVE_IB_STS_FIRST_REPLAY_SHIFT, SQ_WAVE_IB_STS_FIRST_REPLAY_SIZE)	//save FIRST_REPLAY
+	s_lshl_b32		s_save_tmp, s_save_tmp, S_SAVE_PC_HI_FIRST_REPLAY_SHIFT
+	s_or_b32		s_save_pc_hi, s_save_pc_hi, s_save_tmp
+	s_getreg_b32	s_save_tmp, hwreg(HW_REG_IB_STS)										//clear RCNT and FIRST_REPLAY in IB_STS
+	s_and_b32		s_save_tmp, s_save_tmp, SQ_WAVE_IB_STS_RCNT_FIRST_REPLAY_MASK_NEG
+
+	s_setreg_b32	hwreg(HW_REG_IB_STS), s_save_tmp
+    
+	/*		inform SPI the readiness and wait for SPI's go signal */
+	s_mov_b32		s_save_exec_lo,	exec_lo													//save EXEC and use EXEC for the go signal from SPI
+	s_mov_b32		s_save_exec_hi,	exec_hi
+	s_mov_b64		exec, 	0x0																//clear EXEC to get ready to receive
+
+if G8SR_DEBUG_TIMESTAMP
+        s_memrealtime  s_g8sr_ts_sq_save_msg
+        s_waitcnt lgkmcnt(0)
+end
+
+	if (EMU_RUN_HACK)
+	
+	else
+		s_sendmsg	sendmsg(MSG_SAVEWAVE)  //send SPI a message and wait for SPI's write to EXEC  
+	end
+
+  L_SLEEP:		
+	s_sleep 0x2                // sleep 1 (64clk) is not enough for 8 waves per SIMD, which will cause SQ hang, since the 7,8th wave could not get arbit to exec inst, while other waves are stuck into the sleep-loop and waiting for wrexec!=0
+	
+	if (EMU_RUN_HACK)
+																							
+	else
+		s_cbranch_execz	L_SLEEP                                                         
+	end
+
+if G8SR_DEBUG_TIMESTAMP
+        s_memrealtime  s_g8sr_ts_spi_wrexec
+        s_waitcnt lgkmcnt(0)
+end
+
+	/*      setup Resource Contants    */
+	if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_SAVE_SINGLE_WAVE))	
+		//calculate wd_addr using absolute thread id 
+		v_readlane_b32 s_save_tmp, v9, 0
+		s_lshr_b32 s_save_tmp, s_save_tmp, 6
+		s_mul_i32 s_save_tmp, s_save_tmp, WAVE_SPACE
+		s_add_i32 s_save_spi_init_lo, s_save_tmp, WG_BASE_ADDR_LO
+		s_mov_b32 s_save_spi_init_hi, WG_BASE_ADDR_HI
+		s_and_b32 s_save_spi_init_hi, s_save_spi_init_hi, CTX_SAVE_CONTROL		
+	else
+	end
+	if ((EMU_RUN_HACK) && (EMU_RUN_HACK_SAVE_SINGLE_WAVE))
+		s_add_i32 s_save_spi_init_lo, s_save_tmp, WG_BASE_ADDR_LO
+		s_mov_b32 s_save_spi_init_hi, WG_BASE_ADDR_HI
+		s_and_b32 s_save_spi_init_hi, s_save_spi_init_hi, CTX_SAVE_CONTROL		
+	else
+	end
+	
+	
+	s_mov_b32		s_save_buf_rsrc0, 	s_save_spi_init_lo														//base_addr_lo
+	s_and_b32		s_save_buf_rsrc1, 	s_save_spi_init_hi, 0x0000FFFF											//base_addr_hi
+	s_or_b32		s_save_buf_rsrc1, 	s_save_buf_rsrc1,  S_SAVE_BUF_RSRC_WORD1_STRIDE
+    s_mov_b32       s_save_buf_rsrc2,   0                                               						//NUM_RECORDS initial value = 0 (in bytes) although not neccessarily inited
+	s_mov_b32		s_save_buf_rsrc3, 	S_SAVE_BUF_RSRC_WORD3_MISC
+	s_and_b32		s_save_tmp,         s_save_spi_init_hi, S_SAVE_SPI_INIT_ATC_MASK		
+	s_lshr_b32		s_save_tmp,  		s_save_tmp, (S_SAVE_SPI_INIT_ATC_SHIFT-SQ_BUF_RSRC_WORD1_ATC_SHIFT)			//get ATC bit into position
+	s_or_b32		s_save_buf_rsrc3, 	s_save_buf_rsrc3,  s_save_tmp											//or ATC
+	s_and_b32		s_save_tmp,         s_save_spi_init_hi, S_SAVE_SPI_INIT_MTYPE_MASK		
+	s_lshr_b32		s_save_tmp,  		s_save_tmp, (S_SAVE_SPI_INIT_MTYPE_SHIFT-SQ_BUF_RSRC_WORD3_MTYPE_SHIFT)		//get MTYPE bits into position
+	s_or_b32		s_save_buf_rsrc3, 	s_save_buf_rsrc3,  s_save_tmp											//or MTYPE	
+	
+	//FIXME  right now s_save_m0/s_save_mem_offset use tma_lo/tma_hi  (might need to save them before using them?)
+	s_mov_b32		s_save_m0,			m0																	//save M0
+	
+	/* 		global mem offset			*/
+	s_mov_b32		s_save_mem_offset, 	0x0																		//mem offset initial value = 0
+
+
+
+
+	/* 		save HW registers	*/
+	//////////////////////////////
+
+  L_SAVE_HWREG:
+        // HWREG SR memory offset : size(VGPR)+size(SGPR)
+       get_vgpr_size_bytes(s_save_mem_offset)
+       get_sgpr_size_bytes(s_save_tmp)
+       s_add_u32 s_save_mem_offset, s_save_mem_offset, s_save_tmp
+
+
+    s_mov_b32		s_save_buf_rsrc2, 0x4								//NUM_RECORDS	in bytes
+	if (SWIZZLE_EN)
+		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
+	else
+		s_mov_b32		s_save_buf_rsrc2,  0x1000000								//NUM_RECORDS in bytes
+	end
+
+	
+	write_hwreg_to_mem(s_save_m0, s_save_buf_rsrc0, s_save_mem_offset)					//M0
+
+	if ((EMU_RUN_HACK) && (EMU_RUN_HACK_SAVE_FIRST_TIME))      
+		s_add_u32 s_save_pc_lo, s_save_pc_lo, 4             //pc[31:0]+4
+		s_addc_u32 s_save_pc_hi, s_save_pc_hi, 0x0			//carry bit over
+		s_mov_b32	tba_lo, EMU_RUN_HACK_SAVE_FIRST_TIME_TBA_LO
+	    s_mov_b32	tba_hi, EMU_RUN_HACK_SAVE_FIRST_TIME_TBA_HI	
+	end
+
+	write_hwreg_to_mem(s_save_pc_lo, s_save_buf_rsrc0, s_save_mem_offset)					//PC
+	write_hwreg_to_mem(s_save_pc_hi, s_save_buf_rsrc0, s_save_mem_offset)
+	write_hwreg_to_mem(s_save_exec_lo, s_save_buf_rsrc0, s_save_mem_offset)				//EXEC
+	write_hwreg_to_mem(s_save_exec_hi, s_save_buf_rsrc0, s_save_mem_offset)
+	write_hwreg_to_mem(s_save_status, s_save_buf_rsrc0, s_save_mem_offset)				//STATUS 
+	
+	//s_save_trapsts conflicts with s_save_alloc_size
+	s_getreg_b32    s_save_trapsts, hwreg(HW_REG_TRAPSTS)
+	write_hwreg_to_mem(s_save_trapsts, s_save_buf_rsrc0, s_save_mem_offset)				//TRAPSTS
+	
+	write_hwreg_to_mem(s_save_xnack_mask_lo, s_save_buf_rsrc0, s_save_mem_offset)			//XNACK_MASK_LO
+	write_hwreg_to_mem(s_save_xnack_mask_hi, s_save_buf_rsrc0, s_save_mem_offset)			//XNACK_MASK_HI
+	
+	//use s_save_tmp would introduce conflict here between s_save_tmp and s_save_buf_rsrc2
+	s_getreg_b32 	s_save_m0, hwreg(HW_REG_MODE)                                                   //MODE
+	write_hwreg_to_mem(s_save_m0, s_save_buf_rsrc0, s_save_mem_offset)
+	write_hwreg_to_mem(tba_lo, s_save_buf_rsrc0, s_save_mem_offset)						//TBA_LO
+	write_hwreg_to_mem(tba_hi, s_save_buf_rsrc0, s_save_mem_offset)						//TBA_HI
+
+
+
+	/*      the first wave in the threadgroup    */
+        // save fist_wave bits in tba_hi unused bit.26
+	s_and_b32		s_save_tmp, s_save_spi_init_hi, S_SAVE_SPI_INIT_FIRST_WAVE_MASK     // extract fisrt wave bit
+    //s_or_b32        tba_hi, s_save_tmp, tba_hi                                        // save first wave bit to tba_hi.bits[26]
+    s_mov_b32        s_save_exec_hi, 0x0                                
+    s_or_b32         s_save_exec_hi, s_save_tmp, s_save_exec_hi                          // save first wave bit to s_save_exec_hi.bits[26]
+
+
+	/*      	save SGPRs	    */
+        // Save SGPR before LDS save, then the s0 to s4 can be used during LDS save...
+	//////////////////////////////
+
+    // SGPR SR memory offset : size(VGPR)	
+    get_vgpr_size_bytes(s_save_mem_offset)
+    // TODO, change RSRC word to rearrange memory layout for SGPRS
+
+	s_getreg_b32 	s_save_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE) 				//spgr_size
+	s_add_u32 		s_save_alloc_size, s_save_alloc_size, 1
+	s_lshl_b32 		s_save_alloc_size, s_save_alloc_size, 4 						//Number of SGPRs = (sgpr_size + 1) * 16   (non-zero value) 
+	
+	if (SGPR_SAVE_USE_SQC)
+		s_lshl_b32		s_save_buf_rsrc2,	s_save_alloc_size, 2					//NUM_RECORDS in bytes 
+	else
+		s_lshl_b32		s_save_buf_rsrc2,	s_save_alloc_size, 8					//NUM_RECORDS in bytes (64 threads)
+	end
+	
+	if (SWIZZLE_EN)
+		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
+	else
+		s_mov_b32		s_save_buf_rsrc2,  0x1000000								//NUM_RECORDS in bytes
+	end
+
+
+	// backup s_save_buf_rsrc0,1 to s_save_pc_lo/hi, since write_16sgpr_to_mem function will change the rsrc0
+    //s_mov_b64 s_save_pc_lo, s_save_buf_rsrc0
+    s_mov_b64 s_save_xnack_mask_lo, s_save_buf_rsrc0
+    s_add_u32 s_save_buf_rsrc0, s_save_buf_rsrc0, s_save_mem_offset
+    s_addc_u32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0
+
+	s_mov_b32 		m0, 0x0 						//SGPR initial index value =0		
+  L_SAVE_SGPR_LOOP: 					
+    // SGPR is allocated in 16 SGPR granularity					
+	s_movrels_b64 	s0, s0     //s0 = s[0+m0], s1 = s[1+m0]
+    s_movrels_b64   s2, s2     //s2 = s[2+m0], s3 = s[3+m0]
+	s_movrels_b64 	s4, s4     //s4 = s[4+m0], s5 = s[5+m0]
+    s_movrels_b64   s6, s6     //s6 = s[6+m0], s7 = s[7+m0]
+	s_movrels_b64 	s8, s8     //s8 = s[8+m0], s9 = s[9+m0]
+    s_movrels_b64   s10, s10   //s10 = s[10+m0], s11 = s[11+m0]
+	s_movrels_b64 	s12, s12   //s12 = s[12+m0], s13 = s[13+m0]
+    s_movrels_b64   s14, s14   //s14 = s[14+m0], s15 = s[15+m0]
+
+	write_16sgpr_to_mem(s0, s_save_buf_rsrc0, s_save_mem_offset) //PV: the best performance should be using s_buffer_store_dwordx4
+	s_add_u32		m0, m0, 16														//next sgpr index
+	s_cmp_lt_u32 	m0, s_save_alloc_size 											//scc = (m0 < s_save_alloc_size) ? 1 : 0
+	s_cbranch_scc1 	L_SAVE_SGPR_LOOP									//SGPR save is complete?
+    // restore s_save_buf_rsrc0,1
+    //s_mov_b64 s_save_buf_rsrc0, s_save_pc_lo
+    s_mov_b64 s_save_buf_rsrc0, s_save_xnack_mask_lo
+
+
+
+
+	/*      	save first 4 VGPR, then LDS save could use   */
+        // each wave will alloc 4 vgprs at least...
+	/////////////////////////////////////////////////////////////////////////////////////
+
+    s_mov_b32       s_save_mem_offset, 0
+ 	s_mov_b32		exec_lo, 0xFFFFFFFF 											//need every thread from now on
+	s_mov_b32		exec_hi, 0xFFFFFFFF
+	
+	if (SWIZZLE_EN)
+		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
+	else
+		s_mov_b32		s_save_buf_rsrc2,  0x1000000								//NUM_RECORDS in bytes
+	end
+
+
+	// VGPR Allocated in 4-GPR granularity
+
+if G8SR_VGPR_SR_IN_DWX4
+        // the const stride for DWx4 is 4*4 bytes
+        s_and_b32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
+        s_or_b32  s_save_buf_rsrc1, s_save_buf_rsrc1, G8SR_SAVE_BUF_RSRC_WORD1_STRIDE_DWx4  // const stride to 4*4 bytes
+        
+        buffer_store_dwordx4 v0, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1
+
+        s_and_b32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
+        s_or_b32  s_save_buf_rsrc1, s_save_buf_rsrc1, S_SAVE_BUF_RSRC_WORD1_STRIDE  // reset const stride to 4 bytes
+else
+		buffer_store_dword v0, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1
+		buffer_store_dword v1, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256
+		buffer_store_dword v2, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256*2
+		buffer_store_dword v3, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256*3
+end
+
+
+
+	/*      	save LDS	    */
+	//////////////////////////////
+	
+  L_SAVE_LDS:
+
+        // Change EXEC to all threads...	
+	s_mov_b32		exec_lo, 0xFFFFFFFF   //need every thread from now on   
+	s_mov_b32		exec_hi, 0xFFFFFFFF
+	
+	s_getreg_b32 	s_save_alloc_size, hwreg(HW_REG_LDS_ALLOC,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE) 			//lds_size
+	s_and_b32		s_save_alloc_size, s_save_alloc_size, 0xFFFFFFFF				//lds_size is zero?
+	s_cbranch_scc0	L_SAVE_LDS_DONE                                                                            //no lds used? jump to L_SAVE_DONE
+
+	s_barrier               //LDS is used? wait for other waves in the same TG 
+	//s_and_b32		s_save_tmp, tba_hi, S_SAVE_SPI_INIT_FIRST_WAVE_MASK                //exec is still used here     
+    s_and_b32		s_save_tmp, s_save_exec_hi, S_SAVE_SPI_INIT_FIRST_WAVE_MASK                //exec is still used here
+	s_cbranch_scc0	L_SAVE_LDS_DONE
+
+        // first wave do LDS save;
+        
+	s_lshl_b32 		s_save_alloc_size, s_save_alloc_size, 6 						//LDS size in dwords = lds_size * 64dw
+	s_lshl_b32 		s_save_alloc_size, s_save_alloc_size, 2 						//LDS size in bytes
+	s_mov_b32		s_save_buf_rsrc2,  s_save_alloc_size  							//NUM_RECORDS in bytes
+
+    // LDS at offset: size(VGPR)+SIZE(SGPR)+SIZE(HWREG)
+    // 
+    get_vgpr_size_bytes(s_save_mem_offset)
+    get_sgpr_size_bytes(s_save_tmp)
+    s_add_u32  s_save_mem_offset, s_save_mem_offset, s_save_tmp
+    s_add_u32 s_save_mem_offset, s_save_mem_offset, get_hwreg_size_bytes()
+
+
+	if (SWIZZLE_EN)
+		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0	      //FIXME need to use swizzle to enable bounds checking?
+	else
+		s_mov_b32		s_save_buf_rsrc2,  0x1000000                  //NUM_RECORDS in bytes
+	end
+
+	s_mov_b32 		m0, 0x0                                               //lds_offset initial value = 0
+
+
+var LDS_DMA_ENABLE = 0
+var UNROLL = 0
+if UNROLL==0 && LDS_DMA_ENABLE==1
+        s_mov_b32  s3, 256*2
+        s_nop 0
+        s_nop 0
+        s_nop 0
+  L_SAVE_LDS_LOOP:
+        //TODO: looks the 2 buffer_store/load clause for s/r will hurt performance.???
+	if (SAVE_LDS)     //SPI always alloc LDS space in 128DW granularity
+            buffer_store_lds_dword s_save_buf_rsrc0, s_save_mem_offset lds:1            // first 64DW
+            buffer_store_lds_dword s_save_buf_rsrc0, s_save_mem_offset lds:1 offset:256 // second 64DW
+	end
+
+	s_add_u32		m0, m0, s3											//every buffer_store_lds does 256 bytes
+	s_add_u32		s_save_mem_offset, s_save_mem_offset, s3							//mem offset increased by 256 bytes
+	s_cmp_lt_u32	m0, s_save_alloc_size												//scc=(m0 < s_save_alloc_size) ? 1 : 0
+	s_cbranch_scc1  L_SAVE_LDS_LOOP														//LDS save is complete?
+
+elsif LDS_DMA_ENABLE==1 && UNROLL==1 // UNROOL  , has ichace miss
+      // store from higest LDS address to lowest
+      s_mov_b32  s3, 256*2
+      s_sub_u32  m0, s_save_alloc_size, s3
+      s_add_u32 s_save_mem_offset, s_save_mem_offset, m0
+      s_lshr_b32 s_save_alloc_size, s_save_alloc_size, 9   // how many 128 trunks...
+      s_sub_u32 s_save_alloc_size, 128, s_save_alloc_size   // store from higheset addr to lowest
+      s_mul_i32 s_save_alloc_size, s_save_alloc_size, 6*4   // PC offset increment,  each LDS save block cost 6*4 Bytes instruction
+      s_add_u32 s_save_alloc_size, s_save_alloc_size, 3*4   //2is the below 2 inst...//s_addc and s_setpc
+      s_nop 0
+      s_nop 0
+      s_nop 0   //pad 3 dw to let LDS_DMA align with 64Bytes
+      s_getpc_b64 s[0:1]                              // reuse s[0:1], since s[0:1] already saved
+      s_add_u32   s0, s0,s_save_alloc_size
+      s_addc_u32  s1, s1, 0
+      s_setpc_b64 s[0:1]
+      
+
+       for var i =0; i< 128; i++    
+            // be careful to make here a 64Byte aligned address, which could improve performance...
+            buffer_store_lds_dword s_save_buf_rsrc0, s_save_mem_offset lds:1 offset:0           // first 64DW
+            buffer_store_lds_dword s_save_buf_rsrc0, s_save_mem_offset lds:1 offset:256           // second 64DW
+
+	    if i!=127
+		s_sub_u32  m0, m0, s3      // use a sgpr to shrink 2DW-inst to 1DW inst to improve performance , i.e.  pack more LDS_DMA inst to one Cacheline
+	    	s_sub_u32  s_save_mem_offset, s_save_mem_offset,  s3
+            end
+       end
+
+else   // BUFFER_STORE
+      v_mbcnt_lo_u32_b32 v2, 0xffffffff, 0x0
+      v_mbcnt_hi_u32_b32 v3, 0xffffffff, v2     // tid
+      v_mul_i32_i24 v2, v3, 8   // tid*8
+      v_mov_b32 v3, 256*2
+      s_mov_b32 m0, 0x10000
+      s_mov_b32 s0, s_save_buf_rsrc3
+      s_and_b32 s_save_buf_rsrc3, s_save_buf_rsrc3, 0xFF7FFFFF    // disable add_tid 
+      s_or_b32 s_save_buf_rsrc3, s_save_buf_rsrc3, 0x58000   //DFMT
+
+L_SAVE_LDS_LOOP_VECTOR:
+      ds_read_b64 v[0:1], v2    //x =LDS[a], byte address
+      s_waitcnt lgkmcnt(0)
+      buffer_store_dwordx2  v[0:1], v2, s_save_buf_rsrc0, s_save_mem_offset offen:1  glc:1  slc:1
+//      s_waitcnt vmcnt(0)
+      v_add_u32 v2, vcc[0:1], v2, v3
+      v_cmp_lt_u32 vcc[0:1], v2, s_save_alloc_size
+      s_cbranch_vccnz L_SAVE_LDS_LOOP_VECTOR
+
+      // restore rsrc3
+      s_mov_b32 s_save_buf_rsrc3, s0
+      
+end
+
+L_SAVE_LDS_DONE:	
+
+
+	/*      	save VGPRs  - set the Rest VGPRs	    */
+	//////////////////////////////////////////////////////////////////////////////////////
+  L_SAVE_VGPR:
+    // VGPR SR memory offset: 0
+    // TODO rearrange the RSRC words to use swizzle for VGPR save...
+  
+    s_mov_b32       s_save_mem_offset, (0+256*4)                                    // for the rest VGPRs
+ 	s_mov_b32		exec_lo, 0xFFFFFFFF 											//need every thread from now on
+	s_mov_b32		exec_hi, 0xFFFFFFFF
+	
+	s_getreg_b32 	s_save_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE) 					//vpgr_size
+	s_add_u32 		s_save_alloc_size, s_save_alloc_size, 1
+	s_lshl_b32 		s_save_alloc_size, s_save_alloc_size, 2 						//Number of VGPRs = (vgpr_size + 1) * 4    (non-zero value)   //FIXME for GFX, zero is possible 
+	s_lshl_b32		s_save_buf_rsrc2,  s_save_alloc_size, 8							//NUM_RECORDS in bytes (64 threads*4)
+	if (SWIZZLE_EN)
+		s_add_u32		s_save_buf_rsrc2, s_save_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
+	else
+		s_mov_b32		s_save_buf_rsrc2,  0x1000000								//NUM_RECORDS in bytes
+	end
+
+
+	// VGPR Allocated in 4-GPR granularity
+
+if G8SR_VGPR_SR_IN_DWX4
+        // the const stride for DWx4 is 4*4 bytes
+        s_and_b32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
+        s_or_b32  s_save_buf_rsrc1, s_save_buf_rsrc1, G8SR_SAVE_BUF_RSRC_WORD1_STRIDE_DWx4  // const stride to 4*4 bytes
+        
+        s_mov_b32         m0, 4     // skip first 4 VGPRs
+        s_cmp_lt_u32      m0, s_save_alloc_size
+        s_cbranch_scc0    L_SAVE_VGPR_LOOP_END      // no more vgprs
+
+        s_set_gpr_idx_on  m0, 0x1   // This will change M0
+        s_add_u32         s_save_alloc_size, s_save_alloc_size, 0x1000  // because above inst change m0
+L_SAVE_VGPR_LOOP:
+        v_mov_b32         v0, v0   // v0 = v[0+m0]
+        v_mov_b32         v1, v1
+        v_mov_b32         v2, v2
+        v_mov_b32         v3, v3
+
+
+        buffer_store_dwordx4 v0, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1
+        s_add_u32         m0, m0, 4
+        s_add_u32         s_save_mem_offset, s_save_mem_offset, 256*4
+        s_cmp_lt_u32      m0, s_save_alloc_size
+	s_cbranch_scc1 	L_SAVE_VGPR_LOOP												//VGPR save is complete?
+	s_set_gpr_idx_off
+L_SAVE_VGPR_LOOP_END:
+
+        s_and_b32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
+        s_or_b32  s_save_buf_rsrc1, s_save_buf_rsrc1, S_SAVE_BUF_RSRC_WORD1_STRIDE  // reset const stride to 4 bytes
+else
+    // VGPR store using dw burst	
+    s_mov_b32 		  m0, 0x4   //VGPR initial index value =0
+    s_cmp_lt_u32      m0, s_save_alloc_size
+    s_cbranch_scc0    L_SAVE_VGPR_END
+
+
+    s_set_gpr_idx_on    m0, 0x1 //M0[7:0] = M0[7:0] and M0[15:12] = 0x1
+    s_add_u32		s_save_alloc_size, s_save_alloc_size, 0x1000					//add 0x1000 since we compare m0 against it later	
+
+  L_SAVE_VGPR_LOOP: 										
+	v_mov_b32 		v0, v0				//v0 = v[0+m0]	
+	v_mov_b32 		v1, v1				//v0 = v[0+m0]	
+	v_mov_b32 		v2, v2				//v0 = v[0+m0]	
+	v_mov_b32 		v3, v3				//v0 = v[0+m0]	
+	    
+    if(USE_MTBUF_INSTEAD_OF_MUBUF)       
+		tbuffer_store_format_x v0, v0, s_save_buf_rsrc0, s_save_mem_offset format:BUF_NUM_FORMAT_FLOAT format: BUF_DATA_FORMAT_32 slc:1 glc:1
+    else
+		buffer_store_dword v0, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1
+		buffer_store_dword v1, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256
+		buffer_store_dword v2, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256*2
+		buffer_store_dword v3, v0, s_save_buf_rsrc0, s_save_mem_offset slc:1 glc:1  offset:256*3
+	end
+
+    s_add_u32		m0, m0, 4														//next vgpr index
+	s_add_u32		s_save_mem_offset, s_save_mem_offset, 256*4						//every buffer_store_dword does 256 bytes
+	s_cmp_lt_u32 	m0,	s_save_alloc_size 											//scc = (m0 < s_save_alloc_size) ? 1 : 0
+	s_cbranch_scc1 	L_SAVE_VGPR_LOOP												//VGPR save is complete?
+	s_set_gpr_idx_off
+end
+	
+L_SAVE_VGPR_END:
+
+
+
+
+
+	
+	/*     S_PGM_END_SAVED  */    							//FIXME  graphics ONLY
+	if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_SAVE_NORMAL_EXIT))	
+		s_and_b32 s_save_pc_hi, s_save_pc_hi, 0x0000ffff    //pc[47:32]
+		s_add_u32 s_save_pc_lo, s_save_pc_lo, 4             //pc[31:0]+4
+		s_addc_u32 s_save_pc_hi, s_save_pc_hi, 0x0			//carry bit over
+		s_rfe_b64 s_save_pc_lo                              //Return to the main shader program
+	else
+	end
+
+// Save Done timestamp 
+if G8SR_DEBUG_TIMESTAMP
+        s_memrealtime	s_g8sr_ts_save_d
+        // SGPR SR memory offset : size(VGPR)	
+        get_vgpr_size_bytes(s_save_mem_offset)
+        s_add_u32 s_save_mem_offset, s_save_mem_offset, G8SR_DEBUG_TS_SAVE_D_OFFSET
+        s_waitcnt lgkmcnt(0)         //FIXME, will cause xnack??
+        // Need reset rsrc2??
+        s_mov_b32 m0, s_save_mem_offset
+        s_mov_b32 s_save_buf_rsrc2,  0x1000000                                  //NUM_RECORDS in bytes
+        s_buffer_store_dwordx2 s_g8sr_ts_save_d, s_save_buf_rsrc0, m0		glc:1	
+end
+
+	
+    s_branch	L_END_PGM
+	
+
+				
+/**************************************************************************/
+/*                     	restore routine						              */
+/**************************************************************************/
+
+L_RESTORE:
+    /*      Setup Resource Contants    */
+    if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_RESTORE_NORMAL))
+		//calculate wd_addr using absolute thread id
+		v_readlane_b32 s_restore_tmp, v9, 0
+		s_lshr_b32 s_restore_tmp, s_restore_tmp, 6
+		s_mul_i32 s_restore_tmp, s_restore_tmp, WAVE_SPACE
+		s_add_i32 s_restore_spi_init_lo, s_restore_tmp, WG_BASE_ADDR_LO
+		s_mov_b32 s_restore_spi_init_hi, WG_BASE_ADDR_HI
+		s_and_b32 s_restore_spi_init_hi, s_restore_spi_init_hi, CTX_RESTORE_CONTROL	
+	else
+	end
+
+if G8SR_DEBUG_TIMESTAMP
+        s_memrealtime	s_g8sr_ts_restore_s
+        s_waitcnt lgkmcnt(0)         //FIXME, will cause xnack??
+        // tma_lo/hi are sgpr 110, 111, which will not used for 112 SGPR allocated case...
+        s_mov_b32 s_restore_pc_lo, s_g8sr_ts_restore_s[0]
+        s_mov_b32 s_restore_pc_hi, s_g8sr_ts_restore_s[1]   //backup ts to ttmp0/1, sicne exec will be finally restored..
+end
+
+
+	
+    s_mov_b32		s_restore_buf_rsrc0, 	s_restore_spi_init_lo															//base_addr_lo
+	s_and_b32		s_restore_buf_rsrc1, 	s_restore_spi_init_hi, 0x0000FFFF												//base_addr_hi
+	s_or_b32		s_restore_buf_rsrc1, 	s_restore_buf_rsrc1,  S_RESTORE_BUF_RSRC_WORD1_STRIDE
+    s_mov_b32       s_restore_buf_rsrc2,   	0                                               								//NUM_RECORDS initial value = 0 (in bytes)
+	s_mov_b32		s_restore_buf_rsrc3, 	S_RESTORE_BUF_RSRC_WORD3_MISC
+	s_and_b32		s_restore_tmp,         	s_restore_spi_init_hi, S_RESTORE_SPI_INIT_ATC_MASK		
+	s_lshr_b32		s_restore_tmp,  		s_restore_tmp, (S_RESTORE_SPI_INIT_ATC_SHIFT-SQ_BUF_RSRC_WORD1_ATC_SHIFT)		//get ATC bit into position
+	s_or_b32		s_restore_buf_rsrc3, 	s_restore_buf_rsrc3,  s_restore_tmp												//or ATC
+	s_and_b32		s_restore_tmp,         	s_restore_spi_init_hi, S_RESTORE_SPI_INIT_MTYPE_MASK		
+	s_lshr_b32		s_restore_tmp,  		s_restore_tmp, (S_RESTORE_SPI_INIT_MTYPE_SHIFT-SQ_BUF_RSRC_WORD3_MTYPE_SHIFT)	//get MTYPE bits into position
+	s_or_b32		s_restore_buf_rsrc3, 	s_restore_buf_rsrc3,  s_restore_tmp												//or MTYPE
+	
+	/* 		global mem offset			*/
+//	s_mov_b32		s_restore_mem_offset, 0x0								//mem offset initial value = 0
+	
+	/*      the first wave in the threadgroup    */
+	s_and_b32		s_restore_tmp, s_restore_spi_init_hi, S_RESTORE_SPI_INIT_FIRST_WAVE_MASK			
+	s_cbranch_scc0	L_RESTORE_VGPR
+
+    /*      	restore LDS	    */
+	//////////////////////////////
+  L_RESTORE_LDS:
+
+	s_mov_b32		exec_lo, 0xFFFFFFFF 													//need every thread from now on   //be consistent with SAVE although can be moved ahead
+	s_mov_b32		exec_hi, 0xFFFFFFFF
+	
+	s_getreg_b32 	s_restore_alloc_size, hwreg(HW_REG_LDS_ALLOC,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE) 				//lds_size
+	s_and_b32		s_restore_alloc_size, s_restore_alloc_size, 0xFFFFFFFF					//lds_size is zero?
+	s_cbranch_scc0	L_RESTORE_VGPR															//no lds used? jump to L_RESTORE_VGPR
+	s_lshl_b32 		s_restore_alloc_size, s_restore_alloc_size, 6 							//LDS size in dwords = lds_size * 64dw
+	s_lshl_b32 		s_restore_alloc_size, s_restore_alloc_size, 2 							//LDS size in bytes
+	s_mov_b32		s_restore_buf_rsrc2,	s_restore_alloc_size							//NUM_RECORDS in bytes
+
+    // LDS at offset: size(VGPR)+SIZE(SGPR)+SIZE(HWREG)
+    // 
+    get_vgpr_size_bytes(s_restore_mem_offset)
+    get_sgpr_size_bytes(s_restore_tmp)
+    s_add_u32  s_restore_mem_offset, s_restore_mem_offset, s_restore_tmp
+    s_add_u32  s_restore_mem_offset, s_restore_mem_offset, get_hwreg_size_bytes()            //FIXME, Check if offset overflow???
+
+
+	if (SWIZZLE_EN)
+		s_add_u32		s_restore_buf_rsrc2, s_restore_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
+	else
+		s_mov_b32		s_restore_buf_rsrc2,  0x1000000										//NUM_RECORDS in bytes
+	end
+	s_mov_b32 		m0, 0x0 																//lds_offset initial value = 0
+
+  L_RESTORE_LDS_LOOP:									
+	if (SAVE_LDS)
+	    buffer_load_dword	v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset lds:1                    // first 64DW
+	    buffer_load_dword	v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset lds:1 offset:256         // second 64DW
+	end
+    s_add_u32		m0, m0, 256*2					                            // 128 DW
+	s_add_u32		s_restore_mem_offset, s_restore_mem_offset, 256*2           //mem offset increased by 128DW
+	s_cmp_lt_u32	m0, s_restore_alloc_size                                    //scc=(m0 < s_restore_alloc_size) ? 1 : 0
+	s_cbranch_scc1  L_RESTORE_LDS_LOOP														//LDS restore is complete?
+
+    
+    /*      	restore VGPRs	    */
+	//////////////////////////////
+  L_RESTORE_VGPR:
+        // VGPR SR memory offset : 0	
+	s_mov_b32		s_restore_mem_offset, 0x0
+ 	s_mov_b32		exec_lo, 0xFFFFFFFF 													//need every thread from now on   //be consistent with SAVE although can be moved ahead
+	s_mov_b32		exec_hi, 0xFFFFFFFF
+	
+	s_getreg_b32 	s_restore_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE) 	//vpgr_size
+	s_add_u32 		s_restore_alloc_size, s_restore_alloc_size, 1
+	s_lshl_b32 		s_restore_alloc_size, s_restore_alloc_size, 2 							//Number of VGPRs = (vgpr_size + 1) * 4    (non-zero value)
+    s_lshl_b32		s_restore_buf_rsrc2,  s_restore_alloc_size, 8						    //NUM_RECORDS in bytes (64 threads*4)
+	if (SWIZZLE_EN)
+		s_add_u32		s_restore_buf_rsrc2, s_restore_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
+	else
+		s_mov_b32		s_restore_buf_rsrc2,  0x1000000										//NUM_RECORDS in bytes
+	end
+
+if G8SR_VGPR_SR_IN_DWX4
+     get_vgpr_size_bytes(s_restore_mem_offset)
+     s_sub_u32         s_restore_mem_offset, s_restore_mem_offset, 256*4
+
+     // the const stride for DWx4 is 4*4 bytes
+     s_and_b32 s_restore_buf_rsrc1, s_restore_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
+     s_or_b32  s_restore_buf_rsrc1, s_restore_buf_rsrc1, G8SR_RESTORE_BUF_RSRC_WORD1_STRIDE_DWx4  // const stride to 4*4 bytes
+
+     s_mov_b32         m0, s_restore_alloc_size
+     s_set_gpr_idx_on  m0, 0x8    // Note.. This will change m0
+
+L_RESTORE_VGPR_LOOP:
+     buffer_load_dwordx4 v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset slc:1 glc:1
+     s_waitcnt vmcnt(0)
+     s_sub_u32         m0, m0, 4
+     v_mov_b32         v0, v0   // v[0+m0] = v0
+     v_mov_b32         v1, v1
+     v_mov_b32         v2, v2
+     v_mov_b32         v3, v3
+     s_sub_u32         s_restore_mem_offset, s_restore_mem_offset, 256*4
+     s_cmp_eq_u32      m0, 0x8000
+     s_cbranch_scc0    L_RESTORE_VGPR_LOOP
+     s_set_gpr_idx_off
+
+     s_and_b32 s_restore_buf_rsrc1, s_restore_buf_rsrc1, 0x0000FFFF   // reset const stride to 0
+     s_or_b32  s_restore_buf_rsrc1, s_restore_buf_rsrc1, S_RESTORE_BUF_RSRC_WORD1_STRIDE  // const stride to 4*4 bytes
+
+else
+	// VGPR load using dw burst
+	s_mov_b32		s_restore_mem_offset_save, s_restore_mem_offset		// restore start with v1, v0 will be the last
+	s_add_u32		s_restore_mem_offset, s_restore_mem_offset, 256*4
+    s_mov_b32 		m0, 4 			     				//VGPR initial index value = 1
+	s_set_gpr_idx_on  m0, 0x8						//M0[7:0] = M0[7:0] and M0[15:12] = 0x8
+    s_add_u32		s_restore_alloc_size, s_restore_alloc_size, 0x8000						//add 0x8000 since we compare m0 against it later	
+
+  L_RESTORE_VGPR_LOOP: 										
+    if(USE_MTBUF_INSTEAD_OF_MUBUF)       
+		tbuffer_load_format_x v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset format:BUF_NUM_FORMAT_FLOAT format: BUF_DATA_FORMAT_32 slc:1 glc:1
+    else
+		buffer_load_dword v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset	slc:1 glc:1	
+		buffer_load_dword v1, v0, s_restore_buf_rsrc0, s_restore_mem_offset	slc:1 glc:1	offset:256
+		buffer_load_dword v2, v0, s_restore_buf_rsrc0, s_restore_mem_offset	slc:1 glc:1	offset:256*2
+		buffer_load_dword v3, v0, s_restore_buf_rsrc0, s_restore_mem_offset	slc:1 glc:1	offset:256*3
+	end
+	s_waitcnt		vmcnt(0)																//ensure data ready
+	v_mov_b32		v0, v0																	//v[0+m0] = v0
+    v_mov_b32       v1, v1
+    v_mov_b32       v2, v2
+    v_mov_b32       v3, v3
+    s_add_u32		m0, m0, 4																//next vgpr index
+	s_add_u32		s_restore_mem_offset, s_restore_mem_offset, 256*4							//every buffer_load_dword does 256 bytes
+	s_cmp_lt_u32 	m0,	s_restore_alloc_size 												//scc = (m0 < s_restore_alloc_size) ? 1 : 0
+	s_cbranch_scc1 	L_RESTORE_VGPR_LOOP														//VGPR restore (except v0) is complete?
+	s_set_gpr_idx_off
+																							/* VGPR restore on v0 */
+    if(USE_MTBUF_INSTEAD_OF_MUBUF)       
+		tbuffer_load_format_x v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save format:BUF_NUM_FORMAT_FLOAT format: BUF_DATA_FORMAT_32 slc:1 glc:1
+    else
+		buffer_load_dword v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save	slc:1 glc:1	
+		buffer_load_dword v1, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save	slc:1 glc:1	offset:256
+		buffer_load_dword v2, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save	slc:1 glc:1	offset:256*2
+		buffer_load_dword v3, v0, s_restore_buf_rsrc0, s_restore_mem_offset_save	slc:1 glc:1	offset:256*3
+	end
+
+end
+	
+    /*      	restore SGPRs	    */
+	//////////////////////////////
+
+    // SGPR SR memory offset : size(VGPR)	
+    get_vgpr_size_bytes(s_restore_mem_offset)
+    get_sgpr_size_bytes(s_restore_tmp)
+    s_add_u32 s_restore_mem_offset, s_restore_mem_offset, s_restore_tmp
+    s_sub_u32 s_restore_mem_offset, s_restore_mem_offset, 16*4     // restore SGPR from S[n] to S[0], by 16 sgprs group
+    // TODO, change RSRC word to rearrange memory layout for SGPRS
+
+	s_getreg_b32 	s_restore_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE) 				//spgr_size
+	s_add_u32 		s_restore_alloc_size, s_restore_alloc_size, 1
+	s_lshl_b32 		s_restore_alloc_size, s_restore_alloc_size, 4 							//Number of SGPRs = (sgpr_size + 1) * 16   (non-zero value)
+
+	if (SGPR_SAVE_USE_SQC)
+		s_lshl_b32		s_restore_buf_rsrc2,	s_restore_alloc_size, 2						//NUM_RECORDS in bytes 
+	else
+		s_lshl_b32		s_restore_buf_rsrc2,	s_restore_alloc_size, 8						//NUM_RECORDS in bytes (64 threads)
+	end
+	if (SWIZZLE_EN)
+		s_add_u32		s_restore_buf_rsrc2, s_restore_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
+	else
+		s_mov_b32		s_restore_buf_rsrc2,  0x1000000										//NUM_RECORDS in bytes
+	end
+
+    /* If 112 SGPRs ar allocated, 4 sgprs are not used TBA(108,109),TMA(110,111),
+       However, we are safe to restore these 4 SGPRs anyway, since TBA,TMA will later be restored by HWREG
+    */
+    s_mov_b32 m0, s_restore_alloc_size
+
+ L_RESTORE_SGPR_LOOP: 
+	read_16sgpr_from_mem(s0, s_restore_buf_rsrc0, s_restore_mem_offset)  //PV: further performance improvement can be made
+	s_waitcnt		lgkmcnt(0)																//ensure data ready
+
+    s_sub_u32 m0, m0, 16    // Restore from S[n] to S[0]
+
+	s_movreld_b64 	s0, s0 		//s[0+m0] = s0
+    s_movreld_b64 	s2, s2
+    s_movreld_b64 	s4, s4
+    s_movreld_b64 	s6, s6
+    s_movreld_b64 	s8, s8
+    s_movreld_b64 	s10, s10
+    s_movreld_b64 	s12, s12
+    s_movreld_b64 	s14, s14
+
+	s_cmp_eq_u32 	m0, 0				//scc = (m0 < s_restore_alloc_size) ? 1 : 0
+	s_cbranch_scc0 	L_RESTORE_SGPR_LOOP             //SGPR restore (except s0) is complete?
+	
+    /* 		restore HW registers	*/
+	//////////////////////////////
+  L_RESTORE_HWREG:
+
+
+if G8SR_DEBUG_TIMESTAMP
+      s_mov_b32 s_g8sr_ts_restore_s[0], s_restore_pc_lo
+      s_mov_b32 s_g8sr_ts_restore_s[1], s_restore_pc_hi
+end
+
+    // HWREG SR memory offset : size(VGPR)+size(SGPR)
+    get_vgpr_size_bytes(s_restore_mem_offset)
+    get_sgpr_size_bytes(s_restore_tmp)
+    s_add_u32 s_restore_mem_offset, s_restore_mem_offset, s_restore_tmp
+
+
+    s_mov_b32		s_restore_buf_rsrc2, 0x4												//NUM_RECORDS	in bytes
+	if (SWIZZLE_EN)
+		s_add_u32		s_restore_buf_rsrc2, s_restore_buf_rsrc2, 0x0						//FIXME need to use swizzle to enable bounds checking?
+	else
+		s_mov_b32		s_restore_buf_rsrc2,  0x1000000										//NUM_RECORDS in bytes
+	end
+
+	read_hwreg_from_mem(s_restore_m0, s_restore_buf_rsrc0, s_restore_mem_offset)					//M0
+	read_hwreg_from_mem(s_restore_pc_lo, s_restore_buf_rsrc0, s_restore_mem_offset)				//PC
+	read_hwreg_from_mem(s_restore_pc_hi, s_restore_buf_rsrc0, s_restore_mem_offset)
+	read_hwreg_from_mem(s_restore_exec_lo, s_restore_buf_rsrc0, s_restore_mem_offset)				//EXEC
+	read_hwreg_from_mem(s_restore_exec_hi, s_restore_buf_rsrc0, s_restore_mem_offset)
+	read_hwreg_from_mem(s_restore_status, s_restore_buf_rsrc0, s_restore_mem_offset)				//STATUS
+	read_hwreg_from_mem(s_restore_trapsts, s_restore_buf_rsrc0, s_restore_mem_offset)				//TRAPSTS
+    read_hwreg_from_mem(xnack_mask_lo, s_restore_buf_rsrc0, s_restore_mem_offset)					//XNACK_MASK_LO
+	read_hwreg_from_mem(xnack_mask_hi, s_restore_buf_rsrc0, s_restore_mem_offset)					//XNACK_MASK_HI
+	read_hwreg_from_mem(s_restore_mode, s_restore_buf_rsrc0, s_restore_mem_offset)				//MODE
+	read_hwreg_from_mem(tba_lo, s_restore_buf_rsrc0, s_restore_mem_offset)						//TBA_LO
+	read_hwreg_from_mem(tba_hi, s_restore_buf_rsrc0, s_restore_mem_offset)						//TBA_HI
+	
+	s_waitcnt		lgkmcnt(0)																						//from now on, it is safe to restore STATUS and IB_STS
+
+	s_and_b32 s_restore_pc_hi, s_restore_pc_hi, 0x0000ffff    	//pc[47:32]        //Do it here in order not to affect STATUS
+
+	//for normal save & restore, the saved PC points to the next inst to execute, no adjustment needs to be made, otherwise:
+	if ((EMU_RUN_HACK) && (!EMU_RUN_HACK_RESTORE_NORMAL))
+		s_add_u32 s_restore_pc_lo, s_restore_pc_lo, 8            //pc[31:0]+8	  //two back-to-back s_trap are used (first for save and second for restore)
+		s_addc_u32	s_restore_pc_hi, s_restore_pc_hi, 0x0		 //carry bit over
+	end	
+	if ((EMU_RUN_HACK) && (EMU_RUN_HACK_RESTORE_NORMAL))	      
+		s_add_u32 s_restore_pc_lo, s_restore_pc_lo, 4            //pc[31:0]+4     // save is hack through s_trap but restore is normal
+		s_addc_u32	s_restore_pc_hi, s_restore_pc_hi, 0x0		 //carry bit over
+	end
+	
+	s_mov_b32 		m0, 		s_restore_m0
+	s_mov_b32 		exec_lo, 	s_restore_exec_lo
+	s_mov_b32 		exec_hi, 	s_restore_exec_hi
+	
+	s_and_b32		s_restore_m0, SQ_WAVE_TRAPSTS_PRE_SAVECTX_MASK, s_restore_trapsts
+	s_setreg_b32	hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_PRE_SAVECTX_SHIFT, SQ_WAVE_TRAPSTS_PRE_SAVECTX_SIZE), s_restore_m0
+	s_and_b32		s_restore_m0, SQ_WAVE_TRAPSTS_POST_SAVECTX_MASK, s_restore_trapsts
+	s_lshr_b32		s_restore_m0, s_restore_m0, SQ_WAVE_TRAPSTS_POST_SAVECTX_SHIFT
+	s_setreg_b32	hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_POST_SAVECTX_SHIFT, SQ_WAVE_TRAPSTS_POST_SAVECTX_SIZE), s_restore_m0
+	//s_setreg_b32 	hwreg(HW_REG_TRAPSTS), 	s_restore_trapsts      //don't overwrite SAVECTX bit as it may be set through external SAVECTX during restore
+	s_setreg_b32 	hwreg(HW_REG_MODE), 	s_restore_mode
+	//reuse s_restore_m0 as a temp register
+	s_and_b32		s_restore_m0, s_restore_pc_hi, S_SAVE_PC_HI_RCNT_MASK
+	s_lshr_b32		s_restore_m0, s_restore_m0, S_SAVE_PC_HI_RCNT_SHIFT
+	s_lshl_b32		s_restore_m0, s_restore_m0, SQ_WAVE_IB_STS_RCNT_SHIFT
+	s_mov_b32		s_restore_tmp, 0x0																				//IB_STS is zero
+	s_or_b32		s_restore_tmp, s_restore_tmp, s_restore_m0
+	s_and_b32		s_restore_m0, s_restore_pc_hi, S_SAVE_PC_HI_FIRST_REPLAY_MASK
+	s_lshr_b32		s_restore_m0, s_restore_m0, S_SAVE_PC_HI_FIRST_REPLAY_SHIFT
+	s_lshl_b32		s_restore_m0, s_restore_m0, SQ_WAVE_IB_STS_FIRST_REPLAY_SHIFT
+	s_or_b32		s_restore_tmp, s_restore_tmp, s_restore_m0
+    s_and_b32       s_restore_m0, s_restore_status, SQ_WAVE_STATUS_INST_ATC_MASK 
+    s_lshr_b32		s_restore_m0, s_restore_m0, SQ_WAVE_STATUS_INST_ATC_SHIFT    
+    s_setreg_b32 	hwreg(HW_REG_IB_STS), 	s_restore_tmp
+
+	s_and_b64    exec, exec, exec  // Restore STATUS.EXECZ, not writable by s_setreg_b32
+	s_and_b64    vcc, vcc, vcc  // Restore STATUS.VCCZ, not writable by s_setreg_b32
+	s_setreg_b32 	hwreg(HW_REG_STATUS), 	s_restore_status     // SCC is included, which is changed by previous salu
+	
+	s_barrier													//barrier to ensure the readiness of LDS before access attemps from any other wave in the same TG //FIXME not performance-optimal at this time
+
+if G8SR_DEBUG_TIMESTAMP
+	s_memrealtime s_g8sr_ts_restore_d
+	s_waitcnt lgkmcnt(0)
+end	
+	
+//	s_rfe_b64 s_restore_pc_lo                              		//Return to the main shader program and resume execution
+    s_rfe_restore_b64  s_restore_pc_lo, s_restore_m0            // s_restore_m0[0] is used to set STATUS.inst_atc 
+
+
+/**************************************************************************/
+/*                     	the END								              */
+/**************************************************************************/	
+L_END_PGM:	
+	s_endpgm
+	
+end	
+
+
+/**************************************************************************/
+/*                     	the helper functions							  */
+/**************************************************************************/
+
+//Only for save hwreg to mem
+function write_hwreg_to_mem(s, s_rsrc, s_mem_offset)
+		s_mov_b32 exec_lo, m0					//assuming exec_lo is not needed anymore from this point on
+		s_mov_b32 m0, s_mem_offset
+		s_buffer_store_dword s, s_rsrc, m0		glc:1
+		s_add_u32		s_mem_offset, s_mem_offset, 4
+		s_mov_b32	m0, exec_lo
+end
+
+
+// HWREG are saved before SGPRs, so all HWREG could be use.
+function write_16sgpr_to_mem(s, s_rsrc, s_mem_offset)
+
+		s_buffer_store_dwordx4 s[0], s_rsrc, 0  glc:1
+		s_buffer_store_dwordx4 s[4], s_rsrc, 16  glc:1
+		s_buffer_store_dwordx4 s[8], s_rsrc, 32  glc:1
+		s_buffer_store_dwordx4 s[12], s_rsrc, 48 glc:1
+        s_add_u32       s_rsrc[0], s_rsrc[0], 4*16
+        s_addc_u32 		s_rsrc[1], s_rsrc[1], 0x0			  // +scc
+end
+
+
+function read_hwreg_from_mem(s, s_rsrc, s_mem_offset)
+	s_buffer_load_dword s, s_rsrc, s_mem_offset		glc:1
+	s_add_u32		s_mem_offset, s_mem_offset, 4
+end
+
+function read_16sgpr_from_mem(s, s_rsrc, s_mem_offset)
+	s_buffer_load_dwordx16 s, s_rsrc, s_mem_offset		glc:1
+	s_sub_u32		s_mem_offset, s_mem_offset, 4*16
+end
+
+
+
+function get_lds_size_bytes(s_lds_size_byte)
+    // SQ LDS granularity is 64DW, while PGM_RSRC2.lds_size is in granularity 128DW	 
+    s_getreg_b32   s_lds_size_byte, hwreg(HW_REG_LDS_ALLOC, SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT, SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE) 			// lds_size
+    s_lshl_b32     s_lds_size_byte, s_lds_size_byte, 8 						//LDS size in dwords = lds_size * 64 *4Bytes    // granularity 64DW
+end
+
+function get_vgpr_size_bytes(s_vgpr_size_byte)
+    s_getreg_b32   s_vgpr_size_byte, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE)  //vpgr_size
+    s_add_u32      s_vgpr_size_byte, s_vgpr_size_byte, 1
+    s_lshl_b32     s_vgpr_size_byte, s_vgpr_size_byte, (2+8) //Number of VGPRs = (vgpr_size + 1) * 4 * 64 * 4   (non-zero value)   //FIXME for GFX, zero is possible 
+end
+
+function get_sgpr_size_bytes(s_sgpr_size_byte)
+    s_getreg_b32   s_sgpr_size_byte, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE)  //spgr_size
+    s_add_u32      s_sgpr_size_byte, s_sgpr_size_byte, 1
+    s_lshl_b32     s_sgpr_size_byte, s_sgpr_size_byte, 6 //Number of SGPRs = (sgpr_size + 1) * 16 *4   (non-zero value) 
+end
+
+function get_hwreg_size_bytes
+    return 128 //HWREG size 128 bytes
+end
+
+
+#endif
+
+static const uint32_t cwsr_trap_carrizo_hex[] = {
+	0xbf820001, 0xbf820123,
+	0xb8f4f802, 0x89748674,
+	0xb8f5f803, 0x8675ff75,
+	0x00000400, 0xbf850011,
+	0xc00a1e37, 0x00000000,
+	0xbf8c007f, 0x87777978,
+	0xbf840002, 0xb974f802,
+	0xbe801d78, 0xb8f5f803,
+	0x8675ff75, 0x000001ff,
+	0xbf850002, 0x80708470,
+	0x82718071, 0x8671ff71,
+	0x0000ffff, 0xb974f802,
+	0xbe801f70, 0xb8f5f803,
+	0x8675ff75, 0x00000100,
+	0xbf840006, 0xbefa0080,
+	0xb97a0203, 0x8671ff71,
+	0x0000ffff, 0x80f08870,
+	0x82f18071, 0xbefa0080,
+	0xb97a0283, 0xbef60068,
+	0xbef70069, 0xb8fa1c07,
+	0x8e7a9c7a, 0x87717a71,
+	0xb8fa03c7, 0x8e7a9b7a,
+	0x87717a71, 0xb8faf807,
+	0x867aff7a, 0x00007fff,
+	0xb97af807, 0xbef2007e,
+	0xbef3007f, 0xbefe0180,
+	0xbf900004, 0xbf8e0002,
+	0xbf88fffe, 0xbef8007e,
+	0x8679ff7f, 0x0000ffff,
+	0x8779ff79, 0x00040000,
+	0xbefa0080, 0xbefb00ff,
+	0x00807fac, 0x867aff7f,
+	0x08000000, 0x8f7a837a,
+	0x877b7a7b, 0x867aff7f,
+	0x70000000, 0x8f7a817a,
+	0x877b7a7b, 0xbeef007c,
+	0xbeee0080, 0xb8ee2a05,
+	0x806e816e, 0x8e6e8a6e,
+	0xb8fa1605, 0x807a817a,
+	0x8e7a867a, 0x806e7a6e,
+	0xbefa0084, 0xbefa00ff,
+	0x01000000, 0xbefe007c,
+	0xbefc006e, 0xc0611bfc,
+	0x0000007c, 0x806e846e,
+	0xbefc007e, 0xbefe007c,
+	0xbefc006e, 0xc0611c3c,
+	0x0000007c, 0x806e846e,
+	0xbefc007e, 0xbefe007c,
+	0xbefc006e, 0xc0611c7c,
+	0x0000007c, 0x806e846e,
+	0xbefc007e, 0xbefe007c,
+	0xbefc006e, 0xc0611cbc,
+	0x0000007c, 0x806e846e,
+	0xbefc007e, 0xbefe007c,
+	0xbefc006e, 0xc0611cfc,
+	0x0000007c, 0x806e846e,
+	0xbefc007e, 0xbefe007c,
+	0xbefc006e, 0xc0611d3c,
+	0x0000007c, 0x806e846e,
+	0xbefc007e, 0xb8f5f803,
+	0xbefe007c, 0xbefc006e,
+	0xc0611d7c, 0x0000007c,
+	0x806e846e, 0xbefc007e,
+	0xbefe007c, 0xbefc006e,
+	0xc0611dbc, 0x0000007c,
+	0x806e846e, 0xbefc007e,
+	0xbefe007c, 0xbefc006e,
+	0xc0611dfc, 0x0000007c,
+	0x806e846e, 0xbefc007e,
+	0xb8eff801, 0xbefe007c,
+	0xbefc006e, 0xc0611bfc,
+	0x0000007c, 0x806e846e,
+	0xbefc007e, 0xbefe007c,
+	0xbefc006e, 0xc0611b3c,
+	0x0000007c, 0x806e846e,
+	0xbefc007e, 0xbefe007c,
+	0xbefc006e, 0xc0611b7c,
+	0x0000007c, 0x806e846e,
+	0xbefc007e, 0x867aff7f,
+	0x04000000, 0xbef30080,
+	0x8773737a, 0xb8ee2a05,
+	0x806e816e, 0x8e6e8a6e,
+	0xb8f51605, 0x80758175,
+	0x8e758475, 0x8e7a8275,
+	0xbefa00ff, 0x01000000,
+	0xbef60178, 0x80786e78,
+	0x82798079, 0xbefc0080,
+	0xbe802b00, 0xbe822b02,
+	0xbe842b04, 0xbe862b06,
+	0xbe882b08, 0xbe8a2b0a,
+	0xbe8c2b0c, 0xbe8e2b0e,
+	0xc06b003c, 0x00000000,
+	0xc06b013c, 0x00000010,
+	0xc06b023c, 0x00000020,
+	0xc06b033c, 0x00000030,
+	0x8078c078, 0x82798079,
+	0x807c907c, 0xbf0a757c,
+	0xbf85ffeb, 0xbef80176,
+	0xbeee0080, 0xbefe00c1,
+	0xbeff00c1, 0xbefa00ff,
+	0x01000000, 0xe0724000,
+	0x6e1e0000, 0xe0724100,
+	0x6e1e0100, 0xe0724200,
+	0x6e1e0200, 0xe0724300,
+	0x6e1e0300, 0xbefe00c1,
+	0xbeff00c1, 0xb8f54306,
+	0x8675c175, 0xbf84002c,
+	0xbf8a0000, 0x867aff73,
+	0x04000000, 0xbf840028,
+	0x8e758675, 0x8e758275,
+	0xbefa0075, 0xb8ee2a05,
+	0x806e816e, 0x8e6e8a6e,
+	0xb8fa1605, 0x807a817a,
+	0x8e7a867a, 0x806e7a6e,
+	0x806eff6e, 0x00000080,
+	0xbefa00ff, 0x01000000,
+	0xbefc0080, 0xd28c0002,
+	0x000100c1, 0xd28d0003,
+	0x000204c1, 0xd1060002,
+	0x00011103, 0x7e0602ff,
+	0x00000200, 0xbefc00ff,
+	0x00010000, 0xbe80007b,
+	0x867bff7b, 0xff7fffff,
+	0x877bff7b, 0x00058000,
+	0xd8ec0000, 0x00000002,
+	0xbf8c007f, 0xe0765000,
+	0x6e1e0002, 0x32040702,
+	0xd0c9006a, 0x0000eb02,
+	0xbf87fff7, 0xbefb0000,
+	0xbeee00ff, 0x00000400,
+	0xbefe00c1, 0xbeff00c1,
+	0xb8f52a05, 0x80758175,
+	0x8e758275, 0x8e7a8875,
+	0xbefa00ff, 0x01000000,
+	0xbefc0084, 0xbf0a757c,
+	0xbf840015, 0xbf11017c,
+	0x8075ff75, 0x00001000,
+	0x7e000300, 0x7e020301,
+	0x7e040302, 0x7e060303,
+	0xe0724000, 0x6e1e0000,
+	0xe0724100, 0x6e1e0100,
+	0xe0724200, 0x6e1e0200,
+	0xe0724300, 0x6e1e0300,
+	0x807c847c, 0x806eff6e,
+	0x00000400, 0xbf0a757c,
+	0xbf85ffef, 0xbf9c0000,
+	0xbf8200ca, 0xbef8007e,
+	0x8679ff7f, 0x0000ffff,
+	0x8779ff79, 0x00040000,
+	0xbefa0080, 0xbefb00ff,
+	0x00807fac, 0x8676ff7f,
+	0x08000000, 0x8f768376,
+	0x877b767b, 0x8676ff7f,
+	0x70000000, 0x8f768176,
+	0x877b767b, 0x8676ff7f,
+	0x04000000, 0xbf84001e,
+	0xbefe00c1, 0xbeff00c1,
+	0xb8f34306, 0x8673c173,
+	0xbf840019, 0x8e738673,
+	0x8e738273, 0xbefa0073,
+	0xb8f22a05, 0x80728172,
+	0x8e728a72, 0xb8f61605,
+	0x80768176, 0x8e768676,
+	0x80727672, 0x8072ff72,
+	0x00000080, 0xbefa00ff,
+	0x01000000, 0xbefc0080,
+	0xe0510000, 0x721e0000,
+	0xe0510100, 0x721e0000,
+	0x807cff7c, 0x00000200,
+	0x8072ff72, 0x00000200,
+	0xbf0a737c, 0xbf85fff6,
+	0xbef20080, 0xbefe00c1,
+	0xbeff00c1, 0xb8f32a05,
+	0x80738173, 0x8e738273,
+	0x8e7a8873, 0xbefa00ff,
+	0x01000000, 0xbef60072,
+	0x8072ff72, 0x00000400,
+	0xbefc0084, 0xbf11087c,
+	0x8073ff73, 0x00008000,
+	0xe0524000, 0x721e0000,
+	0xe0524100, 0x721e0100,
+	0xe0524200, 0x721e0200,
+	0xe0524300, 0x721e0300,
+	0xbf8c0f70, 0x7e000300,
+	0x7e020301, 0x7e040302,
+	0x7e060303, 0x807c847c,
+	0x8072ff72, 0x00000400,
+	0xbf0a737c, 0xbf85ffee,
+	0xbf9c0000, 0xe0524000,
+	0x761e0000, 0xe0524100,
+	0x761e0100, 0xe0524200,
+	0x761e0200, 0xe0524300,
+	0x761e0300, 0xb8f22a05,
+	0x80728172, 0x8e728a72,
+	0xb8f61605, 0x80768176,
+	0x8e768676, 0x80727672,
+	0x80f2c072, 0xb8f31605,
+	0x80738173, 0x8e738473,
+	0x8e7a8273, 0xbefa00ff,
+	0x01000000, 0xbefc0073,
+	0xc031003c, 0x00000072,
+	0x80f2c072, 0xbf8c007f,
+	0x80fc907c, 0xbe802d00,
+	0xbe822d02, 0xbe842d04,
+	0xbe862d06, 0xbe882d08,
+	0xbe8a2d0a, 0xbe8c2d0c,
+	0xbe8e2d0e, 0xbf06807c,
+	0xbf84fff1, 0xb8f22a05,
+	0x80728172, 0x8e728a72,
+	0xb8f61605, 0x80768176,
+	0x8e768676, 0x80727672,
+	0xbefa0084, 0xbefa00ff,
+	0x01000000, 0xc0211cfc,
+	0x00000072, 0x80728472,
+	0xc0211c3c, 0x00000072,
+	0x80728472, 0xc0211c7c,
+	0x00000072, 0x80728472,
+	0xc0211bbc, 0x00000072,
+	0x80728472, 0xc0211bfc,
+	0x00000072, 0x80728472,
+	0xc0211d3c, 0x00000072,
+	0x80728472, 0xc0211d7c,
+	0x00000072, 0x80728472,
+	0xc0211a3c, 0x00000072,
+	0x80728472, 0xc0211a7c,
+	0x00000072, 0x80728472,
+	0xc0211dfc, 0x00000072,
+	0x80728472, 0xc0211b3c,
+	0x00000072, 0x80728472,
+	0xc0211b7c, 0x00000072,
+	0x80728472, 0xbf8c007f,
+	0x8671ff71, 0x0000ffff,
+	0xbefc0073, 0xbefe006e,
+	0xbeff006f, 0x867375ff,
+	0x000003ff, 0xb9734803,
+	0x867375ff, 0xfffff800,
+	0x8f738b73, 0xb973a2c3,
+	0xb977f801, 0x8673ff71,
+	0xf0000000, 0x8f739c73,
+	0x8e739073, 0xbef60080,
+	0x87767376, 0x8673ff71,
+	0x08000000, 0x8f739b73,
+	0x8e738f73, 0x87767376,
+	0x8673ff74, 0x00800000,
+	0x8f739773, 0xb976f807,
+	0x86fe7e7e, 0x86ea6a6a,
+	0xb974f802, 0xbf8a0000,
+	0x95807370, 0xbf810000,
+};
+
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
index 01c8b19..b2e8c4f 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
@@ -25,7 +25,7 @@
 #include <linux/err.h>
 #include <linux/fs.h>
 #include <linux/sched.h>
-#include <linux/sched/mm.h>
+#include <linux/mm.h>
 #include <linux/slab.h>
 #include <linux/uaccess.h>
 #include <linux/compat.h>
@@ -1696,7 +1696,7 @@ static int kfd_ioctl_cross_memory_copy(struct file *filep,
 	struct task_struct *remote_task;
 	struct mm_struct *remote_mm;
 	struct pid *remote_pid;
-	struct dma_fence *fence = NULL, *lfence = NULL;
+	struct fence *fence = NULL, *lfence = NULL;
 	uint64_t dst_va_addr;
 	uint64_t copied, total_copied = 0;
 	uint64_t src_offset, dst_offset, dst_va_addr_end;
@@ -1866,7 +1866,7 @@ static int kfd_ioctl_cross_memory_copy(struct file *filep,
 
 			/* Later fence available. Release old fence */
 			if (fence && lfence) {
-				dma_fence_put(lfence);
+				fence_put(lfence);
 				lfence = NULL;
 			}
 
@@ -1914,14 +1914,14 @@ static int kfd_ioctl_cross_memory_copy(struct file *filep,
 
 	/* Wait for the last fence irrespective of error condition */
 	if (fence) {
-		if (dma_fence_wait_timeout(fence, false, msecs_to_jiffies(1000))
+		if (fence_wait_timeout(fence, false, msecs_to_jiffies(1000))
 			< 0)
 			pr_err("CMA %s failed. BO timed out\n", cma_op);
-		dma_fence_put(fence);
+		fence_put(fence);
 	} else if (lfence) {
 		pr_debug("GPU copy fail. But wait for prev DMA to finish\n");
-		dma_fence_wait_timeout(lfence, true, msecs_to_jiffies(1000));
-		dma_fence_put(lfence);
+		fence_wait_timeout(lfence, true, msecs_to_jiffies(1000));
+		fence_put(lfence);
 	}
 
 kfd_process_fail:
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device.c b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
index b489101..0890647 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
@@ -23,7 +23,7 @@
 #include <linux/bsearch.h>
 #include <linux/pci.h>
 #include <linux/slab.h>
-#include <linux/dma-fence.h>
+#include <linux/fence.h>
 #include "kfd_priv.h"
 #include "kfd_device_queue_manager.h"
 #include "kfd_pm4_headers_vi.h"
@@ -366,6 +366,7 @@ struct kfd_dev *kgd2kfd_probe(struct kgd_dev *kgd,
 		 * 32 and 64-bit requests are possible and must be
 		 * supported.
 		 */
+#if 0
 		if (pci_enable_atomic_ops_to_root(pdev,
 				PCI_EXP_DEVCAP2_ATOMIC_COMP32 |
 				PCI_EXP_DEVCAP2_ATOMIC_COMP64) < 0) {
@@ -374,6 +375,14 @@ struct kfd_dev *kgd2kfd_probe(struct kgd_dev *kgd,
 				 pdev->vendor, pdev->device);
 			return NULL;
 		}
+#else
+                if (pci_enable_atomic_ops_to_root(pdev) < 0) {
+                        dev_info(kfd_device,
+                                "skipped device %x:%x, PCI rejects atomics",
+                                 pdev->vendor, pdev->device);
+                        return NULL;
+                }
+#endif
 	}
 
 	kfd = kzalloc(sizeof(*kfd), GFP_KERNEL);
@@ -704,7 +713,7 @@ int kgd2kfd_resume_mm(struct kfd_dev *kfd, struct mm_struct *mm)
  *
  */
 int kgd2kfd_schedule_evict_and_restore_process(struct mm_struct *mm,
-                                              struct dma_fence *fence)
+						struct fence *fence)
 {
        struct kfd_process *p;
        unsigned long active_time;
@@ -713,7 +722,7 @@ int kgd2kfd_schedule_evict_and_restore_process(struct mm_struct *mm,
        if (!fence)
                return -EINVAL;
 
-       if (dma_fence_is_signaled(fence))
+	if (fence_is_signaled(fence))
                return 0;
 
        p = kfd_lookup_process_by_mm(mm);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_events.c b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
index a92ca78..2fc5f0d 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_events.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
@@ -23,8 +23,8 @@
 #include <linux/mm_types.h>
 #include <linux/slab.h>
 #include <linux/types.h>
-#include <linux/sched/signal.h>
-#include <linux/sched/mm.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
 #include <linux/uaccess.h>
 #include <linux/mman.h>
 #include <linux/memory.h>
@@ -37,7 +37,7 @@
  * Wrapper around wait_queue_entry_t
  */
 struct kfd_event_waiter {
-	wait_queue_entry_t wait;
+	wait_queue_t wait;
 	struct kfd_event *event; /* Event to wait for */
 	bool activated;		 /* Becomes true when event is signaled */
 };
@@ -253,7 +253,7 @@ static void destroy_event(struct kfd_process *p, struct kfd_event *ev)
 	struct kfd_event_waiter *waiter;
 
 	/* Wake up pending waiters. They will return failure */
-	list_for_each_entry(waiter, &ev->wq.head, wait.entry)
+	list_for_each_entry(waiter, &ev->wq.task_list, wait.task_list)
 		waiter->event = NULL;
 	wake_up_all(&ev->wq);
 
@@ -393,7 +393,7 @@ static void set_event(struct kfd_event *ev)
 	/* Auto reset if the list is non-empty and we're waking someone. */
 	ev->signaled = !ev->auto_reset || !waitqueue_active(&ev->wq);
 
-	list_for_each_entry(waiter, &ev->wq.head, wait.entry)
+	list_for_each_entry(waiter, &ev->wq.task_list, wait.task_list)
 		waiter->activated = true;
 
 	wake_up_all(&ev->wq);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_priv.h b/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
index 4e31f20..614ff59 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
@@ -593,7 +593,7 @@ struct qcm_process_device {
 #define PROCESS_ACTIVE_TIME_MS 10
 
 int kgd2kfd_schedule_evict_and_restore_process(struct mm_struct *mm,
-					       struct dma_fence *fence);
+					       struct fence *fence);
 int kfd_process_evict_queues(struct kfd_process *p);
 int kfd_process_restore_queues(struct kfd_process *p);
 
@@ -715,7 +715,7 @@ struct kfd_process {
 	size_t signal_event_count;
 	bool signal_event_limit_reached;
 
-	struct rb_root_cached bo_interval_tree;
+	struct rb_root bo_interval_tree;
 
 	/* Information used for memory eviction */
 	void *process_info;
@@ -723,7 +723,7 @@ struct kfd_process {
 	 * fence will be triggered during eviction and new one will be created
 	 * during restore
 	 */
-	struct dma_fence *ef;
+	struct fence *ef;
 
 	/* Work items for evicting and restoring BOs */
 	struct delayed_work eviction_work;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process.c b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
index c627b63..3156dba 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_process.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
@@ -23,8 +23,7 @@
 #include <linux/mutex.h>
 #include <linux/log2.h>
 #include <linux/sched.h>
-#include <linux/sched/mm.h>
-#include <linux/sched/task.h>
+#include <linux/mm.h>
 #include <linux/slab.h>
 #include <linux/amd-iommu.h>
 #include <linux/notifier.h>
@@ -374,7 +373,7 @@ static void kfd_process_wq_release(struct work_struct *work)
 	kfd_process_free_outstanding_kfd_bos(p);
 
 	kfd_process_destroy_pdds(p);
-	dma_fence_put(p->ef);
+	fence_put(p->ef);
 
 	kfd_event_free_process(p);
 
@@ -540,7 +539,7 @@ static struct kfd_process *create_process(const struct task_struct *thread,
 	if (!process)
 		goto err_alloc_process;
 
-	process->bo_interval_tree = RB_ROOT_CACHED;
+	process->bo_interval_tree = RB_ROOT;
 
 	process->pasid = kfd_pasid_alloc();
 	if (process->pasid == 0)
@@ -950,8 +949,8 @@ void kfd_suspend_all_processes(void)
 
 		if (kfd_process_evict_queues(p))
 			pr_err("Failed to suspend process %d\n", p->pasid);
-		dma_fence_signal(p->ef);
-		dma_fence_put(p->ef);
+		fence_signal(p->ef);
+		fence_put(p->ef);
 		p->ef = NULL;
 	}
 	srcu_read_unlock(&kfd_processes_srcu, idx);
@@ -1076,8 +1075,8 @@ static void evict_process_worker(struct work_struct *work)
 	pr_info("Started evicting process of pasid %d\n", p->pasid);
 	ret = kfd_process_evict_queues(p);
 	if (!ret) {
-		dma_fence_signal(p->ef);
-		dma_fence_put(p->ef);
+		fence_signal(p->ef);
+		fence_put(p->ef);
 		p->ef = NULL;
 		queue_delayed_work(kfd_restore_wq, &p->restore_work,
 				msecs_to_jiffies(PROCESS_RESTORE_TIME_MS));
diff --git a/drivers/gpu/drm/amd/include/kgd_kfd_interface.h b/drivers/gpu/drm/amd/include/kgd_kfd_interface.h
index cf921ab..3a5ad2a 100644
--- a/drivers/gpu/drm/amd/include/kgd_kfd_interface.h
+++ b/drivers/gpu/drm/amd/include/kgd_kfd_interface.h
@@ -145,6 +145,7 @@ struct kgd2kfd_shared_resources {
 
 	/* GPUVM address space size in bytes */
 	uint64_t gpuvm_size;
+	int drm_render_minor;
 };
 
 struct tile_config {
@@ -202,8 +203,6 @@ struct tile_config {
  * @set_pasid_vmid_mapping: Exposes pasid/vmid pair to the H/W for no cp
  * scheduling mode. Only used for no cp scheduling mode.
  *
- * @init_pipeline: Initialized the compute pipelines.
- *
  * @hqd_load: Loads the mqd structure to a H/W hqd slot. used only for no cp
  * sceduling mode.
  *
@@ -231,9 +230,6 @@ struct tile_config {
  *
  * @get_fw_version: Returns FW versions from the header
  *
- * @set_num_of_requests: Sets number of Peripheral Page Request (PPR) sent to
- * IOMMU when address translation failed
- *
  * @get_cu_info: Retrieves activated cu info
  *
  * @get_dmabuf_info: Returns information about a dmabuf if it was
@@ -253,10 +249,6 @@ struct tile_config {
  *
  * @get_vram_usage: Returns current VRAM usage
  *
- * @set_scratch_backing_va: Sets VA for scratch backing memory of a VMID.
- * Only used for no cp scheduling mode
- *
- *
  * @gpu_recover: let kgd reset gpu after kfd detect CPC hang
  *
  * This structure contains function pointers to services that the kgd driver
@@ -277,7 +269,10 @@ struct kfd2kgd_calls {
 	uint32_t (*get_max_engine_clock_in_mhz)(struct kgd_dev *kgd);
 
 	int (*create_process_vm)(struct kgd_dev *kgd, void **vm,
-				 void **process_info);
+				void **process_info, struct fence **ef);
+	int (*acquire_process_vm)(struct kgd_dev *kgd, struct file *filp,
+				void **vm, void **process_info,
+				struct fence **ef);
 	void (*destroy_process_vm)(struct kgd_dev *kgd, void *vm);
 
 	int (*create_process_gpumem)(struct kgd_dev *kgd, uint64_t va, size_t size, void *vm, struct kgd_mem **mem);
@@ -285,10 +280,8 @@ struct kfd2kgd_calls {
 
 	uint32_t (*get_process_page_dir)(void *vm);
 
-	int (*open_graphic_handle)(struct kgd_dev *kgd, uint64_t va, void *vm, int fd, uint32_t handle, struct kgd_mem **mem);
-
-        int (*alloc_pasid)(unsigned int bits);
-        void (*free_pasid)(unsigned int pasid);
+	int (*alloc_pasid)(unsigned int bits);
+	void (*free_pasid)(unsigned int pasid);
 
 	/* Register access functions */
 	void (*program_sh_mem_settings)(struct kgd_dev *kgd, uint32_t vmid,
@@ -298,9 +291,6 @@ struct kfd2kgd_calls {
 	int (*set_pasid_vmid_mapping)(struct kgd_dev *kgd, unsigned int pasid,
 					unsigned int vmid);
 
-	int (*init_pipeline)(struct kgd_dev *kgd, uint32_t pipe_id,
-				uint32_t hpd_size, uint64_t hpd_gpu_addr);
-
 	int (*init_interrupts)(struct kgd_dev *kgd, uint32_t pipe_id);
 	
 
@@ -351,10 +341,9 @@ struct kfd2kgd_calls {
 					struct kgd_dev *kgd,
 					uint8_t vmid);
 	uint32_t (*read_vmid_from_vmfault_reg)(struct kgd_dev *kgd);
-	void (*write_vmid_invalidate_request)(struct kgd_dev *kgd,
-					uint8_t vmid);
 
 	int (*invalidate_tlbs)(struct kgd_dev *kgd, uint16_t pasid);
+	int (*invalidate_tlbs_vmid)(struct kgd_dev *kgd, uint16_t vmid);
 
 	int (*sync_memory)(struct kgd_dev *kgd, struct kgd_mem *mem, bool intr);
 
@@ -362,8 +351,7 @@ struct kfd2kgd_calls {
 			uint64_t size, void *vm,
 			struct kgd_mem **mem, uint64_t *offset,
 			uint32_t flags);
-	int (*free_memory_of_gpu)(struct kgd_dev *kgd, struct kgd_mem *mem,
-			void *vm);
+	int (*free_memory_of_gpu)(struct kgd_dev *kgd, struct kgd_mem *mem);
 	int (*map_memory_to_gpu)(struct kgd_dev *kgd, struct kgd_mem *mem,
 			void *vm);
 	int (*unmap_memory_to_gpu)(struct kgd_dev *kgd, struct kgd_mem *mem,
@@ -372,18 +360,13 @@ struct kfd2kgd_calls {
 	uint16_t (*get_fw_version)(struct kgd_dev *kgd,
 				enum kgd_engine_type type);
 
-        void (*set_scratch_backing_va)(struct kgd_dev *kgd,
-                                uint64_t va, uint32_t vmid);
 
-	void (*set_num_of_requests)(struct kgd_dev *kgd,
-			uint8_t num_of_requests);
 	int (*alloc_memory_of_scratch)(struct kgd_dev *kgd,
 			uint64_t va, uint32_t vmid);
 	int (*write_config_static_mem)(struct kgd_dev *kgd, bool swizzle_enable,
 		uint8_t element_size, uint8_t index_stride, uint8_t mtype);
 	void (*get_cu_info)(struct kgd_dev *kgd,
 			struct kfd_cu_info *cu_info);
-	int (*mmap_bo)(struct kgd_dev *kgd, struct vm_area_struct *vma);
 	int (*map_gtt_bo_to_kernel)(struct kgd_dev *kgd,
 			struct kgd_mem *mem, void **kptr);
 	void (*set_vm_context_page_table_base)(struct kgd_dev *kgd, uint32_t vmid,
@@ -412,12 +395,11 @@ struct kfd2kgd_calls {
 			uint32_t *ib_cmd, uint32_t ib_len);
 	int (*get_tile_config)(struct kgd_dev *kgd,
 			struct tile_config *config);
-
-	int (*restore_process_bos)(void *process_info);
+	int (*restore_process_bos)(void *process_info, struct fence **ef);
 	int (*copy_mem_to_mem)(struct kgd_dev *kgd, struct kgd_mem *src_mem,
 			uint64_t src_offset, struct kgd_mem *dst_mem,
-			uint64_t dest_offset, uint64_t size, struct fence **f,
-			uint64_t *actual_size);
+			uint64_t dest_offset, uint64_t size,
+			struct fence **f, uint64_t *actual_size);
 	uint64_t (*get_vram_usage)(struct kgd_dev *kgd);
 
 	void (*gpu_recover)(struct kgd_dev *kgd);
diff --git a/drivers/gpu/drm/amd/powerplay/hwmgr/cz_clockpowergating.c b/drivers/gpu/drm/amd/powerplay/hwmgr/cz_clockpowergating.c
index 44de087..416abeb 100644
--- a/drivers/gpu/drm/amd/powerplay/hwmgr/cz_clockpowergating.c
+++ b/drivers/gpu/drm/amd/powerplay/hwmgr/cz_clockpowergating.c
@@ -166,10 +166,10 @@ void cz_dpm_powergate_uvd(struct pp_hwmgr *hwmgr, bool bgate)
 		cz_dpm_powerup_uvd(hwmgr);
 		cgs_set_clockgating_state(hwmgr->device,
 						AMD_IP_BLOCK_TYPE_UVD,
-						AMD_PG_STATE_UNGATE);
+						AMD_CG_STATE_UNGATE);
 		cgs_set_powergating_state(hwmgr->device,
 						AMD_IP_BLOCK_TYPE_UVD,
-						AMD_CG_STATE_UNGATE);
+						AMD_PG_STATE_UNGATE);
 		cz_dpm_update_uvd_dpm(hwmgr, false);
 	}
 
@@ -197,11 +197,11 @@ void cz_dpm_powergate_vce(struct pp_hwmgr *hwmgr, bool bgate)
 		cgs_set_clockgating_state(
 					hwmgr->device,
 					AMD_IP_BLOCK_TYPE_VCE,
-					AMD_PG_STATE_UNGATE);
+					AMD_CG_STATE_UNGATE);
 		cgs_set_powergating_state(
 					hwmgr->device,
 					AMD_IP_BLOCK_TYPE_VCE,
-					AMD_CG_STATE_UNGATE);
+					AMD_PG_STATE_UNGATE);
 		cz_dpm_update_vce_dpm(hwmgr);
 		cz_enable_disable_vce_dpm(hwmgr, true);
 	}
diff --git a/include/uapi/drm/amdgpu_drm.h b/include/uapi/drm/amdgpu_drm.h
index 71ab384..195dffe 100755
--- a/include/uapi/drm/amdgpu_drm.h
+++ b/include/uapi/drm/amdgpu_drm.h
@@ -110,7 +110,7 @@ extern "C" {
 /* Flag that BO sharing will be explicitly synchronized */
 #define AMDGPU_GEM_CREATE_EXPLICIT_SYNC                (1 << 7)
 
-/* hybrid
+/* hybrid */
 /* Flag that the memory should be in SPARSE resource */
 #define AMDGPU_GEM_CREATE_SPARSE		(1ULL << 29)
 /* Flag that the memory allocation should be from top of domain */
diff --git a/include/uapi/linux/kfd_ioctl.h b/include/uapi/linux/kfd_ioctl.h
index 2fed63e..b0226ea 100644
--- a/include/uapi/linux/kfd_ioctl.h
+++ b/include/uapi/linux/kfd_ioctl.h
@@ -208,14 +208,7 @@ struct kfd_ioctl_dbg_wave_control_args {
 #define KFD_IOC_WAIT_RESULT_TIMEOUT	1
 #define KFD_IOC_WAIT_RESULT_FAIL	2
 
-/*
- * The added 512 is because, currently, 8*(4096/256) signal events are
- * reserved for debugger events, and we want to provide at least 4K signal
- * events for EOP usage.
- * We add 512 to make the allocated size (KFD_SIGNAL_EVENT_LIMIT * 8) be
- * page aligned.
- */
-#define KFD_SIGNAL_EVENT_LIMIT		(4096 + 512)
+#define KFD_SIGNAL_EVENT_LIMIT		4096
 
 struct kfd_ioctl_create_event_args {
 	uint64_t event_page_offset;	/* from KFD */
@@ -285,6 +278,11 @@ struct kfd_ioctl_alloc_memory_of_scratch_args {
 	uint32_t pad;
 };
 
+struct kfd_ioctl_acquire_vm_args {
+	uint32_t drm_fd;	/* to KFD */
+	uint32_t gpu_id;	/* to KFD */
+};
+
 /* Allocation flags: memory types */
 #define KFD_IOC_ALLOC_MEM_FLAGS_VRAM		(1 << 0)
 #define KFD_IOC_ALLOC_MEM_FLAGS_GTT		(1 << 1)
@@ -368,22 +366,22 @@ struct kfd_ioctl_ipc_import_handle_args {
 
 struct kfd_ioctl_get_tile_config_args {
 	/* to KFD: pointer to tile array */
-	uint64_t tile_config_ptr;
+	__u64 tile_config_ptr;
 	/* to KFD: pointer to macro tile array */
-	uint64_t macro_tile_config_ptr;
+	__u64 macro_tile_config_ptr;
 	/* to KFD: array size allocated by user mode
 	 * from KFD: array size filled by kernel
 	 */
-	uint32_t num_tile_configs;
+	__u32 num_tile_configs;
 	/* to KFD: array size allocated by user mode
 	 * from KFD: array size filled by kernel
 	 */
-	uint32_t num_macro_tile_configs;
+	__u32 num_macro_tile_configs;
 
-	uint32_t gpu_id;		/* to KFD */
-	uint32_t gb_addr_config;	/* from KFD */
-	uint32_t num_banks;		/* from KFD */
-	uint32_t num_ranks;		/* from KFD */
+	__u32 gpu_id;		/* to KFD */
+	__u32 gb_addr_config;	/* from KFD */
+	__u32 num_banks;		/* from KFD */
+	__u32 num_ranks;		/* from KFD */
 	/* struct size can be extended later if needed
 	 * without breaking ABI compatibility
 	 */
@@ -523,7 +521,10 @@ struct kfd_ioctl_cross_memory_copy_args {
 #define AMDKFD_IOC_GET_QUEUE_WAVE_STATE		\
 		AMDKFD_IOWR(0x20, struct kfd_ioctl_get_queue_wave_state_args)
 
+#define AMDKFD_IOC_ACQUIRE_VM			\
+		AMDKFD_IOW(0x21, struct kfd_ioctl_acquire_vm_args)
+
 #define AMDKFD_COMMAND_START		0x01
-#define AMDKFD_COMMAND_END		0x21
+#define AMDKFD_COMMAND_END		0x22
 
 #endif
-- 
2.7.4

