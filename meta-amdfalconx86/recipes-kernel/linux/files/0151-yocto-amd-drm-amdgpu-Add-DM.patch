From 29dc30048bc15b7fd5d6773b67c10589993dc0ff Mon Sep 17 00:00:00 2001
From: Harry Wentland <harry.wentland@amd.com>
Date: Tue, 9 Jun 2015 11:10:37 -0400
Subject: [PATCH 268/401] drm/amdgpu: Add DM

Signed-off-by: Harry Wentland <harry.wentland@amd.com>
Acked-by: Tim Writer <Tim.Writer@amd.com>
Signed-off-by: Sanjay R Mehta <sanju.mehta@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/Kconfig               |    3 +
 drivers/gpu/drm/amd/amdgpu/Makefile              |   15 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu.h              |    5 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_dal_services.c |  384 ++++++
 drivers/gpu/drm/amd/amdgpu/amdgpu_dal_types.h    |   46 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_dm.c           | 1091 +++++++++++++++
 drivers/gpu/drm/amd/amdgpu/amdgpu_dm.h           |  124 ++
 drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.c       |  442 ++++++
 drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.h       |  104 ++
 drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.c     | 1595 ++++++++++++++++++++++
 drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.h     |   74 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c          |   24 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c          |    6 +
 drivers/gpu/drm/amd/amdgpu/vi.c                  |    7 +-
 14 files changed, 3916 insertions(+), 4 deletions(-)
 create mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_dal_services.c
 create mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_dal_types.h
 create mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_dm.c
 create mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_dm.h
 create mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.c
 create mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.h
 create mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.c
 create mode 100644 drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.h

diff --git a/drivers/gpu/drm/amd/amdgpu/Kconfig b/drivers/gpu/drm/amd/amdgpu/Kconfig
index b30fcfa..3a37e37 100644
--- a/drivers/gpu/drm/amd/amdgpu/Kconfig
+++ b/drivers/gpu/drm/amd/amdgpu/Kconfig
@@ -15,3 +15,6 @@ config DRM_AMDGPU_USERPTR
 	help
 	  This option selects CONFIG_MMU_NOTIFIER if it isn't already
 	  selected to enabled full userptr support.
+
+source "drivers/gpu/drm/amd/dal/Kconfig"
+
diff --git a/drivers/gpu/drm/amd/amdgpu/Makefile b/drivers/gpu/drm/amd/amdgpu/Makefile
index ddf4549..075e4ce 100644
--- a/drivers/gpu/drm/amd/amdgpu/Makefile
+++ b/drivers/gpu/drm/amd/amdgpu/Makefile
@@ -5,8 +5,10 @@
 ccflags-y := -Iinclude/drm -Idrivers/gpu/drm/amd/include/asic_reg \
 		-Idrivers/gpu/drm/amd/include \
 		-Idrivers/gpu/drm/amd/include/bus \
-		-Idrivers/gpu/drm/amd/acp/include
-
+		-Idrivers/gpu/drm/amd/acp/include \
+		-Idrivers/gpu/drm/amd/amdgpu \
+		-Idrivers/gpu/drm/amd/dal \
+		-Idrivers/gpu/drm/amd/dal/include
 amdgpu-y := amdgpu_drv.o
 
 # add KMS driver
@@ -91,6 +93,15 @@ amdgpu-$(CONFIG_VGA_SWITCHEROO) += amdgpu_atpx_handler.o
 amdgpu-$(CONFIG_ACPI) += amdgpu_acpi.o
 amdgpu-$(CONFIG_MMU_NOTIFIER) += amdgpu_mn.o
 
+ifneq ($(CONFIG_DRM_AMD_DAL),)
+amdgpu-y += amdgpu_dal_services.o amdgpu_dm_types.o amdgpu_dm.o \
+	amdgpu_dm_irq.o
+
+include drivers/gpu/drm/amd/dal/Makefile
+
+amdgpu-y += $(AMD_DAL_FILES)
+endif
+
 obj-$(CONFIG_DRM_AMDGPU)+= amdgpu.o
 
 CFLAGS_amdgpu_trace_points.o := -I$(src)
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu.h b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
index ced4398..4c80744 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
@@ -54,6 +54,7 @@
 #include "amdgpu_ucode.h"
 #include "amdgpu_gds.h"
 #include "amdgpu_acp.h"
+#include "amdgpu_dm.h"
 
 /*
  * Modules parameters.
@@ -1968,6 +1969,7 @@ struct amdgpu_device {
 
 	/* display */
 	struct amdgpu_mode_info		mode_info;
+	/* For pre-DCE11. DCE11 and later are in "struct amdgpu_device->dm" */
 	struct work_struct		hotplug_work;
 	struct amdgpu_irq_src		crtc_irq;
 	struct amdgpu_irq_src		pageflip_irq;
@@ -2014,6 +2016,9 @@ struct amdgpu_device {
 	/* GDS */
 	struct amdgpu_gds		gds;
 
+	/* display related functionality */
+	struct amdgpu_display_manager dm;
+
 	const struct amdgpu_ip_block_version *ip_blocks;
 	int				num_ip_blocks;
 	bool				*ip_block_enabled;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dal_services.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_dal_services.c
new file mode 100644
index 0000000..e404521
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dal_services.c
@@ -0,0 +1,384 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+/*
+ * TODO review this function and organize it to align with Brick design
+ */
+
+#include <linux/string.h>
+#include <linux/acpi.h>
+
+#include <drm/drmP.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/amdgpu_drm.h>
+
+#include "amdgpu.h"
+#include "dal_services.h"
+#include "atom.h"
+#include "bios_parser_interface.h"
+#include "amdgpu_dm.h"
+#include "amdgpu_dm_irq.h"
+#include "include/dal_interface.h"
+
+/*
+#include "logger_interface.h"
+#include "acpimethod_atif.h"
+#include "amdgpu_powerplay.h"
+#include "amdgpu_notifications.h"
+*/
+
+uint32_t dal_read_reg(
+	struct dal_context *ctx,
+	uint32_t address)
+{
+	return amdgpu_mm_rreg(ctx->driver_context, address, false);
+}
+
+
+void dal_write_reg(
+	struct dal_context *ctx,
+	uint32_t address,
+	uint32_t value)
+{
+	amdgpu_mm_wreg(ctx->driver_context, address, value, false);
+}
+
+/* if the pointer is not NULL, the allocated memory is zeroed */
+void *dal_alloc(uint32_t size)
+{
+	return kzalloc(size, GFP_KERNEL);
+}
+
+/* Reallocate memory. The contents will remain unchanged.*/
+void *dal_realloc(const void *ptr, uint32_t size)
+{
+	return krealloc(ptr, size, GFP_KERNEL);
+}
+
+void dal_memmove(void *dst, const void *src, uint32_t size)
+{
+	memmove(dst, src, size);
+}
+
+void dal_free(void *p)
+{
+	kfree(p);
+}
+
+void dal_memset(void *p, int32_t c, uint32_t count)
+{
+	memset(p, c, count);
+}
+
+int32_t dal_memcmp(const void *p1, const void *p2, uint32_t count)
+{
+	return memcmp(p1, p2, count);
+}
+
+int32_t dal_strcmp(const int8_t *p1, const int8_t *p2)
+{
+	return strcmp(p1, p2);
+}
+int32_t dal_strncmp(const int8_t *p1, const int8_t *p2, uint32_t count)
+{
+	return strncmp(p1, p2, count);
+}
+
+void dal_sleep_in_milliseconds(uint32_t milliseconds)
+{
+	if (milliseconds >= 20)
+		msleep(milliseconds);
+	else
+		usleep_range(milliseconds*1000, milliseconds*1000+1);
+}
+
+void dal_delay_in_nanoseconds(uint32_t nanoseconds)
+{
+	ndelay(nanoseconds);
+}
+
+void dal_delay_in_microseconds(uint32_t microseconds)
+{
+	udelay(microseconds);
+}
+
+uint32_t dal_bios_cmd_table_para_revision(
+	struct dal_context *ctx,
+	uint32_t index)
+{
+	uint8_t frev;
+	uint8_t crev;
+
+	if (!amdgpu_atom_parse_cmd_header(
+		((struct amdgpu_device *)ctx->driver_context)->
+		mode_info.atom_context,
+		index,
+		&frev,
+		&crev))
+		return 0;
+
+	return crev;
+}
+
+bool dal_bios_cmd_table_revision(
+	struct dal_context *ctx,
+	uint32_t index,
+	struct cmd_table_revision *table_revision)
+{
+	uint8_t frev;
+	uint8_t crev;
+
+	if (!table_revision ||
+		!amdgpu_atom_parse_cmd_header(
+			((struct amdgpu_device *)ctx->driver_context)->
+			mode_info.atom_context,
+			index,
+			&frev,
+			&crev))
+		return false;
+
+	table_revision->revision = crev;
+	table_revision->content_revision = 0;
+
+	return true;
+}
+
+/******************************************************************************
+ * IRQ Interfaces.
+ *****************************************************************************/
+
+void dal_register_timer_interrupt(
+	struct dal_context *context,
+	struct dal_timer_interrupt_params *int_params,
+	interrupt_handler ih,
+	void *args)
+{
+	struct amdgpu_device *adev = context->driver_context;
+
+	if (!adev || !int_params) {
+		DRM_ERROR("DM_IRQ: invalid input!\n");
+		return;
+	}
+
+	if (int_params->int_context != INTERRUPT_LOW_IRQ_CONTEXT) {
+		/* only low irq ctx is supported. */
+		DRM_ERROR("DM_IRQ: invalid context: %d!\n",
+				int_params->int_context);
+		return;
+	}
+
+	amdgpu_dm_irq_register_timer(adev, int_params, ih, args);
+}
+
+irq_handler_idx dal_register_interrupt(
+	struct dal_context *context,
+	struct dal_interrupt_params *int_params,
+	interrupt_handler ih,
+	void *handler_args)
+{
+	struct amdgpu_device *adev = context->driver_context;
+
+	if (NULL == int_params || NULL == ih) {
+		DRM_ERROR("DM_IRQ: invalid input!\n");
+		return DAL_INVALID_IRQ_HANDLER_IDX;
+	}
+
+	if (int_params->int_context >= INTERRUPT_CONTEXT_NUMBER) {
+		DRM_ERROR("DM_IRQ: invalid context: %d!\n",
+				int_params->int_context);
+		return DAL_INVALID_IRQ_HANDLER_IDX;
+	}
+
+	if (!DAL_VALID_IRQ_SRC_NUM(int_params->irq_source)) {
+		DRM_ERROR("DM_IRQ: invalid irq_source: %d!\n",
+				int_params->irq_source);
+		return DAL_INVALID_IRQ_HANDLER_IDX;
+	}
+
+	return amdgpu_dm_irq_register_interrupt(adev, int_params, ih,
+			handler_args);
+}
+
+void dal_unregister_interrupt(
+	struct dal_context *context,
+	enum dal_irq_source irq_source,
+	irq_handler_idx handler_idx)
+{
+	struct amdgpu_device *adev = context->driver_context;
+
+	if (DAL_INVALID_IRQ_HANDLER_IDX == handler_idx) {
+		DRM_ERROR("DM_IRQ: invalid handler_idx==NULL!\n");
+		return;
+	}
+
+	if (!DAL_VALID_IRQ_SRC_NUM(irq_source)) {
+		DRM_ERROR("DM_IRQ: invalid irq_source:%d!\n", irq_source);
+		return;
+	}
+
+	amdgpu_dm_irq_unregister_interrupt(adev, irq_source, handler_idx);
+}
+
+
+void dal_isr_acquire_lock(struct dal_context *context)
+{
+	/*TODO*/
+}
+
+void dal_isr_release_lock(struct dal_context *context)
+{
+	/*TODO*/
+}
+
+/******************************************************************************
+ * End-of-IRQ Interfaces.
+ *****************************************************************************/
+
+bool dal_exec_bios_cmd_table(
+	struct dal_context *ctx,
+	uint32_t index,
+	void *params)
+{
+	/* TODO */
+	return amdgpu_atom_execute_table(
+		((struct amdgpu_device *)ctx->driver_context)->
+		mode_info.atom_context,
+		index, params) == 0;
+	return false;
+}
+
+bool dal_get_platform_info(struct dal_context *dal_context,
+			struct platform_info_params *params)
+{
+	/*TODO*/
+	return false;
+}
+
+/* Next calls are to power component */
+bool dal_pp_pre_dce_clock_change(struct dal_context *ctx,
+				struct dal_to_power_info *input,
+				struct power_to_dal_info *output)
+{
+	/*TODO*/
+	return false;
+}
+
+bool dal_pp_post_dce_clock_change(struct dal_context *ctx)
+{
+	/*TODO*/
+	return false;
+}
+
+bool dal_get_system_clocks_range(struct dal_context *ctx,
+				struct dal_system_clock_range *sys_clks)
+{
+	/*TODO*/
+	return false;
+}
+
+
+bool dal_pp_set_display_clock(struct dal_context *ctx,
+			     struct dal_to_power_dclk *dclk)
+{
+	/* TODO: need power component to provide appropriate interface */
+	return false;
+}
+
+/* end of calls to power component */
+
+/* Calls to notification */
+
+/* dal_notify_hotplug
+ *
+ * Notify display manager for hotplug event
+ *
+ * @param
+ * struct dal_context *dal_context - [in] pointer to specific DAL context
+ *
+ * @return
+ * void
+ * */
+void dal_notify_hotplug(
+	struct dal_context *ctx,
+	uint32_t display_index,
+	bool is_connected)
+{
+	struct amdgpu_device *adev = ctx->driver_context;
+	struct drm_device *dev = adev->ddev;
+	struct drm_connector *connector = NULL;
+	struct amdgpu_connector *aconnector = NULL;
+
+	/* 1. Update status of drm connectors
+	 * 2. Send a uevent and let userspace tell us what to do */
+	drm_helper_hpd_irq_event(dev);
+
+	list_for_each_entry(connector,
+		&dev->mode_config.connector_list, head) {
+		aconnector = to_amdgpu_connector(connector);
+
+		/*aconnector->connector_id means display_index*/
+		if (aconnector->connector_id == display_index) {
+			if (is_connected)
+				drm_mode_connector_update_edid_property(
+					connector,
+					(struct edid *)
+					dal_get_display_edid(
+						adev->dm.dal,
+						display_index,
+						NULL));
+			else
+				drm_mode_connector_update_edid_property(
+					connector, NULL);
+		}
+	}
+}
+
+void dal_notify_capability_change(
+	struct dal_context *ctx,
+	uint32_t display_index)
+{
+	/*TODO*/
+}
+
+void dal_notify_setmode_complete(struct dal_context *ctx,
+	uint32_t h_total,
+	uint32_t v_total,
+	uint32_t h_active,
+	uint32_t v_active,
+	uint32_t pix_clk_in_khz)
+{
+	/*TODO*/
+}
+/* End of calls to notification */
+
+long dal_get_pid(void)
+{
+	return current->pid;
+}
+
+long dal_get_tgid(void)
+{
+	return current->tgid;
+}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dal_types.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_dal_types.h
new file mode 100644
index 0000000..cf46918
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dal_types.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __AMDGPU_DAL_TYPES_H__
+#define __AMDGPU_DAL_TYPES_H__
+
+#include <asm/byteorder.h>
+#include <linux/types.h>
+#include <drm/drmP.h>
+
+
+/* The same symbols are defined in amdgpu.h, redefined here just in
+ * case DAL need to work with other drivers besides amdgpu. */
+#if defined(__BIG_ENDIAN) && !defined(BIGENDIAN_CPU)
+#define BIGENDIAN_CPU
+#elif defined(__LITTLE_ENDIAN) && !defined(LITTLEENDIAN_CPU)
+#define LITTLEENDIAN_CPU
+#endif
+
+#undef READ
+#undef WRITE
+#undef FRAME_SIZE
+
+#endif
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm.c
new file mode 100644
index 0000000..6fa8564
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm.c
@@ -0,0 +1,1091 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "amdgpu_dal_types.h"
+#include "include/dal_interface.h"
+#include "include/mode_query_interface.h"
+
+#include "vid.h"
+#include "amdgpu.h"
+#include "atom.h"
+#include "amdgpu_dm.h"
+#include "amdgpu_dm_types.h"
+
+#include "amdgpu_family.h"
+#include "amdgpu_dm_irq.h"
+
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+#include "dce/dce_11_0_enum.h"
+
+#include "oss/oss_3_0_d.h"
+#include "oss/oss_3_0_sh_mask.h"
+#include "gmc/gmc_8_1_d.h"
+#include "gmc/gmc_8_1_sh_mask.h"
+
+
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+
+
+/* Define variables here
+ * These values will be passed to DAL for feature enable purpose
+ * Disable ALL for HDMI light up
+ * TODO: follow up if need this mechanism*/
+struct dal_override_parameters display_param = {
+	.bool_param_enable_mask = 0,
+	.bool_param_values = 0,
+	.int_param_values[DAL_PARAM_MAX_COFUNC_NON_DP_DISPLAYS] = DAL_PARAM_INVALID_INT,
+	.int_param_values[DAL_PARAM_DRR_SUPPORT] = DAL_PARAM_INVALID_INT,
+};
+
+/* Debug facilities */
+#define AMDGPU_DM_NOT_IMPL(fmt, ...) \
+	DRM_INFO("DM_NOT_IMPL: " fmt, ##__VA_ARGS__)
+
+static enum dal_irq_source amdgpu_dm_crtc_to_dal_irq_source(unsigned type)
+{
+	switch (type) {
+	case AMDGPU_CRTC_IRQ_VBLANK1:
+		return DAL_IRQ_SOURCE_CRTC1VSYNC;
+	case AMDGPU_CRTC_IRQ_VBLANK2:
+		return DAL_IRQ_SOURCE_CRTC2VSYNC;
+	case AMDGPU_CRTC_IRQ_VBLANK3:
+		return DAL_IRQ_SOURCE_CRTC3VSYNC;
+	case AMDGPU_CRTC_IRQ_VBLANK4:
+		return DAL_IRQ_SOURCE_CRTC4VSYNC;
+	case AMDGPU_CRTC_IRQ_VBLANK5:
+		return DAL_IRQ_SOURCE_CRTC5VSYNC;
+	case AMDGPU_CRTC_IRQ_VBLANK6:
+		return DAL_IRQ_SOURCE_CRTC6VSYNC;
+	default:
+		return DAL_IRQ_SOURCE_INVALID;
+	}
+}
+
+static enum dal_irq_source amdgpu_dm_hpd_to_dal_irq_source(unsigned type)
+{
+	switch (type) {
+	case AMDGPU_HPD_1:
+		return DAL_IRQ_SOURCE_HPD1;
+	case AMDGPU_HPD_2:
+		return DAL_IRQ_SOURCE_HPD2;
+	case AMDGPU_HPD_3:
+		return DAL_IRQ_SOURCE_HPD3;
+	case AMDGPU_HPD_4:
+		return DAL_IRQ_SOURCE_HPD4;
+	case AMDGPU_HPD_5:
+		return DAL_IRQ_SOURCE_HPD5;
+	case AMDGPU_HPD_6:
+		return DAL_IRQ_SOURCE_HPD6;
+	default:
+		return DAL_IRQ_SOURCE_INVALID;
+	}
+}
+
+static u32 dm_vblank_get_counter(struct amdgpu_device *adev, int crtc)
+{
+	if (crtc >= adev->mode_info.num_crtc)
+		return 0;
+	else
+		return dal_get_vblank_counter(adev->dm.dal, crtc);
+}
+
+static int dm_crtc_get_scanoutpos(struct amdgpu_device *adev, int crtc,
+					u32 *vbl, u32 *position)
+{
+	if ((crtc < 0) || (crtc >= adev->mode_info.num_crtc))
+		return -EINVAL;
+
+	dal_get_crtc_scanoutpos(adev->dm.dal, crtc, vbl, position);
+
+	return 0;
+}
+
+
+/**
+ * dce_v11_0_hpd_init - hpd setup callback.
+ *
+ * @adev: amdgpu_device pointer
+ *
+ * Setup the hpd pins used by the card (evergreen+).
+ * Enable the pin, set the polarity, and enable the hpd interrupts.
+ */
+/*TODOOOOOO  */
+static void amdgpu_dm_hpd_init(struct amdgpu_device *adev)
+{
+	struct drm_device *dev = adev->ddev;
+	struct drm_connector *connector;
+
+	list_for_each_entry(connector, &dev->mode_config.connector_list, head) {
+		struct amdgpu_connector *amdgpu_connector =
+				to_amdgpu_connector(connector);
+		enum dal_irq_source src =
+			amdgpu_dm_hpd_to_dal_irq_source(
+				amdgpu_connector->hpd.hpd);
+
+		if (connector->connector_type == DRM_MODE_CONNECTOR_eDP ||
+			connector->connector_type == DRM_MODE_CONNECTOR_LVDS) {
+			/* don't try to enable hpd on eDP or LVDS avoid breaking
+			 * the aux dp channel on imac and help (but not
+			 * completely fix)
+			 * https://bugzilla.redhat.com/show_bug.cgi?id=726143
+			 * also avoid interrupt storms during dpms.
+			 */
+			continue;
+		}
+
+		dal_interrupt_set(adev->dm.dal, src, true);
+		amdgpu_irq_get(adev, &adev->hpd_irq, amdgpu_connector->hpd.hpd);
+	}
+}
+
+/**
+ * dce_v11_0_hpd_fini - hpd tear down callback.
+ *
+ * @adev: amdgpu_device pointer
+ *
+ * Tear down the hpd pins used by the card (evergreen+).
+ * Disable the hpd interrupts.
+ */
+static void amdgpu_dm_hpd_fini(struct amdgpu_device *adev)
+{
+	struct drm_device *dev = adev->ddev;
+	struct drm_connector *connector;
+
+	list_for_each_entry(connector, &dev->mode_config.connector_list, head) {
+		struct amdgpu_connector *amdgpu_connector =
+				to_amdgpu_connector(connector);
+		enum dal_irq_source src =
+			amdgpu_dm_hpd_to_dal_irq_source(
+				amdgpu_connector->hpd.hpd);
+
+		dal_interrupt_set(adev->dm.dal, src, false);
+		amdgpu_irq_put(adev, &adev->hpd_irq, amdgpu_connector->hpd.hpd);
+	}
+}
+
+static u32 dm_hpd_get_gpio_reg(struct amdgpu_device *adev)
+{
+	return mmDC_GPIO_HPD_A;
+}
+
+
+static bool dm_is_display_hung(struct amdgpu_device *adev)
+{
+	u32 crtc_hung = 0;
+	u32 i, j, tmp;
+
+	crtc_hung = dal_get_connected_targets_vector(adev->dm.dal);
+
+	for (j = 0; j < 10; j++) {
+		for (i = 0; i < adev->mode_info.num_crtc; i++) {
+			if (crtc_hung & (1 << i)) {
+				int32_t vpos1, hpos1;
+				int32_t vpos2, hpos2;
+
+				tmp = dal_get_crtc_scanoutpos(
+					adev->dm.dal,
+					i,
+					&vpos1,
+					&hpos1);
+				udelay(10);
+				tmp = dal_get_crtc_scanoutpos(
+					adev->dm.dal,
+					i,
+					&vpos2,
+					&hpos2);
+
+				if (hpos1 != hpos2 && vpos1 != vpos2)
+					crtc_hung &= ~(1 << i);
+			}
+		}
+
+		if (crtc_hung == 0)
+			return false;
+	}
+
+	return true;
+}
+
+static void dm_stop_mc_access(struct amdgpu_device *adev,
+				     struct amdgpu_mode_mc_save *save)
+{
+#if 0
+	u32 crtc_enabled, tmp;
+	int i;
+
+	save->vga_render_control = RREG32(mmVGA_RENDER_CONTROL);
+	save->vga_hdp_control = RREG32(mmVGA_HDP_CONTROL);
+
+	/* disable VGA render */
+	tmp = RREG32(mmVGA_RENDER_CONTROL);
+	tmp = REG_SET_FIELD(tmp, VGA_RENDER_CONTROL, VGA_VSTATUS_CNTL, 0);
+	WREG32(mmVGA_RENDER_CONTROL, tmp);
+
+	/* blank the display controllers */
+	for (i = 0; i < adev->mode_info.num_crtc; i++) {
+		crtc_enabled = REG_GET_FIELD(RREG32(mmCRTC_CONTROL + crtc_offsets[i]),
+					     CRTC_CONTROL, CRTC_MASTER_EN);
+		if (crtc_enabled) {
+#if 0
+			u32 frame_count;
+			int j;
+
+			save->crtc_enabled[i] = true;
+			tmp = RREG32(mmCRTC_BLANK_CONTROL + crtc_offsets[i]);
+			if (REG_GET_FIELD(tmp, CRTC_BLANK_CONTROL, CRTC_BLANK_DATA_EN) == 0) {
+				amdgpu_display_vblank_wait(adev, i);
+				WREG32(mmCRTC_UPDATE_LOCK + crtc_offsets[i], 1);
+				tmp = REG_SET_FIELD(tmp, CRTC_BLANK_CONTROL, CRTC_BLANK_DATA_EN, 1);
+				WREG32(mmCRTC_BLANK_CONTROL + crtc_offsets[i], tmp);
+				WREG32(mmCRTC_UPDATE_LOCK + crtc_offsets[i], 0);
+			}
+			/* wait for the next frame */
+			frame_count = amdgpu_display_vblank_get_counter(adev, i);
+			for (j = 0; j < adev->usec_timeout; j++) {
+				if (amdgpu_display_vblank_get_counter(adev, i) != frame_count)
+					break;
+				udelay(1);
+			}
+			tmp = RREG32(mmGRPH_UPDATE + crtc_offsets[i]);
+			if (REG_GET_FIELD(tmp, GRPH_UPDATE, GRPH_UPDATE_LOCK) == 0) {
+				tmp = REG_SET_FIELD(tmp, GRPH_UPDATE, GRPH_UPDATE_LOCK, 1);
+				WREG32(mmGRPH_UPDATE + crtc_offsets[i], tmp);
+			}
+			tmp = RREG32(mmCRTC_MASTER_UPDATE_LOCK + crtc_offsets[i]);
+			if (REG_GET_FIELD(tmp, CRTC_MASTER_UPDATE_LOCK, MASTER_UPDATE_LOCK) == 0) {
+				tmp = REG_SET_FIELD(tmp, CRTC_MASTER_UPDATE_LOCK, MASTER_UPDATE_LOCK, 1);
+				WREG32(mmCRTC_MASTER_UPDATE_LOCK + crtc_offsets[i], tmp);
+			}
+#else
+			/* XXX this is a hack to avoid strange behavior with EFI on certain systems */
+			WREG32(mmCRTC_UPDATE_LOCK + crtc_offsets[i], 1);
+			tmp = RREG32(mmCRTC_CONTROL + crtc_offsets[i]);
+			tmp = REG_SET_FIELD(tmp, CRTC_CONTROL, CRTC_MASTER_EN, 0);
+			WREG32(mmCRTC_CONTROL + crtc_offsets[i], tmp);
+			WREG32(mmCRTC_UPDATE_LOCK + crtc_offsets[i], 0);
+			save->crtc_enabled[i] = false;
+			/* ***** */
+#endif
+		} else {
+			save->crtc_enabled[i] = false;
+		}
+	}
+#endif
+}
+
+static void dm_resume_mc_access(struct amdgpu_device *adev,
+				       struct amdgpu_mode_mc_save *save)
+{
+#if 0
+	u32 tmp, frame_count;
+	int i, j;
+
+	/* update crtc base addresses */
+	for (i = 0; i < adev->mode_info.num_crtc; i++) {
+		WREG32(mmGRPH_PRIMARY_SURFACE_ADDRESS_HIGH + crtc_offsets[i],
+		       upper_32_bits(adev->mc.vram_start));
+		WREG32(mmGRPH_SECONDARY_SURFACE_ADDRESS_HIGH + crtc_offsets[i],
+		       upper_32_bits(adev->mc.vram_start));
+		WREG32(mmGRPH_PRIMARY_SURFACE_ADDRESS + crtc_offsets[i],
+		       (u32)adev->mc.vram_start);
+		WREG32(mmGRPH_SECONDARY_SURFACE_ADDRESS + crtc_offsets[i],
+		       (u32)adev->mc.vram_start);
+
+		if (save->crtc_enabled[i]) {
+			tmp = RREG32(mmCRTC_MASTER_UPDATE_MODE + crtc_offsets[i]);
+			if (REG_GET_FIELD(tmp, CRTC_MASTER_UPDATE_MODE, MASTER_UPDATE_MODE) != 3) {
+				tmp = REG_SET_FIELD(tmp, CRTC_MASTER_UPDATE_MODE, MASTER_UPDATE_MODE, 3);
+				WREG32(mmCRTC_MASTER_UPDATE_MODE + crtc_offsets[i], tmp);
+			}
+			tmp = RREG32(mmGRPH_UPDATE + crtc_offsets[i]);
+			if (REG_GET_FIELD(tmp, GRPH_UPDATE, GRPH_UPDATE_LOCK)) {
+				tmp = REG_SET_FIELD(tmp, GRPH_UPDATE, GRPH_UPDATE_LOCK, 0);
+				WREG32(mmGRPH_UPDATE + crtc_offsets[i], tmp);
+			}
+			tmp = RREG32(mmCRTC_MASTER_UPDATE_LOCK + crtc_offsets[i]);
+			if (REG_GET_FIELD(tmp, CRTC_MASTER_UPDATE_LOCK, MASTER_UPDATE_LOCK)) {
+				tmp = REG_SET_FIELD(tmp, CRTC_MASTER_UPDATE_LOCK, MASTER_UPDATE_LOCK, 0);
+				WREG32(mmCRTC_MASTER_UPDATE_LOCK + crtc_offsets[i], tmp);
+			}
+			for (j = 0; j < adev->usec_timeout; j++) {
+				tmp = RREG32(mmGRPH_UPDATE + crtc_offsets[i]);
+				if (REG_GET_FIELD(tmp, GRPH_UPDATE, GRPH_SURFACE_UPDATE_PENDING) == 0)
+					break;
+				udelay(1);
+			}
+			tmp = RREG32(mmCRTC_BLANK_CONTROL + crtc_offsets[i]);
+			tmp = REG_SET_FIELD(tmp, CRTC_BLANK_CONTROL, CRTC_BLANK_DATA_EN, 0);
+			WREG32(mmCRTC_UPDATE_LOCK + crtc_offsets[i], 1);
+			WREG32(mmCRTC_BLANK_CONTROL + crtc_offsets[i], tmp);
+			WREG32(mmCRTC_UPDATE_LOCK + crtc_offsets[i], 0);
+			/* wait for the next frame */
+			frame_count = amdgpu_display_vblank_get_counter(adev, i);
+			for (j = 0; j < adev->usec_timeout; j++) {
+				if (amdgpu_display_vblank_get_counter(adev, i) != frame_count)
+					break;
+				udelay(1);
+			}
+		}
+	}
+
+	WREG32(mmVGA_MEMORY_BASE_ADDRESS_HIGH, upper_32_bits(adev->mc.vram_start));
+	WREG32(mmVGA_MEMORY_BASE_ADDRESS, lower_32_bits(adev->mc.vram_start));
+
+	/* Unlock vga access */
+	WREG32(mmVGA_HDP_CONTROL, save->vga_hdp_control);
+	mdelay(1);
+	WREG32(mmVGA_RENDER_CONTROL, save->vga_render_control);
+#endif
+}
+
+static bool dm_is_idle(void *handle)
+{
+	/* XXX todo */
+	return true;
+}
+
+static int dm_wait_for_idle(void *handle)
+{
+	/* XXX todo */
+	return 0;
+}
+
+static void dm_print_status(void *handle)
+{
+	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
+	dev_info(adev->dev, "DCE 10.x registers\n");
+	/* XXX todo */
+}
+
+static int dm_soft_reset(void *handle)
+{
+	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
+	u32 srbm_soft_reset = 0, tmp;
+
+	if (dm_is_display_hung(adev))
+		srbm_soft_reset |= SRBM_SOFT_RESET__SOFT_RESET_DC_MASK;
+
+	if (srbm_soft_reset) {
+		dm_print_status(adev);
+
+		tmp = RREG32(mmSRBM_SOFT_RESET);
+		tmp |= srbm_soft_reset;
+		dev_info(adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
+		WREG32(mmSRBM_SOFT_RESET, tmp);
+		tmp = RREG32(mmSRBM_SOFT_RESET);
+
+		udelay(50);
+
+		tmp &= ~srbm_soft_reset;
+		WREG32(mmSRBM_SOFT_RESET, tmp);
+		tmp = RREG32(mmSRBM_SOFT_RESET);
+
+		/* Wait a little for things to settle down */
+		udelay(50);
+		dm_print_status(adev);
+	}
+	return 0;
+}
+
+static int amdgpu_dm_set_hpd_irq_state(struct amdgpu_device *adev,
+					struct amdgpu_irq_src *source,
+					unsigned type,
+					enum amdgpu_interrupt_state state)
+{
+	enum dal_irq_source src = amdgpu_dm_hpd_to_dal_irq_source(type);
+	bool st = (state == AMDGPU_IRQ_STATE_ENABLE);
+
+	dal_interrupt_set(adev->dm.dal, src, st);
+	return 0;
+}
+
+static int amdgpu_dm_set_pflip_irq_state(struct amdgpu_device *adev,
+					struct amdgpu_irq_src *source,
+					unsigned type,
+					enum amdgpu_interrupt_state state)
+{
+	enum dal_irq_source src = dal_get_pflip_irq_src_from_display_index(
+			adev->dm.dal,
+			type,	/* this is the display_index because passed
+				 * via work->crtc_id*/
+			0	/* plane_no */);
+	bool st = (state == AMDGPU_IRQ_STATE_ENABLE);
+
+	dal_interrupt_set(adev->dm.dal, src, st);
+	return 0;
+}
+
+static int amdgpu_dm_set_crtc_irq_state(struct amdgpu_device *adev,
+					struct amdgpu_irq_src *source,
+					unsigned type,
+					enum amdgpu_interrupt_state state)
+{
+	enum dal_irq_source src = amdgpu_dm_crtc_to_dal_irq_source(type);
+	bool st = (state == AMDGPU_IRQ_STATE_ENABLE);
+
+	dal_interrupt_set(adev->dm.dal, src, st);
+	return 0;
+}
+
+
+static int amdgpu_dm_pflip_irq(struct amdgpu_device *adev,
+				  struct amdgpu_irq_src *source,
+				  struct amdgpu_iv_entry *entry)
+{
+	unsigned long flags;
+	enum dal_irq_source src =
+		dal_interrupt_to_irq_source(
+			adev->dm.dal,
+			entry->src_id,
+			entry->src_data);
+	uint32_t display_index =
+		dal_get_display_index_from_int_src(adev->dm.dal, src);
+	struct amdgpu_crtc *amdgpu_crtc = adev->mode_info.crtcs[display_index];
+	struct amdgpu_flip_work *works;
+
+	dal_interrupt_ack(adev->dm.dal, src);
+
+	/* IRQ could occur when in initial stage */
+	if(amdgpu_crtc == NULL)
+		return 0;
+
+	spin_lock_irqsave(&adev->ddev->event_lock, flags);
+	works = amdgpu_crtc->pflip_works;
+	if (amdgpu_crtc->pflip_status != AMDGPU_FLIP_SUBMITTED){
+		DRM_DEBUG_DRIVER("amdgpu_crtc->pflip_status = %d != "
+						 "AMDGPU_FLIP_SUBMITTED(%d)\n",
+						 amdgpu_crtc->pflip_status,
+						 AMDGPU_FLIP_SUBMITTED);
+		spin_unlock_irqrestore(&adev->ddev->event_lock, flags);
+		return 0;
+	}
+
+	/* page flip completed. clean up */
+	amdgpu_crtc->pflip_status = AMDGPU_FLIP_NONE;
+	amdgpu_crtc->pflip_works = NULL;
+
+	/* wakeup usersapce */
+	if(works->event)
+		drm_send_vblank_event(
+			adev->ddev,
+			amdgpu_crtc->crtc_id,
+			works->event);
+
+	spin_unlock_irqrestore(&adev->ddev->event_lock, flags);
+
+	drm_vblank_put(adev->ddev, amdgpu_crtc->crtc_id);
+	amdgpu_irq_put(adev, &adev->pageflip_irq, amdgpu_crtc->crtc_id);
+	queue_work(amdgpu_crtc->pflip_queue, &works->unpin_work);
+
+	return 0;
+}
+
+
+static int amdgpu_dm_crtc_irq(
+		struct amdgpu_device *adev,
+		struct amdgpu_irq_src *source,
+		struct amdgpu_iv_entry *entry)
+{
+	enum dal_irq_source src =
+		dal_interrupt_to_irq_source(
+			adev->dm.dal,
+			entry->src_id,
+			entry->src_data);
+	uint32_t display_index =
+			dal_get_display_index_from_int_src(adev->dm.dal, src);
+
+	if (src < DAL_IRQ_SOURCE_CRTC1VSYNC || src > DAL_IRQ_SOURCE_CRTC6VSYNC)
+		return -EINVAL;
+
+	dal_interrupt_ack(adev->dm.dal, src);
+	drm_handle_vblank(adev->ddev, display_index);
+
+	return 0;
+}
+
+static int amdgpu_dm_hpd_irq(
+	struct amdgpu_device *adev,
+	struct amdgpu_irq_src *source,
+	struct amdgpu_iv_entry *entry)
+{
+	enum dal_irq_source src =
+		dal_interrupt_to_irq_source(
+			adev->dm.dal,
+			entry->src_id,
+			entry->src_data);
+
+	if (src < DAL_IRQ_SOURCE_HPD1 || src > DAL_IRQ_SOURCE_HPD6)
+		return -EINVAL;
+
+	dal_interrupt_ack(adev->dm.dal, src);
+
+	amdgpu_dm_irq_schedule_work(adev, src);
+
+	return 0;
+}
+
+static int dm_set_clockgating_state(void *handle,
+		  enum amd_clockgating_state state)
+{
+	return 0;
+}
+
+static int dm_set_powergating_state(void *handle,
+		  enum amd_powergating_state state)
+{
+	return 0;
+}
+
+/* Prototypes of private functions */
+static int dm_early_init(void* handle);
+
+
+
+/* Init display KMS
+ *
+ * Returns 0 on success
+ */
+int amdgpu_dm_init(struct amdgpu_device *adev)
+{
+	struct dal_init_data init_data;
+	struct drm_device *ddev = adev->ddev;
+	adev->dm.ddev = adev->ddev;
+	adev->dm.adev = adev;
+
+	/* Zero all the fields */
+	memset(&init_data, 0, sizeof(init_data));
+
+	/* initialize DAL's mutex */
+	/*mutex_init(&adev->dm.dal_mutex);*/
+
+	/* initialize DAL's lock (for SYNC context use) */
+	spin_lock_init(&adev->dm.dal_lock);
+
+	if(amdgpu_dm_irq_init(adev)) {
+		DRM_ERROR("amdgpu: failed to initialize DM IRQ support.\n");
+		goto error;
+	}
+
+	if (ddev->pdev) {
+		init_data.bdf_info.DEVICE_NUMBER = PCI_SLOT(ddev->pdev->devfn);
+		init_data.bdf_info.FUNCTION_NUMBER =
+			PCI_FUNC(ddev->pdev->devfn);
+		if (ddev->pdev->bus)
+			init_data.bdf_info.BUS_NUMBER = ddev->pdev->bus->number;
+	}
+
+	init_data.display_param = display_param;
+
+	init_data.asic_id.chip_family = adev->family;
+
+	init_data.asic_id.chip_id = adev->rev_id;
+	init_data.asic_id.hw_internal_rev = adev->external_rev_id;
+
+	init_data.asic_id.vram_width = adev->mc.vram_width;
+	/* TODO: initialize init_data.asic_id.vram_type here!!!! */
+	init_data.asic_id.atombios_base_address =
+		adev->mode_info.atom_context->bios;
+	init_data.asic_id.runtime_flags.bits.SKIP_POWER_DOWN_ON_RESUME = 1;
+
+	if ((adev->family == CHIP_CARRIZO))
+		init_data.asic_id.runtime_flags.bits.GNB_WAKEUP_SUPPORTED = 1;
+
+	init_data.driver = adev;
+
+	adev->dm.dal = NULL;
+
+	/* enable gpu scaling in DAL */
+	init_data.display_param.bool_param_enable_mask |=
+		1 << DAL_PARAM_ENABLE_GPU_SCALING;
+	init_data.display_param.bool_param_values |=
+		1 << DAL_PARAM_ENABLE_GPU_SCALING;
+
+	/*TODO: check if needed  Initialize display related interrupts */
+	/*if (amdgpu_dm_irq_init(adev)) {
+		DRM_ERROR(
+		"amdgpu: failed to initialize irq for display support.\n");
+		return -1;
+	}*/
+
+	adev->dm.dal = dal_create(&init_data);
+
+	if (!adev->dm.dal) {
+		DRM_ERROR(
+		"amdgpu: failed to initialize hw for display support.\n");
+		/* Do not fail and cleanup, try to run without display */
+	}
+
+	if (amdgpu_dm_initialize_drm_device(&adev->dm)) {
+		DRM_ERROR(
+		"amdgpu: failed to initialize sw for display support.\n");
+		goto error;
+	}
+
+	/* Update the actual used number of crtc */
+	adev->mode_info.num_crtc = adev->dm.display_indexes_num;
+
+	/* TODO: Add_display_info? */
+
+	/* TODO use dynamic cursor width */
+	adev->ddev->mode_config.cursor_width = 128;
+	adev->ddev->mode_config.cursor_height = 128;
+
+	/*TODO: check pageflip, vblank implementation*/
+	/*amdgpu_dm_vblank_irq_state_disable(adev);*/
+
+	if (drm_vblank_init(adev->ddev, adev->dm.display_indexes_num)) {
+		DRM_ERROR(
+		"amdgpu: failed to initialize sw for display support.\n");
+		goto error;
+	}
+
+	DRM_INFO("KMS initialized.\n");
+
+	return 0;
+error:
+	amdgpu_dm_fini(adev);
+
+	return -1;
+}
+
+void amdgpu_dm_fini(struct amdgpu_device *adev)
+{
+	/*TODO: pageflip, vlank interrupt
+	 * amdgpu_dm_vblank_irq_state_disable(adev);*/
+
+	amdgpu_dm_destroy_drm_device(&adev->dm);
+
+	/*amdgpu_dm_irq_fini(adev);*/
+
+	dal_destroy(&adev->dm.dal);
+	return;
+}
+
+/* moved from amdgpu_dm_kms.c */
+void amdgpu_dm_destroy()
+{
+}
+
+/*
+ * amdgpu_dm_get_vblank_counter
+ *
+ * @brief
+ * Get counter for number of vertical blanks
+ *
+ * @param
+ * struct amdgpu_device *adev - [in] desired amdgpu device
+ * int disp_idx - [in] which CRTC to get the counter from
+ *
+ * @return
+ * Counter for vertical blanks
+ */
+u32 amdgpu_dm_get_vblank_counter(struct amdgpu_device *adev, int disp_idx)
+{
+	return dal_get_vblank_counter(adev->dm.dal, disp_idx);
+}
+
+static int dm_sw_init(void *handle)
+{
+	return 0;
+}
+
+static int dm_sw_fini(void *handle)
+{
+	return 0;
+}
+
+static int dm_hw_init(void *handle)
+{
+	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
+	/* Create DAL display manager */
+	amdgpu_dm_init(adev);
+
+	amdgpu_dm_hpd_init(adev);
+
+	return 0;
+}
+
+static int dm_hw_fini(void *handle)
+{
+	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
+
+	amdgpu_dm_hpd_fini(adev);
+
+	amdgpu_dm_irq_fini(adev);
+
+	return 0;
+}
+
+const struct amd_ip_funcs amdgpu_dm_funcs = {
+	.early_init = dm_early_init,
+	.late_init = NULL,
+	.sw_init = dm_sw_init,
+	.sw_fini = dm_sw_fini,
+	.hw_init = dm_hw_init,
+	.hw_fini = dm_hw_fini,
+	.suspend = NULL,
+	.resume = NULL,
+	.is_idle = dm_is_idle,
+	.wait_for_idle = dm_wait_for_idle,
+	.soft_reset = dm_soft_reset,
+	.print_status = dm_print_status,
+	.set_clockgating_state = dm_set_clockgating_state,
+	.set_powergating_state = dm_set_powergating_state,
+};
+
+static int amdgpu_dm_mode_config_init(struct amdgpu_device *adev)
+{
+	int r, i;
+	/* Jordan: Removing this, since already done beforehand in
+	 * amdgpu_device.c
+	 */
+	/*drm_mode_config_init(adev->ddev);*/
+
+	/* TODO: copy from DCE11 */
+	for (i = 0; i < adev->mode_info.num_crtc; i++) {
+		r = amdgpu_irq_add_id(adev, i + 1, &adev->crtc_irq);
+		if (r)
+		return r;
+	}
+
+	for (i = 8; i < 20; i += 2) {
+		r = amdgpu_irq_add_id(adev, i, &adev->pageflip_irq);
+		if (r)
+			return r;
+	}
+
+	/* HPD hotplug */
+	r = amdgpu_irq_add_id(adev, 42, &adev->hpd_irq);
+	if (r)
+		return r;
+
+	adev->mode_info.mode_config_initialized = true;
+
+	adev->ddev->mode_config.funcs = (void *)&amdgpu_mode_funcs;
+
+	adev->ddev->mode_config.max_width = 16384;
+	adev->ddev->mode_config.max_height = 16384;
+
+	adev->ddev->mode_config.preferred_depth = 24;
+	adev->ddev->mode_config.prefer_shadow = 1;
+
+	adev->ddev->mode_config.fb_base = adev->mc.aper_base;
+
+	r = amdgpu_modeset_create_props(adev);
+	if (r)
+		return r;
+
+	/* this is a part of HPD initialization  */
+	drm_kms_helper_poll_init(adev->ddev);
+
+	return r;
+}
+
+
+/* In this architecture, the association
+ * connector -> encoder -> crtc
+ * id not really requried. The crtc and connector will hold the
+ * display_index as an abstraction to use with DAL component
+ *
+ * Returns 0 on success
+ */
+int amdgpu_dm_initialize_drm_device(struct amdgpu_display_manager *dm)
+{
+	int current_display_index = 0;
+	struct amdgpu_connector *aconnector;
+	struct amdgpu_encoder *aencoder;
+	struct amdgpu_crtc *acrtc;
+
+	uint32_t connected_displays_vector =
+		dal_get_connected_targets_vector(dm->dal);
+	uint32_t total_supported_displays_vector =
+		dal_get_supported_displays_vector(dm->dal);
+
+
+	if (amdgpu_dm_mode_config_init(dm->adev)) {
+		DRM_ERROR("KMS: Failed to initialize mode config\n");
+		return -1;
+	}
+
+	/*TODO: HDMI audio*/
+	/*dm->hdmi_audio_dev.name = "hdmi_audio";
+
+	if (switch_dev_register(&dm->hdmi_audio_dev) < 0)
+		goto fail_switch_dev;
+	*/
+
+	/* loops over all the connected displays*/
+	for (; total_supported_displays_vector != 0;
+				total_supported_displays_vector >>= 1,
+				connected_displays_vector >>= 1,
+				++current_display_index) {
+
+		if (current_display_index > AMDGPU_DM_MAX_DISPLAY_INDEX) {
+			DRM_ERROR(
+				"KMS: Cannot support more than %d display indeces\n",
+					AMDGPU_DM_MAX_DISPLAY_INDEX);
+			continue;
+		}
+
+		aconnector = kzalloc(sizeof(*aconnector), GFP_KERNEL);
+		if (!aconnector)
+			goto fail_connector;
+
+		aencoder = kzalloc(sizeof(*aencoder), GFP_KERNEL);
+		if (!aencoder)
+			goto fail_encoder;
+
+		acrtc = kzalloc(sizeof(struct amdgpu_crtc), GFP_KERNEL);
+		if (!acrtc)
+			goto fail_crtc;
+
+		if (amdgpu_dm_crtc_init(
+			dm,
+			acrtc,
+			current_display_index)) {
+			DRM_ERROR("KMS: Failed to initialize crtc\n");
+			goto fail;
+		}
+
+		if (amdgpu_dm_encoder_init(
+			dm->ddev,
+			aencoder,
+			current_display_index,
+			acrtc)) {
+			DRM_ERROR("KMS: Failed to initialize encoder\n");
+			goto fail;
+		}
+
+		if (amdgpu_dm_connector_init(
+			dm,
+			aconnector,
+			current_display_index,
+			(connected_displays_vector & 1) == 1,
+			aencoder)) {
+			DRM_ERROR("KMS: Failed to initialize connector\n");
+			goto fail;
+		}
+	}
+
+	dm->display_indexes_num = current_display_index;
+	dm->mode_query_option = QUERY_OPTION_NO_PAN;
+
+	return 0;
+
+fail:
+	/* clean any dongling drm structure for the last (corrupted)
+	display target */
+	amdgpu_dm_crtc_destroy(&acrtc->base);
+fail_crtc:
+	amdgpu_dm_encoder_destroy(&aencoder->base);
+fail_encoder:
+	amdgpu_dm_connector_destroy(&aconnector->base);
+fail_connector:
+	/*switch_dev_unregister(&dm->hdmi_audio_dev);
+fail_switch_dev:*/
+	return -1;
+}
+
+void amdgpu_dm_destroy_drm_device(
+				struct amdgpu_display_manager *dm)
+{
+	drm_mode_config_cleanup(dm->ddev);
+	/*switch_dev_unregister(&dm->hdmi_audio_dev);*/
+	return;
+}
+
+/******************************************************************************
+ * amdgpu_display_funcs functions
+ *****************************************************************************/
+
+
+static void dm_set_vga_render_state(struct amdgpu_device *adev,
+					   bool render)
+{
+	u32 tmp;
+
+	/* Lockout access through VGA aperture*/
+	tmp = RREG32(mmVGA_HDP_CONTROL);
+	if (render)
+		tmp = REG_SET_FIELD(tmp, VGA_HDP_CONTROL, VGA_MEMORY_DISABLE, 0);
+	else
+		tmp = REG_SET_FIELD(tmp, VGA_HDP_CONTROL, VGA_MEMORY_DISABLE, 1);
+	WREG32(mmVGA_HDP_CONTROL, tmp);
+
+	/* disable VGA render */
+	tmp = RREG32(mmVGA_RENDER_CONTROL);
+	if (render)
+		tmp = REG_SET_FIELD(tmp, VGA_RENDER_CONTROL, VGA_VSTATUS_CNTL, 1);
+	else
+		tmp = REG_SET_FIELD(tmp, VGA_RENDER_CONTROL, VGA_VSTATUS_CNTL, 0);
+	WREG32(mmVGA_RENDER_CONTROL, tmp);
+}
+
+/**
+ * dm_bandwidth_update - program display watermarks
+ *
+ * @adev: amdgpu_device pointer
+ *
+ * Calculate and program the display watermarks and line buffer allocation.
+ */
+static void dm_bandwidth_update(struct amdgpu_device *adev)
+{
+	AMDGPU_DM_NOT_IMPL("%s\n", __func__);
+}
+
+static void dm_set_backlight_level(struct amdgpu_encoder *amdgpu_encoder,
+				     u8 level)
+{
+	/* TODO: translate amdgpu_encoder to display_index and call DAL */
+	AMDGPU_DM_NOT_IMPL("%s\n", __func__);
+}
+
+static u8 dm_get_backlight_level(struct amdgpu_encoder *amdgpu_encoder)
+{
+	/* TODO: translate amdgpu_encoder to display_index and call DAL */
+	AMDGPU_DM_NOT_IMPL("%s\n", __func__);
+	return 0;
+}
+
+/******************************************************************************
+ * Page Flip functions
+ ******************************************************************************/
+/**
+ * dm_page_flip - called by amdgpu_flip_work_func(), which is triggered
+ * 			via DRM IOCTL, by user mode.
+ *
+ * @adev: amdgpu_device pointer
+ * @crtc_id: crtc to cleanup pageflip on
+ * @crtc_base: new address of the crtc (GPU MC address)
+ *
+ * Does the actual pageflip (surface address update).
+ */
+static void dm_page_flip(struct amdgpu_device *adev,
+			      int crtc_id, u64 crtc_base)
+{
+	struct amdgpu_crtc *amdgpu_crtc = adev->mode_info.crtcs[crtc_id];
+	struct plane_addr_flip_info flip_info;
+	const unsigned int num_of_planes = 1;
+
+	memset(&flip_info, 0, sizeof(flip_info));
+
+	flip_info.display_index = amdgpu_crtc->crtc_id;
+	flip_info.address_info.address.type = PLN_ADDR_TYPE_GRAPHICS;
+	flip_info.address_info.layer_index = LAYER_INDEX_PRIMARY;
+	flip_info.address_info.flags.bits.ENABLE = 1;
+
+	flip_info.address_info.address.grph.addr.low_part =
+			       lower_32_bits(crtc_base);
+
+	flip_info.address_info.address.grph.addr.high_part =
+			       upper_32_bits(crtc_base);
+
+	dal_update_plane_addresses(adev->dm.dal, num_of_planes, &flip_info);
+}
+
+static const struct amdgpu_display_funcs display_funcs = {
+	.set_vga_render_state = dm_set_vga_render_state,
+	.bandwidth_update = dm_bandwidth_update, /* called unconditionally */
+	.vblank_get_counter = dm_vblank_get_counter,/* called unconditionally */
+	.vblank_wait = NULL, /* not called anywhere */
+	.is_display_hung = dm_is_display_hung,/* called unconditionally */
+	.backlight_set_level =
+		dm_set_backlight_level,/* called unconditionally */
+	.backlight_get_level =
+		dm_get_backlight_level,/* called unconditionally */
+	.hpd_sense = NULL,/* called unconditionally */
+	.hpd_set_polarity = NULL, /* called unconditionally */
+	.hpd_get_gpio_reg = dm_hpd_get_gpio_reg,/* called unconditionally */
+	.page_flip = dm_page_flip, /* called unconditionally */
+	.page_flip_get_scanoutpos =
+		dm_crtc_get_scanoutpos,/* called unconditionally */
+	.add_encoder = NULL, /* VBIOS parsing. DAL does it. */
+	.add_connector = NULL, /* VBIOS parsing. DAL does it. */
+	.stop_mc_access = dm_stop_mc_access, /* called unconditionally */
+	.resume_mc_access = dm_resume_mc_access, /* called unconditionally */
+};
+
+static void set_display_funcs(struct amdgpu_device *adev)
+{
+	if (adev->mode_info.funcs == NULL)
+		adev->mode_info.funcs = &display_funcs;
+}
+
+static const struct amdgpu_irq_src_funcs dm_crtc_irq_funcs = {
+	.set = amdgpu_dm_set_crtc_irq_state,
+	.process = amdgpu_dm_crtc_irq,
+};
+
+static const struct amdgpu_irq_src_funcs dm_pageflip_irq_funcs = {
+	.set = amdgpu_dm_set_pflip_irq_state,
+	.process = amdgpu_dm_pflip_irq,
+};
+
+static const struct amdgpu_irq_src_funcs dm_hpd_irq_funcs = {
+	.set = amdgpu_dm_set_hpd_irq_state,
+	.process = amdgpu_dm_hpd_irq,
+};
+
+static void dm_set_irq_funcs(struct amdgpu_device *adev)
+{
+	adev->crtc_irq.num_types = AMDGPU_CRTC_IRQ_LAST;
+	adev->crtc_irq.funcs = &dm_crtc_irq_funcs;
+
+	adev->pageflip_irq.num_types = AMDGPU_PAGEFLIP_IRQ_LAST;
+	adev->pageflip_irq.funcs = &dm_pageflip_irq_funcs;
+
+	adev->hpd_irq.num_types = AMDGPU_HPD_LAST;
+
+	adev->hpd_irq.funcs = &dm_hpd_irq_funcs;
+}
+
+static int dm_early_init(void *handle)
+{
+	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
+	set_display_funcs(adev);
+	dm_set_irq_funcs(adev);
+
+	switch (adev->asic_type) {
+	case CHIP_CARRIZO:
+		adev->mode_info.num_crtc = 3;
+		adev->mode_info.num_hpd = 6;
+		adev->mode_info.num_dig = 9;
+		break;
+	default:
+		DRM_ERROR("Usupported ASIC type: 0x%X\n", adev->asic_type);
+		return -EINVAL;
+	}
+
+	/* Note: Do NOT change adev->audio_endpt_rreg and
+	 * adev->audio_endpt_wreg because they are initialised in
+	 * amdgpu_device_init() */
+
+
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm.h
new file mode 100644
index 0000000..b93224f
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm.h
@@ -0,0 +1,124 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __AMDGPU_DM_H__
+#define __AMDGPU_DM_H__
+
+/*
+#include "linux/switch.h"
+*/
+
+/*
+ * This file contains the definition for amdgpu_display_manager
+ * and its API for amdgpu driver's use.
+ * This component provides all the display related functionality
+ * and this is the only component that calls DAL API.
+ * The API contained here intended for amdgpu driver use.
+ * The API that is called directly from KMS framework is located
+ * in amdgpu_dm_kms.h file
+ */
+
+#define AMDGPU_DM_MAX_DISPLAY_INDEX 31
+/*
+#include "include/amdgpu_dal_power_if.h"
+#include "amdgpu_dm_irq.h"
+*/
+
+#include "irq_types.h"
+
+/* Forward declarations */
+struct amdgpu_device;
+struct drm_device;
+struct amdgpu_dm_irq_handler_data;
+
+struct amdgpu_dm_prev_state {
+	struct drm_framebuffer *fb;
+	int32_t x;
+	int32_t y;
+	struct drm_display_mode mode;
+};
+
+
+struct amdgpu_display_manager {
+	struct dal *dal;
+	/* lock to be used when DAL is called from SYNC IRQ context */
+	spinlock_t dal_lock;
+
+	struct amdgpu_device *adev;	/*AMD base driver*/
+	struct drm_device *ddev;	/*DRM base driver*/
+	u16 display_indexes_num;
+
+	u32 mode_query_option;
+
+	struct amdgpu_dm_prev_state prev_state;
+
+	/*
+	 * 'irq_source_handler_table' holds a list of handlers
+	 * per (DAL) IRQ source.
+	 *
+	 * Each IRQ source may need to be handled at different contexts.
+	 * By 'context' we mean, for example:
+	 * - The ISR context, which is the direct interrupt handler.
+	 * - The 'deferred' context - this is the post-processing of the
+	 *	interrupt, but at a lower priority.
+	 *
+	 * Note that handlers are called in the same order as they were
+	 * registered (FIFO).
+	 */
+	struct list_head irq_handler_list_table[INTERRUPT_CONTEXT_NUMBER]
+					  [DAL_IRQ_SOURCES_NUMBER];
+
+	/* this spin lock synchronizes access to 'irq_handler_list_table' */
+	spinlock_t irq_handler_list_table_lock;
+
+	/* Timer-related data. */
+	struct list_head timer_handler_list;
+	struct workqueue_struct *timer_workqueue;
+};
+
+
+/* basic init/fini API */
+int amdgpu_dm_init(struct amdgpu_device *adev);
+
+void amdgpu_dm_fini(struct amdgpu_device *adev);
+
+void amdgpu_dm_destroy(void);
+
+/* initializes drm_device display related structures, based on the information
+ * provided by DAL. The drm strcutures are: drm_crtc, drm_connector,
+ * drm_encoder, drm_mode_config
+ *
+ * Returns 0 on success
+ */
+int amdgpu_dm_initialize_drm_device(
+	struct amdgpu_display_manager *dm);
+
+/* removes and deallocates the drm structures, created by the above function */
+void amdgpu_dm_destroy_drm_device(
+	struct amdgpu_display_manager *dm);
+
+extern const struct amd_ip_funcs amdgpu_dm_funcs;
+
+#endif /* __AMDGPU_DM_H__ */
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.c
new file mode 100644
index 0000000..8e7b43b
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.c
@@ -0,0 +1,442 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include <drm/drmP.h>
+
+#include "amdgpu.h"
+#include "amdgpu_dm.h"
+#include "amdgpu_dm_irq.h"
+
+
+/******************************************************************************
+ * Private declarations.
+ *****************************************************************************/
+
+struct handler_common_data {
+	struct list_head list;
+	interrupt_handler handler;
+	void *handler_arg;
+
+	/* DM which this handler belongs to */
+	struct amdgpu_display_manager *dm;
+};
+
+struct amdgpu_dm_irq_handler_data {
+	struct handler_common_data hcd;
+	/* DAL irq source which registered for this interrupt. */
+	enum dal_irq_source irq_source;
+	/* In case this interrupt needs post-processing, 'work' will be queued*/
+	struct work_struct work;
+};
+
+struct amdgpu_dm_timer_handler_data {
+	struct handler_common_data hcd;
+	struct delayed_work d_work;
+};
+
+
+#define DM_IRQ_TABLE_LOCK(adev, flags) \
+	spin_lock_irqsave(&adev->dm.irq_handler_list_table_lock, flags)
+
+#define DM_IRQ_TABLE_UNLOCK(adev, flags) \
+	spin_unlock_irqrestore(&adev->dm.irq_handler_list_table_lock, flags)
+
+/******************************************************************************
+ * Private functions.
+ *****************************************************************************/
+
+static void init_handler_common_data(
+	struct handler_common_data *hcd,
+	void (*ih)(void *),
+	void *args,
+	struct amdgpu_display_manager *dm)
+{
+	hcd->handler = ih;
+	hcd->handler_arg = args;
+	hcd->dm = dm;
+}
+
+/**
+ * dm_irq_work_func - Handle an IRQ outside of the interrupt handler proper.
+ *
+ * @work: work struct
+ */
+static void dm_irq_work_func(
+	struct work_struct *work)
+{
+	struct amdgpu_dm_irq_handler_data *handler_data =
+		container_of(work, struct amdgpu_dm_irq_handler_data, work);
+
+	DRM_DEBUG_KMS("DM_IRQ: work_func: for dal_src=%d\n",
+			handler_data->irq_source);
+
+	/* Call a DAL subcomponent which registered for interrupt notification
+	 * at INTERRUPT_LOW_IRQ_CONTEXT.
+	 * (The most common use is HPD interrupt) */
+	handler_data->hcd.handler(handler_data->hcd.handler_arg);
+}
+
+/**
+ * Remove a handler and return a pointer to hander list from which the
+ * handler was removed.
+ */
+static struct list_head *remove_irq_handler(
+	struct amdgpu_device *adev,
+	void *ih,
+	const struct dal_interrupt_params *int_params)
+{
+	struct list_head *handler_list;
+	struct list_head *entry, *tmp;
+	struct amdgpu_dm_irq_handler_data *handler;
+	unsigned long irq_table_flags;
+	bool handler_removed = false;
+
+	DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+
+	handler_list =
+		&adev->dm.irq_handler_list_table[int_params->int_context]
+						  [int_params->irq_source];
+	list_for_each_safe(entry, tmp, handler_list) {
+
+		handler = list_entry(entry, struct amdgpu_dm_irq_handler_data,
+				hcd.list);
+
+		if (ih == handler) {
+			/* Found our handler. Remove it from the list. */
+			list_del(&handler->hcd.list);
+			handler_removed = true;
+			break;
+		}
+	}
+
+	DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+
+	if (handler_removed == false) {
+		/* Not necessarily an error - caller may not
+		 * know the context. */
+		return NULL;
+	}
+
+	/* The handler was removed from the table,
+	 * it means it is safe to flush all the 'work'
+	 * (because no code can schedule a new one). */
+	flush_work(&handler->work);
+
+	kfree(handler);
+
+	DRM_DEBUG_KMS(
+	"DM_IRQ: removed irq handler: %p for: dal_src=%d, irq context=%d\n",
+		ih, int_params->irq_source, int_params->int_context);
+
+	return handler_list;
+}
+
+/* If 'handler_in == NULL' then remove ALL handlers. */
+static void remove_timer_handler(
+	struct amdgpu_device *adev,
+	struct amdgpu_dm_timer_handler_data *handler_in)
+{
+	struct amdgpu_dm_timer_handler_data *handler_temp;
+	struct list_head *handler_list;
+	struct list_head *entry, *tmp;
+	unsigned long irq_table_flags;
+	bool handler_removed = false;
+
+	DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+
+	handler_list = &adev->dm.timer_handler_list;
+
+	list_for_each_safe(entry, tmp, handler_list) {
+		/* Note that list_for_each_safe() guarantees that
+		 * handler_temp is NOT null. */
+		handler_temp = list_entry(entry,
+				struct amdgpu_dm_timer_handler_data, hcd.list);
+
+		if (handler_in == NULL || handler_in == handler_temp) {
+			list_del(&handler_temp->hcd.list);
+			DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+
+			DRM_DEBUG_KMS("DM_IRQ: removing timer handler: %p\n",
+					handler_temp);
+
+			if (handler_in == NULL) {
+				/* Since it is still in the queue, it must
+				 * be cancelled. */
+				cancel_delayed_work_sync(&handler_temp->d_work);
+			}
+
+			kfree(handler_temp);
+			handler_removed = true;
+
+			DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+		}
+
+		if (handler_in == NULL) {
+			/* Remove ALL handlers. */
+			continue;
+		}
+
+		if (handler_in == handler_temp) {
+			/* Remove a SPECIFIC handler.
+			 * Found our handler - we can stop here. */
+			break;
+		}
+	}
+
+	DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+
+	if (handler_in != NULL && handler_removed == false) {
+		DRM_ERROR("DM_IRQ: handler: %p is not in the list!\n",
+				handler_in);
+	}
+}
+
+/**
+ * dm_timer_work_func - Handle a timer.
+ *
+ * @work: work struct
+ */
+static void dm_timer_work_func(
+	struct work_struct *work)
+{
+	struct amdgpu_dm_timer_handler_data *handler_data =
+		container_of(work, struct amdgpu_dm_timer_handler_data,
+				d_work.work);
+
+	DRM_DEBUG_KMS("DM_IRQ: work_func: handler_data=%p\n", handler_data);
+
+	/* Call a DAL subcomponent which registered for timer notification. */
+	handler_data->hcd.handler(handler_data->hcd.handler_arg);
+
+	/* We support only "single shot" timers. That means we must delete
+	 * the handler after it was called. */
+	remove_timer_handler(handler_data->hcd.dm->adev, handler_data);
+}
+
+/******************************************************************************
+ * Public functions.
+ *
+ * Note: caller is responsible for input validation.
+ *****************************************************************************/
+
+void *amdgpu_dm_irq_register_interrupt(
+	struct amdgpu_device *adev,
+	struct dal_interrupt_params *int_params,
+	void (*ih)(void *),
+	void *handler_args)
+{
+	struct list_head *handler_list;
+	struct amdgpu_dm_irq_handler_data *handler_data;
+	unsigned long irq_table_flags;
+
+	handler_data = kzalloc(sizeof(*handler_data), GFP_KERNEL);
+	if (!handler_data) {
+		DRM_ERROR("DM_IRQ: failed to allocate irq handler!\n");
+		return DAL_INVALID_IRQ_HANDLER_IDX;
+	}
+
+	memset(handler_data, 0, sizeof(*handler_data));
+
+	init_handler_common_data(&handler_data->hcd, ih, handler_args,
+			&adev->dm);
+
+	handler_data->irq_source = int_params->irq_source;
+
+	INIT_WORK(&handler_data->work, dm_irq_work_func);
+
+	/* Lock the list, add the handler. */
+	DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+
+	handler_list = &adev->dm.irq_handler_list_table[int_params->int_context]
+						  [int_params->irq_source];
+
+	list_add_tail(&handler_data->hcd.list, handler_list);
+
+	DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+
+	/* This pointer will be stored by code which requested interrupt
+	 * registration.
+	 * The same pointer will be needed in order to unregister the
+	 * interrupt. */
+
+	DRM_DEBUG_KMS(
+	"DM_IRQ: added irq handler: %p for: dal_src=%d, irq context=%d\n",
+		handler_data, int_params->irq_source, int_params->int_context);
+
+	return handler_data;
+}
+
+void amdgpu_dm_irq_unregister_interrupt(
+	struct amdgpu_device *adev,
+	enum dal_irq_source irq_source,
+	void *ih)
+{
+	struct list_head *handler_list;
+	struct dal_interrupt_params int_params;
+	int i;
+
+	memset(&int_params, 0, sizeof(int_params));
+
+	int_params.irq_source = irq_source;
+
+	for (i = 0; i < INTERRUPT_CONTEXT_NUMBER; i++) {
+
+		int_params.int_context = i;
+
+		handler_list = remove_irq_handler(adev, ih, &int_params);
+
+		if (handler_list != NULL)
+			break;
+	}
+
+	if (handler_list == NULL) {
+		/* If we got here, it means we searched all irq contexts
+		 * for this irq source, but the handler was not found. */
+		DRM_ERROR(
+		"DM_IRQ: failed to find irq handler:%p for irq_source:%d!\n",
+			ih, irq_source);
+	}
+}
+
+/**
+ * amdgpu_dm_irq_schedule_work - schedule all work items registered for
+ * 				the "irq_source".
+ */
+void amdgpu_dm_irq_schedule_work(
+	struct amdgpu_device *adev,
+	enum dal_irq_source irq_source)
+{
+	struct list_head *handler_list;
+	struct amdgpu_dm_irq_handler_data *handler;
+	struct list_head *entry, *tmp;
+	unsigned long irq_table_flags;
+
+	DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+
+	/* Since the caller is interested in 'work_struct' then
+	 * the irq will be post-processed at "INTERRUPT_LOW_IRQ_CONTEXT". */
+	handler_list =
+		&adev->dm.irq_handler_list_table[INTERRUPT_LOW_IRQ_CONTEXT]
+						     [irq_source];
+
+	list_for_each_safe(entry, tmp, handler_list) {
+
+		handler = list_entry(entry, struct amdgpu_dm_irq_handler_data,
+				hcd.list);
+
+		DRM_DEBUG_KMS("DM_IRQ: schedule_work: for dal_src=%d\n",
+				handler->irq_source);
+
+		schedule_work(&handler->work);
+	}
+
+	DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+}
+
+int amdgpu_dm_irq_init(
+	struct amdgpu_device *adev)
+{
+	int ctx;
+	int src;
+	struct list_head *lh;
+
+	DRM_DEBUG_KMS("DM_IRQ\n");
+
+	spin_lock_init(&adev->dm.irq_handler_list_table_lock);
+
+	for (ctx = 0; ctx < INTERRUPT_CONTEXT_NUMBER; ctx++) {
+		for (src = 0; src < DAL_IRQ_SOURCES_NUMBER; src++) {
+
+			lh = &adev->dm.irq_handler_list_table[ctx][src];
+			INIT_LIST_HEAD(lh);
+		}
+	}
+
+	INIT_LIST_HEAD(&adev->dm.timer_handler_list);
+
+	/* allocate and initialize the workqueue for DM timer */
+	adev->dm.timer_workqueue = create_singlethread_workqueue(
+			"dm_timer_queue");
+	if (adev->dm.timer_workqueue == NULL) {
+		DRM_ERROR("DM_IRQ: unable to create timer queue!\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+void amdgpu_dm_irq_register_timer(
+	struct amdgpu_device *adev,
+	struct dal_timer_interrupt_params *int_params,
+	interrupt_handler ih,
+	void *args)
+{
+	unsigned long jf_delay;
+	struct list_head *handler_list;
+	struct amdgpu_dm_timer_handler_data *handler_data;
+	unsigned long irq_table_flags;
+
+	handler_data = kzalloc(sizeof(*handler_data), GFP_KERNEL);
+	if (!handler_data) {
+		DRM_ERROR("DM_IRQ: failed to allocate timer handler!\n");
+		return;
+	}
+
+	memset(handler_data, 0, sizeof(*handler_data));
+
+	init_handler_common_data(&handler_data->hcd, ih, args, &adev->dm);
+
+	INIT_DELAYED_WORK(&handler_data->d_work, dm_timer_work_func);
+
+	/* Lock the list, add the handler. */
+	DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+
+	handler_list = &adev->dm.timer_handler_list;
+
+	list_add_tail(&handler_data->hcd.list, handler_list);
+
+	DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+
+	jf_delay = usecs_to_jiffies(int_params->micro_sec_interval);
+
+	queue_delayed_work(adev->dm.timer_workqueue, &handler_data->d_work,
+			jf_delay);
+
+	DRM_DEBUG_KMS("DM_IRQ: added handler:%p with micro_sec_interval=%llu\n",
+			handler_data, int_params->micro_sec_interval);
+	return;
+}
+
+/* DM IRQ and timer resource release */
+void amdgpu_dm_irq_fini(
+	struct amdgpu_device *adev)
+{
+	DRM_DEBUG_KMS("DM_IRQ: releasing resources.\n");
+
+	/* Cancel ALL timers and release handlers (if any). */
+	remove_timer_handler(adev, NULL);
+	/* Release the queue itself. */
+	destroy_workqueue(adev->dm.timer_workqueue);
+}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.h
new file mode 100644
index 0000000..4549fb4
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_irq.h
@@ -0,0 +1,104 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef __AMDGPU_DM_IRQ_H__
+#define __AMDGPU_DM_IRQ_H__
+
+#include "include/irq_types.h" /* DAL irq definitions */
+
+/*
+ * Display Manager IRQ-related interfaces (for use by DAL).
+ */
+
+/**
+ * amdgpu_dm_irq_init - Initialize internal structures of 'amdgpu_dm_irq'.
+ *
+ * This function should be called exactly once - during DM initialization.
+ *
+ * Returns:
+ *	0 - success
+ *	non-zero - error
+ */
+int amdgpu_dm_irq_init(
+	struct amdgpu_device *adev);
+
+/**
+ * amdgpu_dm_irq_fini - deallocate internal structures of 'amdgpu_dm_irq'.
+ *
+ * This function should be called exactly once - during DM destruction.
+ *
+ */
+void amdgpu_dm_irq_fini(
+	struct amdgpu_device *adev);
+
+/**
+ * amdgpu_dm_irq_register_interrupt - register irq handler for Display block.
+ *
+ * @adev: AMD DRM device
+ * @int_params: parameters for the irq
+ * @ih: pointer to the irq hander function
+ * @handler_args: arguments which will be passed to ih
+ *
+ * Returns:
+ * 	IRQ Handler Index on success.
+ * 	NULL on failure.
+ *
+ * Cannot be called from an interrupt handler.
+ */
+void *amdgpu_dm_irq_register_interrupt(
+		struct amdgpu_device *adev,
+		struct dal_interrupt_params *int_params,
+		void (*ih)(void *),
+		void *handler_args);
+
+/**
+ * amdgpu_dm_irq_unregister_interrupt - unregister handler which was registered
+ *	by amdgpu_dm_irq_register_interrupt().
+ *
+ * @adev: AMD DRM device.
+ * @ih_index: irq handler index which was returned by
+ *	amdgpu_dm_irq_register_interrupt
+ */
+void amdgpu_dm_irq_unregister_interrupt(
+		struct amdgpu_device *adev,
+		enum dal_irq_source irq_source,
+		void *ih_index);
+
+/**
+ * amdgpu_dm_irq_schedule_work - schedule all work items registered for
+ * 				the "irq_source".
+ *
+ * @adev: AMD DRM device
+ */
+void amdgpu_dm_irq_schedule_work(
+		struct amdgpu_device *adev,
+		enum dal_irq_source irq_source);
+
+
+void amdgpu_dm_irq_register_timer(
+	struct amdgpu_device *adev,
+	struct dal_timer_interrupt_params *int_params,
+	interrupt_handler ih,
+	void *args);
+
+#endif /* __AMDGPU_DM_IRQ_H__ */
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.c
new file mode 100644
index 0000000..cec89c7
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.c
@@ -0,0 +1,1595 @@
+/*
+ * Copyright 2012-13 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dal_services_types.h"
+
+#include <linux/types.h>
+#include <drm/drmP.h>
+#include "amdgpu.h"
+// We need to #undef FRAME_SIZE and DEPRECATED because they conflict
+// with ptrace-abi.h's #define's of them.
+#undef FRAME_SIZE
+#undef DEPRECATED
+
+#include "amdgpu_dm_types.h"
+
+#include "include/dal_interface.h"
+#include "include/timing_service_types.h"
+#include "include/set_mode_interface.h"
+#include "include/mode_query_interface.h"
+#include "include/dcs_types.h"
+#include "include/mode_manager_interface.h"
+#include "include/mode_manager_types.h"
+
+/*#include "amdgpu_buffer.h"*/
+
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+#include "dce/dce_11_0_enum.h"
+
+void amdgpu_dm_encoder_destroy(struct drm_encoder *encoder)
+{
+	drm_encoder_cleanup(encoder);
+	kfree(encoder);
+}
+
+static const struct drm_encoder_funcs amdgpu_dm_encoder_funcs = {
+	.reset = NULL,
+	.destroy = amdgpu_dm_encoder_destroy,
+};
+
+static void init_dal_topology(
+	struct amdgpu_display_manager *dm,
+	struct topology *tp,
+	uint32_t current_display_index)
+{
+	DRM_DEBUG_KMS("current_display_index: %d\n", current_display_index);
+
+/* TODO: the following code is used for clone mode, uncomment in case needed
+	uint32_t current_display_index;
+	uint32_t connected_displays_vector =
+		dal_get_connected_targets_vector(dm->dal);
+	uint32_t total_connected_displays = 0;
+
+	for (current_display_index = 0;
+		connected_displays_vector != 0;
+		connected_displays_vector >>= 1,
+		++current_display_index) {
+		if ((connected_displays_vector & 1) == 1) {
+			tp->display_index[total_connected_displays] =
+				current_display_index;
+			++total_connected_displays;
+		}
+	} */
+
+	tp->display_index[0] = current_display_index;
+	tp->disp_path_num = 1;
+}
+
+
+static enum pixel_format convert_to_dal_pixel_format(uint32_t drm_pf)
+{
+	switch (drm_pf) {
+	case DRM_FORMAT_RGB888:
+		return PIXEL_FORMAT_INDEX8;
+	case DRM_FORMAT_RGB565:
+		return PIXEL_FORMAT_RGB565;
+	case DRM_FORMAT_ARGB8888:
+		return PIXEL_FORMAT_ARGB8888;
+	case DRM_FORMAT_ARGB2101010:
+		return PIXEL_FORMAT_ARGB2101010;
+	default:
+		return PIXEL_FORMAT_ARGB8888;
+	}
+}
+
+/* TODO: Use DAL define if available */
+#define DAL_MODE_FLAG_VIDEO_OPTIMIZED (1 << 0)
+
+/*TODO define max plane nums*/
+#define AMDGPU_PFLIP_IRQ_SRC_NUM  6
+
+
+static int dm_set_cursor(struct drm_crtc *crtc, struct drm_gem_object *obj,
+		uint32_t width, uint32_t height)
+{
+	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
+	struct amdgpu_bo *robj;
+	struct cursor_attributes attributes;
+	struct amdgpu_device *adev = crtc->dev->dev_private;
+	uint64_t gpu_addr;
+	int ret;
+
+	if ((width > amdgpu_crtc->max_cursor_width) ||
+	    (height > amdgpu_crtc->max_cursor_height)) {
+		DRM_ERROR("bad cursor width or height %d x %d\n", width, height);
+		return -EINVAL;
+	}
+
+	robj = gem_to_amdgpu_bo(obj);
+	ret = amdgpu_bo_reserve(robj, false);
+	if (unlikely(ret != 0))
+		return ret;
+	ret = amdgpu_bo_pin_restricted(robj, AMDGPU_GEM_DOMAIN_VRAM,
+				       0, 0, &gpu_addr);
+	amdgpu_bo_unreserve(robj);
+	if (ret)
+		return ret;
+
+	amdgpu_crtc->cursor_width = width;
+	amdgpu_crtc->cursor_height = height;
+
+	attributes.address.high_part = upper_32_bits(gpu_addr);
+	attributes.address.low_part = lower_32_bits(gpu_addr);
+	attributes.width = width-1;
+	attributes.height = height-1;
+	attributes.x_hot = 0;
+	attributes.y_hot = 0;
+	attributes.color_format = CURSOR_MODE_COLOR_PRE_MULTIPLIED_ALPHA;
+	attributes.rotation_angle = 0;
+	attributes.attribute_flags.value = 0;
+
+	dal_set_cursor_attributes(adev->dm.dal, amdgpu_crtc->crtc_id, &attributes);
+
+	return 0;
+}
+
+
+static int dm_crtc_cursor_set(struct drm_crtc *crtc,
+				    struct drm_file *file_priv,
+				    uint32_t handle,
+				    uint32_t width,
+				    uint32_t height)
+{
+	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
+	struct drm_gem_object *obj;
+	struct amdgpu_bo *robj;
+
+	struct amdgpu_device *adev = crtc->dev->dev_private;
+	int ret;
+	struct cursor_position position;
+
+
+	if (!handle) {
+		/* turn off cursor */
+		position.enable = false;
+		position.x = 0;
+		position.y = 0;
+		dal_set_cursor_position(adev->dm.dal, amdgpu_crtc->crtc_id, &position);
+
+		obj = NULL;
+		goto unpin;
+	}
+
+	obj = drm_gem_object_lookup(crtc->dev, file_priv, handle);
+	if (!obj) {
+		DRM_ERROR("Cannot find cursor object %x for crtc %d\n", handle, amdgpu_crtc->crtc_id);
+		return -ENOENT;
+	}
+
+	ret = dm_set_cursor(crtc, obj, width, height);
+
+	if (ret) {
+		drm_gem_object_unreference_unlocked(obj);
+		return ret;
+	}
+
+unpin:
+	if (amdgpu_crtc->cursor_bo) {
+		robj = gem_to_amdgpu_bo(amdgpu_crtc->cursor_bo);
+		ret = amdgpu_bo_reserve(robj, false);
+		if (likely(ret == 0)) {
+			amdgpu_bo_unpin(robj);
+			amdgpu_bo_unreserve(robj);
+		}
+		drm_gem_object_unreference_unlocked(amdgpu_crtc->cursor_bo);
+	}
+
+	amdgpu_crtc->cursor_bo = obj;
+	return 0;
+
+}
+
+static int dm_crtc_cursor_move(struct drm_crtc *crtc,
+				     int x, int y)
+{
+	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
+	struct amdgpu_device *adev = crtc->dev->dev_private;
+	int xorigin = 0, yorigin = 0;
+	struct cursor_position position;
+
+	/* avivo cursor are offset into the total surface */
+	x += crtc->x;
+	y += crtc->y;
+	DRM_DEBUG("x %d y %d c->x %d c->y %d\n", x, y, crtc->x, crtc->y);
+
+	if (x < 0) {
+		xorigin = min(-x, amdgpu_crtc->max_cursor_width - 1);
+		x = 0;
+	}
+	if (y < 0) {
+		yorigin = min(-y, amdgpu_crtc->max_cursor_height - 1);
+		y = 0;
+	}
+
+	position.enable = true;
+	position.x = x;
+	position.y = y;
+
+	dal_set_cursor_position(adev->dm.dal, amdgpu_crtc->crtc_id, &position);
+
+	return 0;
+}
+
+static int dm_crtc_cursor_reset(struct drm_crtc *crtc)
+{
+	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
+	int ret = 0;
+
+	if (amdgpu_crtc->cursor_bo) {
+		ret = dm_set_cursor(crtc, amdgpu_crtc->cursor_bo,
+			amdgpu_crtc->cursor_width, amdgpu_crtc->cursor_height);
+	}
+
+	return ret;
+}
+
+static void fill_plane_attributes(
+			struct amdgpu_device* adev,
+			struct plane_config *pl_config,
+			struct drm_crtc *crtc) {
+
+	uint64_t tiling_flags;
+	struct drm_gem_object *obj;
+	struct amdgpu_bo *rbo;
+	int r;
+	struct amdgpu_framebuffer *amdgpu_fb;
+
+	amdgpu_fb = to_amdgpu_framebuffer(crtc->primary->fb);
+	obj = amdgpu_fb->obj;
+	rbo = gem_to_amdgpu_bo(obj);
+	r = amdgpu_bo_reserve(rbo, false);
+	if (unlikely(r != 0)){
+		DRM_ERROR("Unable to reserve buffer\n");
+		return;
+	}
+
+	amdgpu_bo_get_tiling_flags(rbo, &tiling_flags);
+	amdgpu_bo_unreserve(rbo);
+
+	switch (amdgpu_fb->base.pixel_format) {
+	case DRM_FORMAT_C8:
+		pl_config->config.format =
+			SURFACE_PIXEL_FORMAT_GRPH_PALETA_256_COLORS;
+		break;
+	case DRM_FORMAT_RGB565:
+		pl_config->config.format =
+			SURFACE_PIXEL_FORMAT_GRPH_RGB565;
+		break;
+	case DRM_FORMAT_XRGB8888:
+	case DRM_FORMAT_ARGB8888:
+		pl_config->config.format =
+			SURFACE_PIXEL_FORMAT_GRPH_ARGB8888;
+		break;
+	default:
+		DRM_ERROR("Unsupported screen depth %d\n",
+				amdgpu_fb->base.bits_per_pixel);
+		return;
+	}
+
+	pl_config->config.tiling_info.value = 0;
+
+       if (AMDGPU_TILING_GET(tiling_flags, ARRAY_MODE) == ARRAY_2D_TILED_THIN1) {
+               unsigned bankw, bankh, mtaspect, tile_split, num_banks;
+
+               bankw = AMDGPU_TILING_GET(tiling_flags, BANK_WIDTH);
+               bankh = AMDGPU_TILING_GET(tiling_flags, BANK_HEIGHT);
+               mtaspect = AMDGPU_TILING_GET(tiling_flags, MACRO_TILE_ASPECT);
+               tile_split = AMDGPU_TILING_GET(tiling_flags, TILE_SPLIT);
+               num_banks = AMDGPU_TILING_GET(tiling_flags, NUM_BANKS);
+
+
+		/* XXX fix me for VI */
+		pl_config->config.tiling_info.grph.NUM_BANKS = num_banks;
+		pl_config->config.tiling_info.grph.ARRAY_MODE = ARRAY_2D_TILED_THIN1;
+		pl_config->config.tiling_info.grph.TILE_SPLIT = tile_split;
+		pl_config->config.tiling_info.grph.BANK_WIDTH = bankw;
+		pl_config->config.tiling_info.grph.BANK_HEIGHT = bankh;
+		pl_config->config.tiling_info.grph.TILE_ASPECT = mtaspect;
+		pl_config->config.tiling_info.grph.TILE_MODE = ADDR_SURF_MICRO_TILING_DISPLAY;
+	} else if (AMDGPU_TILING_GET(tiling_flags, ARRAY_MODE) == ARRAY_1D_TILED_THIN1) {
+		pl_config->config.tiling_info.grph.ARRAY_MODE = ARRAY_1D_TILED_THIN1;
+	}
+
+	pl_config->config.tiling_info.grph.PIPE_CONFIG =
+			AMDGPU_TILING_GET(tiling_flags, PIPE_CONFIG);
+
+	pl_config->config.plane_size.grph.surface_size.x = 0;
+	pl_config->config.plane_size.grph.surface_size.y = 0;
+	pl_config->config.plane_size.grph.surface_size.width = amdgpu_fb->base.width;
+	pl_config->config.plane_size.grph.surface_size.height = amdgpu_fb->base.height;
+	pl_config->config.plane_size.grph.surface_pitch =
+			amdgpu_fb->base.pitches[0] / (amdgpu_fb->base.bits_per_pixel / 8);
+
+	/* TODO ACHTUNG ACHTUNG - NICHT SCHIESSEN
+	 * Correctly program src, dst, and clip */
+	pl_config->attributes.dst_rect.width  = crtc->mode.hdisplay;
+	pl_config->attributes.dst_rect.height = crtc->mode.vdisplay;
+	pl_config->attributes.dst_rect.x = 0;
+	pl_config->attributes.dst_rect.y = 0;
+	pl_config->attributes.clip_rect = pl_config->attributes.dst_rect;
+	pl_config->attributes.src_rect = pl_config->attributes.dst_rect;
+
+	pl_config->mp_scaling_data.viewport.x = crtc->x;
+	pl_config->mp_scaling_data.viewport.y = crtc->y;
+	pl_config->mp_scaling_data.viewport.width = crtc->mode.hdisplay;
+	pl_config->mp_scaling_data.viewport.height = crtc->mode.vdisplay;
+
+	pl_config->config.rotation = ROTATION_ANGLE_0;
+	pl_config->config.layer_index = LAYER_INDEX_PRIMARY;
+	pl_config->config.enabled = 1;
+	pl_config->mask.bits.SURFACE_CONFIG_IS_VALID = 1;
+
+}
+
+/* this function will be called in the future from other files as well */
+void amdgpu_dm_fill_surface_address(struct drm_crtc *crtc,
+			struct plane_addr_flip_info *info,
+			struct amdgpu_framebuffer *afb,
+			struct drm_framebuffer *old_fb)
+{
+	struct drm_gem_object *obj;
+	struct amdgpu_bo *rbo;
+	uint64_t fb_location;
+	struct amdgpu_device *adev = crtc->dev->dev_private;
+	int r;
+
+	info->address_info.address.type = PLN_ADDR_TYPE_GRAPHICS;
+	//DD-ToDo
+	//	info->flip_immediate = amdgpu_pflip_vsync ? false : true;
+
+	info->address_info.layer_index = LAYER_INDEX_PRIMARY;
+	info->address_info.flags.bits.ENABLE = 1;
+
+	/*Get fb location*/
+	/* no fb bound */
+	if (!crtc->primary->fb) {
+		DRM_DEBUG_KMS("No FB bound\n");
+		return ;
+	}
+
+	DRM_DEBUG_KMS("Pin new framebuffer: %p\n", afb);
+	obj = afb->obj;
+	rbo = gem_to_amdgpu_bo(obj);
+	r = amdgpu_bo_reserve(rbo, false);
+	if (unlikely(r != 0))
+		return;
+
+	r = amdgpu_bo_pin(rbo, AMDGPU_GEM_DOMAIN_VRAM, &fb_location);
+
+	amdgpu_bo_unreserve(rbo);
+
+	if (unlikely(r != 0)) {
+		DRM_ERROR("Failed to pin framebuffer\n");
+		return ;
+	}
+
+	info->address_info.address.grph.addr.low_part =
+		lower_32_bits(fb_location);
+	info->address_info.address.grph.addr.high_part =
+		upper_32_bits(fb_location);
+
+	dal_update_plane_addresses(adev->dm.dal, 1, info);
+
+	/* unpin the old FB if surface change*/
+	if (old_fb && old_fb != crtc->primary->fb)	{
+		struct amdgpu_framebuffer *afb;
+		/*struct amdgpu_bo *rbo;
+		int r;*/
+
+		afb = to_amdgpu_framebuffer(old_fb);
+		DRM_DEBUG_KMS("Unpin old framebuffer: %p\n", afb);
+		rbo = gem_to_amdgpu_bo(afb->obj);
+		r = amdgpu_bo_reserve(rbo, false);
+		if (unlikely(r)) {
+			DRM_ERROR("failed to reserve rbo before unpin\n");
+			return;
+		} else {
+			amdgpu_bo_unpin(rbo);
+			amdgpu_bo_unreserve(rbo);
+		}
+	}
+}
+
+
+/**
+ * drm_crtc_set_mode - set a mode
+ * @crtc: CRTC to program
+ * @mode: mode to use
+ * @x: width of mode
+ * @y: height of mode
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Try to set @mode on @crtc.  Give @crtc and its associated connectors a chance
+ * to fixup or reject the mode prior to trying to set it.
+ *
+ * RETURNS:
+ * True if the mode was set successfully, or false otherwise.
+ */
+bool amdgpu_dm_mode_set(
+	struct drm_crtc *crtc,
+	struct drm_display_mode *mode,
+	int x,
+	int y,
+	struct drm_framebuffer *old_fb)
+{
+	struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+	struct amdgpu_display_manager *dm =
+		&((struct amdgpu_device *)crtc->dev->dev_private)->dm;
+	struct drm_device *dev = crtc->dev;
+	struct drm_display_mode *adjusted_mode, saved_mode, saved_hwmode;
+	int saved_x, saved_y;
+	bool ret = true;
+	struct amdgpu_device *adev = crtc->dev->dev_private;
+	struct plane_config pl_config = { { { 0 } } };
+	struct plane_addr_flip_info addr_flip_info = { 0 };
+	struct amdgpu_framebuffer *afb = NULL;
+	int num_planes = 1;
+	struct mode_query *mq;
+	const struct path_mode_set *pms;
+
+	DRM_DEBUG_KMS("amdgpu_dm_mode_set called\n");
+
+	if (!adev->dm.dal)
+		return false;
+
+	adjusted_mode = drm_mode_duplicate(dev, mode);
+	if (!adjusted_mode)
+		return false;
+
+	/* for now, no support for atomic mode_set, thus old_fb is not used */
+	afb = to_amdgpu_framebuffer(crtc->primary->fb);
+
+	saved_hwmode = crtc->hwmode;
+	saved_mode = crtc->mode;
+	saved_x = crtc->x;
+	saved_y = crtc->y;
+
+	/* Update crtc values up front so the driver can rely on them for mode
+	* setting.
+	*/
+	crtc->mode = *mode;
+	crtc->x = x;
+	crtc->y = y;
+
+	DRM_DEBUG_KMS("[CRTC: %d, DISPLAY_IDX: %d]\n",
+		      crtc->base.id, acrtc->crtc_id);
+
+	/* Currently no support for atomic mode set */
+	{
+		struct render_mode rm;
+		struct refresh_rate rf = { 0 };
+		struct topology tp;
+		init_dal_topology(dm, &tp, acrtc->crtc_id);
+		rm.view.width = mode->hdisplay;
+		rm.view.height = mode->vdisplay;
+		rm.pixel_format =
+			convert_to_dal_pixel_format(crtc->primary->fb->pixel_format);
+		rf.field_rate = drm_mode_vrefresh(mode);
+		rf.VIDEO_OPTIMIZED_RATE = (mode->private_flags &
+			DAL_MODE_FLAG_VIDEO_OPTIMIZED) != 0;
+		rf.INTERLACED = (mode->flags & DRM_MODE_FLAG_INTERLACE) != 0;
+
+		mq = dal_get_mode_query(
+			adev->dm.dal,
+			&tp,
+			dm->mode_query_option);
+		if (!mq)
+			return false;
+
+		if (!dal_mode_query_select_render_mode(mq, &rm)) {
+			dal_mode_query_destroy(&mq);
+			DRM_ERROR("dal_mode_query_select_render_mode failed\n");
+			return -1;
+		}
+
+		if (!dal_mode_query_select_refresh_rate(mq, &rf)) {
+			dal_mode_query_destroy(&mq);
+			DRM_ERROR("dal_mode_query_select_refresh_rate failed\n");
+			return false;
+		}
+		pms = dal_mode_query_get_current_path_mode_set(mq);
+
+		if (!pms) {
+			dal_mode_query_destroy(&mq);
+			DRM_ERROR("dal_mode_query_get_current_path_mode_set failed\n");
+			return false;
+		}
+	}
+
+
+	/* the actual mode set call */
+	ret = dal_set_path_mode(adev->dm.dal, pms);
+
+	dal_mode_query_destroy(&mq);
+
+	/* Surface programming */
+	pl_config.display_index = acrtc->crtc_id;
+	addr_flip_info.display_index = acrtc->crtc_id;
+
+	fill_plane_attributes(adev, &pl_config, crtc);
+
+	dal_setup_plane_configurations(adev->dm.dal, num_planes, &pl_config);
+
+	/*programs the surface addr and flip control*/
+	amdgpu_dm_fill_surface_address(crtc, &addr_flip_info, afb, old_fb);
+
+	dal_set_blanking(adev->dm.dal, acrtc->crtc_id, false);
+
+	/* Turn vblank on after reset */
+	drm_crtc_vblank_on(crtc);
+
+	if (ret) {
+		/* Store real post-adjustment hardware mode. */
+		crtc->hwmode = *adjusted_mode;
+		crtc->enabled = true;
+
+		/* Calculate and store various constants which
+		 * are later needed by vblank and swap-completion
+		 * timestamping. They are derived from true hwmode.
+		 */
+		drm_calc_timestamping_constants(crtc, &crtc->hwmode);
+	}
+
+	drm_mode_destroy(dev, adjusted_mode);
+	if (!ret) {
+		crtc->hwmode = saved_hwmode;
+		crtc->mode = saved_mode;
+		crtc->x = saved_x;
+		crtc->y = saved_y;
+	}
+
+	return true;
+}
+
+bool amdgpu_dm_mode_reset(struct drm_crtc *crtc)
+{
+	bool ret;
+	struct amdgpu_device *adev = crtc->dev->dev_private;
+	struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+	uint32_t display_index = acrtc->crtc_id;
+
+	DRM_DEBUG_KMS("dal_reset_path_mode\n");
+
+	/* Turn vblank off before reset */
+	drm_crtc_vblank_off(crtc);
+
+	/* Blank the display */
+	dal_set_blanking(adev->dm.dal, acrtc->crtc_id, true);
+
+	/* the actual reset mode call */
+	ret = dal_reset_path_mode(adev->dm.dal, 1, &display_index);
+
+	/* unpin the FB */
+	if (crtc->primary->fb)	{
+		struct amdgpu_framebuffer *afb;
+		struct amdgpu_bo *rbo;
+		int r;
+
+		afb = to_amdgpu_framebuffer(crtc->primary->fb);
+		DRM_DEBUG_KMS("Unpin old framebuffer: %p\n", afb);
+		rbo = gem_to_amdgpu_bo(afb->obj);
+		r = amdgpu_bo_reserve(rbo, false);
+		if (unlikely(r))
+			DRM_ERROR("failed to reserve rbo before unpin\n");
+		else {
+			amdgpu_bo_unpin(rbo);
+			amdgpu_bo_unreserve(rbo);
+		}
+	}
+
+	if (ret)
+		crtc->enabled = false;
+
+	return ret;
+}
+
+/**
+ * amdgpu_dm_set_config - set a new config from userspace
+ * @crtc: CRTC to setup
+ * @crtc_info: user provided configuration
+ * @new_mode: new mode to set
+ * @connector_set: set of connectors for the new config
+ * @fb: new framebuffer
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Setup a new configuration, provided by the user in @crtc_info, and enable
+ * it.
+ *
+ * RETURNS:
+ * Zero on success
+ */
+int amdgpu_dm_set_config(struct drm_mode_set *set)
+{
+	/* TODO:
+	 * Save + restore mode + fb info
+	 * Call dm_set_mode to do the folloring:
+	 * 	Fill Modes
+	 * 	Set Mode
+	 * 	SetPlaneConfig
+	 * 	UpdatePlaneAddress
+	 * 	FillPlaneAttributes
+	 */
+
+	struct drm_device *dev;
+	struct drm_crtc *save_crtcs, *crtc;
+	struct drm_encoder *save_encoders, *encoder, *new_encoder;
+	bool mode_changed = false; /* if true do a full mode set */
+	bool fb_changed = false; /* if true and !mode_changed just do a flip */
+				 /* if true and mode_changed do reset_mode */
+	struct drm_connector *save_connectors, *connector;
+	const struct drm_connector_helper_funcs *connector_funcs;
+	struct amdgpu_crtc *acrtc = NULL;
+	int count = 0, fail = 0;
+	struct drm_mode_set save_set;
+	int ret = 0;
+	int i;
+
+	DRM_DEBUG_KMS("\n");
+	DRM_DEBUG_KMS("--- DM set_config called ---\n");
+
+	BUG_ON(!set);
+	BUG_ON(!set->crtc);
+	BUG_ON(!set->crtc->helper_private);
+
+	/* Enforce sane interface api - has been abused by the fb helper. */
+	BUG_ON(!set->mode && set->fb);
+	BUG_ON(set->fb && set->num_connectors == 0);
+
+	if (set->num_connectors > 1) {
+		DRM_ERROR("Trying to set %zu connectors, but code only assumes max of one\n",
+				set->num_connectors);
+		return -EINVAL;
+	}
+
+	dev = set->crtc->dev;
+
+	if (!set->mode)
+		set->fb = NULL;
+
+
+	/* Allocate space for the backup of all (non-pointer) crtc, encoder and
+	 * connector data. */
+	save_crtcs = kzalloc(dev->mode_config.num_crtc *
+			     sizeof(struct drm_crtc), GFP_KERNEL);
+	if (!save_crtcs)
+		return -ENOMEM;
+
+	save_encoders = kzalloc(dev->mode_config.num_encoder *
+				sizeof(struct drm_encoder), GFP_KERNEL);
+	if (!save_encoders) {
+		kfree(save_crtcs);
+		return -ENOMEM;
+	}
+
+	save_connectors = kzalloc(dev->mode_config.num_connector *
+				sizeof(struct drm_connector), GFP_KERNEL);
+	if (!save_connectors) {
+		kfree(save_encoders);
+		kfree(save_crtcs);
+		return -ENOMEM;
+	}
+
+	/* Copy data. Note that driver private data is not affected.
+	 * Should anything bad happen only the expected state is
+	 * restored, not the drivers personal bookkeeping.
+	 */
+	count = 0;
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		save_crtcs[count++] = *crtc;
+	}
+
+	count = 0;
+	list_for_each_entry(encoder, &dev->mode_config.encoder_list, head) {
+		save_encoders[count++] = *encoder;
+	}
+
+	count = 0;
+	list_for_each_entry(connector, &dev->mode_config.connector_list, head) {
+		save_connectors[count++] = *connector;
+	}
+
+
+	if (set->fb) {
+		DRM_DEBUG_KMS(
+			"[CRTC:%d] [FB:%d] #connectors=%d (x y) (%i %i)\n",
+				set->crtc->base.id, set->fb->base.id,
+				(int)set->num_connectors, set->x, set->y);
+	} else {
+		/*TODO: Move mode reset to crtc->disable instead, and
+		call drm_helper_disable_unused_functions here?*/
+
+		DRM_DEBUG_KMS("[CRTC:%d] [NOFB]\n", set->crtc->base.id);
+
+		if (!amdgpu_dm_mode_reset(set->crtc)) {
+			DRM_ERROR("### Failed to reset mode on [CRTC:%d] ###\n",
+					set->crtc->base.id);
+			ret = -EINVAL;
+			goto fail;
+		}
+		DRM_DEBUG_KMS("=== Early exit dm_set_config ===\n");
+		return 0;
+	}
+
+	save_set.crtc = set->crtc;
+	save_set.mode = &set->crtc->mode;
+	save_set.x = set->crtc->x;
+	save_set.y = set->crtc->y;
+	save_set.fb = set->crtc->primary->fb;
+
+	/* We should be able to check here if the fb has the same properties
+	 * and then just flip_or_move it */
+	if (set->crtc->primary->fb != set->fb) {
+		DRM_DEBUG_KMS("Old FB: %p, New FB[%d]: %p",
+				set->crtc->primary->fb,
+				set->fb->base.id,
+				set->fb);
+		/* If we have no fb then treat it as a full mode set */
+		if (set->crtc->primary->fb == NULL) {
+			DRM_DEBUG_KMS("crtc has no fb, full mode set\n");
+			mode_changed = true;
+		} else if (set->fb == NULL) {
+			mode_changed = true;
+		} else if (set->fb->depth != set->crtc->primary->fb->depth) {
+			mode_changed = true;
+		} else if (set->fb->bits_per_pixel !=
+			   set->crtc->primary->fb->bits_per_pixel) {
+			mode_changed = true;
+		} else {
+			fb_changed = true;
+			DRM_DEBUG_KMS("fbs do not match, set fb_changed to true\n");
+		}
+	} else {
+		DRM_DEBUG_KMS("FB hasn't changed since last set\n");
+	}
+
+	if (set->x != set->crtc->x || set->y != set->crtc->y) {
+		fb_changed = true;
+		DRM_DEBUG_KMS("Viewport Changed. Original: (%d,%d), New: (%d,%d)\n",
+						set->x,
+						set->y,
+						set->crtc->x,
+						set->crtc->y);
+	}
+
+	if (set->mode && !drm_mode_equal(set->mode, &set->crtc->mode)) {
+		DRM_DEBUG_KMS("modes are different, set mode_changed=true\n");
+		drm_mode_debug_printmodeline(&set->crtc->mode);
+		drm_mode_debug_printmodeline(set->mode);
+		mode_changed = true;
+	}
+
+	/* traverse and find the appropriate connector,
+	use its encoder and crtc */
+	count = 0;
+	fail = 1;
+	acrtc = to_amdgpu_crtc(set->crtc);
+	list_for_each_entry(connector, &dev->mode_config.connector_list, head) {
+		/* matching the display index */
+		if (to_amdgpu_connector(connector)->connector_id ==
+			acrtc->crtc_id) {
+			fail = 0;
+			break;
+		}
+	}
+
+	if (fail) {
+		ret = -EINVAL;
+		DRM_ERROR("Couldn't find a matching connector\n");
+		goto fail;
+	}
+
+	/* Get best encoder for connector found above
+	 * TODO: Might need to traverse entire connector list at some point
+	 */
+	connector_funcs = connector->helper_private;
+	new_encoder = connector->encoder;
+
+	/* NOTE: We're assuming a max of one connector per set, so no need to
+	 * loop through connectors in set to find the correct one */
+	if (set->connectors[0] == connector) {
+		new_encoder = connector_funcs->best_encoder(connector);
+		/* if we can't get an encoder for a connector
+		   we are setting now - then fail */
+		if (new_encoder == NULL)
+			/* don't break so fail path works correct */
+			fail = 1;
+		if (connector->dpms != DRM_MODE_DPMS_ON) {
+			DRM_DEBUG_KMS("connector dpms not on, full mode switch\n");
+			mode_changed = true;
+		}
+	}
+
+	if (new_encoder != connector->encoder) {
+		DRM_DEBUG_KMS("encoder changed, full mode switch\n");
+		mode_changed = true;
+		/* If the encoder is reused for another connector, then
+		 * the appropriate crtc will be set later.
+		 */
+		if (connector->encoder)
+			connector->encoder->crtc = NULL;
+		connector->encoder = new_encoder;
+	}
+
+
+	if (fail) {
+		ret = -EINVAL;
+		DRM_ERROR("Couldn't find an encoder\n");
+		goto fail;
+	}
+
+	if (connector->encoder->crtc != set->crtc) {
+		mode_changed = true;
+		connector->encoder->crtc = set->crtc;
+
+		DRM_DEBUG_KMS("New CRTC being used. Full mode set: [CONNECTOR:%d] to [CRTC:%d]\n",
+			connector->base.id,
+			set->crtc->base.id);
+	} else {
+		DRM_DEBUG_KMS("Crtc didn't change: [CONNECTOR:%d] on [CRTC:%d]\n",
+			connector->base.id,
+			set->crtc->base.id);
+	}
+
+	if (mode_changed) {
+		DRM_DEBUG_KMS(
+				"Attempting to set mode from userspace\n");
+		drm_mode_debug_printmodeline(set->mode);
+
+		set->crtc->primary->fb = set->fb;
+		if (!amdgpu_dm_mode_set(
+				set->crtc,
+				set->mode,
+				set->x,
+				set->y,
+				save_set.fb)) {
+			DRM_ERROR(
+					"failed to set mode on [CRTC:%d, DISPLAY_IDX: %d]\n",
+					set->crtc->base.id,
+					acrtc->crtc_id);
+			set->crtc->primary->fb = save_set.fb;
+			ret = -EINVAL;
+			goto fail;
+		}
+
+		DRM_DEBUG_KMS("Setting connector DPMS state to on\n");
+		for (i = 0; i < set->num_connectors; i++) {
+			DRM_DEBUG_KMS(
+					"\t[CONNECTOR:%d] set DPMS on\n",
+					set->connectors[i]->base.id);
+
+			 set->connectors[i]->funcs->dpms(
+					set->connectors[i], DRM_MODE_DPMS_ON);
+
+		}
+
+		/* Re-set the cursor attributes after a successful set mode */
+		dm_crtc_cursor_reset(set->crtc);
+
+	} else if (fb_changed) { /* no mode change just surface change. */
+		struct plane_addr_flip_info addr_flip_info = { 0 };
+		struct amdgpu_framebuffer *afb = to_amdgpu_framebuffer(set->fb);
+		struct amdgpu_device *adev = dev->dev_private;
+		struct plane_config pl_config = { { { 0 } } };
+
+		DRM_DEBUG_KMS("FB Changed, update address\n");
+		set->crtc->primary->fb = set->fb;
+		set->crtc->x = set->x;
+		set->crtc->y = set->y;
+
+		/* program plane config */
+		pl_config.display_index = acrtc->crtc_id;
+		fill_plane_attributes(adev, &pl_config, set->crtc);
+		dal_setup_plane_configurations(adev->dm.dal, 1, &pl_config);
+
+		/* program the surface addr and flip control*/
+		addr_flip_info.display_index = acrtc->crtc_id;
+		amdgpu_dm_fill_surface_address(set->crtc, &addr_flip_info, afb, save_set.fb);
+	}
+	DRM_DEBUG_KMS("=== Finished dm_set_config ===\n");
+
+	kfree(save_connectors);
+	kfree(save_encoders);
+	kfree(save_crtcs);
+	return 0;
+
+fail:
+	/* Restore all previous data. */
+	DRM_ERROR("### Failed set_config. Attempting to restore previous data ###\n");
+	count = 0;
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		*crtc = save_crtcs[count++];
+	}
+
+	count = 0;
+	list_for_each_entry(encoder, &dev->mode_config.encoder_list, head) {
+		*encoder = save_encoders[count++];
+	}
+
+	count = 0;
+	list_for_each_entry(connector, &dev->mode_config.connector_list, head) {
+		*connector = save_connectors[count++];
+	}
+
+	/* Try to restore the config */
+	if (mode_changed && save_set.crtc->primary->fb &&
+		!amdgpu_dm_mode_set(
+			save_set.crtc,
+			save_set.mode,
+			save_set.x,
+			save_set.y,
+			save_set.fb))
+		DRM_ERROR("failed to restore config after modeset failure\n");
+
+	kfree(save_connectors);
+	kfree(save_encoders);
+	kfree(save_crtcs);
+
+	DRM_DEBUG_KMS("=== Finished dm_set_config ===\n");
+	return ret;
+}
+
+void amdgpu_dm_crtc_destroy(struct drm_crtc *crtc)
+{
+	struct amdgpu_crtc *dm_crtc = to_amdgpu_crtc(crtc);
+
+	drm_crtc_cleanup(crtc);
+	destroy_workqueue(dm_crtc->pflip_queue);
+	kfree(crtc);
+}
+
+
+
+/* Implemented only the options currently availible for the driver */
+static const struct drm_crtc_funcs amdgpu_dm_crtc_funcs = {
+/*	.save = NULL,
+	.restore = NULL,
+	.reset = NULL,*/
+	.cursor_set = dm_crtc_cursor_set,
+	.cursor_move = dm_crtc_cursor_move,
+	.destroy = amdgpu_dm_crtc_destroy,
+	.set_config = amdgpu_dm_set_config,
+	.page_flip = amdgpu_crtc_page_flip /* this function is common for
+				all implementations of DCE code (original and
+				DAL) */
+
+	/*.set_property = NULL*/
+};
+
+
+/*
+ * dm_add_display_info
+ *
+ * @brief:
+ * Update required display info from mode
+ *
+ * @param
+ * disp_info: [out] display information
+ * mode: [in] mode containing display information
+ *
+ * @return
+ * void
+ */
+void dm_add_display_info(
+		struct drm_display_info *disp_info,
+		struct amdgpu_display_manager *dm,
+		uint32_t display_index)
+{
+}
+
+static inline bool compare_mode_query_info_and_mode_timing(
+	const struct render_mode *rm,
+	const struct refresh_rate *rr,
+	const struct mode_info *mi)
+{
+	if (rm->view.height == mi->pixel_height &&
+		rm->view.width == mi->pixel_width &&
+		rr->field_rate == mi->field_rate &&
+		rr->INTERLACED == mi->flags.INTERLACE &&
+		rr->VIDEO_OPTIMIZED_RATE == mi->flags.VIDEO_OPTIMIZED_RATE)
+		return true;
+	else
+		return false;
+}
+
+static inline void fill_drm_mode_info(
+	struct drm_display_mode *drm_mode,
+	const struct mode_timing *mode_timing,
+	const struct refresh_rate *rr)
+{
+	drm_mode->hsync_start = mode_timing->mode_info.pixel_width +
+		mode_timing->crtc_timing.h_front_porch;
+	drm_mode->hsync_end = mode_timing->mode_info.pixel_width +
+		mode_timing->crtc_timing.h_front_porch +
+		mode_timing->crtc_timing.h_sync_width;
+	drm_mode->htotal = mode_timing->crtc_timing.h_total;
+	drm_mode->hdisplay = mode_timing->crtc_timing.h_addressable;
+	drm_mode->vsync_start = mode_timing->mode_info.pixel_height +
+		mode_timing->crtc_timing.v_front_porch;
+	drm_mode->vsync_end = mode_timing->mode_info.pixel_height +
+		mode_timing->crtc_timing.v_front_porch +
+		mode_timing->crtc_timing.v_sync_width;
+	drm_mode->vtotal = mode_timing->crtc_timing.v_total;
+	drm_mode->vdisplay = mode_timing->crtc_timing.v_addressable;
+
+	drm_mode->clock = mode_timing->crtc_timing.pix_clk_khz;
+	drm_mode->vrefresh = rr->field_rate;
+	if (mode_timing->crtc_timing.flags.HSYNC_POSITIVE_POLARITY)
+		drm_mode->flags |= DRM_MODE_FLAG_PHSYNC;
+	if (mode_timing->crtc_timing.flags.VSYNC_POSITIVE_POLARITY)
+		drm_mode->flags |= DRM_MODE_FLAG_PVSYNC;
+	if (mode_timing->crtc_timing.flags.INTERLACE)
+		drm_mode->flags |= DRM_MODE_FLAG_INTERLACE;
+	if (mode_timing->mode_info.flags.PREFERRED)
+		drm_mode->type |= DRM_MODE_TYPE_PREFERRED;
+
+	drm_mode_set_name(drm_mode);
+}
+
+/**
+ * amdgpu_display_manager_add_mode -
+ *      add mode for the connector
+ * @connector: drm connector
+ * @mode: the mode
+ *
+ * Add the specified modes to the connector's mode list.
+ *
+ * Return 0 on success.
+ */
+static int dm_add_mode(
+	struct drm_connector *connector,
+	const struct path_mode *pm,
+	const struct render_mode *rm,
+	const struct refresh_rate *rr,
+	const struct dal_timing_list_query *tlq)
+{
+	struct drm_display_mode *drm_mode;
+
+	if (!pm || !tlq)
+		return -1;
+
+	if (pm->scaling == SCALING_TRANSFORMATION_IDENTITY) {
+		/* in this case crtc timing is the same as render mode,
+		 * we can use it for timing values calculations right away
+		 */
+		drm_mode = drm_mode_create(connector->dev);
+
+		if (!drm_mode)
+			return -1;
+
+		fill_drm_mode_info(drm_mode, pm->mode_timing, rr);
+	} else {
+		uint32_t tlq_index;
+		uint32_t tlq_count =
+			dal_timing_list_query_get_mode_timing_count(tlq);
+		const struct mode_timing *tlq_mt;
+
+		/* trying to find proper mode_timing */
+		for (tlq_index = 0; tlq_index < tlq_count; tlq_index++) {
+			tlq_mt = dal_timing_list_query_get_mode_timing_at_index(
+				tlq, tlq_index);
+
+			if (compare_mode_query_info_and_mode_timing(
+				rm,
+				rr,
+				&tlq_mt->mode_info))
+				break;
+		}
+
+		if (tlq_index == tlq_count) {
+			/* in this case we did not find mode_timing in timing
+			 * list query and rely on gtf (generalized timing
+			 * formula) to calculate timing values reported to OS
+			 */
+
+			drm_mode =
+				drm_gtf_mode(
+					connector->dev,
+					rm->view.width,
+					rm->view.height,
+					rr->field_rate,
+					rr->INTERLACED,
+					0);
+
+			if (!drm_mode)
+				return -1;
+		} else {
+			/* in this case we found mode_timing in timing list
+			 * query and can use it to fill mode information
+			 */
+			drm_mode = drm_mode_create(connector->dev);
+
+			if (!drm_mode)
+				return -1;
+
+			fill_drm_mode_info(drm_mode, tlq_mt, rr);
+		}
+	}
+
+	list_add(&drm_mode->head, &connector->modes);
+
+	return 0;
+}
+
+static void add_to_mq_helper(void *what, const struct path_mode *pm)
+{
+	dal_mode_query_pin_path_mode(what, pm);
+}
+
+
+static void amdgpu_dm_connector_dpms(struct drm_connector *connector, int mode)
+{
+	/* TODO: uncomment these definitions to enable DPMS call later
+	struct amdgpu_connector *aconnector = to_amdgpu_connector(connector);
+	struct amdgpu_device *adev = connector->dev->dev_private;
+	uint32_t display_index = aconnector->connector_id; */
+	enum dal_power_state ps;
+
+	if (mode == connector->dpms)
+		return;
+
+	switch (mode) {
+	case DRM_MODE_DPMS_ON:
+		ps = DAL_POWER_STATE_ON;
+		break;
+	case DRM_MODE_DPMS_OFF:
+		ps = DAL_POWER_STATE_OFF;
+		break;
+	case DRM_MODE_DPMS_STANDBY:
+		ps = DAL_POWER_STATE_STANDBY;
+		break;
+	case DRM_MODE_DPMS_SUSPEND:
+		ps = DAL_POWER_STATE_SUSPEND;
+		break;
+	default:
+		DRM_ERROR("Invalid DPMS mode requested\n");
+		return;
+	}
+
+	/* dal_set_display_dpms(adev->dm.dal, display_index, ps); */
+
+	connector->dpms = mode;
+}
+
+static enum drm_connector_status
+amdgpu_dm_connector_detect(struct drm_connector *connector, bool force)
+{
+	struct amdgpu_connector *aconnector =
+		to_amdgpu_connector(connector);
+	struct drm_device *dev = connector->dev;
+	struct amdgpu_device *adev = dev->dev_private;
+	bool connected;
+	uint32_t display_index = aconnector->connector_id;
+
+	if (!adev->dm.dal)
+		return 0;
+
+	connected = (dal_get_connected_targets_vector(adev->dm.dal)
+			& (1 << display_index));
+
+	return (connected ? connector_status_connected :
+			connector_status_disconnected);
+}
+static void amdgpu_dm_crtc_disable(struct drm_crtc *crtc)
+{
+	DRM_DEBUG_KMS("NOT IMPLEMENTED\n");
+}
+
+static void amdgpu_dm_encoder_disable(struct drm_encoder *encoder)
+{
+	DRM_DEBUG_KMS("NOT IMPLEMENTED\n");
+}
+
+/**
+ * amdgpu_display_manager_fill_modes - get complete set of
+ * display timing modes per drm_connector
+ *
+ * @connector: DRM device connector
+ * @maxX: max width for modes
+ * @maxY: max height for modes
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Query connector and try to detect modes on it. Received
+ * modes are assumed to be filtered and validated and supported
+ * by the connector assuming set_mode for that connector will
+ * come immediately after this function call.
+ *
+ * Therefore all these modes will be put into the normal modes
+ * list.
+ *
+ * Intended to be used either at bootup time or when major configuration
+ * changes have occurred.
+ *
+ *
+ * RETURNS:
+ * Number of modes found on @connector.
+ */
+int amdgpu_display_manager_fill_modes(struct drm_connector *connector,
+				      uint32_t maxX, uint32_t maxY)
+{
+	struct amdgpu_connector *aconnector =
+		to_amdgpu_connector(connector);
+	struct drm_device *dev = connector->dev;
+	struct amdgpu_device *adev = dev->dev_private;
+	struct drm_display_mode *mode, *t;
+	bool stop;
+	unsigned int non_filtered_modes_num = 0;
+	struct mode_query *mq;
+	struct topology tp;
+	struct dal_timing_list_query *tlq;
+
+	if (!adev->dm.dal)
+		return 0;
+
+	DRM_DEBUG_KMS("[CONNECTOR:%d, DISPLAY_IDX: %d]\n",
+		connector->connector_type_id, aconnector->connector_id);
+
+	/* clean all the previous modes on this connector */
+	list_for_each_entry_safe(mode, t, &connector->modes, head) {
+		list_del(&mode->head);
+		drm_mode_debug_printmodeline(mode);
+		DRM_DEBUG_KMS("Not using %s mode %d\n",
+				mode->name, mode->status);
+		drm_mode_destroy(dev, mode);
+	}
+
+
+	if (connector->status == connector_status_disconnected) {
+		DRM_DEBUG_KMS("[CONNECTOR:%d] disconnected\n",
+			connector->connector_type_id);
+		drm_mode_connector_update_edid_property(connector, NULL);
+		goto prune;
+	}
+
+	/* get the mode list from DAL, iterate over it and add
+	the modes to drm connector mode list */
+
+	/* the critical assumtion here is that the returned list is
+	clean: no duplicates, all the modes are valid, ordered, and
+	can be actually set on the hardware */
+
+	init_dal_topology(&adev->dm, &tp, aconnector->connector_id);
+	mq = dal_get_mode_query(adev->dm.dal, &tp, adev->dm.mode_query_option);
+
+	if (!mq)
+		goto prune;
+
+	tlq = dal_create_timing_list_query(
+		adev->dm.dal,
+		aconnector->connector_id);
+
+	if (!tlq)
+		goto free_mode_query;
+
+	dal_pin_active_path_modes(
+		adev->dm.dal,
+		mq,
+		aconnector->connector_id,
+		add_to_mq_helper);
+
+	if (!dal_mode_query_select_first(mq))
+		goto free_timing_list_query;
+
+	stop = false;
+
+	do {
+		const struct render_mode *rm =
+			dal_mode_query_get_current_render_mode(mq);
+
+		if (rm->pixel_format != PIXEL_FORMAT_ARGB8888)
+			continue;
+
+		do {
+			const struct refresh_rate *rr =
+				dal_mode_query_get_current_refresh_rate(mq);
+			const struct path_mode_set *pms =
+				dal_mode_query_get_current_path_mode_set(mq);
+			const struct path_mode *pm =
+				dal_pms_get_path_mode_for_display_index(
+					pms,
+					aconnector->connector_id);
+
+			const struct mode_timing *mt = pm->mode_timing;
+
+			if (mt->mode_info.pixel_height > maxY ||
+				mt->mode_info.pixel_width > maxX ||
+				mt->mode_info.flags.INTERLACE)
+				continue;
+
+			if (dm_add_mode(connector, pm, rm, rr, tlq))
+				stop = true;
+			else
+				++non_filtered_modes_num;
+		} while (!stop && dal_mode_query_select_next_refresh_rate(mq));
+	} while (!stop && dal_mode_query_select_next_render_mode(mq));
+
+	if (stop)
+		DRM_ERROR("KMS: failed to add mode on [CONNECTOR: %d, DISPLAY_IDX: %d]\n",
+			connector->connector_type_id,
+			aconnector->connector_id);
+
+free_timing_list_query:
+	dal_timing_list_query_destroy(&tlq);
+
+free_mode_query:
+	dal_mode_query_destroy(&mq);
+
+prune:
+	DRM_DEBUG_KMS("[CONNECTOR:%d] probed modes :\n",
+		      connector->connector_type_id);
+
+	list_for_each_entry(mode, &connector->modes, head) {
+		drm_mode_set_crtcinfo(mode, 0);
+		drm_mode_debug_printmodeline(mode);
+	}
+
+	return non_filtered_modes_num;
+}
+
+static int amdgpu_dm_connector_set_property(struct drm_connector *connector,
+					  struct drm_property *property,
+					  uint64_t val)
+{
+	DRM_ERROR("NOT IMPLEMENTED\n");
+	return 0;
+}
+
+void amdgpu_dm_connector_destroy(struct drm_connector *connector)
+{
+	/*drm_sysfs_connector_remove(connector);*/
+	drm_connector_cleanup(connector);
+	kfree(connector);
+}
+
+static void amdgpu_dm_connector_force(struct drm_connector *connector)
+{
+	DRM_ERROR("NOT IMPLEMENTED\n");
+}
+
+static const struct drm_connector_funcs amdgpu_dm_connector_funcs = {
+	.dpms = amdgpu_dm_connector_dpms,
+/*	.save = NULL,
+	.restore = NULL,
+	.reset = NULL,*/
+	.detect = amdgpu_dm_connector_detect,
+	.fill_modes = amdgpu_display_manager_fill_modes,
+	.set_property = amdgpu_dm_connector_set_property,
+	.destroy = amdgpu_dm_connector_destroy,
+	.force = amdgpu_dm_connector_force
+};
+
+static const struct drm_crtc_helper_funcs amdgpu_dm_crtc_helper_funcs = {
+	.disable = amdgpu_dm_crtc_disable,
+	.load_lut = NULL
+};
+
+static const struct drm_encoder_helper_funcs dm_encoder_helper_funcs = {
+	.disable = amdgpu_dm_encoder_disable,
+};
+
+
+int amdgpu_dm_crtc_init(struct amdgpu_display_manager *dm,
+			struct amdgpu_crtc *acrtc,
+			int display_idx)
+{
+	int res = drm_crtc_init(
+			dm->ddev,
+			&acrtc->base,
+			&amdgpu_dm_crtc_funcs);
+
+	if (res)
+		goto fail;
+
+	drm_crtc_helper_add(&acrtc->base, &amdgpu_dm_crtc_helper_funcs);
+
+	acrtc->max_cursor_width = 128;
+	acrtc->max_cursor_height = 128;
+
+	acrtc->crtc_id = display_idx;
+	acrtc->base.enabled = false;
+
+	dm->adev->mode_info.crtcs[display_idx] = acrtc;
+	drm_mode_crtc_set_gamma_size(&acrtc->base, 256);
+
+	acrtc->pflip_queue =
+		create_singlethread_workqueue("amdgpu-pageflip-queue");
+
+	return 0;
+fail:
+	acrtc->crtc_id = -1;
+	return res;
+}
+
+static struct drm_encoder *best_encoder(struct drm_connector *connector)
+{
+	int enc_id = connector->encoder_ids[0];
+	struct drm_mode_object *obj;
+	struct drm_encoder *encoder;
+
+	DRM_DEBUG_KMS("Finding the best encoder\n");
+
+	/* pick the encoder ids */
+	if (enc_id) {
+		obj = drm_mode_object_find(connector->dev, enc_id, DRM_MODE_OBJECT_ENCODER);
+		if (!obj) {
+			DRM_ERROR("Couldn't find a matching encoder for our connector\n");
+			return NULL;
+		}
+		encoder = obj_to_encoder(obj);
+		return encoder;
+	}
+	DRM_ERROR("No encoder id\n");
+	return NULL;
+}
+
+
+static const struct drm_connector_helper_funcs
+amdgpu_dm_connector_helper_funcs = {
+	.best_encoder = best_encoder
+};
+
+static int to_drm_connector_type(enum signal_type st)
+{
+	switch (st) {
+	case SIGNAL_TYPE_HDMI_TYPE_A:
+		return DRM_MODE_CONNECTOR_HDMIA;
+	case SIGNAL_TYPE_EDP:
+		return DRM_MODE_CONNECTOR_eDP;
+	case SIGNAL_TYPE_RGB:
+		return DRM_MODE_CONNECTOR_VGA;
+	case SIGNAL_TYPE_DISPLAY_PORT:
+	case SIGNAL_TYPE_DISPLAY_PORT_MST:
+		return DRM_MODE_CONNECTOR_DisplayPort;
+	case SIGNAL_TYPE_DVI_DUAL_LINK:
+	case SIGNAL_TYPE_DVI_SINGLE_LINK:
+	case SIGNAL_TYPE_DVI_SINGLE_LINK1:
+		return DRM_MODE_CONNECTOR_DVID;
+
+	default:
+		return DRM_MODE_CONNECTOR_Unknown;
+	}
+}
+
+int amdgpu_dm_connector_init(
+	struct amdgpu_display_manager *dm,
+	struct amdgpu_connector *aconnector,
+	int display_idx,
+	bool is_connected,
+	struct amdgpu_encoder *aencoder)
+{
+	int res, connector_type;
+	enum signal_type st = SIGNAL_TYPE_HDMI_TYPE_A;
+
+	DRM_DEBUG_KMS("amdgpu_dm_connector_init\n");
+
+	if (dm->dal != NULL)
+		st = dal_get_display_signal(dm->dal, display_idx);
+	connector_type = to_drm_connector_type(st);
+
+	res = drm_connector_init(
+			dm->ddev,
+			&aconnector->base,
+			&amdgpu_dm_connector_funcs,
+			connector_type);
+
+	if (res) {
+		DRM_ERROR("connector_init failed\n");
+		aconnector->connector_id = -1;
+		return res;
+	}
+
+	drm_connector_helper_add(
+			&aconnector->base,
+			&amdgpu_dm_connector_helper_funcs);
+
+	aconnector->connector_id = display_idx;
+	aconnector->base.interlace_allowed = true;
+	aconnector->base.doublescan_allowed = true;
+	aconnector->hpd.hpd = display_idx; /* maps to 'enum amdgpu_hpd_id' */
+
+	if (is_connected)
+		aconnector->base.status = connector_status_connected;
+	else
+		aconnector->base.status = connector_status_disconnected;
+
+	/*configure suport HPD hot plug connector_>polled default value is 0
+	 * which means HPD hot plug not supported*/
+	switch (connector_type) {
+	case DRM_MODE_CONNECTOR_HDMIA:
+		aconnector->base.polled = DRM_CONNECTOR_POLL_HPD;
+		break;
+	case DRM_MODE_CONNECTOR_DisplayPort:
+		aconnector->base.polled = DRM_CONNECTOR_POLL_HPD;
+		break;
+	case DRM_MODE_CONNECTOR_DVID:
+		aconnector->base.polled = DRM_CONNECTOR_POLL_HPD;
+		break;
+	default:
+		break;
+	}
+
+	/* TODO: Don't do this manually anymore
+	aconnector->base.encoder = &aencoder->base;
+	*/
+
+	drm_mode_connector_attach_encoder(
+		&aconnector->base, &aencoder->base);
+
+	/*drm_sysfs_connector_add(&dm_connector->base);*/
+
+	/* TODO: this switch should be updated during hotplug/unplug*/
+	if (dm->dal != NULL && is_connected) {
+		DRM_DEBUG_KMS("Connector is connected\n");
+		drm_mode_connector_update_edid_property(
+			&aconnector->base,
+			(struct edid *)
+			dal_get_display_edid(dm->dal, display_idx, NULL));
+	}
+
+	drm_connector_register(&aconnector->base);
+
+	return 0;
+}
+
+int amdgpu_dm_encoder_init(
+	struct drm_device *dev,
+	struct amdgpu_encoder *aencoder,
+	int display_idx,
+	struct amdgpu_crtc *acrtc)
+{
+	int res = drm_encoder_init(dev,
+				   &aencoder->base,
+				   &amdgpu_dm_encoder_funcs,
+				   DRM_MODE_ENCODER_TMDS);
+
+	aencoder->base.possible_crtcs = 1 << display_idx;
+	aencoder->base.crtc = &acrtc->base;
+
+	if (!res)
+		aencoder->encoder_id = display_idx;
+
+	else
+		aencoder->encoder_id = -1;
+
+	drm_encoder_helper_add(&aencoder->base, &dm_encoder_helper_funcs);
+
+	return res;
+}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.h
new file mode 100644
index 0000000..fbf2215
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_dm_types.h
@@ -0,0 +1,74 @@
+/*
+ * Copyright 2012-13 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+
+#ifndef __AMDGPU_DM_TYPES_H__
+#define __AMDGPU_DM_TYPES_H__
+
+#include <drm/drmP.h>
+
+struct plane_addr_flip_info;
+struct amdgpu_framebuffer;
+struct amdgpu_display_manager;
+
+/*TODO Jodan Hersen use the one in amdgpu_dm*/
+int amdgpu_dm_crtc_init(struct amdgpu_display_manager *dm,
+			struct amdgpu_crtc *amdgpu_crtc,
+			int display_idx);
+int amdgpu_dm_connector_init(struct amdgpu_display_manager *dm,
+			struct amdgpu_connector *amdgpu_connector,
+			int display_idx,
+			bool is_connected,
+			struct amdgpu_encoder *amdgpu_encoder);
+int amdgpu_dm_encoder_init(struct drm_device *dev,
+			struct amdgpu_encoder *amdgpu_encoder,
+			int display_idx,
+			struct amdgpu_crtc *amdgpu_crtc);
+
+
+void amdgpu_dm_fill_surface_address(struct drm_crtc *crtc,
+			struct plane_addr_flip_info *info,
+			struct amdgpu_framebuffer *afb,
+			struct drm_framebuffer *old_fb);
+
+void amdgpu_dm_crtc_destroy(struct drm_crtc *crtc);
+void amdgpu_dm_connector_destroy(struct drm_connector *connector);
+void amdgpu_dm_encoder_destroy(struct drm_encoder *encoder);
+
+bool amdgpu_dm_mode_reset(struct drm_crtc *crtc);
+
+bool amdgpu_dm_mode_set(
+	struct drm_crtc *crtc,
+	struct drm_display_mode *mode,
+	int x,
+	int y,
+	struct drm_framebuffer *old_fb);
+
+void dm_add_display_info(
+	struct drm_display_info *disp_info,
+	struct amdgpu_display_manager *dm,
+	uint32_t display_index);
+
+#endif		/* __AMDGPU_DM_TYPES_H__ */
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c
index 0aba8e9..8d63a8e 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c
@@ -35,6 +35,10 @@
 
 #include <linux/pm_runtime.h>
 
+#ifdef CONFIG_DRM_AMD_DAL
+#include "amdgpu_dm_irq.h"
+#endif
+
 #define AMDGPU_WAIT_IDLE_TIMEOUT 200
 
 /*
@@ -229,7 +233,25 @@ int amdgpu_irq_init(struct amdgpu_device *adev)
 		}
 	}
 
-	INIT_WORK(&adev->hotplug_work, amdgpu_hotplug_work_func);
+#ifdef CONFIG_DRM_AMD_DAL
+			switch(adev->asic_type)
+			{
+			case CHIP_CARRIZO:
+				/* DAL handles DCE11 and up.
+				 * See amdgpu_dm_irq.c. */
+				break;
+			default:
+				/* pre DCE11 */
+				INIT_WORK(&adev->hotplug_work,
+						amdgpu_hotplug_work_func);
+				break;
+			}
+#else
+			INIT_WORK(&adev->hotplug_work,
+					amdgpu_hotplug_work_func);
+#endif
+
+
 	INIT_WORK(&adev->reset_work, amdgpu_irq_reset_work_func);
 
 	adev->irq.installed = true;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
index 5533434..bb94bda 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c
@@ -672,6 +672,12 @@ int amdgpu_get_vblank_timestamp_kms(struct drm_device *dev, int crtc,
 
 	/* Get associated drm_crtc: */
 	drmcrtc = &adev->mode_info.crtcs[crtc]->base;
+	if (!drmcrtc) {
+		/* This can occur on driver load if some component fails to
+		 * initialize completely and driver is unloaded */
+		DRM_ERROR("Uninitialized crtc %d\n", crtc);
+		return -EINVAL;
+	}
 
 	/* Helper routine in DRM core does all the work: */
 	return drm_calc_vbltimestamp_from_scanoutpos(dev, crtc, max_error,
diff --git a/drivers/gpu/drm/amd/amdgpu/vi.c b/drivers/gpu/drm/amd/amdgpu/vi.c
index a6bca56..94f71d7 100644
--- a/drivers/gpu/drm/amd/amdgpu/vi.c
+++ b/drivers/gpu/drm/amd/amdgpu/vi.c
@@ -74,6 +74,7 @@
 #if defined(CONFIG_DRM_AMD_ACP)
 #include "amdgpu_acp.h"
 #endif
+#include "amdgpu_dm.h"
 
 /*
  * Indirect registers accessor
@@ -592,12 +593,12 @@ u32 vi_gpu_check_soft_reset(struct amdgpu_device *adev)
 			reset_mask |= AMDGPU_RESET_VCE1;
 
 	}
+#endif
 
 	if (adev->asic_type != CHIP_TOPAZ) {
 		if (amdgpu_display_is_display_hung(adev))
 			reset_mask |= AMDGPU_RESET_DISPLAY;
 	}
-#endif
 
 	/* Skip MC reset as it's mostly likely not hung, just busy */
 	if (reset_mask & AMDGPU_RESET_MC) {
@@ -1140,7 +1141,11 @@ static const struct amdgpu_ip_block_version cz_ip_blocks[] =
 		.major = 11,
 		.minor = 0,
 		.rev = 0,
+#ifdef CONFIG_DRM_AMD_DAL
+		.funcs = &amdgpu_dm_funcs,
+#else
 		.funcs = &dce_v11_0_ip_funcs,
+#endif
 	},
 	{
 		.type = AMD_IP_BLOCK_TYPE_GFX,
-- 
1.9.1

