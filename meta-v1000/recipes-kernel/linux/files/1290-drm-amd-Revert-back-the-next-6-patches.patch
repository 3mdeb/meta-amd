From 8785d6fc39ac5588a5f3d3adee11b67c0002e252 Mon Sep 17 00:00:00 2001
From: Kalyan Alle <kalyan.alle@amd.com>
Date: Thu, 22 Dec 2016 12:47:34 +0530
Subject: [PATCH 1290/1722] drm/amd: Revert back the next 6 patches

These 6 patches are reverted to reposition them

Signed-off-by: Kalyan Alle <kalyan.alle@amd.com
---
 drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c              |  18 +-
 drivers/gpu/drm/amd/dal/dc/core/dc.c               |  80 +++----
 drivers/gpu/drm/amd/dal/dc/core/dc_resource.c      |  11 -
 drivers/gpu/drm/amd/dal/dc/dc_types.h              |   2 -
 drivers/gpu/drm/amd/dal/dc/inc/core_dc.h           |   1 -
 drivers/gpu/drm/amd/dal/dc/inc/resource.h          |   3 +-
 drivers/gpu/drm/amd/dal/include/i2caux_interface.h |   1 -
 drivers/gpu/drm/drm_gem.c                          | 243 ++++++---------------
 include/drm/drm_gem.h                              |  24 +-
 9 files changed, 132 insertions(+), 251 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
index 8142c38..a0202df 100644
--- a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c
@@ -5930,24 +5930,29 @@ static void gfx_v8_0_update_coarse_grain_clock_gating(struct amdgpu_device *adev
 	adev->gfx.rlc.funcs->enter_safe_mode(adev);
 
 	if (enable && (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGCG)) {
+		/* 1 enable cntx_empty_int_enable/cntx_busy_int_enable/
+		 * Cmp_busy/GFX_Idle interrupts
+		 */
+		gfx_v8_0_enable_gui_idle_interrupt(adev, true);
+
 		temp1 = data1 =	RREG32(mmRLC_CGTT_MGCG_OVERRIDE);
 		data1 &= ~RLC_CGTT_MGCG_OVERRIDE__CGCG_MASK;
 		if (temp1 != data1)
 			WREG32(mmRLC_CGTT_MGCG_OVERRIDE, data1);
 
-		/* : wait for RLC_SERDES_CU_MASTER & RLC_SERDES_NONCU_MASTER idle */
+		/* 2 wait for RLC_SERDES_CU_MASTER & RLC_SERDES_NONCU_MASTER idle */
 		gfx_v8_0_wait_for_rlc_serdes(adev);
 
-		/* 2 - clear cgcg override */
+		/* 3 - clear cgcg override */
 		gfx_v8_0_send_serdes_cmd(adev, BPM_REG_CGCG_OVERRIDE, CLE_BPM_SERDES_CMD);
 
 		/* wait for RLC_SERDES_CU_MASTER & RLC_SERDES_NONCU_MASTER idle */
 		gfx_v8_0_wait_for_rlc_serdes(adev);
 
-		/* 3 - write cmd to set CGLS */
+		/* 4 - write cmd to set CGLS */
 		gfx_v8_0_send_serdes_cmd(adev, BPM_REG_CGLS_EN, SET_BPM_SERDES_CMD);
 
-		/* 4 - enable cgcg */
+		/* 5 - enable cgcg */
 		data |= RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
 
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGLS) {
@@ -5965,11 +5970,6 @@ static void gfx_v8_0_update_coarse_grain_clock_gating(struct amdgpu_device *adev
 
 		if (temp != data)
 			WREG32(mmRLC_CGCG_CGLS_CTRL, data);
-
-		/* 5 enable cntx_empty_int_enable/cntx_busy_int_enable/
-		 * Cmp_busy/GFX_Idle interrupts
-		 */
-		gfx_v8_0_enable_gui_idle_interrupt(adev, true);
 	} else {
 		/* disable cntx_empty_int_enable & GFX Idle interrupt */
 		gfx_v8_0_enable_gui_idle_interrupt(adev, false);
diff --git a/drivers/gpu/drm/amd/dal/dc/core/dc.c b/drivers/gpu/drm/amd/dal/dc/core/dc.c
index eacacb2..76dc9ac 100644
--- a/drivers/gpu/drm/amd/dal/dc/core/dc.c
+++ b/drivers/gpu/drm/amd/dal/dc/core/dc.c
@@ -561,30 +561,15 @@ ctx_fail:
 static void destruct(struct core_dc *dc)
 {
 	resource_validate_ctx_destruct(dc->current_context);
-	
+	dm_free(dc->current_context);
 	dm_free(dc->temp_flip_context);
-        dc->current_context = NULL;
-	
+	dc->current_context = NULL;
 	destroy_links(dc);
-	
-	if (dc->res_pool)
-                dc->res_pool->funcs->destroy(&dc->res_pool);
-
-        if (dc->ctx->gpio_service)
-                dal_gpio_service_destroy(&dc->ctx->gpio_service);
-
-        if (dc->ctx->i2caux)
-                dal_i2caux_destroy(&dc->ctx->i2caux);
-
-        if (dc->ctx->created_bios)
-                dal_bios_parser_destroy(&dc->ctx->dc_bios);
-
-        if (dc->ctx->logger)
-                dal_logger_destroy(&dc->ctx->logger);
-	
-	dm_free(dc->current_context);
+	dc->res_pool->funcs->destroy(&dc->res_pool);
+	dal_logger_destroy(&dc->ctx->logger);
+	if (dc->ctx->created_bios)
+		dal_bios_parser_destroy(&dc->ctx->dc_bios);
 	dm_free(dc->ctx);
-
 	dc->ctx = NULL;
 }
 
@@ -1339,7 +1324,9 @@ void dc_update_surfaces_for_target(struct dc *dc, struct dc_surface_update *upda
 	struct validate_context *context = core_dc->temp_flip_context;
 	int i, j;
 	bool is_new_pipe_surface[MAX_SURFACES];
-        const struct dc_surface *new_surfaces[MAX_SURFACES] = { 0 };
+
+	for (j = 0; j < MAX_SURFACES; j++)
+		is_new_pipe_surface[j] = true;
 
 	*context = *core_dc->current_context;
 
@@ -1355,22 +1342,8 @@ void dc_update_surfaces_for_target(struct dc *dc, struct dc_surface_update *upda
 				&context->res_ctx.pipe_ctx[cur_pipe->bottom_pipe->pipe_idx];
 	}
 
-	for (j = 0; j < MAX_SURFACES; j++)
-		is_new_pipe_surface[j] = true;
-
-	for (i = 0 ; i < surface_count; i++) {
-		struct core_surface *surface = DC_SURFACE_TO_CORE(updates[i].surface);
-
-		new_surfaces[i] = updates[i].surface;
-		for (j = 0; j < context->res_ctx.pool->pipe_count; j++) {
-			struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j];
-
-			if (surface == pipe_ctx->surface)
-				is_new_pipe_surface[i] = false;
-		}
-	}
-
 	if (dc_target) {
+		const struct dc_surface *new_surfaces[MAX_SURFACES] = { 0 };
 		struct core_target *target = DC_TARGET_TO_CORE(dc_target);
 
 		if (core_dc->current_context->target_count == 0)
@@ -1383,6 +1356,18 @@ void dc_update_surfaces_for_target(struct dc *dc, struct dc_surface_update *upda
 		if (i == core_dc->current_context->target_count)
 			return;
 
+		for (i = 0 ; i < surface_count; i++) {
+			struct core_surface *surface = DC_SURFACE_TO_CORE(updates[i].surface);
+
+			new_surfaces[i] = updates[i].surface;
+			for (j = 0; j < context->res_ctx.pool->pipe_count; j++) {
+				struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j];
+
+				if (surface == pipe_ctx->surface)
+					is_new_pipe_surface[i] = false;
+			}
+		}
+
 		if (!resource_attach_surfaces_to_context(
 				new_surfaces, surface_count, dc_target, context)) {
 			BREAK_TO_DEBUGGER();
@@ -1567,24 +1552,13 @@ void dc_flip_surface_addrs(
 		struct dc_flip_addrs flip_addrs[],
 		uint32_t count)
 {
-	struct core_dc *core_dc = DC_TO_CORE(dc);
-	int i, j;
-
+	int i;
+	struct dc_surface_update updates[MAX_SURFACE_NUM] = { { 0 } };
 	for (i = 0; i < count; i++) {
-		struct core_surface *surface = DC_SURFACE_TO_CORE(surfaces[i]);
-
-		surface->public.address = flip_addrs[i].address;
-		surface->public.flip_immediate = flip_addrs[i].flip_immediate;
-
-		for (j = 0; j < core_dc->res_pool->pipe_count; j++) {
-			struct pipe_ctx *pipe_ctx = &core_dc->current_context->res_ctx.pipe_ctx[j];
-
-			if (pipe_ctx->surface != surface)
-				continue;
-
-			core_dc->hwss.update_plane_addr(core_dc, pipe_ctx);
-		}
+		updates[i].flip_addr = &flip_addrs[i];
+		updates[i].surface = surfaces[i];
 	}
+	dc_update_surfaces_for_target(dc, updates, count, NULL);
 }
 
 enum dc_irq_source dc_interrupt_to_irq_source(
diff --git a/drivers/gpu/drm/amd/dal/dc/core/dc_resource.c b/drivers/gpu/drm/amd/dal/dc/core/dc_resource.c
index 9d88540..82cac0a 100644
--- a/drivers/gpu/drm/amd/dal/dc/core/dc_resource.c
+++ b/drivers/gpu/drm/amd/dal/dc/core/dc_resource.c
@@ -121,17 +121,6 @@ struct resource_pool *dc_create_resource_pool(struct adapter_service *adapter_se
 	return false;
 }
 
-void dc_destroy_resource_pool(struct core_dc *dc)
-{
-        if (dc) {
-                if (dc->res_pool)
-                        dc->res_pool->funcs->destroy(&dc->res_pool);
- 
-                if (dc->hwseq)
-                        dm_free(dc->hwseq);
-        }
-}
-
 void resource_unreference_clock_source(
 		struct resource_context *res_ctx,
 		struct clock_source *clock_source)
diff --git a/drivers/gpu/drm/amd/dal/dc/dc_types.h b/drivers/gpu/drm/amd/dal/dc/dc_types.h
index 31f9611..c71f81f 100644
--- a/drivers/gpu/drm/amd/dal/dc/dc_types.h
+++ b/drivers/gpu/drm/amd/dal/dc/dc_types.h
@@ -75,8 +75,6 @@ struct dc_context {
 
 	struct dc_bios *dc_bios;
 	bool created_bios;
-	struct gpio_service *gpio_service;
-        struct i2caux *i2caux;
 };
 
 /*
diff --git a/drivers/gpu/drm/amd/dal/dc/inc/core_dc.h b/drivers/gpu/drm/amd/dal/dc/inc/core_dc.h
index 640d578..6c4f9e1 100644
--- a/drivers/gpu/drm/amd/dal/dc/inc/core_dc.h
+++ b/drivers/gpu/drm/amd/dal/dc/inc/core_dc.h
@@ -39,7 +39,6 @@ struct core_dc {
 
 	/* HW functions */
 	struct hw_sequencer_funcs hwss;
-        struct dce_hwseq *hwseq;
 };
 
 #endif /* __CORE_DC_H__ */
diff --git a/drivers/gpu/drm/amd/dal/dc/inc/resource.h b/drivers/gpu/drm/amd/dal/dc/inc/resource.h
index 5237347..4254f32 100644
--- a/drivers/gpu/drm/amd/dal/dc/inc/resource.h
+++ b/drivers/gpu/drm/amd/dal/dc/inc/resource.h
@@ -42,7 +42,8 @@ struct resource_pool *dc_create_resource_pool(struct adapter_service *adapter_se
 				enum dce_version dc_version,
 				struct hw_asic_id asic_id);
 
-void dc_destroy_resource_pool(struct core_dc *dc);
+void dc_destroy_resource_pool(struct resource_pool **pool,
+				enum dce_version dc_version);
 
 enum dc_status resource_map_pool_resources(
 		const struct core_dc *dc,
diff --git a/drivers/gpu/drm/amd/dal/include/i2caux_interface.h b/drivers/gpu/drm/amd/dal/include/i2caux_interface.h
index 4dc28ed..b23e6c0 100644
--- a/drivers/gpu/drm/amd/dal/include/i2caux_interface.h
+++ b/drivers/gpu/drm/amd/dal/include/i2caux_interface.h
@@ -28,7 +28,6 @@
 
 #include "ddc_interface.h"
 #include "adapter_service_interface.h"
-#include "gpio_service_interface.h" 
 
 #define DEFAULT_AUX_MAX_DATA_SIZE 16
 #define AUX_MAX_DEFER_WRITE_RETRY 20
diff --git a/drivers/gpu/drm/drm_gem.c b/drivers/gpu/drm/drm_gem.c
index 7ba52b3..c7de454 100644
--- a/drivers/gpu/drm/drm_gem.c
+++ b/drivers/gpu/drm/drm_gem.c
@@ -220,9 +220,6 @@ static void drm_gem_object_exported_dma_buf_free(struct drm_gem_object *obj)
 static void
 drm_gem_object_handle_unreference_unlocked(struct drm_gem_object *obj)
 {
-	struct drm_device *dev = obj->dev;
-	bool final = false;
-
 	if (WARN_ON(obj->handle_count == 0))
 		return;
 
@@ -232,39 +229,14 @@ drm_gem_object_handle_unreference_unlocked(struct drm_gem_object *obj)
 	* checked for a name
 	*/
 
-	mutex_lock(&dev->object_name_lock);
+	mutex_lock(&obj->dev->object_name_lock);
 	if (--obj->handle_count == 0) {
 		drm_gem_object_handle_free(obj);
 		drm_gem_object_exported_dma_buf_free(obj);
-		final = true;
 	}
-	mutex_unlock(&dev->object_name_lock);
+	mutex_unlock(&obj->dev->object_name_lock);
 
-	if (final)
-		drm_gem_object_unreference_unlocked(obj);
-}
-
-/*
- * Called at device or object close to release the file's
- * handle references on objects.
- */
-static int
-drm_gem_object_release_handle(int id, void *ptr, void *data)
-{
-	struct drm_file *file_priv = data;
-	struct drm_gem_object *obj = ptr;
-	struct drm_device *dev = obj->dev;
-
-	if (drm_core_check_feature(dev, DRIVER_PRIME))
-		drm_gem_remove_prime_handles(obj, file_priv);
-	drm_vma_node_revoke(&obj->vma_node, file_priv->filp);
-
-	if (dev->driver->gem_close_object)
-		dev->driver->gem_close_object(obj, file_priv);
-
-	drm_gem_object_handle_unreference_unlocked(obj);
-
-	return 0;
+	drm_gem_object_unreference_unlocked(obj);
 }
 
 /**
@@ -272,13 +244,13 @@ drm_gem_object_release_handle(int id, void *ptr, void *data)
  * @filp: drm file-private structure to use for the handle look up
  * @handle: userspace handle to delete
  *
- * Removes the GEM handle from the @filp lookup table which has been added with
- * drm_gem_handle_create(). If this is the last handle also cleans up linked
- * resources like GEM names.
+ * Removes the GEM handle from the @filp lookup table and if this is the last
+ * handle also cleans up linked resources like GEM names.
  */
 int
 drm_gem_handle_delete(struct drm_file *filp, u32 handle)
 {
+	struct drm_device *dev;
 	struct drm_gem_object *obj;
 
 	/* This is gross. The idr system doesn't let us try a delete and
@@ -293,19 +265,25 @@ drm_gem_handle_delete(struct drm_file *filp, u32 handle)
 	spin_lock(&filp->table_lock);
 
 	/* Check if we currently have a reference on the object */
-	obj = idr_replace(&filp->object_idr, NULL, handle);
-	spin_unlock(&filp->table_lock);
-	if (IS_ERR_OR_NULL(obj))
+	obj = idr_find(&filp->object_idr, handle);
+	if (obj == NULL) {
+		spin_unlock(&filp->table_lock);
 		return -EINVAL;
+	}
+	dev = obj->dev;
 
-	/* Release driver's reference and decrement refcount. */
-	drm_gem_object_release_handle(handle, obj, filp);
-
-	/* And finally make the handle available for future allocations. */
-	spin_lock(&filp->table_lock);
+	/* Release reference and decrement refcount. */
 	idr_remove(&filp->object_idr, handle);
 	spin_unlock(&filp->table_lock);
 
+	if (drm_core_check_feature(dev, DRIVER_PRIME))
+		drm_gem_remove_prime_handles(obj, filp);
+	drm_vma_node_revoke(&obj->vma_node, filp->filp);
+
+	if (dev->driver->gem_close_object)
+		dev->driver->gem_close_object(obj, filp);
+	drm_gem_object_handle_unreference_unlocked(obj);
+
 	return 0;
 }
 EXPORT_SYMBOL(drm_gem_handle_delete);
@@ -336,10 +314,6 @@ EXPORT_SYMBOL(drm_gem_dumb_destroy);
  * This expects the dev->object_name_lock to be held already and will drop it
  * before returning. Used to avoid races in establishing new handles when
  * importing an object from either an flink name or a dma-buf.
- *
- * Handles must be release again through drm_gem_handle_delete(). This is done
- * when userspace closes @file_priv for all attached handles, or through the
- * GEM_CLOSE ioctl for individual handles.
  */
 int
 drm_gem_handle_create_tail(struct drm_file *file_priv,
@@ -347,12 +321,9 @@ drm_gem_handle_create_tail(struct drm_file *file_priv,
 			   u32 *handlep)
 {
 	struct drm_device *dev = obj->dev;
-	u32 handle;
 	int ret;
 
 	WARN_ON(!mutex_is_locked(&dev->object_name_lock));
-	if (obj->handle_count++ == 0)
-		drm_gem_object_reference(obj);
 
 	/*
 	 * Get the user-visible handle using idr.  Preload and perform
@@ -362,38 +333,32 @@ drm_gem_handle_create_tail(struct drm_file *file_priv,
 	spin_lock(&file_priv->table_lock);
 
 	ret = idr_alloc(&file_priv->object_idr, obj, 1, 0, GFP_NOWAIT);
-
+	drm_gem_object_reference(obj);
+	obj->handle_count++;
 	spin_unlock(&file_priv->table_lock);
 	idr_preload_end();
-
 	mutex_unlock(&dev->object_name_lock);
-	if (ret < 0)
-		goto err_unref;
-
-	handle = ret;
+	if (ret < 0) {
+		drm_gem_object_handle_unreference_unlocked(obj);
+		return ret;
+	}
+	*handlep = ret;
 
 	ret = drm_vma_node_allow(&obj->vma_node, file_priv->filp);
-	if (ret)
-		goto err_remove;
+	if (ret) {
+		drm_gem_handle_delete(file_priv, *handlep);
+		return ret;
+	}
 
 	if (dev->driver->gem_open_object) {
 		ret = dev->driver->gem_open_object(obj, file_priv);
-		if (ret)
-			goto err_revoke;
+		if (ret) {
+			drm_gem_handle_delete(file_priv, *handlep);
+			return ret;
+		}
 	}
 
-	*handlep = handle;
 	return 0;
-
-err_revoke:
-	drm_vma_node_revoke(&obj->vma_node, file_priv->filp);
-err_remove:
-	spin_lock(&file_priv->table_lock);
-	idr_remove(&file_priv->object_idr, handle);
-	spin_unlock(&file_priv->table_lock);
-err_unref:
-	drm_gem_object_handle_unreference_unlocked(obj);
-	return ret;
 }
 
 /**
@@ -422,10 +387,6 @@ EXPORT_SYMBOL(drm_gem_handle_create);
  * @obj: obj in question
  *
  * This routine frees fake offsets allocated by drm_gem_create_mmap_offset().
- *
- * Note that drm_gem_object_release() already calls this function, so drivers
- * don't have to take care of releasing the mmap offset themselves when freeing
- * the GEM object.
  */
 void
 drm_gem_free_mmap_offset(struct drm_gem_object *obj)
@@ -449,9 +410,6 @@ EXPORT_SYMBOL(drm_gem_free_mmap_offset);
  * This routine allocates and attaches a fake offset for @obj, in cases where
  * the virtual size differs from the physical size (ie. obj->size).  Otherwise
  * just use drm_gem_create_mmap_offset().
- *
- * This function is idempotent and handles an already allocated mmap offset
- * transparently. Drivers do not need to check for this case.
  */
 int
 drm_gem_create_mmap_offset_size(struct drm_gem_object *obj, size_t size)
@@ -473,9 +431,6 @@ EXPORT_SYMBOL(drm_gem_create_mmap_offset_size);
  * structures.
  *
  * This routine allocates and attaches a fake offset for @obj.
- *
- * Drivers can call drm_gem_free_mmap_offset() before freeing @obj to release
- * the fake offset again.
  */
 int drm_gem_create_mmap_offset(struct drm_gem_object *obj)
 {
@@ -544,7 +499,7 @@ struct page **drm_gem_get_pages(struct drm_gem_object *obj)
 
 fail:
 	while (i--)
-		put_page(pages[i]);
+		page_cache_release(pages[i]);
 
 	drm_free_large(pages);
 	return ERR_CAST(p);
@@ -579,23 +534,14 @@ void drm_gem_put_pages(struct drm_gem_object *obj, struct page **pages,
 			mark_page_accessed(pages[i]);
 
 		/* Undo the reference we took when populating the table */
-		put_page(pages[i]);
+		page_cache_release(pages[i]);
 	}
 
 	drm_free_large(pages);
 }
 EXPORT_SYMBOL(drm_gem_put_pages);
 
-/**
- * drm_gem_object_lookup - look up a GEM object from it's handle
- * @filp: DRM file private date
- * @handle: userspace handle
- *
- * Returns:
- *
- * A reference to the object named by the handle if such exists on @filp, NULL
- * otherwise.
- */
+/** Returns a reference to the object named by the handle. */
 struct drm_gem_object *
 drm_gem_object_lookup(struct drm_device *dev, struct drm_file *filp,
 		      u32 handle)
@@ -606,8 +552,12 @@ drm_gem_object_lookup(struct drm_device *dev, struct drm_file *filp,
 
 	/* Check if we currently have a reference on the object */
 	obj = idr_find(&filp->object_idr, handle);
-	if (obj)
-		drm_gem_object_reference(obj);
+	if (obj == NULL) {
+		spin_unlock(&filp->table_lock);
+		return NULL;
+	}
+
+	drm_gem_object_reference(obj);
 
 	spin_unlock(&filp->table_lock);
 
@@ -665,6 +615,7 @@ drm_gem_flink_ioctl(struct drm_device *dev, void *data,
 		return -ENOENT;
 
 	mutex_lock(&dev->object_name_lock);
+	idr_preload(GFP_KERNEL);
 	/* prevent races with concurrent gem_close. */
 	if (obj->handle_count == 0) {
 		ret = -ENOENT;
@@ -672,7 +623,7 @@ drm_gem_flink_ioctl(struct drm_device *dev, void *data,
 	}
 
 	if (!obj->name) {
-		ret = idr_alloc(&dev->object_name_idr, obj, 1, 0, GFP_KERNEL);
+		ret = idr_alloc(&dev->object_name_idr, obj, 1, 0, GFP_NOWAIT);
 		if (ret < 0)
 			goto err;
 
@@ -683,6 +634,7 @@ drm_gem_flink_ioctl(struct drm_device *dev, void *data,
 	ret = 0;
 
 err:
+	idr_preload_end();
 	mutex_unlock(&dev->object_name_lock);
 	drm_gem_object_unreference_unlocked(obj);
 	return ret;
@@ -747,6 +699,29 @@ drm_gem_open(struct drm_device *dev, struct drm_file *file_private)
 	spin_lock_init(&file_private->table_lock);
 }
 
+/*
+ * Called at device close to release the file's
+ * handle references on objects.
+ */
+static int
+drm_gem_object_release_handle(int id, void *ptr, void *data)
+{
+	struct drm_file *file_priv = data;
+	struct drm_gem_object *obj = ptr;
+	struct drm_device *dev = obj->dev;
+
+	if (drm_core_check_feature(dev, DRIVER_PRIME))
+		drm_gem_remove_prime_handles(obj, file_priv);
+	drm_vma_node_revoke(&obj->vma_node, file_priv->filp);
+
+	if (dev->driver->gem_close_object)
+		dev->driver->gem_close_object(obj, file_priv);
+
+	drm_gem_object_handle_unreference_unlocked(obj);
+
+	return 0;
+}
+
 /**
  * drm_gem_release - release file-private GEM resources
  * @dev: drm_device which is being closed by userspace
@@ -764,13 +739,6 @@ drm_gem_release(struct drm_device *dev, struct drm_file *file_private)
 	idr_destroy(&file_private->object_idr);
 }
 
-/**
- * drm_gem_object_release - release GEM buffer object resources
- * @obj: GEM buffer object
- *
- * This releases any structures and resources used by @obj and is the invers of
- * drm_gem_object_init().
- */
 void
 drm_gem_object_release(struct drm_gem_object *obj)
 {
@@ -799,73 +767,13 @@ drm_gem_object_free(struct kref *kref)
 		container_of(kref, struct drm_gem_object, refcount);
 	struct drm_device *dev = obj->dev;
 
-	if (dev->driver->gem_free_object_unlocked) {
-		dev->driver->gem_free_object_unlocked(obj);
-	} else if (dev->driver->gem_free_object) {
-		WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
 
+	if (dev->driver->gem_free_object != NULL)
 		dev->driver->gem_free_object(obj);
-	}
 }
 EXPORT_SYMBOL(drm_gem_object_free);
 
-/**
- * drm_gem_object_unreference_unlocked - release a GEM BO reference
- * @obj: GEM buffer object
- *
- * This releases a reference to @obj. Callers must not hold the
- * dev->struct_mutex lock when calling this function.
- *
- * See also __drm_gem_object_unreference().
- */
-void
-drm_gem_object_unreference_unlocked(struct drm_gem_object *obj)
-{
-	struct drm_device *dev;
-
-	if (!obj)
-		return;
-
-	dev = obj->dev;
-	might_lock(&dev->struct_mutex);
-
-	if (dev->driver->gem_free_object_unlocked)
-		kref_put(&obj->refcount, drm_gem_object_free);
-	else if (kref_put_mutex(&obj->refcount, drm_gem_object_free,
-				&dev->struct_mutex))
-		mutex_unlock(&dev->struct_mutex);
-}
-EXPORT_SYMBOL(drm_gem_object_unreference_unlocked);
-
-/**
- * drm_gem_object_unreference - release a GEM BO reference
- * @obj: GEM buffer object
- *
- * This releases a reference to @obj. Callers must hold the dev->struct_mutex
- * lock when calling this function, even when the driver doesn't use
- * dev->struct_mutex for anything.
- *
- * For drivers not encumbered with legacy locking use
- * drm_gem_object_unreference_unlocked() instead.
- */
-void
-drm_gem_object_unreference(struct drm_gem_object *obj)
-{
-	if (obj) {
-		WARN_ON(!mutex_is_locked(&obj->dev->struct_mutex));
-
-		kref_put(&obj->refcount, drm_gem_object_free);
-	}
-}
-EXPORT_SYMBOL(drm_gem_object_unreference);
-
-/**
- * drm_gem_vm_open - vma->ops->open implementation for GEM
- * @vma: VM area structure
- *
- * This function implements the #vm_operations_struct open() callback for GEM
- * drivers. This must be used together with drm_gem_vm_close().
- */
 void drm_gem_vm_open(struct vm_area_struct *vma)
 {
 	struct drm_gem_object *obj = vma->vm_private_data;
@@ -874,13 +782,6 @@ void drm_gem_vm_open(struct vm_area_struct *vma)
 }
 EXPORT_SYMBOL(drm_gem_vm_open);
 
-/**
- * drm_gem_vm_close - vma->ops->close implementation for GEM
- * @vma: VM area structure
- *
- * This function implements the #vm_operations_struct close() callback for GEM
- * drivers. This must be used together with drm_gem_vm_open().
- */
 void drm_gem_vm_close(struct vm_area_struct *vma)
 {
 	struct drm_gem_object *obj = vma->vm_private_data;
diff --git a/include/drm/drm_gem.h b/include/drm/drm_gem.h
index 89c05cd..15e7f00 100644
--- a/include/drm/drm_gem.h
+++ b/include/drm/drm_gem.h
@@ -139,10 +139,30 @@ drm_gem_object_reference(struct drm_gem_object *obj)
 	kref_get(&obj->refcount);
 }
 
+static inline void
+drm_gem_object_unreference(struct drm_gem_object *obj)
+{
+	if (obj != NULL) {
+		WARN_ON(!mutex_is_locked(&obj->dev->struct_mutex));
 
+		kref_put(&obj->refcount, drm_gem_object_free);
+	}
+}
 
-void drm_gem_object_unreference_unlocked(struct drm_gem_object *obj);
-void drm_gem_object_unreference(struct drm_gem_object *obj);
+static inline void
+drm_gem_object_unreference_unlocked(struct drm_gem_object *obj)
+{
+	struct drm_device *dev;
+
+	if (!obj)
+		return;
+
+	dev = obj->dev;
+	if (kref_put_mutex(&obj->refcount, drm_gem_object_free, &dev->struct_mutex))
+		mutex_unlock(&dev->struct_mutex);
+	else
+		might_lock(&dev->struct_mutex);
+}
 
 int drm_gem_handle_create(struct drm_file *file_priv,
 			  struct drm_gem_object *obj,
-- 
2.7.4

