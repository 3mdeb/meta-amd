From 55b2ea8af4b3818cbaf3e84dcb36360c8b8909c0 Mon Sep 17 00:00:00 2001
From: Harry Wentland <harry.wentland@amd.com>
Date: Wed, 23 Aug 2017 15:53:40 -0400
Subject: [PATCH 3423/5855] drm/amd/dc: Add dc display driver (v2)

Supported DCE versions: 8.0, 10.0, 11.0, 11.2

v2: rebase against 4.11

Signed-off-by: Harry Wentland <harry.wentland@amd.com>
Acked-by: Alex Deucher <alexander.deucher@amd.com>
Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu.h                |   16 +
 drivers/gpu/drm/amd/amdgpu/vi.c                    |    1 +
 .../drm/amd/display/amdgpu_dm/amdgpu_dm_types.c    | 3313 ++++++++++++++++++++
 .../drm/amd/display/amdgpu_dm/amdgpu_dm_types.h    |  101 +
 .../drm/amd/display/dc/basics/register_logger.c    |  197 ++
 .../gpu/drm/amd/display/dc/calcs/bandwidth_calcs.c | 3108 ++++++++++++++++++
 drivers/gpu/drm/amd/display/dc/calcs/gamma_calcs.c | 1382 ++++++++
 drivers/gpu/drm/amd/display/dc/core/dc_target.c    |  334 ++
 drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.c |   62 +
 drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.h |   76 +
 .../drm/amd/display/dc/dce110/dce110_ipp_cursor.c  |  253 ++
 .../drm/amd/display/dc/dce110/dce110_ipp_gamma.c   |  303 ++
 .../drm/amd/display/dc/dce110/dce110_mem_input.c   |  535 ++++
 .../drm/amd/display/dc/dce110/dce110_mem_input.h   |  131 +
 drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.c |   77 +
 drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.h |  149 +
 .../gpu/drm/amd/display/dc/dce110/dce110_opp_csc.c |  363 +++
 .../amd/display/dc/dce110/dce110_opp_formatter.c   |  627 ++++
 .../drm/amd/display/dc/dce110/dce110_opp_regamma.c |  537 ++++
 .../drm/amd/display/dc/dce112/dce112_mem_input.c   |   54 +
 .../drm/amd/display/dc/dce112/dce112_mem_input.h   |   38 +
 drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.c |   72 +
 drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.h |   48 +
 .../amd/display/dc/dce112/dce112_opp_formatter.c   |  215 ++
 drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.c   |   64 +
 drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.h   |   47 +
 .../gpu/drm/amd/display/dc/dce80/dce80_ipp_gamma.c |   76 +
 .../gpu/drm/amd/display/dc/dce80/dce80_mem_input.c |   83 +
 .../gpu/drm/amd/display/dc/dce80/dce80_mem_input.h |   36 +
 drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.c   |  136 +
 drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.h   |  130 +
 .../gpu/drm/amd/display/dc/dce80/dce80_opp_csc.c   |  363 +++
 .../drm/amd/display/dc/dce80/dce80_opp_formatter.c |  577 ++++
 .../drm/amd/display/dc/dce80/dce80_opp_regamma.c   |  543 ++++
 drivers/gpu/drm/amd/display/dc/gpu/Makefile        |   36 +
 .../display/dc/gpu/dce110/display_clock_dce110.c   | 1035 ++++++
 .../display/dc/gpu/dce110/display_clock_dce110.h   |   53 +
 .../display/dc/gpu/dce112/display_clock_dce112.c   |  964 ++++++
 .../display/dc/gpu/dce112/display_clock_dce112.h   |  114 +
 .../amd/display/dc/gpu/dce80/display_clock_dce80.c |  934 ++++++
 .../amd/display/dc/gpu/dce80/display_clock_dce80.h |   57 +
 drivers/gpu/drm/amd/display/dc/gpu/display_clock.c |  217 ++
 drivers/gpu/drm/amd/display/dc/gpu/display_clock.h |   89 +
 drivers/gpu/drm/amd/display/dc/gpu/divider_range.c |  127 +
 drivers/gpu/drm/amd/display/dc/gpu/divider_range.h |   62 +
 .../gpu/drm/amd/display/dc/inc/bandwidth_calcs.h   |  503 +++
 drivers/gpu/drm/amd/display/dc/inc/gamma_calcs.h   |   19 +
 drivers/gpu/drm/amd/display/dc/inc/gamma_types.h   |   38 +
 .../display/include/asic_capability_interface.h    |   55 +
 .../amd/display/include/asic_capability_types.h    |  116 +
 .../drm/amd/display/include/dal_register_logger.h  |   42 +
 .../amd/display/include/display_clock_interface.h  |  175 ++
 .../gpu/drm/amd/display/include/irq_interface.h    |   31 +
 drivers/gpu/drm/amd/display/modules/color/color.c  | 2094 +++++++++++++
 .../gpu/drm/amd/display/modules/inc/mod_color.h    |  179 ++
 .../gpu/drm/amd/display/modules/inc/mod_power.h    |  112 +
 drivers/gpu/drm/amd/display/modules/power/power.c  |  784 +++++
 include/uapi/drm/amdgpu_drm.h                      |   15 +
 58 files changed, 21898 insertions(+)
 create mode 100644 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_types.c
 create mode 100644 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_types.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/basics/register_logger.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/calcs/bandwidth_calcs.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/calcs/gamma_calcs.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/core/dc_target.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp_cursor.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp_gamma.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_mem_input.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_mem_input.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_csc.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_formatter.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_regamma.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce112/dce112_mem_input.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce112/dce112_mem_input.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce112/dce112_opp_formatter.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp_gamma.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_mem_input.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_mem_input.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_csc.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_formatter.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_regamma.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/Makefile
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/dce110/display_clock_dce110.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/dce110/display_clock_dce110.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/dce112/display_clock_dce112.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/dce112/display_clock_dce112.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/dce80/display_clock_dce80.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/dce80/display_clock_dce80.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/display_clock.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/display_clock.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/divider_range.c
 create mode 100644 drivers/gpu/drm/amd/display/dc/gpu/divider_range.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/inc/bandwidth_calcs.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/inc/gamma_calcs.h
 create mode 100644 drivers/gpu/drm/amd/display/dc/inc/gamma_types.h
 create mode 100644 drivers/gpu/drm/amd/display/include/asic_capability_interface.h
 create mode 100644 drivers/gpu/drm/amd/display/include/asic_capability_types.h
 create mode 100644 drivers/gpu/drm/amd/display/include/dal_register_logger.h
 create mode 100644 drivers/gpu/drm/amd/display/include/display_clock_interface.h
 create mode 100644 drivers/gpu/drm/amd/display/include/irq_interface.h
 create mode 100644 drivers/gpu/drm/amd/display/modules/color/color.c
 create mode 100644 drivers/gpu/drm/amd/display/modules/inc/mod_color.h
 create mode 100644 drivers/gpu/drm/amd/display/modules/inc/mod_power.h
 create mode 100644 drivers/gpu/drm/amd/display/modules/power/power.c

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu.h b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
index 176b783..79e075b 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
@@ -68,6 +68,7 @@
 #include "amdgpu_uvd.h"
 #include "amdgpu_vce.h"
 #include "amdgpu_vcn.h"
+#include "amdgpu_dm.h"
 
 #include "gpu_scheduler.h"
 #include "amdgpu_virt.h"
@@ -1355,6 +1356,9 @@ int amdgpu_sem_add_cs(struct amdgpu_ctx *ctx, struct amdgpu_ring *ring,
 
 void amdgpu_sem_destroy(struct amdgpu_fpriv *fpriv, u32 handle);
 
+int amdgpu_freesync_ioctl(struct drm_device *dev, void *data,
+			    struct drm_file *filp);
+
 /* VRAM scratch page for HDP bug, default vram page */
 struct amdgpu_vram_scratch {
 	struct amdgpu_bo		*robj;
@@ -1629,6 +1633,9 @@ struct amdgpu_device {
 	/* display related functionality */
 	struct amdgpu_display_manager dm;
 
+	/* display related functionality */
+	struct amdgpu_display_manager dm;
+
 	struct amdgpu_ip_block          ip_blocks[AMDGPU_MAX_IP_NUM];
 	int				num_ip_blocks;
 	struct mutex	mn_lock;
@@ -1695,6 +1702,9 @@ void amdgpu_mm_wdoorbell64(struct amdgpu_device *adev, u32 index, u64 v);
 bool amdgpu_device_asic_has_dc_support(enum amd_asic_type asic_type);
 bool amdgpu_device_has_dc_support(struct amdgpu_device *adev);
 
+bool amdgpu_device_asic_has_dc_support(enum amd_asic_type asic_type);
+bool amdgpu_device_has_dc_support(struct amdgpu_device *adev);
+
 /*
  * Registers read & write functions.
  */
@@ -1964,6 +1974,12 @@ int amdgpu_dm_display_resume(struct amdgpu_device *adev );
 static inline int amdgpu_dm_display_resume(struct amdgpu_device *adev) { return 0; }
 #endif
 
+#if defined(CONFIG_DRM_AMD_DC)
+int amdgpu_dm_display_resume(struct amdgpu_device *adev );
+#else
+static inline int amdgpu_dm_display_resume(struct amdgpu_device *adev) { return 0; }
+#endif
+
 #include "amdgpu_object.h"
 #endif
 
diff --git a/drivers/gpu/drm/amd/amdgpu/vi.c b/drivers/gpu/drm/amd/amdgpu/vi.c
index 3d9cbbf..57f93cf 100644
--- a/drivers/gpu/drm/amd/amdgpu/vi.c
+++ b/drivers/gpu/drm/amd/amdgpu/vi.c
@@ -78,6 +78,7 @@
 #include "dce_virtual.h"
 #include "amdgpu_dm.h"
 #include "mxgpu_vi.h"
+#include "amdgpu_dm.h"
 
 /*
  * Indirect registers accessor
diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_types.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_types.c
new file mode 100644
index 0000000..3d494e6
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_types.c
@@ -0,0 +1,3313 @@
+/*
+ * Copyright 2012-13 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/version.h>
+
+#include <drm/drmP.h>
+#include <drm/drm_atomic_helper.h>
+#include <drm/drm_fb_helper.h>
+#include <drm/drm_atomic.h>
+
+#include "amdgpu.h"
+#include "amdgpu_pm.h"
+#include "dm_services_types.h"
+
+#include "drm_edid.h"
+
+// We need to #undef FRAME_SIZE and DEPRECATED because they conflict
+// with ptrace-abi.h's #define's of them.
+#undef FRAME_SIZE
+#undef DEPRECATED
+
+#include "dc.h"
+
+#include "amdgpu_dm_types.h"
+#include "amdgpu_dm_mst_types.h"
+
+#include "modules/inc/mod_freesync.h"
+
+struct dm_connector_state {
+	struct drm_connector_state base;
+
+	enum amdgpu_rmx_type scaling;
+	uint8_t underscan_vborder;
+	uint8_t underscan_hborder;
+	bool underscan_enable;
+};
+
+#define to_dm_connector_state(x)\
+	container_of((x), struct dm_connector_state, base)
+
+#define AMDGPU_CRTC_MODE_PRIVATE_FLAGS_GAMMASET 1
+
+void amdgpu_dm_encoder_destroy(struct drm_encoder *encoder)
+{
+	drm_encoder_cleanup(encoder);
+	kfree(encoder);
+}
+
+static const struct drm_encoder_funcs amdgpu_dm_encoder_funcs = {
+	.destroy = amdgpu_dm_encoder_destroy,
+};
+
+static void dm_set_cursor(
+	struct amdgpu_crtc *amdgpu_crtc,
+	uint64_t gpu_addr,
+	uint32_t width,
+	uint32_t height)
+{
+	struct dc_cursor_attributes attributes;
+	amdgpu_crtc->cursor_width = width;
+	amdgpu_crtc->cursor_height = height;
+
+	attributes.address.high_part = upper_32_bits(gpu_addr);
+	attributes.address.low_part  = lower_32_bits(gpu_addr);
+	attributes.width             = width;
+	attributes.height            = height;
+	attributes.x_hot             = 0;
+	attributes.y_hot             = 0;
+	attributes.color_format      = CURSOR_MODE_COLOR_PRE_MULTIPLIED_ALPHA;
+	attributes.rotation_angle    = 0;
+	attributes.attribute_flags.value = 0;
+
+	if (!dc_target_set_cursor_attributes(
+				amdgpu_crtc->target,
+				&attributes)) {
+		DRM_ERROR("DC failed to set cursor attributes\n");
+	}
+}
+
+static int dm_crtc_unpin_cursor_bo_old(
+	struct amdgpu_crtc *amdgpu_crtc)
+{
+	struct amdgpu_bo *robj;
+	int ret = 0;
+
+	if (NULL != amdgpu_crtc && NULL != amdgpu_crtc->cursor_bo) {
+		robj = gem_to_amdgpu_bo(amdgpu_crtc->cursor_bo);
+
+		ret = amdgpu_bo_reserve(robj, false);
+
+		if (likely(ret == 0)) {
+			ret = amdgpu_bo_unpin(robj);
+
+			if (unlikely(ret != 0)) {
+				DRM_ERROR(
+					"%s: unpin failed (ret=%d), bo %p\n",
+					__func__,
+					ret,
+					amdgpu_crtc->cursor_bo);
+			}
+
+			amdgpu_bo_unreserve(robj);
+		} else {
+			DRM_ERROR(
+				"%s: reserve failed (ret=%d), bo %p\n",
+				__func__,
+				ret,
+				amdgpu_crtc->cursor_bo);
+		}
+
+		drm_gem_object_unreference_unlocked(amdgpu_crtc->cursor_bo);
+		amdgpu_crtc->cursor_bo = NULL;
+	}
+
+	return ret;
+}
+
+static int dm_crtc_pin_cursor_bo_new(
+	struct drm_crtc *crtc,
+	struct drm_file *file_priv,
+	uint32_t handle,
+	struct amdgpu_bo **ret_obj)
+{
+	struct amdgpu_crtc *amdgpu_crtc;
+	struct amdgpu_bo *robj;
+	struct drm_gem_object *obj;
+	int ret = -EINVAL;
+
+	if (NULL != crtc) {
+		struct drm_device *dev = crtc->dev;
+		struct amdgpu_device *adev = dev->dev_private;
+		uint64_t gpu_addr;
+
+		amdgpu_crtc = to_amdgpu_crtc(crtc);
+
+		obj = drm_gem_object_lookup(file_priv, handle);
+
+		if (!obj) {
+			DRM_ERROR(
+				"Cannot find cursor object %x for crtc %d\n",
+				handle,
+				amdgpu_crtc->crtc_id);
+			goto release;
+		}
+		robj = gem_to_amdgpu_bo(obj);
+
+		ret  = amdgpu_bo_reserve(robj, false);
+
+		if (unlikely(ret != 0)) {
+			drm_gem_object_unreference_unlocked(obj);
+		DRM_ERROR("dm_crtc_pin_cursor_bo_new ret %x, handle %x\n",
+				 ret, handle);
+			goto release;
+		}
+
+		ret = amdgpu_bo_pin_restricted(robj, AMDGPU_GEM_DOMAIN_VRAM, 0,
+						adev->mc.visible_vram_size,
+						&gpu_addr);
+
+		if (ret == 0) {
+			amdgpu_crtc->cursor_addr = gpu_addr;
+			*ret_obj  = robj;
+		}
+		amdgpu_bo_unreserve(robj);
+		if (ret)
+			drm_gem_object_unreference_unlocked(obj);
+
+	}
+release:
+
+	return ret;
+}
+
+static int dm_crtc_cursor_set(
+	struct drm_crtc *crtc,
+	struct drm_file *file_priv,
+	uint32_t handle,
+	uint32_t width,
+	uint32_t height)
+{
+	struct amdgpu_bo *new_cursor_bo;
+	struct dc_cursor_position position;
+
+	int ret;
+
+	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
+
+	ret		= EINVAL;
+	new_cursor_bo	= NULL;
+
+	DRM_DEBUG_KMS(
+	"%s: crtc_id=%d with handle %d and size %d to %d, bo_object %p\n",
+		__func__,
+		amdgpu_crtc->crtc_id,
+		handle,
+		width,
+		height,
+		amdgpu_crtc->cursor_bo);
+
+	if (!handle) {
+		/* turn off cursor */
+		position.enable = false;
+		position.x = 0;
+		position.y = 0;
+		position.hot_spot_enable = false;
+
+		if (amdgpu_crtc->target) {
+			/*set cursor visible false*/
+			dc_target_set_cursor_position(
+				amdgpu_crtc->target,
+				&position);
+		}
+		/*unpin old cursor buffer and update cache*/
+		ret = dm_crtc_unpin_cursor_bo_old(amdgpu_crtc);
+		goto release;
+
+	}
+
+	if ((width > amdgpu_crtc->max_cursor_width) ||
+		(height > amdgpu_crtc->max_cursor_height)) {
+		DRM_ERROR(
+			"%s: bad cursor width or height %d x %d\n",
+			__func__,
+			width,
+			height);
+		goto release;
+	}
+	/*try to pin new cursor bo*/
+	ret = dm_crtc_pin_cursor_bo_new(crtc, file_priv, handle, &new_cursor_bo);
+	/*if map not successful then return an error*/
+	if (ret)
+		goto release;
+
+	/*program new cursor bo to hardware*/
+	dm_set_cursor(amdgpu_crtc, amdgpu_crtc->cursor_addr, width, height);
+
+	/*un map old, not used anymore cursor bo ,
+	 * return memory and mapping back */
+	dm_crtc_unpin_cursor_bo_old(amdgpu_crtc);
+
+	/*assign new cursor bo to our internal cache*/
+	amdgpu_crtc->cursor_bo = &new_cursor_bo->gem_base;
+
+release:
+	return ret;
+
+}
+
+static int dm_crtc_cursor_move(struct drm_crtc *crtc,
+				     int x, int y)
+{
+	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
+	int xorigin = 0, yorigin = 0;
+	struct dc_cursor_position position;
+
+	/* avivo cursor are offset into the total surface */
+	x += crtc->primary->state->src_x >> 16;
+	y += crtc->primary->state->src_y >> 16;
+
+	/*
+	 * TODO: for cursor debugging unguard the following
+	 */
+#if 0
+	DRM_DEBUG_KMS(
+		"%s: x %d y %d c->x %d c->y %d\n",
+		__func__,
+		x,
+		y,
+		crtc->x,
+		crtc->y);
+#endif
+
+	if (x < 0) {
+		xorigin = min(-x, amdgpu_crtc->max_cursor_width - 1);
+		x = 0;
+	}
+	if (y < 0) {
+		yorigin = min(-y, amdgpu_crtc->max_cursor_height - 1);
+		y = 0;
+	}
+
+	position.enable = true;
+	position.x = x;
+	position.y = y;
+
+	position.hot_spot_enable = true;
+	position.x_hotspot = xorigin;
+	position.y_hotspot = yorigin;
+
+	if (amdgpu_crtc->target) {
+		if (!dc_target_set_cursor_position(
+					amdgpu_crtc->target,
+					&position)) {
+			DRM_ERROR("DC failed to set cursor position\n");
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static void dm_crtc_cursor_reset(struct drm_crtc *crtc)
+{
+	struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
+
+	DRM_DEBUG_KMS(
+		"%s: with cursor_bo %p\n",
+		__func__,
+		amdgpu_crtc->cursor_bo);
+
+	if (amdgpu_crtc->cursor_bo && amdgpu_crtc->target) {
+		dm_set_cursor(
+		amdgpu_crtc,
+		amdgpu_crtc->cursor_addr,
+		amdgpu_crtc->cursor_width,
+		amdgpu_crtc->cursor_height);
+	}
+}
+static bool fill_rects_from_plane_state(
+	const struct drm_plane_state *state,
+	struct dc_surface *surface)
+{
+	surface->src_rect.x = state->src_x >> 16;
+	surface->src_rect.y = state->src_y >> 16;
+	/*we ignore for now mantissa and do not to deal with floating pixels :(*/
+	surface->src_rect.width = state->src_w >> 16;
+
+	if (surface->src_rect.width == 0)
+		return false;
+
+	surface->src_rect.height = state->src_h >> 16;
+	if (surface->src_rect.height == 0)
+		return false;
+
+	surface->dst_rect.x = state->crtc_x;
+	surface->dst_rect.y = state->crtc_y;
+
+	if (state->crtc_w == 0)
+		return false;
+
+	surface->dst_rect.width = state->crtc_w;
+
+	if (state->crtc_h == 0)
+		return false;
+
+	surface->dst_rect.height = state->crtc_h;
+
+	surface->clip_rect = surface->dst_rect;
+
+	switch (state->rotation) {
+	case BIT(DRM_ROTATE_0):
+		surface->rotation = ROTATION_ANGLE_0;
+		break;
+	case BIT(DRM_ROTATE_90):
+		surface->rotation = ROTATION_ANGLE_90;
+		break;
+	case BIT(DRM_ROTATE_180):
+		surface->rotation = ROTATION_ANGLE_180;
+		break;
+	case BIT(DRM_ROTATE_270):
+		surface->rotation = ROTATION_ANGLE_270;
+		break;
+	default:
+		surface->rotation = ROTATION_ANGLE_0;
+		break;
+	}
+
+	return true;
+}
+static bool get_fb_info(
+	const struct amdgpu_framebuffer *amdgpu_fb,
+	uint64_t *tiling_flags,
+	uint64_t *fb_location)
+{
+	struct amdgpu_bo *rbo = gem_to_amdgpu_bo(amdgpu_fb->obj);
+	int r = amdgpu_bo_reserve(rbo, false);
+	if (unlikely(r != 0)){
+		DRM_ERROR("Unable to reserve buffer\n");
+		return false;
+	}
+
+	if (fb_location)
+		*fb_location = amdgpu_bo_gpu_offset(rbo);
+
+	if (tiling_flags)
+		amdgpu_bo_get_tiling_flags(rbo, tiling_flags);
+
+	amdgpu_bo_unreserve(rbo);
+
+	return true;
+}
+static void fill_plane_attributes_from_fb(
+	struct dc_surface *surface,
+	const struct amdgpu_framebuffer *amdgpu_fb, bool addReq)
+{
+	uint64_t tiling_flags;
+	uint64_t fb_location = 0;
+	const struct drm_framebuffer *fb = &amdgpu_fb->base;
+	struct drm_format_name_buf format_name;
+
+	get_fb_info(
+		amdgpu_fb,
+		&tiling_flags,
+		addReq == true ? &fb_location:NULL);
+
+	surface->address.type                = PLN_ADDR_TYPE_GRAPHICS;
+	surface->address.grph.addr.low_part  = lower_32_bits(fb_location);
+	surface->address.grph.addr.high_part = upper_32_bits(fb_location);
+
+	switch (fb->format->format) {
+	case DRM_FORMAT_C8:
+		surface->format = SURFACE_PIXEL_FORMAT_GRPH_PALETA_256_COLORS;
+		break;
+	case DRM_FORMAT_RGB565:
+		surface->format = SURFACE_PIXEL_FORMAT_GRPH_RGB565;
+		break;
+	case DRM_FORMAT_XRGB8888:
+	case DRM_FORMAT_ARGB8888:
+		surface->format = SURFACE_PIXEL_FORMAT_GRPH_ARGB8888;
+		break;
+	case DRM_FORMAT_XRGB2101010:
+	case DRM_FORMAT_ARGB2101010:
+		surface->format = SURFACE_PIXEL_FORMAT_GRPH_ARGB2101010;
+		break;
+	case DRM_FORMAT_XBGR2101010:
+	case DRM_FORMAT_ABGR2101010:
+		surface->format = SURFACE_PIXEL_FORMAT_GRPH_ABGR2101010;
+		break;
+	default:
+		DRM_ERROR("Unsupported screen format %s\n",
+		          drm_get_format_name(fb->format->format, &format_name));
+		return;
+	}
+
+	memset(&surface->tiling_info, 0, sizeof(surface->tiling_info));
+
+	if (AMDGPU_TILING_GET(tiling_flags, ARRAY_MODE) == DC_ARRAY_2D_TILED_THIN1)
+	{
+		unsigned bankw, bankh, mtaspect, tile_split, num_banks;
+
+		bankw = AMDGPU_TILING_GET(tiling_flags, BANK_WIDTH);
+		bankh = AMDGPU_TILING_GET(tiling_flags, BANK_HEIGHT);
+		mtaspect = AMDGPU_TILING_GET(tiling_flags, MACRO_TILE_ASPECT);
+		tile_split = AMDGPU_TILING_GET(tiling_flags, TILE_SPLIT);
+		num_banks = AMDGPU_TILING_GET(tiling_flags, NUM_BANKS);
+
+		/* XXX fix me for VI */
+		surface->tiling_info.gfx8.num_banks = num_banks;
+		surface->tiling_info.gfx8.array_mode =
+				DC_ARRAY_2D_TILED_THIN1;
+		surface->tiling_info.gfx8.tile_split = tile_split;
+		surface->tiling_info.gfx8.bank_width = bankw;
+		surface->tiling_info.gfx8.bank_height = bankh;
+		surface->tiling_info.gfx8.tile_aspect = mtaspect;
+		surface->tiling_info.gfx8.tile_mode =
+				DC_ADDR_SURF_MICRO_TILING_DISPLAY;
+	} else if (AMDGPU_TILING_GET(tiling_flags, ARRAY_MODE)
+			== DC_ARRAY_1D_TILED_THIN1) {
+		surface->tiling_info.gfx8.array_mode = DC_ARRAY_1D_TILED_THIN1;
+	}
+
+	surface->tiling_info.gfx8.pipe_config =
+			AMDGPU_TILING_GET(tiling_flags, PIPE_CONFIG);
+
+	surface->plane_size.grph.surface_size.x = 0;
+	surface->plane_size.grph.surface_size.y = 0;
+	surface->plane_size.grph.surface_size.width = fb->width;
+	surface->plane_size.grph.surface_size.height = fb->height;
+	surface->plane_size.grph.surface_pitch =
+		fb->pitches[0] / fb->format->cpp[0];
+
+	surface->visible = true;
+	surface->scaling_quality.h_taps_c = 0;
+	surface->scaling_quality.v_taps_c = 0;
+
+	/* TODO: unhardcode */
+	surface->color_space = COLOR_SPACE_SRGB;
+	/* is this needed? is surface zeroed at allocation? */
+	surface->scaling_quality.h_taps = 0;
+	surface->scaling_quality.v_taps = 0;
+	surface->stereo_format = PLANE_STEREO_FORMAT_NONE;
+
+}
+
+#define NUM_OF_RAW_GAMMA_RAMP_RGB_256 256
+
+static void fill_gamma_from_crtc(
+	const struct drm_crtc *crtc,
+	struct dc_surface *dc_surface)
+{
+	int i;
+	struct dc_gamma *gamma;
+	uint16_t *red, *green, *blue;
+	int end = (crtc->gamma_size > NUM_OF_RAW_GAMMA_RAMP_RGB_256) ?
+			NUM_OF_RAW_GAMMA_RAMP_RGB_256 : crtc->gamma_size;
+	struct amdgpu_device *adev = crtc->dev->dev_private;
+
+	red = crtc->gamma_store;
+	green = red + crtc->gamma_size;
+	blue = green + crtc->gamma_size;
+
+	gamma = dc_create_gamma(adev->dm.dc);
+
+	if (gamma == NULL)
+		return;
+
+	for (i = 0; i < end; i++) {
+		gamma->gamma_ramp_rgb256x3x16.red[i] =
+				(unsigned short) red[i];
+		gamma->gamma_ramp_rgb256x3x16.green[i] =
+				(unsigned short) green[i];
+		gamma->gamma_ramp_rgb256x3x16.blue[i] =
+				(unsigned short) blue[i];
+	}
+
+	gamma->type = GAMMA_RAMP_RBG256X3X16;
+	gamma->size = sizeof(gamma->gamma_ramp_rgb256x3x16);
+
+	dc_surface->gamma_correction = gamma;
+}
+
+static void fill_plane_attributes(
+			struct dc_surface *surface,
+			struct drm_plane_state *state, bool addrReq)
+{
+	const struct amdgpu_framebuffer *amdgpu_fb =
+		to_amdgpu_framebuffer(state->fb);
+	const struct drm_crtc *crtc = state->crtc;
+
+	fill_rects_from_plane_state(state, surface);
+	fill_plane_attributes_from_fb(
+		surface,
+		amdgpu_fb,
+		addrReq);
+
+	/* In case of gamma set, update gamma value */
+	if (crtc->mode.private_flags &
+		AMDGPU_CRTC_MODE_PRIVATE_FLAGS_GAMMASET) {
+		fill_gamma_from_crtc(crtc, surface);
+	}
+}
+
+/*****************************************************************************/
+
+struct amdgpu_connector *aconnector_from_drm_crtc_id(
+		const struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_connector *connector;
+	struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+	struct amdgpu_connector *aconnector;
+
+	list_for_each_entry(connector,
+			&dev->mode_config.connector_list, head)	{
+
+		aconnector = to_amdgpu_connector(connector);
+
+		if (aconnector->base.state->crtc != &acrtc->base)
+			continue;
+
+		/* Found the connector */
+		return aconnector;
+	}
+
+	/* If we get here, not found. */
+	return NULL;
+}
+
+static void update_stream_scaling_settings(
+		const struct drm_display_mode *mode,
+		const struct dm_connector_state *dm_state,
+		const struct dc_stream *stream)
+{
+	struct amdgpu_device *adev = dm_state->base.crtc->dev->dev_private;
+	enum amdgpu_rmx_type rmx_type;
+
+	struct rect src = { 0 }; /* viewport in target space*/
+	struct rect dst = { 0 }; /* stream addressable area */
+
+	/* Full screen scaling by default */
+	src.width = mode->hdisplay;
+	src.height = mode->vdisplay;
+	dst.width = stream->timing.h_addressable;
+	dst.height = stream->timing.v_addressable;
+
+	rmx_type = dm_state->scaling;
+	if (rmx_type == RMX_ASPECT || rmx_type == RMX_OFF) {
+		if (src.width * dst.height <
+				src.height * dst.width) {
+			/* height needs less upscaling/more downscaling */
+			dst.width = src.width *
+					dst.height / src.height;
+		} else {
+			/* width needs less upscaling/more downscaling */
+			dst.height = src.height *
+					dst.width / src.width;
+		}
+	} else if (rmx_type == RMX_CENTER) {
+		dst = src;
+	}
+
+	dst.x = (stream->timing.h_addressable - dst.width) / 2;
+	dst.y = (stream->timing.v_addressable - dst.height) / 2;
+
+	if (dm_state->underscan_enable) {
+		dst.x += dm_state->underscan_hborder / 2;
+		dst.y += dm_state->underscan_vborder / 2;
+		dst.width -= dm_state->underscan_hborder;
+		dst.height -= dm_state->underscan_vborder;
+	}
+
+	adev->dm.dc->stream_funcs.stream_update_scaling(adev->dm.dc, stream, &src, &dst);
+
+	DRM_DEBUG_KMS("Destination Rectangle x:%d  y:%d  width:%d  height:%d\n",
+			dst.x, dst.y, dst.width, dst.height);
+
+}
+
+static void dm_dc_surface_commit(
+		struct dc *dc,
+		struct drm_crtc *crtc)
+{
+	struct dc_surface *dc_surface;
+	const struct dc_surface *dc_surfaces[1];
+	const struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+	struct dc_target *dc_target = acrtc->target;
+
+	if (!dc_target) {
+		dm_error(
+			"%s: Failed to obtain target on crtc (%d)!\n",
+			__func__,
+			acrtc->crtc_id);
+		goto fail;
+	}
+
+	dc_surface = dc_create_surface(dc);
+
+	if (!dc_surface) {
+		dm_error(
+			"%s: Failed to create a surface!\n",
+			__func__);
+		goto fail;
+	}
+
+	/* Surface programming */
+	fill_plane_attributes(dc_surface, crtc->primary->state, true);
+	if (crtc->mode.private_flags &
+		AMDGPU_CRTC_MODE_PRIVATE_FLAGS_GAMMASET) {
+		/* reset trigger of gamma */
+		crtc->mode.private_flags &=
+			~AMDGPU_CRTC_MODE_PRIVATE_FLAGS_GAMMASET;
+	}
+
+	dc_surfaces[0] = dc_surface;
+
+	if (false == dc_commit_surfaces_to_target(
+			dc,
+			dc_surfaces,
+			1,
+			dc_target)) {
+		dm_error(
+			"%s: Failed to attach surface!\n",
+			__func__);
+	}
+
+	dc_surface_release(dc_surface);
+fail:
+	return;
+}
+
+static enum dc_color_depth convert_color_depth_from_display_info(
+		const struct drm_connector *connector)
+{
+	uint32_t bpc = connector->display_info.bpc;
+
+	/* Limited color depth to 8bit
+	 * TODO: Still need to handle deep color*/
+	if (bpc > 8)
+		bpc = 8;
+
+	switch (bpc) {
+	case 0:
+		/* Temporary Work around, DRM don't parse color depth for
+		 * EDID revision before 1.4
+		 * TODO: Fix edid parsing
+		 */
+		return COLOR_DEPTH_888;
+	case 6:
+		return COLOR_DEPTH_666;
+	case 8:
+		return COLOR_DEPTH_888;
+	case 10:
+		return COLOR_DEPTH_101010;
+	case 12:
+		return COLOR_DEPTH_121212;
+	case 14:
+		return COLOR_DEPTH_141414;
+	case 16:
+		return COLOR_DEPTH_161616;
+	default:
+		return COLOR_DEPTH_UNDEFINED;
+	}
+}
+
+static enum dc_aspect_ratio get_aspect_ratio(
+		const struct drm_display_mode *mode_in)
+{
+	int32_t width = mode_in->crtc_hdisplay * 9;
+	int32_t height = mode_in->crtc_vdisplay * 16;
+	if ((width - height) < 10 && (width - height) > -10)
+		return ASPECT_RATIO_16_9;
+	else
+		return ASPECT_RATIO_4_3;
+}
+
+static enum dc_color_space get_output_color_space(
+				const struct dc_crtc_timing *dc_crtc_timing)
+{
+	enum dc_color_space color_space = COLOR_SPACE_SRGB;
+
+	switch (dc_crtc_timing->pixel_encoding)	{
+	case PIXEL_ENCODING_YCBCR422:
+	case PIXEL_ENCODING_YCBCR444:
+	case PIXEL_ENCODING_YCBCR420:
+	{
+		/*
+		 * 27030khz is the separation point between HDTV and SDTV
+		 * according to HDMI spec, we use YCbCr709 and YCbCr601
+		 * respectively
+		 */
+		if (dc_crtc_timing->pix_clk_khz > 27030) {
+			if (dc_crtc_timing->flags.Y_ONLY)
+				color_space =
+					COLOR_SPACE_YCBCR709_LIMITED;
+			else
+				color_space = COLOR_SPACE_YCBCR709;
+		} else {
+			if (dc_crtc_timing->flags.Y_ONLY)
+				color_space =
+					COLOR_SPACE_YCBCR601_LIMITED;
+			else
+				color_space = COLOR_SPACE_YCBCR601;
+		}
+
+	}
+	break;
+	case PIXEL_ENCODING_RGB:
+		color_space = COLOR_SPACE_SRGB;
+		break;
+
+	default:
+		WARN_ON(1);
+		break;
+	}
+
+	return color_space;
+}
+
+/*****************************************************************************/
+
+static void fill_stream_properties_from_drm_display_mode(
+	struct dc_stream *stream,
+	const struct drm_display_mode *mode_in,
+	const struct drm_connector *connector)
+{
+	struct dc_crtc_timing *timing_out = &stream->timing;
+	memset(timing_out, 0, sizeof(struct dc_crtc_timing));
+
+	timing_out->h_border_left = 0;
+	timing_out->h_border_right = 0;
+	timing_out->v_border_top = 0;
+	timing_out->v_border_bottom = 0;
+	/* TODO: un-hardcode */
+
+	if ((connector->display_info.color_formats & DRM_COLOR_FORMAT_YCRCB444)
+			&& stream->sink->sink_signal == SIGNAL_TYPE_HDMI_TYPE_A)
+		timing_out->pixel_encoding = PIXEL_ENCODING_YCBCR444;
+	else
+		timing_out->pixel_encoding = PIXEL_ENCODING_RGB;
+
+	timing_out->timing_3d_format = TIMING_3D_FORMAT_NONE;
+	timing_out->display_color_depth = convert_color_depth_from_display_info(
+			connector);
+	timing_out->scan_type = SCANNING_TYPE_NODATA;
+	timing_out->hdmi_vic = 0;
+	timing_out->vic = drm_match_cea_mode(mode_in);
+
+	timing_out->h_addressable = mode_in->crtc_hdisplay;
+	timing_out->h_total = mode_in->crtc_htotal;
+	timing_out->h_sync_width =
+		mode_in->crtc_hsync_end - mode_in->crtc_hsync_start;
+	timing_out->h_front_porch =
+		mode_in->crtc_hsync_start - mode_in->crtc_hdisplay;
+	timing_out->v_total = mode_in->crtc_vtotal;
+	timing_out->v_addressable = mode_in->crtc_vdisplay;
+	timing_out->v_front_porch =
+		mode_in->crtc_vsync_start - mode_in->crtc_vdisplay;
+	timing_out->v_sync_width =
+		mode_in->crtc_vsync_end - mode_in->crtc_vsync_start;
+	timing_out->pix_clk_khz = mode_in->crtc_clock;
+	timing_out->aspect_ratio = get_aspect_ratio(mode_in);
+	if (mode_in->flags & DRM_MODE_FLAG_PHSYNC)
+		timing_out->flags.HSYNC_POSITIVE_POLARITY = 1;
+	if (mode_in->flags & DRM_MODE_FLAG_PVSYNC)
+		timing_out->flags.VSYNC_POSITIVE_POLARITY = 1;
+
+	stream->output_color_space = get_output_color_space(timing_out);
+
+}
+
+static void fill_audio_info(
+	struct audio_info *audio_info,
+	const struct drm_connector *drm_connector,
+	const struct dc_sink *dc_sink)
+{
+	int i = 0;
+	int cea_revision = 0;
+	const struct dc_edid_caps *edid_caps = &dc_sink->edid_caps;
+
+	audio_info->manufacture_id = edid_caps->manufacturer_id;
+	audio_info->product_id = edid_caps->product_id;
+
+	cea_revision = drm_connector->display_info.cea_rev;
+
+	while (i < AUDIO_INFO_DISPLAY_NAME_SIZE_IN_CHARS &&
+		edid_caps->display_name[i]) {
+		audio_info->display_name[i] = edid_caps->display_name[i];
+		i++;
+	}
+
+	if(cea_revision >= 3) {
+		audio_info->mode_count = edid_caps->audio_mode_count;
+
+		for (i = 0; i < audio_info->mode_count; ++i) {
+			audio_info->modes[i].format_code =
+					(enum audio_format_code)
+					(edid_caps->audio_modes[i].format_code);
+			audio_info->modes[i].channel_count =
+					edid_caps->audio_modes[i].channel_count;
+			audio_info->modes[i].sample_rates.all =
+					edid_caps->audio_modes[i].sample_rate;
+			audio_info->modes[i].sample_size =
+					edid_caps->audio_modes[i].sample_size;
+		}
+	}
+
+	audio_info->flags.all = edid_caps->speaker_flags;
+
+	/* TODO: We only check for the progressive mode, check for interlace mode too */
+	if(drm_connector->latency_present[0]) {
+		audio_info->video_latency = drm_connector->video_latency[0];
+		audio_info->audio_latency = drm_connector->audio_latency[0];
+	}
+
+	/* TODO: For DP, video and audio latency should be calculated from DPCD caps */
+
+}
+
+static void copy_crtc_timing_for_drm_display_mode(
+		const struct drm_display_mode *src_mode,
+		struct drm_display_mode *dst_mode)
+{
+	dst_mode->crtc_hdisplay = src_mode->crtc_hdisplay;
+	dst_mode->crtc_vdisplay = src_mode->crtc_vdisplay;
+	dst_mode->crtc_clock = src_mode->crtc_clock;
+	dst_mode->crtc_hblank_start = src_mode->crtc_hblank_start;
+	dst_mode->crtc_hblank_end = src_mode->crtc_hblank_end;
+	dst_mode->crtc_hsync_start=  src_mode->crtc_hsync_start;
+	dst_mode->crtc_hsync_end = src_mode->crtc_hsync_end;
+	dst_mode->crtc_htotal = src_mode->crtc_htotal;
+	dst_mode->crtc_hskew = src_mode->crtc_hskew;
+	dst_mode->crtc_vblank_start = src_mode->crtc_vblank_start;;
+	dst_mode->crtc_vblank_end = src_mode->crtc_vblank_end;;
+	dst_mode->crtc_vsync_start = src_mode->crtc_vsync_start;;
+	dst_mode->crtc_vsync_end = src_mode->crtc_vsync_end;;
+	dst_mode->crtc_vtotal = src_mode->crtc_vtotal;;
+}
+
+static void decide_crtc_timing_for_drm_display_mode(
+		struct drm_display_mode *drm_mode,
+		const struct drm_display_mode *native_mode,
+		bool scale_enabled)
+{
+	if (scale_enabled) {
+		copy_crtc_timing_for_drm_display_mode(native_mode, drm_mode);
+	} else if (native_mode->clock == drm_mode->clock &&
+			native_mode->htotal == drm_mode->htotal &&
+			native_mode->vtotal == drm_mode->vtotal) {
+		copy_crtc_timing_for_drm_display_mode(native_mode, drm_mode);
+	} else {
+		/* no scaling nor amdgpu inserted, no need to patch */
+	}
+}
+
+static struct dc_target *create_target_for_sink(
+		const struct amdgpu_connector *aconnector,
+		const struct drm_display_mode *drm_mode,
+		const struct dm_connector_state *dm_state)
+{
+	struct drm_display_mode *preferred_mode = NULL;
+	const struct drm_connector *drm_connector;
+	struct dc_target *target = NULL;
+	struct dc_stream *stream;
+	struct drm_display_mode mode = *drm_mode;
+	bool native_mode_found = false;
+
+	if (NULL == aconnector) {
+		DRM_ERROR("aconnector is NULL!\n");
+		goto drm_connector_null;
+	}
+
+	if (NULL == dm_state) {
+		DRM_ERROR("dm_state is NULL!\n");
+		goto dm_state_null;
+	}
+
+	drm_connector = &aconnector->base;
+	stream = dc_create_stream_for_sink(aconnector->dc_sink);
+
+	if (NULL == stream) {
+		DRM_ERROR("Failed to create stream for sink!\n");
+		goto stream_create_fail;
+	}
+
+	list_for_each_entry(preferred_mode, &aconnector->base.modes, head) {
+		/* Search for preferred mode */
+		if (preferred_mode->type & DRM_MODE_TYPE_PREFERRED) {
+			native_mode_found = true;
+			break;
+		}
+	}
+	if (!native_mode_found)
+		preferred_mode = list_first_entry_or_null(
+				&aconnector->base.modes,
+				struct drm_display_mode,
+				head);
+
+	if (NULL == preferred_mode) {
+		/* This may not be an error, the use case is when we we have no
+		 * usermode calls to reset and set mode upon hotplug. In this
+		 * case, we call set mode ourselves to restore the previous mode
+		 * and the modelist may not be filled in in time.
+		 */
+		DRM_INFO("No preferred mode found\n");
+	} else {
+		decide_crtc_timing_for_drm_display_mode(
+				&mode, preferred_mode,
+				dm_state->scaling != RMX_OFF);
+	}
+
+	fill_stream_properties_from_drm_display_mode(stream,
+			&mode, &aconnector->base);
+	update_stream_scaling_settings(&mode, dm_state, stream);
+
+	fill_audio_info(
+		&stream->audio_info,
+		drm_connector,
+		aconnector->dc_sink);
+
+	target = dc_create_target_for_streams(&stream, 1);
+	dc_stream_release(stream);
+
+	if (NULL == target) {
+		DRM_ERROR("Failed to create target with streams!\n");
+		goto target_create_fail;
+	}
+
+dm_state_null:
+drm_connector_null:
+target_create_fail:
+stream_create_fail:
+	return target;
+}
+
+void amdgpu_dm_crtc_destroy(struct drm_crtc *crtc)
+{
+	drm_crtc_cleanup(crtc);
+	kfree(crtc);
+}
+
+static int amdgpu_dm_atomic_crtc_gamma_set(
+		struct drm_crtc *crtc,
+		u16 *red,
+		u16 *green,
+		u16 *blue,
+		uint32_t size)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_property *prop = dev->mode_config.prop_crtc_id;
+
+	crtc->state->mode.private_flags |= AMDGPU_CRTC_MODE_PRIVATE_FLAGS_GAMMASET;
+
+	return drm_atomic_helper_crtc_set_property(crtc, prop, 0);
+}
+
+static int dm_crtc_funcs_atomic_set_property(
+	struct drm_crtc *crtc,
+	struct drm_crtc_state *crtc_state,
+	struct drm_property *property,
+	uint64_t val)
+{
+	struct drm_plane_state *plane_state;
+
+	crtc_state->planes_changed = true;
+
+	/*
+	 * Bit of magic done here. We need to ensure
+	 * that planes get update after mode is set.
+	 * So, we need to add primary plane to state,
+	 * and this way atomic_update would be called
+	 * for it
+	 */
+	plane_state =
+		drm_atomic_get_plane_state(
+			crtc_state->state,
+			crtc->primary);
+
+	if (!plane_state)
+		return -EINVAL;
+
+	return 0;
+}
+
+
+static int amdgpu_atomic_helper_page_flip(struct drm_crtc *crtc,
+				struct drm_framebuffer *fb,
+				struct drm_pending_vblank_event *event,
+				uint32_t flags)
+{
+	struct drm_plane *plane = crtc->primary;
+	struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+	struct drm_atomic_state *state;
+	struct drm_plane_state *plane_state;
+	struct drm_crtc_state *crtc_state;
+	int ret = 0;
+
+	state = drm_atomic_state_alloc(plane->dev);
+	if (!state)
+		return -ENOMEM;
+
+	state->acquire_ctx = drm_modeset_legacy_acquire_ctx(crtc);
+retry:
+	crtc_state = drm_atomic_get_crtc_state(state, crtc);
+	if (IS_ERR(crtc_state)) {
+		ret = PTR_ERR(crtc_state);
+		goto fail;
+	}
+	crtc_state->event = event;
+
+	plane_state = drm_atomic_get_plane_state(state, plane);
+	if (IS_ERR(plane_state)) {
+		ret = PTR_ERR(plane_state);
+		goto fail;
+	}
+
+	ret = drm_atomic_set_crtc_for_plane(plane_state, crtc);
+	if (ret != 0)
+		goto fail;
+	drm_atomic_set_fb_for_plane(plane_state, fb);
+
+	/* Make sure we don't accidentally do a full modeset. */
+	state->allow_modeset = false;
+	if (!crtc_state->active) {
+		DRM_DEBUG_ATOMIC("[CRTC:%d] disabled, rejecting legacy flip\n",
+				 crtc->base.id);
+		ret = -EINVAL;
+		goto fail;
+	}
+	acrtc->flip_flags = flags;
+	ret = drm_atomic_nonblocking_commit(state);
+	if (ret != 0)
+		goto fail;
+
+	/* Driver takes ownership of state on successful async commit. */
+	return 0;
+fail:
+	if (ret == -EDEADLK)
+		goto backoff;
+
+	drm_atomic_state_put(state);
+
+	return ret;
+backoff:
+	drm_atomic_state_clear(state);
+	drm_atomic_legacy_backoff(state);
+
+	/*
+	 * Someone might have exchanged the framebuffer while we dropped locks
+	 * in the backoff code. We need to fix up the fb refcount tracking the
+	 * core does for us.
+	 */
+	plane->old_fb = plane->fb;
+
+	goto retry;
+}
+
+/* Implemented only the options currently availible for the driver */
+static const struct drm_crtc_funcs amdgpu_dm_crtc_funcs = {
+	.reset = drm_atomic_helper_crtc_reset,
+	.cursor_set = dm_crtc_cursor_set,
+	.cursor_move = dm_crtc_cursor_move,
+	.destroy = amdgpu_dm_crtc_destroy,
+	.gamma_set = amdgpu_dm_atomic_crtc_gamma_set,
+	.set_config = drm_atomic_helper_set_config,
+	.page_flip = amdgpu_atomic_helper_page_flip,
+	.atomic_duplicate_state = drm_atomic_helper_crtc_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_crtc_destroy_state,
+	.atomic_set_property = dm_crtc_funcs_atomic_set_property
+};
+
+static enum drm_connector_status
+amdgpu_dm_connector_detect(struct drm_connector *connector, bool force)
+{
+	bool connected;
+	struct amdgpu_connector *aconnector = to_amdgpu_connector(connector);
+
+	/* Notes:
+	 * 1. This interface is NOT called in context of HPD irq.
+	 * 2. This interface *is called* in context of user-mode ioctl. Which
+	 * makes it a bad place for *any* MST-related activit. */
+
+	if (aconnector->base.force == DRM_FORCE_UNSPECIFIED)
+		connected = (aconnector->dc_sink != NULL);
+	else
+		connected = (aconnector->base.force == DRM_FORCE_ON);
+
+	return (connected ? connector_status_connected :
+			connector_status_disconnected);
+}
+
+/* Compare user free sync property with immunable property free sync capable
+ * and if display is not free sync capable sets free sync property to 0
+ */
+static int amdgpu_freesync_update_property_atomic(
+				struct drm_connector *connector,
+				uint64_t val_capable)
+{
+	struct drm_device *dev;
+	struct amdgpu_device *adev;
+	int ret;
+	uint64_t val;
+
+	dev  = connector->dev;
+	adev = dev->dev_private;
+
+	ret = drm_object_property_get_value(
+			&connector->base,
+			adev->mode_info.freesync_property,
+			&val);
+	if (ret == 0 && val != 0 && val_capable == 0)
+		ret = drm_object_property_set_value(
+				&connector->base,
+				adev->mode_info.freesync_property,
+				val_capable);
+	return ret;
+
+}
+
+static int amdgpu_freesync_set_property_atomic(
+				struct drm_connector *connector,
+				struct drm_connector_state *connector_state,
+				struct drm_property *property, uint64_t val)
+{
+	struct mod_freesync_user_enable user_enable;
+	struct drm_device *dev;
+	struct amdgpu_device *adev;
+	struct amdgpu_crtc *acrtc;
+	int ret;
+	uint64_t val_capable;
+
+	dev  = connector->dev;
+	adev = dev->dev_private;
+	ret  = -EINVAL;
+
+	if (adev->dm.freesync_module && connector_state->crtc) {
+		ret = drm_object_property_get_value(
+				&connector->base,
+				adev->mode_info.freesync_capable_property,
+				&val_capable);
+		/* if user free sync val property is enable, but the capable
+		 * prop is not, then fail the call
+		 */
+		if (ret != 0 || (val_capable == 0 && val != 0)) {
+			ret  = -EINVAL;
+			goto release;
+		}
+
+		user_enable.enable_for_gaming = val ? true : false;
+		user_enable.enable_for_static = user_enable.enable_for_gaming;
+		user_enable.enable_for_video  = user_enable.enable_for_gaming;
+		ret  = -EINVAL;
+		acrtc = to_amdgpu_crtc(connector_state->crtc);
+		if (connector_state->connector == connector && acrtc->target) {
+			mod_freesync_set_user_enable(adev->dm.freesync_module,
+						     acrtc->target->streams,
+						     acrtc->target->stream_count
+						     , &user_enable);
+			ret = 0;
+		}
+	}
+release:
+	return ret;
+}
+
+int amdgpu_dm_connector_atomic_set_property(
+	struct drm_connector *connector,
+	struct drm_connector_state *connector_state,
+	struct drm_property *property,
+	uint64_t val)
+{
+	struct drm_device *dev = connector->dev;
+	struct amdgpu_device *adev = dev->dev_private;
+	struct dm_connector_state *dm_old_state =
+		to_dm_connector_state(connector->state);
+	struct dm_connector_state *dm_new_state =
+		to_dm_connector_state(connector_state);
+
+	struct drm_crtc_state *new_crtc_state;
+	struct drm_crtc *crtc;
+	int i;
+	int ret = -EINVAL;
+
+	if (property == dev->mode_config.scaling_mode_property) {
+		enum amdgpu_rmx_type rmx_type;
+
+		switch (val) {
+		case DRM_MODE_SCALE_CENTER:
+			rmx_type = RMX_CENTER;
+			break;
+		case DRM_MODE_SCALE_ASPECT:
+			rmx_type = RMX_ASPECT;
+			break;
+		case DRM_MODE_SCALE_FULLSCREEN:
+			rmx_type = RMX_FULL;
+			break;
+		case DRM_MODE_SCALE_NONE:
+		default:
+			rmx_type = RMX_OFF;
+			break;
+		}
+
+		if (dm_old_state->scaling == rmx_type)
+			return 0;
+
+		dm_new_state->scaling = rmx_type;
+		ret = 0;
+	} else if (property == adev->mode_info.underscan_hborder_property) {
+		dm_new_state->underscan_hborder = val;
+		ret = 0;
+	} else if (property == adev->mode_info.underscan_vborder_property) {
+		dm_new_state->underscan_vborder = val;
+		ret = 0;
+	} else if (property == adev->mode_info.underscan_property) {
+		dm_new_state->underscan_enable = val;
+		ret = 0;
+	} else if (property == adev->mode_info.freesync_property) {
+		ret = amdgpu_freesync_set_property_atomic(connector,
+							  connector_state,
+							  property, val);
+		return ret;
+	} else if (property == adev->mode_info.freesync_capable_property) {
+		ret = -EINVAL;
+		return ret;
+	}
+
+
+
+	for_each_crtc_in_state(
+		connector_state->state,
+		crtc,
+		new_crtc_state,
+		i) {
+
+		if (crtc == connector_state->crtc) {
+			struct drm_plane_state *plane_state;
+
+			/*
+			 * Bit of magic done here. We need to ensure
+			 * that planes get update after mode is set.
+			 * So, we need to add primary plane to state,
+			 * and this way atomic_update would be called
+			 * for it
+			 */
+			plane_state =
+				drm_atomic_get_plane_state(
+					connector_state->state,
+					crtc->primary);
+
+			if (!plane_state)
+				return -EINVAL;
+		}
+	}
+
+	return ret;
+}
+
+void amdgpu_dm_connector_destroy(struct drm_connector *connector)
+{
+	struct amdgpu_connector *aconnector = to_amdgpu_connector(connector);
+	const struct dc_link *link = aconnector->dc_link;
+	struct amdgpu_device *adev = connector->dev->dev_private;
+	struct amdgpu_display_manager *dm = &adev->dm;
+#if defined(CONFIG_BACKLIGHT_CLASS_DEVICE) ||\
+	defined(CONFIG_BACKLIGHT_CLASS_DEVICE_MODULE)
+
+	if (link->connector_signal & (SIGNAL_TYPE_EDP | SIGNAL_TYPE_LVDS)) {
+		amdgpu_dm_register_backlight_device(dm);
+
+		if (dm->backlight_dev) {
+			backlight_device_unregister(dm->backlight_dev);
+			dm->backlight_dev = NULL;
+		}
+
+	}
+#endif
+	drm_connector_unregister(connector);
+	drm_connector_cleanup(connector);
+	kfree(connector);
+}
+
+void amdgpu_dm_connector_funcs_reset(struct drm_connector *connector)
+{
+	struct dm_connector_state *state =
+		to_dm_connector_state(connector->state);
+
+	kfree(state);
+
+	state = kzalloc(sizeof(*state), GFP_KERNEL);
+
+	if (state) {
+		state->scaling = RMX_OFF;
+		state->underscan_enable = false;
+		state->underscan_hborder = 0;
+		state->underscan_vborder = 0;
+
+		connector->state = &state->base;
+		connector->state->connector = connector;
+	}
+}
+
+struct drm_connector_state *amdgpu_dm_connector_atomic_duplicate_state(
+	struct drm_connector *connector)
+{
+	struct dm_connector_state *state =
+		to_dm_connector_state(connector->state);
+
+	struct dm_connector_state *new_state =
+			kmemdup(state, sizeof(*state), GFP_KERNEL);
+
+	if (new_state) {
+		__drm_atomic_helper_connector_duplicate_state(connector,
+								      &new_state->base);
+		return &new_state->base;
+	}
+
+	return NULL;
+}
+
+static const struct drm_connector_funcs amdgpu_dm_connector_funcs = {
+	.dpms = drm_atomic_helper_connector_dpms,
+	.reset = amdgpu_dm_connector_funcs_reset,
+	.detect = amdgpu_dm_connector_detect,
+	.fill_modes = drm_helper_probe_single_connector_modes,
+	.set_property = drm_atomic_helper_connector_set_property,
+	.destroy = amdgpu_dm_connector_destroy,
+	.atomic_duplicate_state = amdgpu_dm_connector_atomic_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_connector_destroy_state,
+	.atomic_set_property = amdgpu_dm_connector_atomic_set_property
+};
+
+static struct drm_encoder *best_encoder(struct drm_connector *connector)
+{
+	int enc_id = connector->encoder_ids[0];
+	struct drm_mode_object *obj;
+	struct drm_encoder *encoder;
+
+	DRM_DEBUG_KMS("Finding the best encoder\n");
+
+	/* pick the encoder ids */
+	if (enc_id) {
+		obj = drm_mode_object_find(connector->dev, enc_id, DRM_MODE_OBJECT_ENCODER);
+		if (!obj) {
+			DRM_ERROR("Couldn't find a matching encoder for our connector\n");
+			return NULL;
+		}
+		encoder = obj_to_encoder(obj);
+		return encoder;
+	}
+	DRM_ERROR("No encoder id\n");
+	return NULL;
+}
+
+static int get_modes(struct drm_connector *connector)
+{
+	return amdgpu_dm_connector_get_modes(connector);
+}
+
+static void create_eml_sink(struct amdgpu_connector *aconnector)
+{
+	struct dc_sink_init_data init_params = {
+			.link = aconnector->dc_link,
+			.sink_signal = SIGNAL_TYPE_VIRTUAL
+	};
+	struct edid *edid = (struct edid *) aconnector->base.edid_blob_ptr->data;
+
+	if (!aconnector->base.edid_blob_ptr ||
+		!aconnector->base.edid_blob_ptr->data) {
+		DRM_ERROR("No EDID firmware found on connector: %s ,forcing to OFF!\n",
+				aconnector->base.name);
+
+		aconnector->base.force = DRM_FORCE_OFF;
+		aconnector->base.override_edid = false;
+		return;
+	}
+
+	aconnector->edid = edid;
+
+	aconnector->dc_em_sink = dc_link_add_remote_sink(
+		aconnector->dc_link,
+		(uint8_t *)edid,
+		(edid->extensions + 1) * EDID_LENGTH,
+		&init_params);
+
+	if (aconnector->base.force
+					== DRM_FORCE_ON)
+		aconnector->dc_sink = aconnector->dc_link->local_sink ?
+		aconnector->dc_link->local_sink :
+		aconnector->dc_em_sink;
+}
+
+static void handle_edid_mgmt(struct amdgpu_connector *aconnector)
+{
+	struct dc_link *link = (struct dc_link *)aconnector->dc_link;
+
+	/* In case of headless boot with force on for DP managed connector
+	 * Those settings have to be != 0 to get initial modeset
+	 */
+	if (link->connector_signal == SIGNAL_TYPE_DISPLAY_PORT) {
+		link->verified_link_cap.lane_count = LANE_COUNT_FOUR;
+		link->verified_link_cap.link_rate = LINK_RATE_HIGH2;
+	}
+
+
+	aconnector->base.override_edid = true;
+	create_eml_sink(aconnector);
+}
+
+int amdgpu_dm_connector_mode_valid(
+		struct drm_connector *connector,
+		struct drm_display_mode *mode)
+{
+	int result = MODE_ERROR;
+	const struct dc_sink *dc_sink;
+	struct amdgpu_device *adev = connector->dev->dev_private;
+	struct dc_validation_set val_set = { 0 };
+	/* TODO: Unhardcode stream count */
+	struct dc_stream *streams[1];
+	struct dc_target *target;
+	struct amdgpu_connector *aconnector = to_amdgpu_connector(connector);
+
+	if ((mode->flags & DRM_MODE_FLAG_INTERLACE) ||
+			(mode->flags & DRM_MODE_FLAG_DBLSCAN))
+		return result;
+
+	/* Only run this the first time mode_valid is called to initilialize
+	 * EDID mgmt
+	 */
+	if (aconnector->base.force != DRM_FORCE_UNSPECIFIED &&
+		!aconnector->dc_em_sink)
+		handle_edid_mgmt(aconnector);
+
+	dc_sink = to_amdgpu_connector(connector)->dc_sink;
+
+	if (NULL == dc_sink) {
+		DRM_ERROR("dc_sink is NULL!\n");
+		goto stream_create_fail;
+	}
+
+	streams[0] = dc_create_stream_for_sink(dc_sink);
+
+	if (NULL == streams[0]) {
+		DRM_ERROR("Failed to create stream for sink!\n");
+		goto stream_create_fail;
+	}
+
+	drm_mode_set_crtcinfo(mode, 0);
+	fill_stream_properties_from_drm_display_mode(streams[0], mode, connector);
+
+	target = dc_create_target_for_streams(streams, 1);
+	val_set.target = target;
+
+	if (NULL == val_set.target) {
+		DRM_ERROR("Failed to create target with stream!\n");
+		goto target_create_fail;
+	}
+
+	val_set.surface_count = 0;
+	streams[0]->src.width = mode->hdisplay;
+	streams[0]->src.height = mode->vdisplay;
+	streams[0]->dst = streams[0]->src;
+
+	if (dc_validate_resources(adev->dm.dc, &val_set, 1))
+		result = MODE_OK;
+
+	dc_target_release(target);
+target_create_fail:
+	dc_stream_release(streams[0]);
+stream_create_fail:
+	/* TODO: error handling*/
+	return result;
+}
+
+static const struct drm_connector_helper_funcs
+amdgpu_dm_connector_helper_funcs = {
+	/*
+	* If hotplug a second bigger display in FB Con mode, bigger resolution
+	* modes will be filtered by drm_mode_validate_size(), and those modes
+	* is missing after user start lightdm. So we need to renew modes list.
+	* in get_modes call back, not just return the modes count
+	*/
+	.get_modes = get_modes,
+	.mode_valid = amdgpu_dm_connector_mode_valid,
+	.best_encoder = best_encoder
+};
+
+static void dm_crtc_helper_disable(struct drm_crtc *crtc)
+{
+}
+
+static int dm_crtc_helper_atomic_check(
+	struct drm_crtc *crtc,
+	struct drm_crtc_state *state)
+{
+	return 0;
+}
+
+static bool dm_crtc_helper_mode_fixup(
+	struct drm_crtc *crtc,
+	const struct drm_display_mode *mode,
+	struct drm_display_mode *adjusted_mode)
+{
+	return true;
+}
+
+static const struct drm_crtc_helper_funcs amdgpu_dm_crtc_helper_funcs = {
+	.disable = dm_crtc_helper_disable,
+	.atomic_check = dm_crtc_helper_atomic_check,
+	.mode_fixup = dm_crtc_helper_mode_fixup
+};
+
+static void dm_encoder_helper_disable(struct drm_encoder *encoder)
+{
+
+}
+
+static int dm_encoder_helper_atomic_check(
+	struct drm_encoder *encoder,
+	struct drm_crtc_state *crtc_state,
+	struct drm_connector_state *conn_state)
+{
+	return 0;
+}
+
+const struct drm_encoder_helper_funcs amdgpu_dm_encoder_helper_funcs = {
+	.disable = dm_encoder_helper_disable,
+	.atomic_check = dm_encoder_helper_atomic_check
+};
+
+static const struct drm_plane_funcs dm_plane_funcs = {
+	.reset = drm_atomic_helper_plane_reset,
+	.atomic_duplicate_state = drm_atomic_helper_plane_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_plane_destroy_state
+};
+
+static void clear_unrelated_fields(struct drm_plane_state *state)
+{
+	state->crtc = NULL;
+	state->fb = NULL;
+	state->state = NULL;
+	state->fence = NULL;
+}
+
+static bool page_flip_needed(
+	const struct drm_plane_state *new_state,
+	const struct drm_plane_state *old_state,
+	bool commit_surface_required)
+{
+	struct drm_plane_state old_state_tmp;
+	struct drm_plane_state new_state_tmp;
+
+	struct amdgpu_framebuffer *amdgpu_fb_old;
+	struct amdgpu_framebuffer *amdgpu_fb_new;
+	struct amdgpu_crtc *acrtc_new;
+
+	uint64_t old_tiling_flags;
+	uint64_t new_tiling_flags;
+
+	bool page_flip_required;
+
+	if (!old_state)
+		return false;
+
+	if (!old_state->fb)
+		return false;
+
+	if (!new_state)
+		return false;
+
+	if (!new_state->fb)
+		return false;
+
+	old_state_tmp = *old_state;
+	new_state_tmp = *new_state;
+
+	if (!new_state->crtc->state->event)
+		return false;
+
+	amdgpu_fb_old = to_amdgpu_framebuffer(old_state->fb);
+	amdgpu_fb_new = to_amdgpu_framebuffer(new_state->fb);
+
+	if (!get_fb_info(amdgpu_fb_old, &old_tiling_flags, NULL))
+		return false;
+
+	if (!get_fb_info(amdgpu_fb_new, &new_tiling_flags, NULL))
+		return false;
+
+	if (commit_surface_required == true &&
+	    old_tiling_flags != new_tiling_flags)
+		return false;
+
+	clear_unrelated_fields(&old_state_tmp);
+	clear_unrelated_fields(&new_state_tmp);
+
+	page_flip_required = memcmp(&old_state_tmp,
+				    &new_state_tmp,
+				    sizeof(old_state_tmp)) == 0 ? true:false;
+	if (new_state->crtc && page_flip_required == false) {
+		acrtc_new = to_amdgpu_crtc(new_state->crtc);
+		if (acrtc_new->flip_flags & DRM_MODE_PAGE_FLIP_ASYNC)
+			page_flip_required = true;
+	}
+	return page_flip_required;
+}
+
+static int dm_plane_helper_prepare_fb(
+	struct drm_plane *plane,
+	struct drm_plane_state *new_state)
+{
+	struct amdgpu_framebuffer *afb;
+	struct drm_gem_object *obj;
+	struct amdgpu_bo *rbo;
+	int r;
+
+	if (!new_state->fb) {
+		DRM_DEBUG_KMS("No FB bound\n");
+		return 0;
+	}
+
+	afb = to_amdgpu_framebuffer(new_state->fb);
+
+	obj = afb->obj;
+	rbo = gem_to_amdgpu_bo(obj);
+	r = amdgpu_bo_reserve(rbo, false);
+	if (unlikely(r != 0))
+		return r;
+
+	r = amdgpu_bo_pin(rbo, AMDGPU_GEM_DOMAIN_VRAM, NULL);
+
+	amdgpu_bo_unreserve(rbo);
+
+	if (unlikely(r != 0)) {
+		DRM_ERROR("Failed to pin framebuffer\n");
+		return r;
+	}
+
+	return 0;
+}
+
+static void dm_plane_helper_cleanup_fb(
+	struct drm_plane *plane,
+	struct drm_plane_state *old_state)
+{
+	struct amdgpu_bo *rbo;
+	struct amdgpu_framebuffer *afb;
+	int r;
+
+	if (!old_state->fb)
+		return;
+
+	afb = to_amdgpu_framebuffer(old_state->fb);
+	rbo = gem_to_amdgpu_bo(afb->obj);
+	r = amdgpu_bo_reserve(rbo, false);
+	if (unlikely(r)) {
+		DRM_ERROR("failed to reserve rbo before unpin\n");
+		return;
+	} else {
+		amdgpu_bo_unpin(rbo);
+		amdgpu_bo_unreserve(rbo);
+	}
+}
+
+int dm_create_validation_set_for_target(struct drm_connector *connector,
+		struct drm_display_mode *mode, struct dc_validation_set *val_set)
+{
+	int result = MODE_ERROR;
+	const struct dc_sink *dc_sink =
+			to_amdgpu_connector(connector)->dc_sink;
+	/* TODO: Unhardcode stream count */
+	struct dc_stream *streams[1];
+	struct dc_target *target;
+
+	if ((mode->flags & DRM_MODE_FLAG_INTERLACE) ||
+			(mode->flags & DRM_MODE_FLAG_DBLSCAN))
+		return result;
+
+	if (NULL == dc_sink) {
+		DRM_ERROR("dc_sink is NULL!\n");
+		return result;
+	}
+
+	streams[0] = dc_create_stream_for_sink(dc_sink);
+
+	if (NULL == streams[0]) {
+		DRM_ERROR("Failed to create stream for sink!\n");
+		return result;
+	}
+
+	drm_mode_set_crtcinfo(mode, 0);
+
+	fill_stream_properties_from_drm_display_mode(streams[0], mode, connector);
+
+	target = dc_create_target_for_streams(streams, 1);
+	val_set->target = target;
+
+	if (NULL == val_set->target) {
+		DRM_ERROR("Failed to create target with stream!\n");
+		goto fail;
+	}
+
+	streams[0]->src.width = mode->hdisplay;
+	streams[0]->src.height = mode->vdisplay;
+	streams[0]->dst = streams[0]->src;
+
+	return MODE_OK;
+
+fail:
+	dc_stream_release(streams[0]);
+	return result;
+
+}
+
+static const struct drm_plane_helper_funcs dm_plane_helper_funcs = {
+	.prepare_fb = dm_plane_helper_prepare_fb,
+	.cleanup_fb = dm_plane_helper_cleanup_fb,
+};
+
+/*
+ * TODO: these are currently initialized to rgb formats only.
+ * For future use cases we should either initialize them dynamically based on
+ * plane capabilities, or initialize this array to all formats, so internal drm
+ * check will succeed, and let DC to implement proper check
+ */
+static uint32_t rgb_formats[] = {
+	DRM_FORMAT_XRGB4444,
+	DRM_FORMAT_ARGB4444,
+	DRM_FORMAT_RGBA4444,
+	DRM_FORMAT_ARGB1555,
+	DRM_FORMAT_RGB565,
+	DRM_FORMAT_RGB888,
+	DRM_FORMAT_XRGB8888,
+	DRM_FORMAT_ARGB8888,
+	DRM_FORMAT_RGBA8888,
+	DRM_FORMAT_XRGB2101010,
+	DRM_FORMAT_XBGR2101010,
+	DRM_FORMAT_ARGB2101010,
+	DRM_FORMAT_ABGR2101010,
+};
+
+int amdgpu_dm_crtc_init(struct amdgpu_display_manager *dm,
+			struct amdgpu_crtc *acrtc,
+			uint32_t crtc_index)
+{
+	int res = -ENOMEM;
+
+	struct drm_plane *primary_plane =
+		kzalloc(sizeof(*primary_plane), GFP_KERNEL);
+
+	if (!primary_plane)
+		goto fail_plane;
+
+	primary_plane->format_default = true;
+
+	res = drm_universal_plane_init(
+		dm->adev->ddev,
+		primary_plane,
+		0,
+		&dm_plane_funcs,
+		rgb_formats,
+		ARRAY_SIZE(rgb_formats),
+		DRM_PLANE_TYPE_PRIMARY, NULL);
+
+	primary_plane->crtc = &acrtc->base;
+
+	drm_plane_helper_add(primary_plane, &dm_plane_helper_funcs);
+
+	res = drm_crtc_init_with_planes(
+			dm->ddev,
+			&acrtc->base,
+			primary_plane,
+			NULL,
+			&amdgpu_dm_crtc_funcs, NULL);
+
+	if (res)
+		goto fail;
+
+	drm_crtc_helper_add(&acrtc->base, &amdgpu_dm_crtc_helper_funcs);
+
+	acrtc->max_cursor_width = 128;
+	acrtc->max_cursor_height = 128;
+
+	acrtc->crtc_id = crtc_index;
+	acrtc->base.enabled = false;
+
+	dm->adev->mode_info.crtcs[crtc_index] = acrtc;
+	drm_mode_crtc_set_gamma_size(&acrtc->base, 256);
+
+	return 0;
+fail:
+	kfree(primary_plane);
+fail_plane:
+	acrtc->crtc_id = -1;
+	return res;
+}
+
+static int to_drm_connector_type(enum signal_type st)
+{
+	switch (st) {
+	case SIGNAL_TYPE_HDMI_TYPE_A:
+		return DRM_MODE_CONNECTOR_HDMIA;
+	case SIGNAL_TYPE_EDP:
+		return DRM_MODE_CONNECTOR_eDP;
+	case SIGNAL_TYPE_RGB:
+		return DRM_MODE_CONNECTOR_VGA;
+	case SIGNAL_TYPE_DISPLAY_PORT:
+	case SIGNAL_TYPE_DISPLAY_PORT_MST:
+		return DRM_MODE_CONNECTOR_DisplayPort;
+	case SIGNAL_TYPE_DVI_DUAL_LINK:
+	case SIGNAL_TYPE_DVI_SINGLE_LINK:
+		return DRM_MODE_CONNECTOR_DVID;
+	case SIGNAL_TYPE_VIRTUAL:
+		return DRM_MODE_CONNECTOR_VIRTUAL;
+
+	default:
+		return DRM_MODE_CONNECTOR_Unknown;
+	}
+}
+
+static void amdgpu_dm_get_native_mode(struct drm_connector *connector)
+{
+	const struct drm_connector_helper_funcs *helper =
+		connector->helper_private;
+	struct drm_encoder *encoder;
+	struct amdgpu_encoder *amdgpu_encoder;
+
+	encoder = helper->best_encoder(connector);
+
+	if (encoder == NULL)
+		return;
+
+	amdgpu_encoder = to_amdgpu_encoder(encoder);
+
+	amdgpu_encoder->native_mode.clock = 0;
+
+	if (!list_empty(&connector->probed_modes)) {
+		struct drm_display_mode *preferred_mode = NULL;
+		list_for_each_entry(preferred_mode,
+				&connector->probed_modes,
+				head) {
+		if (preferred_mode->type & DRM_MODE_TYPE_PREFERRED) {
+			amdgpu_encoder->native_mode = *preferred_mode;
+		}
+			break;
+		}
+
+	}
+}
+
+static struct drm_display_mode *amdgpu_dm_create_common_mode(
+		struct drm_encoder *encoder, char *name,
+		int hdisplay, int vdisplay)
+{
+	struct drm_device *dev = encoder->dev;
+	struct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);
+	struct drm_display_mode *mode = NULL;
+	struct drm_display_mode *native_mode = &amdgpu_encoder->native_mode;
+
+	mode = drm_mode_duplicate(dev, native_mode);
+
+	if(mode == NULL)
+		return NULL;
+
+	mode->hdisplay = hdisplay;
+	mode->vdisplay = vdisplay;
+	mode->type &= ~DRM_MODE_TYPE_PREFERRED;
+	strncpy(mode->name, name, DRM_DISPLAY_MODE_LEN);
+
+	return mode;
+
+}
+
+static void amdgpu_dm_connector_add_common_modes(struct drm_encoder *encoder,
+					struct drm_connector *connector)
+{
+	struct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);
+	struct drm_display_mode *mode = NULL;
+	struct drm_display_mode *native_mode = &amdgpu_encoder->native_mode;
+	struct amdgpu_connector *amdgpu_connector =
+				to_amdgpu_connector(connector);
+	int i;
+	int n;
+	struct mode_size {
+		char name[DRM_DISPLAY_MODE_LEN];
+		int w;
+		int h;
+	}common_modes[] = {
+		{  "640x480",  640,  480},
+		{  "800x600",  800,  600},
+		{ "1024x768", 1024,  768},
+		{ "1280x720", 1280,  720},
+		{ "1280x800", 1280,  800},
+		{"1280x1024", 1280, 1024},
+		{ "1440x900", 1440,  900},
+		{"1680x1050", 1680, 1050},
+		{"1600x1200", 1600, 1200},
+		{"1920x1080", 1920, 1080},
+		{"1920x1200", 1920, 1200}
+	};
+
+	n = sizeof(common_modes) / sizeof(common_modes[0]);
+
+	for (i = 0; i < n; i++) {
+		struct drm_display_mode *curmode = NULL;
+		bool mode_existed = false;
+
+		if (common_modes[i].w > native_mode->hdisplay ||
+			common_modes[i].h > native_mode->vdisplay ||
+			(common_modes[i].w == native_mode->hdisplay &&
+			common_modes[i].h == native_mode->vdisplay))
+				continue;
+
+		list_for_each_entry(curmode, &connector->probed_modes, head) {
+			if (common_modes[i].w == curmode->hdisplay &&
+				common_modes[i].h == curmode->vdisplay) {
+				mode_existed = true;
+				break;
+			}
+		}
+
+		if (mode_existed)
+			continue;
+
+		mode = amdgpu_dm_create_common_mode(encoder,
+				common_modes[i].name, common_modes[i].w,
+				common_modes[i].h);
+		drm_mode_probed_add(connector, mode);
+		amdgpu_connector->num_modes++;
+	}
+}
+
+static void amdgpu_dm_connector_ddc_get_modes(
+	struct drm_connector *connector,
+	struct edid *edid)
+{
+	struct amdgpu_connector *amdgpu_connector =
+			to_amdgpu_connector(connector);
+
+	if (edid) {
+		/* empty probed_modes */
+		INIT_LIST_HEAD(&connector->probed_modes);
+		amdgpu_connector->num_modes =
+				drm_add_edid_modes(connector, edid);
+
+		drm_edid_to_eld(connector, edid);
+
+		amdgpu_dm_get_native_mode(connector);
+	} else
+		amdgpu_connector->num_modes = 0;
+}
+
+int amdgpu_dm_connector_get_modes(struct drm_connector *connector)
+{
+	const struct drm_connector_helper_funcs *helper =
+			connector->helper_private;
+	struct amdgpu_connector *amdgpu_connector =
+			to_amdgpu_connector(connector);
+	struct drm_encoder *encoder;
+	struct edid *edid = amdgpu_connector->edid;
+
+	encoder = helper->best_encoder(connector);
+
+	amdgpu_dm_connector_ddc_get_modes(connector, edid);
+	amdgpu_dm_connector_add_common_modes(encoder, connector);
+	return amdgpu_connector->num_modes;
+}
+
+void amdgpu_dm_connector_init_helper(
+	struct amdgpu_display_manager *dm,
+	struct amdgpu_connector *aconnector,
+	int connector_type,
+	const struct dc_link *link,
+	int link_index)
+{
+	struct amdgpu_device *adev = dm->ddev->dev_private;
+
+	aconnector->connector_id = link_index;
+	aconnector->dc_link = link;
+	aconnector->base.interlace_allowed = true;
+	aconnector->base.doublescan_allowed = true;
+	aconnector->base.dpms = DRM_MODE_DPMS_OFF;
+	aconnector->hpd.hpd = AMDGPU_HPD_NONE; /* not used */
+
+	mutex_init(&aconnector->hpd_lock);
+
+	/*configure suport HPD hot plug connector_>polled default value is 0
+	 * which means HPD hot plug not supported*/
+	switch (connector_type) {
+	case DRM_MODE_CONNECTOR_HDMIA:
+		aconnector->base.polled = DRM_CONNECTOR_POLL_HPD;
+		break;
+	case DRM_MODE_CONNECTOR_DisplayPort:
+		aconnector->base.polled = DRM_CONNECTOR_POLL_HPD;
+		break;
+	case DRM_MODE_CONNECTOR_DVID:
+		aconnector->base.polled = DRM_CONNECTOR_POLL_HPD;
+		break;
+	default:
+		break;
+	}
+
+	drm_object_attach_property(&aconnector->base.base,
+				dm->ddev->mode_config.scaling_mode_property,
+				DRM_MODE_SCALE_NONE);
+
+	drm_object_attach_property(&aconnector->base.base,
+				adev->mode_info.underscan_property,
+				UNDERSCAN_OFF);
+	drm_object_attach_property(&aconnector->base.base,
+				adev->mode_info.underscan_hborder_property,
+				0);
+	drm_object_attach_property(&aconnector->base.base,
+				adev->mode_info.underscan_vborder_property,
+				0);
+
+	if (connector_type == DRM_MODE_CONNECTOR_HDMIA ||
+	    connector_type == DRM_MODE_CONNECTOR_DisplayPort) {
+		drm_object_attach_property(&aconnector->base.base,
+					  adev->mode_info.freesync_property, 0);
+		drm_object_attach_property(&aconnector->base.base,
+				      adev->mode_info.freesync_capable_property,
+					   0);
+	}
+}
+
+int amdgpu_dm_i2c_xfer(struct i2c_adapter *i2c_adap,
+		      struct i2c_msg *msgs, int num)
+{
+	struct amdgpu_i2c_adapter *i2c = i2c_get_adapdata(i2c_adap);
+	struct i2c_command cmd;
+	int i;
+	int result = -EIO;
+
+	cmd.payloads = kzalloc(num * sizeof(struct i2c_payload), GFP_KERNEL);
+
+	if (!cmd.payloads)
+		return result;
+
+	cmd.number_of_payloads = num;
+	cmd.engine = I2C_COMMAND_ENGINE_DEFAULT;
+	cmd.speed = 100;
+
+	for (i = 0; i < num; i++) {
+		cmd.payloads[i].write = (msgs[i].flags & I2C_M_RD);
+		cmd.payloads[i].address = msgs[i].addr;
+		cmd.payloads[i].length = msgs[i].len;
+		cmd.payloads[i].data = msgs[i].buf;
+	}
+
+	if (dc_submit_i2c(i2c->dm->dc, i2c->link_index, &cmd))
+		result = num;
+
+	kfree(cmd.payloads);
+
+	return result;
+}
+
+u32 amdgpu_dm_i2c_func(struct i2c_adapter *adap)
+{
+	return I2C_FUNC_I2C | I2C_FUNC_SMBUS_EMUL;
+}
+
+static const struct i2c_algorithm amdgpu_dm_i2c_algo = {
+	.master_xfer = amdgpu_dm_i2c_xfer,
+	.functionality = amdgpu_dm_i2c_func,
+};
+
+struct amdgpu_i2c_adapter *create_i2c(unsigned int link_index, struct amdgpu_display_manager *dm, int *res)
+{
+	struct amdgpu_i2c_adapter *i2c;
+
+	i2c = kzalloc(sizeof (struct amdgpu_i2c_adapter), GFP_KERNEL);
+	i2c->dm = dm;
+	i2c->base.owner = THIS_MODULE;
+	i2c->base.class = I2C_CLASS_DDC;
+	i2c->base.dev.parent = &dm->adev->pdev->dev;
+	i2c->base.algo = &amdgpu_dm_i2c_algo;
+	snprintf(i2c->base.name, sizeof (i2c->base.name), "AMDGPU DM i2c hw bus %d", link_index);
+	i2c->link_index = link_index;
+	i2c_set_adapdata(&i2c->base, i2c);
+
+	return i2c;
+}
+
+/* Note: this function assumes that dc_link_detect() was called for the
+ * dc_link which will be represented by this aconnector. */
+int amdgpu_dm_connector_init(
+	struct amdgpu_display_manager *dm,
+	struct amdgpu_connector *aconnector,
+	uint32_t link_index,
+	struct amdgpu_encoder *aencoder)
+{
+	int res = 0;
+	int connector_type;
+	struct dc *dc = dm->dc;
+	const struct dc_link *link = dc_get_link_at_index(dc, link_index);
+	struct amdgpu_i2c_adapter *i2c;
+
+	DRM_DEBUG_KMS("%s()\n", __func__);
+
+	i2c = create_i2c(link->link_index, dm, &res);
+	aconnector->i2c = i2c;
+	res = i2c_add_adapter(&i2c->base);
+
+	if (res) {
+		DRM_ERROR("Failed to register hw i2c %d\n", link->link_index);
+		goto out_free;
+	}
+
+	connector_type = to_drm_connector_type(link->connector_signal);
+
+	res = drm_connector_init(
+			dm->ddev,
+			&aconnector->base,
+			&amdgpu_dm_connector_funcs,
+			connector_type);
+
+	if (res) {
+		DRM_ERROR("connector_init failed\n");
+		aconnector->connector_id = -1;
+		goto out_free;
+	}
+
+	drm_connector_helper_add(
+			&aconnector->base,
+			&amdgpu_dm_connector_helper_funcs);
+
+	amdgpu_dm_connector_init_helper(
+		dm,
+		aconnector,
+		connector_type,
+		link,
+		link_index);
+
+	drm_mode_connector_attach_encoder(
+		&aconnector->base, &aencoder->base);
+
+	drm_connector_register(&aconnector->base);
+
+	if (connector_type == DRM_MODE_CONNECTOR_DisplayPort
+		|| connector_type == DRM_MODE_CONNECTOR_eDP)
+		amdgpu_dm_initialize_mst_connector(dm, aconnector);
+
+#if defined(CONFIG_BACKLIGHT_CLASS_DEVICE) ||\
+	defined(CONFIG_BACKLIGHT_CLASS_DEVICE_MODULE)
+
+	/* NOTE: this currently will create backlight device even if a panel
+	 * is not connected to the eDP/LVDS connector.
+	 *
+	 * This is less than ideal but we don't have sink information at this
+	 * stage since detection happens after. We can't do detection earlier
+	 * since MST detection needs connectors to be created first.
+	 */
+	if (link->connector_signal & (SIGNAL_TYPE_EDP | SIGNAL_TYPE_LVDS)) {
+		/* Event if registration failed, we should continue with
+		 * DM initialization because not having a backlight control
+		 * is better then a black screen. */
+		amdgpu_dm_register_backlight_device(dm);
+
+		if (dm->backlight_dev)
+			dm->backlight_link = link;
+	}
+#endif
+
+out_free:
+	if (res) {
+		kfree(i2c);
+		aconnector->i2c = NULL;
+	}
+	return res;
+}
+
+int amdgpu_dm_get_encoder_crtc_mask(struct amdgpu_device *adev)
+{
+	switch (adev->mode_info.num_crtc) {
+	case 1:
+		return 0x1;
+	case 2:
+		return 0x3;
+	case 3:
+		return 0x7;
+	case 4:
+		return 0xf;
+	case 5:
+		return 0x1f;
+	case 6:
+	default:
+		return 0x3f;
+	}
+}
+
+int amdgpu_dm_encoder_init(
+	struct drm_device *dev,
+	struct amdgpu_encoder *aencoder,
+	uint32_t link_index)
+{
+	struct amdgpu_device *adev = dev->dev_private;
+
+	int res = drm_encoder_init(dev,
+				   &aencoder->base,
+				   &amdgpu_dm_encoder_funcs,
+				   DRM_MODE_ENCODER_TMDS,
+				   NULL);
+
+	aencoder->base.possible_crtcs = amdgpu_dm_get_encoder_crtc_mask(adev);
+
+	if (!res)
+		aencoder->encoder_id = link_index;
+	else
+		aencoder->encoder_id = -1;
+
+	drm_encoder_helper_add(&aencoder->base, &amdgpu_dm_encoder_helper_funcs);
+
+	return res;
+}
+
+enum dm_commit_action {
+	DM_COMMIT_ACTION_NOTHING,
+	DM_COMMIT_ACTION_RESET,
+	DM_COMMIT_ACTION_DPMS_ON,
+	DM_COMMIT_ACTION_DPMS_OFF,
+	DM_COMMIT_ACTION_SET
+};
+
+static enum dm_commit_action get_dm_commit_action(struct drm_crtc_state *state)
+{
+	/* mode changed means either actually mode changed or enabled changed */
+	/* active changed means dpms changed */
+
+	DRM_DEBUG_KMS("crtc_state_flags: enable:%d, active:%d, planes_changed:%d, mode_changed:%d,active_changed:%d,connectors_changed:%d\n",
+			state->enable,
+			state->active,
+			state->planes_changed,
+			state->mode_changed,
+			state->active_changed,
+			state->connectors_changed);
+
+	if (state->mode_changed) {
+		/* if it is got disabled - call reset mode */
+		if (!state->enable)
+			return DM_COMMIT_ACTION_RESET;
+
+		if (state->active)
+			return DM_COMMIT_ACTION_SET;
+		else
+			return DM_COMMIT_ACTION_RESET;
+	} else {
+		/* ! mode_changed */
+
+		/* if it is remain disable - skip it */
+		if (!state->enable)
+			return DM_COMMIT_ACTION_NOTHING;
+
+		if (state->active && state->connectors_changed)
+			return DM_COMMIT_ACTION_SET;
+
+		if (state->active_changed) {
+			if (state->active) {
+				return DM_COMMIT_ACTION_DPMS_ON;
+			} else {
+				return DM_COMMIT_ACTION_DPMS_OFF;
+			}
+		} else {
+			/* ! active_changed */
+			return DM_COMMIT_ACTION_NOTHING;
+		}
+	}
+}
+
+
+typedef bool (*predicate)(struct amdgpu_crtc *acrtc);
+
+static void wait_while_pflip_status(struct amdgpu_device *adev,
+		struct amdgpu_crtc *acrtc, predicate f) {
+	int count = 0;
+	while (f(acrtc)) {
+		/* Spin Wait*/
+		msleep(1);
+		count++;
+		if (count == 1000) {
+			DRM_ERROR("%s - crtc:%d[%p], pflip_stat:%d, probable hang!\n",
+										__func__, acrtc->crtc_id,
+										acrtc,
+										acrtc->pflip_status);
+
+			/* we do not expect to hit this case except on Polaris with PHY PLL
+			 * 1. DP to HDMI passive dongle connected
+			 * 2. unplug (headless)
+			 * 3. plug in DP
+			 * 3a. on plug in, DP will try verify link by training, and training
+			 * would disable PHY PLL which HDMI rely on to drive TG
+			 * 3b. this will cause flip interrupt cannot be generated, and we
+			 * exit when timeout expired.  however we do not have code to clean
+			 * up flip, flip clean up will happen when the address is written
+			 * with the restore mode change
+			 */
+			WARN_ON(1);
+			break;
+		}
+	}
+
+	DRM_DEBUG_DRIVER("%s - Finished waiting for:%d msec, crtc:%d[%p], pflip_stat:%d \n",
+											__func__,
+											count,
+											acrtc->crtc_id,
+											acrtc,
+											acrtc->pflip_status);
+}
+
+static bool pflip_in_progress_predicate(struct amdgpu_crtc *acrtc)
+{
+	return acrtc->pflip_status != AMDGPU_FLIP_NONE;
+}
+
+static void manage_dm_interrupts(
+	struct amdgpu_device *adev,
+	struct amdgpu_crtc *acrtc,
+	bool enable)
+{
+	/*
+	 * this is not correct translation but will work as soon as VBLANK
+	 * constant is the same as PFLIP
+	 */
+	int irq_type =
+		amdgpu_crtc_idx_to_irq_type(
+			adev,
+			acrtc->crtc_id);
+
+	if (enable) {
+		drm_crtc_vblank_on(&acrtc->base);
+		amdgpu_irq_get(
+			adev,
+			&adev->pageflip_irq,
+			irq_type);
+	} else {
+		wait_while_pflip_status(adev, acrtc,
+				pflip_in_progress_predicate);
+
+		amdgpu_irq_put(
+			adev,
+			&adev->pageflip_irq,
+			irq_type);
+		drm_crtc_vblank_off(&acrtc->base);
+	}
+}
+
+
+static bool pflip_pending_predicate(struct amdgpu_crtc *acrtc)
+{
+	return acrtc->pflip_status == AMDGPU_FLIP_PENDING;
+}
+
+static bool is_scaling_state_different(
+		const struct dm_connector_state *dm_state,
+		const struct dm_connector_state *old_dm_state)
+{
+	if (dm_state->scaling != old_dm_state->scaling)
+		return true;
+	if (!dm_state->underscan_enable && old_dm_state->underscan_enable) {
+		if (old_dm_state->underscan_hborder != 0 && old_dm_state->underscan_vborder != 0)
+			return true;
+	} else  if (dm_state->underscan_enable && !old_dm_state->underscan_enable) {
+		if (dm_state->underscan_hborder != 0 && dm_state->underscan_vborder != 0)
+			return true;
+	} else if (dm_state->underscan_hborder != old_dm_state->underscan_hborder
+				|| dm_state->underscan_vborder != old_dm_state->underscan_vborder)
+			return true;
+	return false;
+}
+
+static void remove_target(struct amdgpu_device *adev, struct amdgpu_crtc *acrtc)
+{
+	int i;
+
+	/*
+	 * we evade vblanks and pflips on crtc that
+	 * should be changed
+	 */
+	manage_dm_interrupts(adev, acrtc, false);
+	/* this is the update mode case */
+	if (adev->dm.freesync_module)
+		for (i = 0; i < acrtc->target->stream_count; i++)
+			mod_freesync_remove_stream(
+					adev->dm.freesync_module,
+					acrtc->target->streams[i]);
+	dc_target_release(acrtc->target);
+	acrtc->target = NULL;
+	acrtc->otg_inst = -1;
+	acrtc->enabled = false;
+}
+
+int amdgpu_dm_atomic_commit(
+	struct drm_device *dev,
+	struct drm_atomic_state *state,
+	bool async)
+{
+	struct amdgpu_device *adev = dev->dev_private;
+	struct amdgpu_display_manager *dm = &adev->dm;
+	struct drm_plane *plane;
+	struct drm_plane_state *old_plane_state;
+	uint32_t i, j;
+	int32_t ret = 0;
+	uint32_t commit_targets_count = 0;
+	uint32_t new_crtcs_count = 0;
+	struct drm_crtc *crtc;
+	struct drm_crtc_state *old_crtc_state;
+
+	struct dc_target *commit_targets[MAX_TARGETS];
+	struct amdgpu_crtc *new_crtcs[MAX_TARGETS];
+	struct dc_target *new_target;
+
+	/* In this step all new fb would be pinned */
+
+	/*
+	 * TODO: Revisit when we support true asynchronous commit.
+	 * Right now we receive async commit only from pageflip, in which case
+	 * we should not pin/unpin the fb here, it should be done in
+	 * amdgpu_crtc_flip and from the vblank irq handler.
+	 */
+	if (!async) {
+		ret = drm_atomic_helper_prepare_planes(dev, state);
+		if (ret)
+			return ret;
+	}
+
+	/*
+	 * This is the point of no return - everything below never fails except
+	 * when the hw goes bonghits. Which means we can commit the new state on
+	 * the software side now.
+	 */
+
+	drm_atomic_helper_swap_state(state, true);
+
+	/*
+	 * From this point state become old state really. New state is
+	 * initialized to appropriate objects and could be accessed from there
+	 */
+
+	/*
+	 * there is no fences usage yet in state. We can skip the following line
+	 * wait_for_fences(dev, state);
+	 */
+
+	drm_atomic_helper_update_legacy_modeset_state(dev, state);
+
+	/* update changed items */
+	for_each_crtc_in_state(state, crtc, old_crtc_state, i) {
+		struct amdgpu_crtc *acrtc;
+		struct amdgpu_connector *aconnector = NULL;
+		enum dm_commit_action action;
+		struct drm_crtc_state *new_state = crtc->state;
+
+		acrtc = to_amdgpu_crtc(crtc);
+
+		aconnector =
+			amdgpu_dm_find_first_crct_matching_connector(
+				state,
+				crtc,
+				false);
+
+		/* handles headless hotplug case, updating new_state and
+		 * aconnector as needed
+		 */
+
+		action = get_dm_commit_action(new_state);
+
+		switch (action) {
+		case DM_COMMIT_ACTION_DPMS_ON:
+		case DM_COMMIT_ACTION_SET: {
+			struct dm_connector_state *dm_state = NULL;
+			new_target = NULL;
+
+			if (aconnector)
+				dm_state = to_dm_connector_state(aconnector->base.state);
+
+			new_target = create_target_for_sink(
+					aconnector,
+					&crtc->state->mode,
+					dm_state);
+
+			DRM_INFO("Atomic commit: SET crtc id %d: [%p]\n", acrtc->crtc_id, acrtc);
+
+			if (!new_target) {
+				/*
+				 * this could happen because of issues with
+				 * userspace notifications delivery.
+				 * In this case userspace tries to set mode on
+				 * display which is disconnect in fact.
+				 * dc_sink in NULL in this case on aconnector.
+				 * We expect reset mode will come soon.
+				 *
+				 * This can also happen when unplug is done
+				 * during resume sequence ended
+				 *
+				 * In this case, we want to pretend we still
+				 * have a sink to keep the pipe running so that
+				 * hw state is consistent with the sw state
+				 */
+				DRM_DEBUG_KMS("%s: Failed to create new target for crtc %d\n",
+						__func__, acrtc->base.base.id);
+				break;
+			}
+
+			if (acrtc->target)
+				remove_target(adev, acrtc);
+
+			/*
+			 * this loop saves set mode crtcs
+			 * we needed to enable vblanks once all
+			 * resources acquired in dc after dc_commit_targets
+			 */
+			new_crtcs[new_crtcs_count] = acrtc;
+			new_crtcs_count++;
+
+			acrtc->target = new_target;
+			acrtc->enabled = true;
+			acrtc->hw_mode = crtc->state->mode;
+			crtc->hwmode = crtc->state->mode;
+
+			break;
+		}
+
+		case DM_COMMIT_ACTION_NOTHING: {
+			struct dm_connector_state *dm_state = NULL;
+
+			if (!aconnector)
+				break;
+
+			dm_state = to_dm_connector_state(aconnector->base.state);
+
+			/* Scaling update */
+			update_stream_scaling_settings(
+					&crtc->state->mode,
+					dm_state,
+					acrtc->target->streams[0]);
+
+			break;
+		}
+		case DM_COMMIT_ACTION_DPMS_OFF:
+		case DM_COMMIT_ACTION_RESET:
+			DRM_INFO("Atomic commit: RESET. crtc id %d:[%p]\n", acrtc->crtc_id, acrtc);
+			/* i.e. reset mode */
+			if (acrtc->target)
+				remove_target(adev, acrtc);
+			break;
+		} /* switch() */
+	} /* for_each_crtc_in_state() */
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+
+		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+
+		if (acrtc->target) {
+			commit_targets[commit_targets_count] = acrtc->target;
+			++commit_targets_count;
+		}
+	}
+
+	/*
+	 * Add streams after required streams from new and replaced targets
+	 * are removed from freesync module
+	 */
+	if (adev->dm.freesync_module) {
+		for (i = 0; i < new_crtcs_count; i++) {
+			struct amdgpu_connector *aconnector = NULL;
+			new_target = new_crtcs[i]->target;
+			aconnector =
+				amdgpu_dm_find_first_crct_matching_connector(
+					state,
+					&new_crtcs[i]->base,
+					false);
+			if (!aconnector) {
+				DRM_INFO(
+						"Atomic commit: Failed to find connector for acrtc id:%d "
+						"skipping freesync init\n",
+						new_crtcs[i]->crtc_id);
+				continue;
+			}
+
+			for (j = 0; j < new_target->stream_count; j++)
+				mod_freesync_add_stream(
+						adev->dm.freesync_module,
+						new_target->streams[j], &aconnector->caps);
+		}
+	}
+
+	/* DC is optimized not to do anything if 'targets' didn't change. */
+	dc_commit_targets(dm->dc, commit_targets, commit_targets_count);
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+
+		if (acrtc->target != NULL)
+			acrtc->otg_inst =
+				dc_target_get_status(acrtc->target)->primary_otg_inst;
+	}
+
+	/* update planes when needed */
+	for_each_plane_in_state(state, plane, old_plane_state, i) {
+		struct drm_plane_state *plane_state = plane->state;
+		struct drm_crtc *crtc = plane_state->crtc;
+		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+		struct drm_framebuffer *fb = plane_state->fb;
+		struct drm_connector *connector;
+		struct dm_connector_state *dm_state = NULL;
+		enum dm_commit_action action;
+
+		if (!fb || !crtc || !crtc->state->planes_changed ||
+			!crtc->state->active)
+			continue;
+
+		action = get_dm_commit_action(crtc->state);
+
+		/* Surfaces are created under two scenarios:
+		 * 1. This commit is not a page flip.
+		 * 2. This commit is a page flip, and targets are created.
+		 */
+		if (!page_flip_needed(plane_state, old_plane_state, true) ||
+				action == DM_COMMIT_ACTION_DPMS_ON ||
+				action == DM_COMMIT_ACTION_SET) {
+			list_for_each_entry(connector,
+				&dev->mode_config.connector_list, head)	{
+				if (connector->state->crtc == crtc) {
+					dm_state = to_dm_connector_state(
+						connector->state);
+					break;
+				}
+			}
+
+			/*
+			 * This situation happens in the following case:
+			 * we are about to get set mode for connector who's only
+			 * possible crtc (in encoder crtc mask) is used by
+			 * another connector, that is why it will try to
+			 * re-assing crtcs in order to make configuration
+			 * supported. For our implementation we need to make all
+			 * encoders support all crtcs, then this issue will
+			 * never arise again. But to guard code from this issue
+			 * check is left.
+			 *
+			 * Also it should be needed when used with actual
+			 * drm_atomic_commit ioctl in future
+			 */
+			if (!dm_state)
+				continue;
+
+			/*
+			 * if flip is pending (ie, still waiting for fence to return
+			 * before address is submitted) here, we cannot commit_surface
+			 * as commit_surface will pre-maturely write out the future
+			 * address. wait until flip is submitted before proceeding.
+			 */
+			wait_while_pflip_status(adev, acrtc, pflip_pending_predicate);
+
+			dm_dc_surface_commit(dm->dc, crtc);
+		}
+	}
+
+	for (i = 0; i < new_crtcs_count; i++) {
+		/*
+		 * loop to enable interrupts on newly arrived crtc
+		 */
+		struct amdgpu_crtc *acrtc = new_crtcs[i];
+
+		if (adev->dm.freesync_module) {
+			for (j = 0; j < acrtc->target->stream_count; j++)
+				mod_freesync_notify_mode_change(
+						adev->dm.freesync_module,
+						acrtc->target->streams,
+						acrtc->target->stream_count);
+		}
+
+		manage_dm_interrupts(adev, acrtc, true);
+		dm_crtc_cursor_reset(&acrtc->base);
+
+	}
+
+	/* Page flip if needed */
+	for_each_plane_in_state(state, plane, old_plane_state, i) {
+		struct drm_plane_state *plane_state = plane->state;
+		struct drm_crtc *crtc = plane_state->crtc;
+		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+		struct drm_framebuffer *fb = plane_state->fb;
+
+		if (!fb || !crtc || !crtc->state->planes_changed ||
+			!crtc->state->active)
+			continue;
+
+		if (page_flip_needed(plane_state, old_plane_state, false)) {
+			ret = amdgpu_crtc_page_flip_target(crtc,
+							   fb,
+							   crtc->state->event,
+							   acrtc->flip_flags,
+							   drm_crtc_vblank_count(crtc));
+			/*clean up the flags for next usage*/
+			acrtc->flip_flags = 0;
+			if (ret)
+				return ret;
+		}
+	}
+
+	/* In this state all old framebuffers would be unpinned */
+
+	/* TODO: Revisit when we support true asynchronous commit.*/
+	if (!async)
+		drm_atomic_helper_cleanup_planes(dev, state);
+
+	drm_atomic_state_put(state);
+
+	return ret;
+}
+/*
+ * This functions handle all cases when set mode does not come upon hotplug.
+ * This include when the same display is unplugged then plugged back into the
+ * same port and when we are running without usermode desktop manager supprot
+ */
+void dm_restore_drm_connector_state(struct drm_device *dev, struct drm_connector *connector)
+{
+	struct drm_crtc *crtc;
+	struct amdgpu_device *adev = dev->dev_private;
+	struct dc *dc = adev->dm.dc;
+	struct amdgpu_connector *aconnector = to_amdgpu_connector(connector);
+	struct amdgpu_crtc *disconnected_acrtc;
+	const struct dc_sink *sink;
+	struct dc_target *commit_targets[6];
+	struct dc_target *current_target;
+	uint32_t commit_targets_count = 0;
+	int i;
+
+	if (!aconnector->dc_sink || !connector->state || !connector->encoder)
+		return;
+
+	disconnected_acrtc = to_amdgpu_crtc(connector->encoder->crtc);
+
+	if (!disconnected_acrtc || !disconnected_acrtc->target)
+		return;
+
+	sink = disconnected_acrtc->target->streams[0]->sink;
+
+	/*
+	 * If the previous sink is not released and different from the current,
+	 * we deduce we are in a state where we can not rely on usermode call
+	 * to turn on the display, so we do it here
+	 */
+	if (sink != aconnector->dc_sink) {
+		struct dm_connector_state *dm_state =
+				to_dm_connector_state(aconnector->base.state);
+
+		struct dc_target *new_target =
+			create_target_for_sink(
+				aconnector,
+				&disconnected_acrtc->base.state->mode,
+				dm_state);
+
+		DRM_INFO("Headless hotplug, restoring connector state\n");
+		/*
+		 * we evade vblanks and pflips on crtc that
+		 * should be changed
+		 */
+		manage_dm_interrupts(adev, disconnected_acrtc, false);
+		/* this is the update mode case */
+
+		current_target = disconnected_acrtc->target;
+
+		disconnected_acrtc->target = new_target;
+		disconnected_acrtc->enabled = true;
+		disconnected_acrtc->hw_mode = disconnected_acrtc->base.state->mode;
+
+		commit_targets_count = 0;
+
+		list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+			struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+
+			if (acrtc->target) {
+				commit_targets[commit_targets_count] = acrtc->target;
+				++commit_targets_count;
+			}
+		}
+
+		/* DC is optimized not to do anything if 'targets' didn't change. */
+		if (!dc_commit_targets(dc, commit_targets,
+				commit_targets_count)) {
+			DRM_INFO("Failed to restore connector state!\n");
+			dc_target_release(disconnected_acrtc->target);
+			disconnected_acrtc->target = current_target;
+			manage_dm_interrupts(adev, disconnected_acrtc, true);
+			return;
+		}
+
+		if (adev->dm.freesync_module) {
+
+			for (i = 0; i < current_target->stream_count; i++)
+				mod_freesync_remove_stream(
+						adev->dm.freesync_module,
+						current_target->streams[i]);
+
+			for (i = 0; i < new_target->stream_count; i++)
+				mod_freesync_add_stream(
+						adev->dm.freesync_module,
+						new_target->streams[i],
+						&aconnector->caps);
+		}
+		list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+			struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+
+			if (acrtc->target != NULL) {
+				acrtc->otg_inst =
+					dc_target_get_status(acrtc->target)->primary_otg_inst;
+			}
+		}
+
+		dc_target_release(current_target);
+
+		dm_dc_surface_commit(dc, &disconnected_acrtc->base);
+
+		manage_dm_interrupts(adev, disconnected_acrtc, true);
+		dm_crtc_cursor_reset(&disconnected_acrtc->base);
+
+	}
+}
+
+static uint32_t add_val_sets_surface(
+	struct dc_validation_set *val_sets,
+	uint32_t set_count,
+	const struct dc_target *target,
+	const struct dc_surface *surface)
+{
+	uint32_t i = 0;
+
+	while (i < set_count) {
+		if (val_sets[i].target == target)
+			break;
+		++i;
+	}
+
+	val_sets[i].surfaces[val_sets[i].surface_count] = surface;
+	val_sets[i].surface_count++;
+
+	return val_sets[i].surface_count;
+}
+
+static uint32_t update_in_val_sets_target(
+	struct dc_validation_set *val_sets,
+	struct drm_crtc **crtcs,
+	uint32_t set_count,
+	const struct dc_target *old_target,
+	const struct dc_target *new_target,
+	struct drm_crtc *crtc)
+{
+	uint32_t i = 0;
+
+	while (i < set_count) {
+		if (val_sets[i].target == old_target)
+			break;
+		++i;
+	}
+
+	val_sets[i].target = new_target;
+	crtcs[i] = crtc;
+
+	if (i == set_count) {
+		/* nothing found. add new one to the end */
+		return set_count + 1;
+	}
+
+	return set_count;
+}
+
+static uint32_t remove_from_val_sets(
+	struct dc_validation_set *val_sets,
+	uint32_t set_count,
+	const struct dc_target *target)
+{
+	int i;
+
+	for (i = 0; i < set_count; i++)
+		if (val_sets[i].target == target)
+			break;
+
+	if (i == set_count) {
+		/* nothing found */
+		return set_count;
+	}
+
+	set_count--;
+
+	for (; i < set_count; i++) {
+		val_sets[i] = val_sets[i + 1];
+	}
+
+	return set_count;
+}
+
+int amdgpu_dm_atomic_check(struct drm_device *dev,
+			struct drm_atomic_state *state)
+{
+	struct drm_crtc *crtc;
+	struct drm_crtc_state *crtc_state;
+	struct drm_plane *plane;
+	struct drm_plane_state *plane_state;
+	int i, j;
+	int ret;
+	int set_count;
+	int new_target_count;
+	struct dc_validation_set set[MAX_TARGETS] = {{ 0 }};
+	struct dc_target *new_targets[MAX_TARGETS] = { 0 };
+	struct drm_crtc *crtc_set[MAX_TARGETS] = { 0 };
+	struct amdgpu_device *adev = dev->dev_private;
+	struct dc *dc = adev->dm.dc;
+	bool need_to_validate = false;
+
+	ret = drm_atomic_helper_check(dev, state);
+
+	if (ret) {
+		DRM_ERROR("Atomic state validation failed with error :%d !\n",
+				ret);
+		return ret;
+	}
+
+	ret = -EINVAL;
+
+	/* copy existing configuration */
+	new_target_count = 0;
+	set_count = 0;
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+
+		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+
+		if (acrtc->target) {
+			set[set_count].target = acrtc->target;
+			crtc_set[set_count] = crtc;
+			++set_count;
+		}
+	}
+
+	/* update changed items */
+	for_each_crtc_in_state(state, crtc, crtc_state, i) {
+		struct amdgpu_crtc *acrtc = NULL;
+		struct amdgpu_connector *aconnector = NULL;
+		enum dm_commit_action action;
+
+		acrtc = to_amdgpu_crtc(crtc);
+
+		aconnector = amdgpu_dm_find_first_crct_matching_connector(state, crtc, true);
+
+		action = get_dm_commit_action(crtc_state);
+
+		switch (action) {
+		case DM_COMMIT_ACTION_DPMS_ON:
+		case DM_COMMIT_ACTION_SET: {
+			struct dc_target *new_target = NULL;
+			struct drm_connector_state *conn_state = NULL;
+			struct dm_connector_state *dm_state = NULL;
+
+			if (aconnector) {
+				conn_state = drm_atomic_get_connector_state(state, &aconnector->base);
+				if (IS_ERR(conn_state))
+					return ret;
+				dm_state = to_dm_connector_state(conn_state);
+			}
+
+			new_target = create_target_for_sink(aconnector, &crtc_state->mode, dm_state);
+
+			/*
+			 * we can have no target on ACTION_SET if a display
+			 * was disconnected during S3, in this case it not and
+			 * error, the OS will be updated after detection, and
+			 * do the right thing on next atomic commit
+			 */
+			if (!new_target) {
+				DRM_DEBUG_KMS("%s: Failed to create new target for crtc %d\n",
+						__func__, acrtc->base.base.id);
+				break;
+			}
+
+			new_targets[new_target_count] = new_target;
+			set_count = update_in_val_sets_target(
+					set,
+					crtc_set,
+					set_count,
+					acrtc->target,
+					new_target,
+					crtc);
+
+			new_target_count++;
+			need_to_validate = true;
+			break;
+		}
+
+		case DM_COMMIT_ACTION_NOTHING: {
+			const struct drm_connector *drm_connector = NULL;
+			struct drm_connector_state *conn_state = NULL;
+			struct dm_connector_state *dm_state = NULL;
+			struct dm_connector_state *old_dm_state = NULL;
+			struct dc_target *new_target;
+
+			if (!aconnector)
+				break;
+
+			for_each_connector_in_state(
+				state, drm_connector, conn_state, j) {
+				if (&aconnector->base == drm_connector)
+					break;
+			}
+
+			old_dm_state = to_dm_connector_state(drm_connector->state);
+			dm_state = to_dm_connector_state(conn_state);
+
+			/* Support underscan adjustment*/
+			if (!is_scaling_state_different(dm_state, old_dm_state))
+				break;
+
+			new_target = create_target_for_sink(aconnector, &crtc_state->mode, dm_state);
+
+			if (!new_target) {
+				DRM_ERROR("%s: Failed to create new target for crtc %d\n",
+						__func__, acrtc->base.base.id);
+				break;
+			}
+
+			new_targets[new_target_count] = new_target;
+			set_count = update_in_val_sets_target(
+					set,
+					crtc_set,
+					set_count,
+					acrtc->target,
+					new_target,
+					crtc);
+
+			new_target_count++;
+			need_to_validate = true;
+
+			break;
+		}
+		case DM_COMMIT_ACTION_DPMS_OFF:
+		case DM_COMMIT_ACTION_RESET:
+			/* i.e. reset mode */
+			if (acrtc->target) {
+				set_count = remove_from_val_sets(
+						set,
+						set_count,
+						acrtc->target);
+			}
+			break;
+		}
+	}
+
+	for (i = 0; i < set_count; i++) {
+		for_each_plane_in_state(state, plane, plane_state, j) {
+			struct drm_plane_state *old_plane_state = plane->state;
+			struct drm_crtc *crtc = plane_state->crtc;
+			struct drm_framebuffer *fb = plane_state->fb;
+			struct drm_connector *connector;
+			struct dm_connector_state *dm_state = NULL;
+			enum dm_commit_action action;
+
+			if (!fb || !crtc || crtc_set[i] != crtc ||
+				!crtc->state->planes_changed || !crtc->state->active)
+				continue;
+
+			action = get_dm_commit_action(crtc->state);
+
+			/* Surfaces are created under two scenarios:
+			 * 1. This commit is not a page flip.
+			 * 2. This commit is a page flip, and targets are created.
+			 */
+			if (!page_flip_needed(plane_state, old_plane_state,
+					      true) ||
+					action == DM_COMMIT_ACTION_DPMS_ON ||
+					action == DM_COMMIT_ACTION_SET) {
+				struct dc_surface *surface;
+
+				list_for_each_entry(connector,
+					&dev->mode_config.connector_list, head)	{
+					if (connector->state->crtc == crtc) {
+						dm_state = to_dm_connector_state(
+							connector->state);
+						break;
+					}
+				}
+
+				/*
+				 * This situation happens in the following case:
+				 * we are about to get set mode for connector who's only
+				 * possible crtc (in encoder crtc mask) is used by
+				 * another connector, that is why it will try to
+				 * re-assing crtcs in order to make configuration
+				 * supported. For our implementation we need to make all
+				 * encoders support all crtcs, then this issue will
+				 * never arise again. But to guard code from this issue
+				 * check is left.
+				 *
+				 * Also it should be needed when used with actual
+				 * drm_atomic_commit ioctl in future
+				 */
+				if (!dm_state)
+					continue;
+
+				surface = dc_create_surface(dc);
+				fill_plane_attributes(
+					surface,
+					plane_state,
+					false);
+
+				add_val_sets_surface(
+							set,
+							set_count,
+							set[i].target,
+							surface);
+
+				need_to_validate = true;
+			}
+		}
+	}
+
+	if (need_to_validate == false || set_count == 0 ||
+		dc_validate_resources(dc, set, set_count))
+		ret = 0;
+
+	for (i = 0; i < set_count; i++) {
+		for (j = 0; j < set[i].surface_count; j++) {
+			dc_surface_release(set[i].surfaces[j]);
+		}
+	}
+	for (i = 0; i < new_target_count; i++)
+		dc_target_release(new_targets[i]);
+
+	if (ret != 0)
+		DRM_ERROR("Atomic check failed.\n");
+
+	return ret;
+}
+
+static bool is_dp_capable_without_timing_msa(
+		struct dc *dc,
+		struct amdgpu_connector *amdgpu_connector)
+{
+	uint8_t dpcd_data;
+	bool capable = false;
+	if (amdgpu_connector->dc_link &&
+	    dc_read_dpcd(dc, amdgpu_connector->dc_link->link_index,
+			 DP_DOWN_STREAM_PORT_COUNT,
+			 &dpcd_data, sizeof(dpcd_data)) )
+		capable = dpcd_data & DP_MSA_TIMING_PAR_IGNORED? true:false;
+
+	return capable;
+}
+void amdgpu_dm_add_sink_to_freesync_module(
+		struct drm_connector *connector,
+		struct edid *edid)
+{
+	int i;
+	uint64_t val_capable;
+	bool edid_check_required;
+	struct detailed_timing *timing;
+	struct detailed_non_pixel *data;
+	struct detailed_data_monitor_range *range;
+	struct amdgpu_connector *amdgpu_connector =
+			to_amdgpu_connector(connector);
+
+	struct drm_device *dev = connector->dev;
+	struct amdgpu_device *adev = dev->dev_private;
+	edid_check_required = false;
+	if (!amdgpu_connector->dc_sink) {
+		DRM_ERROR("dc_sink NULL, could not add free_sync module.\n");
+		return;
+	}
+	if (!adev->dm.freesync_module)
+		return;
+	/*
+	 * if edid non zero restrict freesync only for dp and edp
+	 */
+	if (edid) {
+		if (amdgpu_connector->dc_sink->sink_signal == SIGNAL_TYPE_DISPLAY_PORT
+			|| amdgpu_connector->dc_sink->sink_signal == SIGNAL_TYPE_EDP) {
+			edid_check_required = is_dp_capable_without_timing_msa(
+						adev->dm.dc,
+						amdgpu_connector);
+		}
+	}
+	val_capable = 0;
+	if (edid_check_required == true && (edid->version > 1 ||
+	   (edid->version == 1 && edid->revision > 1))) {
+		for (i = 0; i < 4; i++) {
+
+			timing	= &edid->detailed_timings[i];
+			data	= &timing->data.other_data;
+			range	= &data->data.range;
+			/*
+			 * Check if monitor has continuous frequency mode
+			 */
+			if (data->type != EDID_DETAIL_MONITOR_RANGE)
+				continue;
+			/*
+			 * Check for flag range limits only. If flag == 1 then
+			 * no additional timing information provided.
+			 * Default GTF, GTF Secondary curve and CVT are not
+			 * supported
+			 */
+			if (range->flags != 1)
+				continue;
+
+			amdgpu_connector->min_vfreq = range->min_vfreq;
+			amdgpu_connector->max_vfreq = range->max_vfreq;
+			amdgpu_connector->pixel_clock_mhz =
+				range->pixel_clock_mhz * 10;
+			break;
+		}
+
+		if (amdgpu_connector->max_vfreq -
+				amdgpu_connector->min_vfreq > 10) {
+			amdgpu_connector->caps.supported = true;
+			amdgpu_connector->caps.min_refresh_in_micro_hz =
+					amdgpu_connector->min_vfreq * 1000000;
+			amdgpu_connector->caps.max_refresh_in_micro_hz =
+					amdgpu_connector->max_vfreq * 1000000;
+				val_capable = 1;
+		}
+	}
+	drm_object_property_set_value(&connector->base,
+				      adev->mode_info.freesync_capable_property,
+				      val_capable);
+	amdgpu_freesync_update_property_atomic(connector, val_capable);
+
+}
+
+void amdgpu_dm_remove_sink_from_freesync_module(
+		struct drm_connector *connector)
+{
+	struct amdgpu_connector *amdgpu_connector =
+			to_amdgpu_connector(connector);
+
+	struct drm_device *dev = connector->dev;
+	struct amdgpu_device *adev = dev->dev_private;
+
+	if (!amdgpu_connector->dc_sink || !adev->dm.freesync_module) {
+		DRM_ERROR("dc_sink NULL or no free_sync module.\n");
+		return;
+	}
+
+	amdgpu_connector->min_vfreq = 0;
+	amdgpu_connector->max_vfreq = 0;
+	amdgpu_connector->pixel_clock_mhz = 0;
+
+	memset(&amdgpu_connector->caps, 0, sizeof(amdgpu_connector->caps));
+
+	drm_object_property_set_value(&connector->base,
+				      adev->mode_info.freesync_capable_property,
+				      0);
+	amdgpu_freesync_update_property_atomic(connector, 0);
+
+}
diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_types.h b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_types.h
new file mode 100644
index 0000000..4f7bd3b
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_types.h
@@ -0,0 +1,101 @@
+/*
+ * Copyright 2012-13 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __AMDGPU_DM_TYPES_H__
+#define __AMDGPU_DM_TYPES_H__
+
+#include <drm/drmP.h>
+
+struct amdgpu_framebuffer;
+struct amdgpu_display_manager;
+struct dc_validation_set;
+struct dc_surface;
+
+/*TODO Jodan Hersen use the one in amdgpu_dm*/
+int amdgpu_dm_crtc_init(struct amdgpu_display_manager *dm,
+			struct amdgpu_crtc *amdgpu_crtc,
+			uint32_t link_index);
+int amdgpu_dm_connector_init(struct amdgpu_display_manager *dm,
+			struct amdgpu_connector *amdgpu_connector,
+			uint32_t link_index,
+			struct amdgpu_encoder *amdgpu_encoder);
+int amdgpu_dm_encoder_init(
+	struct drm_device *dev,
+	struct amdgpu_encoder *aencoder,
+	uint32_t link_index);
+
+void amdgpu_dm_crtc_destroy(struct drm_crtc *crtc);
+void amdgpu_dm_connector_destroy(struct drm_connector *connector);
+void amdgpu_dm_encoder_destroy(struct drm_encoder *encoder);
+
+int amdgpu_dm_connector_get_modes(struct drm_connector *connector);
+
+int amdgpu_dm_atomic_commit(
+	struct drm_device *dev,
+	struct drm_atomic_state *state,
+	bool async);
+int amdgpu_dm_atomic_check(struct drm_device *dev,
+				struct drm_atomic_state *state);
+
+int dm_create_validation_set_for_target(
+	struct drm_connector *connector,
+	struct drm_display_mode *mode,
+	struct dc_validation_set *val_set);
+
+void amdgpu_dm_connector_funcs_reset(struct drm_connector *connector);
+struct drm_connector_state *amdgpu_dm_connector_atomic_duplicate_state(
+	struct drm_connector *connector);
+
+int amdgpu_dm_connector_atomic_set_property(
+	struct drm_connector *connector,
+	struct drm_connector_state *state,
+	struct drm_property *property,
+	uint64_t val);
+
+int amdgpu_dm_get_encoder_crtc_mask(struct amdgpu_device *adev);
+
+void amdgpu_dm_connector_init_helper(
+	struct amdgpu_display_manager *dm,
+	struct amdgpu_connector *aconnector,
+	int connector_type,
+	const struct dc_link *link,
+	int link_index);
+
+int amdgpu_dm_connector_mode_valid(
+	struct drm_connector *connector,
+	struct drm_display_mode *mode);
+
+void dm_restore_drm_connector_state(struct drm_device *dev, struct drm_connector *connector);
+
+void amdgpu_dm_add_sink_to_freesync_module(
+		struct drm_connector *connector,
+		struct edid *edid);
+
+void amdgpu_dm_remove_sink_from_freesync_module(
+		struct drm_connector *connector);
+
+extern const struct drm_encoder_helper_funcs amdgpu_dm_encoder_helper_funcs;
+
+#endif		/* __AMDGPU_DM_TYPES_H__ */
diff --git a/drivers/gpu/drm/amd/display/dc/basics/register_logger.c b/drivers/gpu/drm/amd/display/dc/basics/register_logger.c
new file mode 100644
index 0000000..b8d57d9
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/basics/register_logger.c
@@ -0,0 +1,197 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "include/dal_types.h"
+#include "include/logger_interface.h"
+#include "logger.h"
+
+/******************************************************************************
+ * Register Logger.
+ * A facility to create register R/W logs.
+ * Currently used for DAL Test.
+ *****************************************************************************/
+
+/******************************************************************************
+ * Private structures
+ *****************************************************************************/
+struct dal_reg_dump_stack_location {
+	const char *current_caller_func;
+	long current_pid;
+	long current_tgid;
+	uint32_t rw_count;/* register access counter for current function. */
+};
+
+/* This the maximum number of nested calls to the 'reg_dump' facility. */
+#define DAL_REG_DUMP_STACK_MAX_SIZE 32
+
+struct dal_reg_dump_stack {
+	int32_t stack_pointer;
+	struct dal_reg_dump_stack_location
+		stack_locations[DAL_REG_DUMP_STACK_MAX_SIZE];
+	uint32_t total_rw_count; /* Total count for *all* functions. */
+};
+
+static struct dal_reg_dump_stack reg_dump_stack = {0};
+
+/******************************************************************************
+ * Private functions
+ *****************************************************************************/
+
+/* Check if current process is the one which requested register dump.
+ * The reason for the check:
+ * mmCRTC_STATUS_FRAME_COUNT is accessed by dal_controller_get_vblank_counter().
+ * Which runs all the time when at least one display is connected.
+ * (Triggered by drm_mode_page_flip_ioctl()). */
+static bool is_reg_dump_process(void)
+{
+	uint32_t i;
+
+	/* walk the list of our processes */
+	for (i = 0; i < reg_dump_stack.stack_pointer; i++) {
+		struct dal_reg_dump_stack_location *stack_location
+					= &reg_dump_stack.stack_locations[i];
+
+		if (stack_location->current_pid == dm_get_pid()
+			&& stack_location->current_tgid == dm_get_tgid())
+			return true;
+	}
+
+	return false;
+}
+
+static bool dal_reg_dump_stack_is_empty(void)
+{
+	if (reg_dump_stack.stack_pointer <= 0)
+		return true;
+	else
+		return false;
+}
+
+static struct dal_reg_dump_stack_location *dal_reg_dump_stack_push(void)
+{
+	struct dal_reg_dump_stack_location *current_location = NULL;
+
+	if (reg_dump_stack.stack_pointer >= DAL_REG_DUMP_STACK_MAX_SIZE) {
+		/* stack is full */
+		dm_output_to_console("[REG_DUMP]: %s: stack is full!\n",
+				__func__);
+	} else {
+		current_location =
+		&reg_dump_stack.stack_locations[reg_dump_stack.stack_pointer];
+		++reg_dump_stack.stack_pointer;
+	}
+
+	return current_location;
+}
+
+static struct dal_reg_dump_stack_location *dal_reg_dump_stack_pop(void)
+{
+	struct dal_reg_dump_stack_location *current_location = NULL;
+
+	if (dal_reg_dump_stack_is_empty()) {
+		/* stack is empty */
+		dm_output_to_console("[REG_DUMP]: %s: stack is empty!\n",
+				__func__);
+	} else {
+		--reg_dump_stack.stack_pointer;
+		current_location =
+		&reg_dump_stack.stack_locations[reg_dump_stack.stack_pointer];
+	}
+
+	return current_location;
+}
+
+/******************************************************************************
+ * Public functions
+ *****************************************************************************/
+
+void dal_reg_logger_push(const char *caller_func)
+{
+	struct dal_reg_dump_stack_location *free_stack_location;
+
+	free_stack_location = dal_reg_dump_stack_push();
+
+	if (NULL == free_stack_location)
+		return;
+
+	memset(free_stack_location, 0, sizeof(*free_stack_location));
+
+	free_stack_location->current_caller_func = caller_func;
+	free_stack_location->current_pid = dm_get_pid();
+	free_stack_location->current_tgid = dm_get_tgid();
+
+	dm_output_to_console("[REG_DUMP]:%s - start (pid:%ld, tgid:%ld)\n",
+		caller_func,
+		free_stack_location->current_pid,
+		free_stack_location->current_tgid);
+}
+
+void dal_reg_logger_pop(void)
+{
+	struct dal_reg_dump_stack_location *top_stack_location;
+
+	top_stack_location = dal_reg_dump_stack_pop();
+
+	if (NULL == top_stack_location) {
+		dm_output_to_console("[REG_DUMP]:%s - Stack is Empty!\n",
+				__func__);
+		return;
+	}
+
+	dm_output_to_console(
+	"[REG_DUMP]:%s - end."\
+	" Reg R/W Count: Total=%d Function=%d. (pid:%ld, tgid:%ld)\n",
+			top_stack_location->current_caller_func,
+			reg_dump_stack.total_rw_count,
+			top_stack_location->rw_count,
+			dm_get_pid(),
+			dm_get_tgid());
+
+	memset(top_stack_location, 0, sizeof(*top_stack_location));
+}
+
+void dal_reg_logger_rw_count_increment(void)
+{
+	++reg_dump_stack.total_rw_count;
+
+	++reg_dump_stack.stack_locations
+		[reg_dump_stack.stack_pointer - 1].rw_count;
+}
+
+bool dal_reg_logger_should_dump_register(void)
+{
+	if (true == dal_reg_dump_stack_is_empty())
+		return false;
+
+	if (false == is_reg_dump_process())
+		return false;
+
+	return true;
+}
+
+/******************************************************************************
+ * End of File.
+ *****************************************************************************/
diff --git a/drivers/gpu/drm/amd/display/dc/calcs/bandwidth_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/bandwidth_calcs.c
new file mode 100644
index 0000000..0b2bb39
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/calcs/bandwidth_calcs.c
@@ -0,0 +1,3108 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "bandwidth_calcs.h"
+#include "dc.h"
+#include "core_types.h"
+
+/*******************************************************************************
+ * Private Functions
+ ******************************************************************************/
+
+static void calculate_bandwidth(
+	const struct bw_calcs_dceip *dceip,
+	const struct bw_calcs_vbios *vbios,
+	struct bw_calcs_data *data)
+
+{
+	const int32_t pixels_per_chunk = 512;
+	const int32_t high = 2;
+	const int32_t mid = 1;
+	const int32_t low = 0;
+	const uint32_t s_low = 0;
+	const uint32_t s_mid1 = 1;
+	const uint32_t s_mid2 = 2;
+	const uint32_t s_mid3 = 3;
+	const uint32_t s_mid4 = 4;
+	const uint32_t s_mid5 = 5;
+	const uint32_t s_mid6 = 6;
+	const uint32_t s_high = 7;
+	const uint32_t bus_efficiency = 1;
+	const uint32_t dmif_chunk_buff_margin = 1;
+
+	uint32_t max_chunks_fbc_mode;
+	int32_t num_cursor_lines;
+
+	int32_t i, j, k;
+	struct bw_fixed yclk[3];
+	struct bw_fixed sclk[8];
+	bool d0_underlay_enable;
+	bool d1_underlay_enable;
+	bool fbc_enabled;
+	bool lpt_enabled;
+	enum bw_defines sclk_message;
+	enum bw_defines yclk_message;
+	enum bw_defines v_filter_init_mode[maximum_number_of_surfaces];
+	enum bw_defines tiling_mode[maximum_number_of_surfaces];
+	enum bw_defines surface_type[maximum_number_of_surfaces];
+	enum bw_defines voltage;
+	enum bw_defines pipe_check;
+	enum bw_defines hsr_check;
+	enum bw_defines vsr_check;
+	enum bw_defines lb_size_check;
+	enum bw_defines fbc_check;
+	enum bw_defines rotation_check;
+	enum bw_defines mode_check;
+	enum bw_defines nbp_state_change_enable_blank;
+	/*initialize variables*/
+	int32_t number_of_displays_enabled = 0;
+	int32_t number_of_displays_enabled_with_margin = 0;
+	int32_t number_of_aligned_displays_with_no_margin = 0;
+
+	yclk[low] = vbios->low_yclk;
+	yclk[mid] = vbios->mid_yclk;
+	yclk[high] = vbios->high_yclk;
+	sclk[s_low] = vbios->low_sclk;
+	sclk[s_mid1] = vbios->mid1_sclk;
+	sclk[s_mid2] = vbios->mid2_sclk;
+	sclk[s_mid3] = vbios->mid3_sclk;
+	sclk[s_mid4] = vbios->mid4_sclk;
+	sclk[s_mid5] = vbios->mid5_sclk;
+	sclk[s_mid6] = vbios->mid6_sclk;
+	sclk[s_high] = vbios->high_sclk;
+	/*''''''''''''''''''*/
+	/* surface assignment:*/
+	/* 0: d0 underlay or underlay luma*/
+	/* 1: d0 underlay chroma*/
+	/* 2: d1 underlay or underlay luma*/
+	/* 3: d1 underlay chroma*/
+	/* 4: d0 graphics*/
+	/* 5: d1 graphics*/
+	/* 6: d2 graphics*/
+	/* 7: d3 graphics, same mode as d2*/
+	/* 8: d4 graphics, same mode as d2*/
+	/* 9: d5 graphics, same mode as d2*/
+	/* ...*/
+	/* maximum_number_of_surfaces-2: d1 display_write_back420 luma*/
+	/* maximum_number_of_surfaces-1: d1 display_write_back420 chroma*/
+	/* underlay luma and chroma surface parameters from spreadsheet*/
+
+
+
+
+	if (data->d0_underlay_mode == bw_def_none) { d0_underlay_enable = 0; }
+	else {
+		d0_underlay_enable = 1;
+	}
+	if (data->d1_underlay_mode == bw_def_none) { d1_underlay_enable = 0; }
+	else {
+		d1_underlay_enable = 1;
+	}
+	data->number_of_underlay_surfaces = d0_underlay_enable + d1_underlay_enable;
+	switch (data->underlay_surface_type) {
+	case bw_def_420:
+		surface_type[0] = bw_def_underlay420_luma;
+		surface_type[2] = bw_def_underlay420_luma;
+		data->bytes_per_pixel[0] = 1;
+		data->bytes_per_pixel[2] = 1;
+		surface_type[1] = bw_def_underlay420_chroma;
+		surface_type[3] = bw_def_underlay420_chroma;
+		data->bytes_per_pixel[1] = 2;
+		data->bytes_per_pixel[3] = 2;
+		data->lb_size_per_component[0] = dceip->underlay420_luma_lb_size_per_component;
+		data->lb_size_per_component[1] = dceip->underlay420_chroma_lb_size_per_component;
+		data->lb_size_per_component[2] = dceip->underlay420_luma_lb_size_per_component;
+		data->lb_size_per_component[3] = dceip->underlay420_chroma_lb_size_per_component;
+		break;
+	case bw_def_422:
+		surface_type[0] = bw_def_underlay422;
+		surface_type[2] = bw_def_underlay422;
+		data->bytes_per_pixel[0] = 2;
+		data->bytes_per_pixel[2] = 2;
+		data->lb_size_per_component[0] = dceip->underlay422_lb_size_per_component;
+		data->lb_size_per_component[2] = dceip->underlay422_lb_size_per_component;
+		break;
+	default:
+		surface_type[0] = bw_def_underlay444;
+		surface_type[2] = bw_def_underlay444;
+		data->bytes_per_pixel[0] = 4;
+		data->bytes_per_pixel[2] = 4;
+		data->lb_size_per_component[0] = dceip->lb_size_per_component444;
+		data->lb_size_per_component[2] = dceip->lb_size_per_component444;
+		break;
+	}
+	if (d0_underlay_enable) {
+		switch (data->underlay_surface_type) {
+		case bw_def_420:
+			data->enable[0] = 1;
+			data->enable[1] = 1;
+			break;
+		default:
+			data->enable[0] = 1;
+			data->enable[1] = 0;
+			break;
+		}
+	}
+	else {
+		data->enable[0] = 0;
+		data->enable[1] = 0;
+	}
+	if (d1_underlay_enable) {
+		switch (data->underlay_surface_type) {
+		case bw_def_420:
+			data->enable[2] = 1;
+			data->enable[3] = 1;
+			break;
+		default:
+			data->enable[2] = 1;
+			data->enable[3] = 0;
+			break;
+		}
+	}
+	else {
+		data->enable[2] = 0;
+		data->enable[3] = 0;
+	}
+	data->use_alpha[0] = 0;
+	data->use_alpha[1] = 0;
+	data->use_alpha[2] = 0;
+	data->use_alpha[3] = 0;
+	data->scatter_gather_enable_for_pipe[0] = vbios->scatter_gather_enable;
+	data->scatter_gather_enable_for_pipe[1] = vbios->scatter_gather_enable;
+	data->scatter_gather_enable_for_pipe[2] = vbios->scatter_gather_enable;
+	data->scatter_gather_enable_for_pipe[3] = vbios->scatter_gather_enable;
+	/*underlay0 same and graphics display pipe0*/
+	data->interlace_mode[0] = data->interlace_mode[4];
+	data->interlace_mode[1] = data->interlace_mode[4];
+	/*underlay1 same and graphics display pipe1*/
+	data->interlace_mode[2] = data->interlace_mode[5];
+	data->interlace_mode[3] = data->interlace_mode[5];
+	/*underlay0 same and graphics display pipe0*/
+	data->h_total[0] = data->h_total[4];
+	data->v_total[0] = data->v_total[4];
+	data->h_total[1] = data->h_total[4];
+	data->v_total[1] = data->v_total[4];
+	/*underlay1 same and graphics display pipe1*/
+	data->h_total[2] = data->h_total[5];
+	data->v_total[2] = data->v_total[5];
+	data->h_total[3] = data->h_total[5];
+	data->v_total[3] = data->v_total[5];
+	/*underlay0 same and graphics display pipe0*/
+	data->pixel_rate[0] = data->pixel_rate[4];
+	data->pixel_rate[1] = data->pixel_rate[4];
+	/*underlay1 same and graphics display pipe1*/
+	data->pixel_rate[2] = data->pixel_rate[5];
+	data->pixel_rate[3] = data->pixel_rate[5];
+	if ((data->underlay_tiling_mode == bw_def_array_linear_general || data->underlay_tiling_mode == bw_def_array_linear_aligned)) {
+		tiling_mode[0] = bw_def_linear;
+		tiling_mode[1] = bw_def_linear;
+		tiling_mode[2] = bw_def_linear;
+		tiling_mode[3] = bw_def_linear;
+	}
+	else {
+		tiling_mode[0] = bw_def_landscape;
+		tiling_mode[1] = bw_def_landscape;
+		tiling_mode[2] = bw_def_landscape;
+		tiling_mode[3] = bw_def_landscape;
+	}
+	data->lb_bpc[0] = data->underlay_lb_bpc;
+	data->lb_bpc[1] = data->underlay_lb_bpc;
+	data->lb_bpc[2] = data->underlay_lb_bpc;
+	data->lb_bpc[3] = data->underlay_lb_bpc;
+	data->compression_rate[0] = bw_int_to_fixed(1);
+	data->compression_rate[1] = bw_int_to_fixed(1);
+	data->compression_rate[2] = bw_int_to_fixed(1);
+	data->compression_rate[3] = bw_int_to_fixed(1);
+	data->access_one_channel_only[0] = 0;
+	data->access_one_channel_only[1] = 0;
+	data->access_one_channel_only[2] = 0;
+	data->access_one_channel_only[3] = 0;
+	data->cursor_width_pixels[0] = bw_int_to_fixed(0);
+	data->cursor_width_pixels[1] = bw_int_to_fixed(0);
+	data->cursor_width_pixels[2] = bw_int_to_fixed(0);
+	data->cursor_width_pixels[3] = bw_int_to_fixed(0);
+	/* graphics surface parameters from spreadsheet*/
+	fbc_enabled = 0;
+	lpt_enabled = 0;
+	for (i = 4; i <= maximum_number_of_surfaces - 3; i++) {
+		if (i < data->number_of_displays + 4) {
+			if (i == 4 && data->d0_underlay_mode == bw_def_underlay_only) {
+				data->enable[i] = 0;
+				data->use_alpha[i] = 0;
+			}
+			else if (i == 4 && data->d0_underlay_mode == bw_def_blend) {
+				data->enable[i] = 1;
+				data->use_alpha[i] = 1;
+			}
+			else if (i == 4) {
+				data->enable[i] = 1;
+				data->use_alpha[i] = 0;
+			}
+			else if (i == 5 && data->d1_underlay_mode == bw_def_underlay_only) {
+				data->enable[i] = 0;
+				data->use_alpha[i] = 0;
+			}
+			else if (i == 5 && data->d1_underlay_mode == bw_def_blend) {
+				data->enable[i] = 1;
+				data->use_alpha[i] = 1;
+			}
+			else {
+				data->enable[i] = 1;
+				data->use_alpha[i] = 0;
+			}
+		}
+		else {
+			data->enable[i] = 0;
+			data->use_alpha[i] = 0;
+		}
+		data->scatter_gather_enable_for_pipe[i] = vbios->scatter_gather_enable;
+		surface_type[i] = bw_def_graphics;
+		data->lb_size_per_component[i] = dceip->lb_size_per_component444;
+		if (data->graphics_tiling_mode == bw_def_array_linear_general || data->graphics_tiling_mode == bw_def_array_linear_aligned) {
+			tiling_mode[i] = bw_def_linear;
+		}
+		else {
+			tiling_mode[i] = bw_def_tiled;
+		}
+		data->lb_bpc[i] = data->graphics_lb_bpc;
+		if ((data->fbc_en[i] == 1 && (dceip->argb_compression_support || data->d0_underlay_mode != bw_def_blended))) {
+			data->compression_rate[i] = bw_int_to_fixed(vbios->average_compression_rate);
+			data->access_one_channel_only[i] = data->lpt_en[i];
+		}
+		else {
+			data->compression_rate[i] = bw_int_to_fixed(1);
+			data->access_one_channel_only[i] = 0;
+		}
+		if (data->fbc_en[i] == 1) {
+			fbc_enabled = 1;
+			if (data->lpt_en[i] == 1) {
+				lpt_enabled = 1;
+			}
+		}
+		data->cursor_width_pixels[i] = bw_int_to_fixed(vbios->cursor_width);
+	}
+	/* display_write_back420*/
+	data->scatter_gather_enable_for_pipe[maximum_number_of_surfaces - 2] = 0;
+	data->scatter_gather_enable_for_pipe[maximum_number_of_surfaces - 1] = 0;
+	if (data->d1_display_write_back_dwb_enable == 1) {
+		data->enable[maximum_number_of_surfaces - 2] = 1;
+		data->enable[maximum_number_of_surfaces - 1] = 1;
+	}
+	else {
+		data->enable[maximum_number_of_surfaces - 2] = 0;
+		data->enable[maximum_number_of_surfaces - 1] = 0;
+	}
+	surface_type[maximum_number_of_surfaces - 2] = bw_def_display_write_back420_luma;
+	surface_type[maximum_number_of_surfaces - 1] = bw_def_display_write_back420_chroma;
+	data->lb_size_per_component[maximum_number_of_surfaces - 2] = dceip->underlay420_luma_lb_size_per_component;
+	data->lb_size_per_component[maximum_number_of_surfaces - 1] = dceip->underlay420_chroma_lb_size_per_component;
+	data->bytes_per_pixel[maximum_number_of_surfaces - 2] = 1;
+	data->bytes_per_pixel[maximum_number_of_surfaces - 1] = 2;
+	data->interlace_mode[maximum_number_of_surfaces - 2] = data->interlace_mode[5];
+	data->interlace_mode[maximum_number_of_surfaces - 1] = data->interlace_mode[5];
+	data->h_taps[maximum_number_of_surfaces - 2] = bw_int_to_fixed(1);
+	data->h_taps[maximum_number_of_surfaces - 1] = bw_int_to_fixed(1);
+	data->v_taps[maximum_number_of_surfaces - 2] = bw_int_to_fixed(1);
+	data->v_taps[maximum_number_of_surfaces - 1] = bw_int_to_fixed(1);
+	data->rotation_angle[maximum_number_of_surfaces - 2] = bw_int_to_fixed(0);
+	data->rotation_angle[maximum_number_of_surfaces - 1] = bw_int_to_fixed(0);
+	tiling_mode[maximum_number_of_surfaces - 2] = bw_def_linear;
+	tiling_mode[maximum_number_of_surfaces - 1] = bw_def_linear;
+	data->lb_bpc[maximum_number_of_surfaces - 2] = 8;
+	data->lb_bpc[maximum_number_of_surfaces - 1] = 8;
+	data->compression_rate[maximum_number_of_surfaces - 2] = bw_int_to_fixed(1);
+	data->compression_rate[maximum_number_of_surfaces - 1] = bw_int_to_fixed(1);
+	data->access_one_channel_only[maximum_number_of_surfaces - 2] = 0;
+	data->access_one_channel_only[maximum_number_of_surfaces - 1] = 0;
+	/*assume display pipe1 has dwb enabled*/
+	data->h_total[maximum_number_of_surfaces - 2] = data->h_total[5];
+	data->h_total[maximum_number_of_surfaces - 1] = data->h_total[5];
+	data->v_total[maximum_number_of_surfaces - 2] = data->v_total[5];
+	data->v_total[maximum_number_of_surfaces - 1] = data->v_total[5];
+	data->pixel_rate[maximum_number_of_surfaces - 2] = data->pixel_rate[5];
+	data->pixel_rate[maximum_number_of_surfaces - 1] = data->pixel_rate[5];
+	data->src_width[maximum_number_of_surfaces - 2] = data->src_width[5];
+	data->src_width[maximum_number_of_surfaces - 1] = data->src_width[5];
+	data->src_height[maximum_number_of_surfaces - 2] = data->src_height[5];
+	data->src_height[maximum_number_of_surfaces - 1] = data->src_height[5];
+	data->pitch_in_pixels[maximum_number_of_surfaces - 2] = data->src_width[5];
+	data->pitch_in_pixels[maximum_number_of_surfaces - 1] = data->src_width[5];
+	data->h_scale_ratio[maximum_number_of_surfaces - 2] = bw_int_to_fixed(1);
+	data->h_scale_ratio[maximum_number_of_surfaces - 1] = bw_int_to_fixed(1);
+	data->v_scale_ratio[maximum_number_of_surfaces - 2] = bw_int_to_fixed(1);
+	data->v_scale_ratio[maximum_number_of_surfaces - 1] = bw_int_to_fixed(1);
+	data->stereo_mode[maximum_number_of_surfaces - 2] = bw_def_mono;
+	data->stereo_mode[maximum_number_of_surfaces - 1] = bw_def_mono;
+	data->cursor_width_pixels[maximum_number_of_surfaces - 2] = bw_int_to_fixed(0);
+	data->cursor_width_pixels[maximum_number_of_surfaces - 1] = bw_int_to_fixed(0);
+	data->use_alpha[maximum_number_of_surfaces - 2] = 0;
+	data->use_alpha[maximum_number_of_surfaces - 1] = 0;
+	/*mode check calculations:*/
+	/* mode within dce ip capabilities*/
+	/* fbc*/
+	/* hsr*/
+	/* vsr*/
+	/* lb size*/
+	/*effective scaling source and ratios:*/
+	/*for graphics, non-stereo, non-interlace surfaces when the size of the source and destination are the same, only one tap is used*/
+	/*420 chroma has half the width, height, horizontal and vertical scaling ratios than luma*/
+	/*rotating a graphic or underlay surface swaps the width, height, horizontal and vertical scaling ratios*/
+	/*in top-bottom stereo mode there is 2:1 vertical downscaling for each eye*/
+	/*in side-by-side stereo mode there is 2:1 horizontal downscaling for each eye*/
+	/*in interlace mode there is 2:1 vertical downscaling for each field*/
+	/*in panning or bezel adjustment mode the source width has an extra 128 pixels*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (bw_equ(data->h_scale_ratio[i], bw_int_to_fixed(1)) && bw_equ(data->v_scale_ratio[i], bw_int_to_fixed(1)) && surface_type[i] == bw_def_graphics && data->stereo_mode[i] == bw_def_mono && data->interlace_mode[i] == 0) {
+				data->h_taps[i] = bw_int_to_fixed(1);
+				data->v_taps[i] = bw_int_to_fixed(1);
+			}
+			if (surface_type[i] == bw_def_display_write_back420_chroma || surface_type[i] == bw_def_underlay420_chroma) {
+				data->pitch_in_pixels_after_surface_type[i] = bw_div(data->pitch_in_pixels[i], bw_int_to_fixed(2));
+				data->src_width_after_surface_type = bw_div(data->src_width[i], bw_int_to_fixed(2));
+				data->src_height_after_surface_type = bw_div(data->src_height[i], bw_int_to_fixed(2));
+				data->hsr_after_surface_type = bw_div(data->h_scale_ratio[i], bw_int_to_fixed(2));
+				data->vsr_after_surface_type = bw_div(data->v_scale_ratio[i], bw_int_to_fixed(2));
+			}
+			else {
+				data->pitch_in_pixels_after_surface_type[i] = data->pitch_in_pixels[i];
+				data->src_width_after_surface_type = data->src_width[i];
+				data->src_height_after_surface_type = data->src_height[i];
+				data->hsr_after_surface_type = data->h_scale_ratio[i];
+				data->vsr_after_surface_type = data->v_scale_ratio[i];
+			}
+			if ((bw_equ(data->rotation_angle[i], bw_int_to_fixed(90)) || bw_equ(data->rotation_angle[i], bw_int_to_fixed(270))) && surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma) {
+				data->src_width_after_rotation = data->src_height_after_surface_type;
+				data->src_height_after_rotation = data->src_width_after_surface_type;
+				data->hsr_after_rotation = data->vsr_after_surface_type;
+				data->vsr_after_rotation = data->hsr_after_surface_type;
+			}
+			else {
+				data->src_width_after_rotation = data->src_width_after_surface_type;
+				data->src_height_after_rotation = data->src_height_after_surface_type;
+				data->hsr_after_rotation = data->hsr_after_surface_type;
+				data->vsr_after_rotation = data->vsr_after_surface_type;
+			}
+			switch (data->stereo_mode[i]) {
+			case bw_def_top_bottom:
+				data->source_width_pixels[i] = data->src_width_after_rotation;
+				data->source_height_pixels = bw_mul(bw_int_to_fixed(2), data->src_height_after_rotation);
+				data->hsr_after_stereo = data->hsr_after_rotation;
+				data->vsr_after_stereo = bw_mul(bw_int_to_fixed(1), data->vsr_after_rotation);
+				break;
+			case bw_def_side_by_side:
+				data->source_width_pixels[i] = bw_mul(bw_int_to_fixed(2), data->src_width_after_rotation);
+				data->source_height_pixels = data->src_height_after_rotation;
+				data->hsr_after_stereo = bw_mul(bw_int_to_fixed(1), data->hsr_after_rotation);
+				data->vsr_after_stereo = data->vsr_after_rotation;
+				break;
+			default:
+				data->source_width_pixels[i] = data->src_width_after_rotation;
+				data->source_height_pixels = data->src_height_after_rotation;
+				data->hsr_after_stereo = data->hsr_after_rotation;
+				data->vsr_after_stereo = data->vsr_after_rotation;
+				break;
+			}
+			data->hsr[i] = data->hsr_after_stereo;
+			if (data->interlace_mode[i]) {
+				data->vsr[i] = bw_mul(data->vsr_after_stereo, bw_int_to_fixed(2));
+			}
+			else {
+				data->vsr[i] = data->vsr_after_stereo;
+			}
+			if (data->panning_and_bezel_adjustment != bw_def_none) {
+				data->source_width_rounded_up_to_chunks[i] = bw_add(bw_floor2(bw_sub(data->source_width_pixels[i], bw_int_to_fixed(1)), bw_int_to_fixed(128)), bw_int_to_fixed(256));
+			}
+			else {
+				data->source_width_rounded_up_to_chunks[i] = bw_ceil2(data->source_width_pixels[i], bw_int_to_fixed(128));
+			}
+			data->source_height_rounded_up_to_chunks[i] = data->source_height_pixels;
+		}
+	}
+	/*mode support checks:*/
+	/*the number of graphics and underlay pipes is limited by the ip support*/
+	/*maximum horizontal and vertical scale ratio is 4, and should not exceed the number of taps*/
+	/*for downscaling with the pre-downscaler, the horizontal scale ratio must be more than the ceiling of one quarter of the number of taps*/
+	/*the pre-downscaler reduces the line buffer source by the horizontal scale ratio*/
+	/*the number of lines in the line buffer has to exceed the number of vertical taps*/
+	/*the size of the line in the line buffer is the product of the source width and the bits per component, rounded up to a multiple of 48*/
+	/*the size of the line in the line buffer in the case of 10 bit per component is the product of the source width rounded up to multiple of 8 and 30.023438 / 3, rounded up to a multiple of 48*/
+	/*the size of the line in the line buffer in the case of 8 bit per component is the product of the source width rounded up to multiple of 8 and 30.023438 / 3, rounded up to a multiple of 48*/
+	/*frame buffer compression is not supported with stereo mode, rotation, or non- 888 formats*/
+	/*rotation is not supported with linear of stereo modes*/
+	if (dceip->number_of_graphics_pipes >= data->number_of_displays && dceip->number_of_underlay_pipes >= data->number_of_underlay_surfaces && !(dceip->display_write_back_supported == 0 && data->d1_display_write_back_dwb_enable == 1)) {
+		pipe_check = bw_def_ok;
+	}
+	else {
+		pipe_check = bw_def_notok;
+	}
+	hsr_check = bw_def_ok;
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (bw_neq(data->hsr[i], bw_int_to_fixed(1))) {
+				if (bw_mtn(data->hsr[i], bw_int_to_fixed(4))) {
+					hsr_check = bw_def_hsr_mtn_4;
+				}
+				else {
+					if (bw_mtn(data->hsr[i], data->h_taps[i])) {
+						hsr_check = bw_def_hsr_mtn_h_taps;
+					}
+					else {
+						if (dceip->pre_downscaler_enabled == 1 && bw_mtn(data->hsr[i], bw_int_to_fixed(1)) && bw_leq(data->hsr[i], bw_ceil2(bw_div(data->h_taps[i], bw_int_to_fixed(4)), bw_int_to_fixed(1)))) {
+							hsr_check = bw_def_ceiling__h_taps_div_4___meq_hsr;
+						}
+					}
+				}
+			}
+		}
+	}
+	vsr_check = bw_def_ok;
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (bw_neq(data->vsr[i], bw_int_to_fixed(1))) {
+				if (bw_mtn(data->vsr[i], bw_int_to_fixed(4))) {
+					vsr_check = bw_def_vsr_mtn_4;
+				}
+				else {
+					if (bw_mtn(data->vsr[i], data->v_taps[i])) {
+						vsr_check = bw_def_vsr_mtn_v_taps;
+					}
+				}
+			}
+		}
+	}
+	lb_size_check = bw_def_ok;
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if ((dceip->pre_downscaler_enabled && bw_mtn(data->hsr[i], bw_int_to_fixed(1)))) {
+				data->source_width_in_lb = bw_div(data->source_width_pixels[i], data->hsr[i]);
+			}
+			else {
+				data->source_width_in_lb = data->source_width_pixels[i];
+			}
+			switch (data->lb_bpc[i]) {
+			case 8:
+				data->lb_line_pitch = bw_ceil2(bw_mul(bw_div(bw_frc_to_fixed(2401171875, 100000000), bw_int_to_fixed(3)), bw_ceil2(data->source_width_in_lb, bw_int_to_fixed(8))), bw_int_to_fixed(48));
+				break;
+			case 10:
+				data->lb_line_pitch = bw_ceil2(bw_mul(bw_div(bw_frc_to_fixed(300234375, 10000000), bw_int_to_fixed(3)), bw_ceil2(data->source_width_in_lb, bw_int_to_fixed(8))), bw_int_to_fixed(48));
+				break;
+			default:
+				data->lb_line_pitch = bw_ceil2(bw_mul(bw_int_to_fixed(data->lb_bpc[i]), data->source_width_in_lb), bw_int_to_fixed(48));
+				break;
+			}
+			data->lb_partitions[i] = bw_floor2(bw_div(data->lb_size_per_component[i], data->lb_line_pitch), bw_int_to_fixed(1));
+			/*clamp the partitions to the maxium number supported by the lb*/
+			if ((surface_type[i] != bw_def_graphics || dceip->graphics_lb_nodownscaling_multi_line_prefetching == 1)) {
+				data->lb_partitions_max[i] = bw_int_to_fixed(10);
+			}
+			else {
+				data->lb_partitions_max[i] = bw_int_to_fixed(7);
+			}
+			data->lb_partitions[i] = bw_min2(data->lb_partitions_max[i], data->lb_partitions[i]);
+			if (bw_mtn(bw_add(data->v_taps[i], bw_int_to_fixed(1)), data->lb_partitions[i])) {
+				lb_size_check = bw_def_notok;
+			}
+		}
+	}
+	fbc_check = bw_def_ok;
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i] && data->fbc_en[i] == 1 && (bw_equ(data->rotation_angle[i], bw_int_to_fixed(90)) || bw_equ(data->rotation_angle[i], bw_int_to_fixed(270)) || data->stereo_mode[i] != bw_def_mono || data->bytes_per_pixel[i] != 4)) {
+			fbc_check = bw_def_invalid_rotation_or_bpp_or_stereo;
+		}
+	}
+	rotation_check = bw_def_ok;
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if ((bw_equ(data->rotation_angle[i], bw_int_to_fixed(90)) || bw_equ(data->rotation_angle[i], bw_int_to_fixed(270))) && (tiling_mode[i] == bw_def_linear || data->stereo_mode[i] != bw_def_mono)) {
+				rotation_check = bw_def_invalid_linear_or_stereo_mode;
+			}
+		}
+	}
+	if (pipe_check == bw_def_ok && hsr_check == bw_def_ok && vsr_check == bw_def_ok && lb_size_check == bw_def_ok && fbc_check == bw_def_ok && rotation_check == bw_def_ok) {
+		mode_check = bw_def_ok;
+	}
+	else {
+		mode_check = bw_def_notok;
+	}
+	/*number of memory channels for write-back client*/
+	data->number_of_dram_wrchannels = vbios->number_of_dram_channels;
+	data->number_of_dram_channels = vbios->number_of_dram_channels;
+	/*modify number of memory channels if lpt mode is enabled*/
+	/* low power tiling mode register*/
+	/* 0 = use channel 0*/
+	/* 1 = use channel 0 and 1*/
+	/* 2 = use channel 0,1,2,3*/
+	if ((fbc_enabled == 1 && lpt_enabled == 1)) {
+		data->dram_efficiency = bw_int_to_fixed(1);
+		if (dceip->low_power_tiling_mode == 0) {
+			data->number_of_dram_channels = 1;
+		}
+		else if (dceip->low_power_tiling_mode == 1) {
+			data->number_of_dram_channels = 2;
+		}
+		else if (dceip->low_power_tiling_mode == 2) {
+			data->number_of_dram_channels = 4;
+		}
+		else {
+			data->number_of_dram_channels = 1;
+		}
+	}
+	else {
+		data->dram_efficiency = bw_frc_to_fixed(8, 10);
+	}
+	/*memory request size and latency hiding:*/
+	/*request size is normally 64 byte, 2-line interleaved, with full latency hiding*/
+	/*the display write-back requests are single line*/
+	/*for tiled graphics surfaces, or undelay surfaces with width higher than the maximum size for full efficiency, request size is 32 byte in 8 and 16 bpp or if the rotation is orthogonal to the tiling grain. only half is useful of the bytes in the request size in 8 bpp or in 32 bpp if the rotation is orthogonal to the tiling grain.*/
+	/*for undelay surfaces with width lower than the maximum size for full efficiency, requests are 4-line interleaved in 16bpp if the rotation is parallel to the tiling grain, and 8-line interleaved with 4-line latency hiding in 8bpp or if the rotation is orthogonal to the tiling grain.*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if ((bw_equ(data->rotation_angle[i], bw_int_to_fixed(90)) || bw_equ(data->rotation_angle[i], bw_int_to_fixed(270)))) {
+				if ((i < 4)) {
+					/*underlay portrait tiling mode is not supported*/
+					data->orthogonal_rotation[i] = 1;
+				}
+				else {
+					/*graphics portrait tiling mode*/
+					if ((data->graphics_micro_tile_mode == bw_def_rotated_micro_tiling)) {
+						data->orthogonal_rotation[i] = 0;
+					}
+					else {
+						data->orthogonal_rotation[i] = 1;
+					}
+				}
+			}
+			else {
+				if ((i < 4)) {
+					/*underlay landscape tiling mode is only supported*/
+					if ((data->underlay_micro_tile_mode == bw_def_display_micro_tiling)) {
+						data->orthogonal_rotation[i] = 0;
+					}
+					else {
+						data->orthogonal_rotation[i] = 1;
+					}
+				}
+				else {
+					/*graphics landscape tiling mode*/
+					if ((data->graphics_micro_tile_mode == bw_def_display_micro_tiling)) {
+						data->orthogonal_rotation[i] = 0;
+					}
+					else {
+						data->orthogonal_rotation[i] = 1;
+					}
+				}
+			}
+			if (bw_equ(data->rotation_angle[i], bw_int_to_fixed(90)) || bw_equ(data->rotation_angle[i], bw_int_to_fixed(270))) {
+				data->underlay_maximum_source_efficient_for_tiling = dceip->underlay_maximum_height_efficient_for_tiling;
+			}
+			else {
+				data->underlay_maximum_source_efficient_for_tiling = dceip->underlay_maximum_width_efficient_for_tiling;
+			}
+			if (surface_type[i] == bw_def_display_write_back420_luma || surface_type[i] == bw_def_display_write_back420_chroma) {
+				data->bytes_per_request[i] = bw_int_to_fixed(64);
+				data->useful_bytes_per_request[i] = bw_int_to_fixed(64);
+				data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(1);
+				data->latency_hiding_lines[i] = bw_int_to_fixed(1);
+			}
+			else if (tiling_mode[i] == bw_def_linear) {
+				data->bytes_per_request[i] = bw_int_to_fixed(64);
+				data->useful_bytes_per_request[i] = bw_int_to_fixed(64);
+				data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(2);
+				data->latency_hiding_lines[i] = bw_int_to_fixed(2);
+			}
+			else {
+				if (surface_type[i] == bw_def_graphics || (bw_mtn(data->source_width_rounded_up_to_chunks[i], bw_ceil2(data->underlay_maximum_source_efficient_for_tiling, bw_int_to_fixed(256))))) {
+					switch (data->bytes_per_pixel[i]) {
+					case 8:
+						data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(2);
+						data->latency_hiding_lines[i] = bw_int_to_fixed(2);
+						if (data->orthogonal_rotation[i]) {
+							data->bytes_per_request[i] = bw_int_to_fixed(32);
+							data->useful_bytes_per_request[i] = bw_int_to_fixed(32);
+						}
+						else {
+							data->bytes_per_request[i] = bw_int_to_fixed(64);
+							data->useful_bytes_per_request[i] = bw_int_to_fixed(64);
+						}
+						break;
+					case 4:
+						if (data->orthogonal_rotation[i]) {
+							data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(2);
+							data->latency_hiding_lines[i] = bw_int_to_fixed(2);
+							data->bytes_per_request[i] = bw_int_to_fixed(32);
+							data->useful_bytes_per_request[i] = bw_int_to_fixed(16);
+						}
+						else {
+							data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(2);
+							data->latency_hiding_lines[i] = bw_int_to_fixed(2);
+							data->bytes_per_request[i] = bw_int_to_fixed(64);
+							data->useful_bytes_per_request[i] = bw_int_to_fixed(64);
+						}
+						break;
+					case 2:
+						data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(2);
+						data->latency_hiding_lines[i] = bw_int_to_fixed(2);
+						data->bytes_per_request[i] = bw_int_to_fixed(32);
+						data->useful_bytes_per_request[i] = bw_int_to_fixed(32);
+						break;
+					default:
+						data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(2);
+						data->latency_hiding_lines[i] = bw_int_to_fixed(2);
+						data->bytes_per_request[i] = bw_int_to_fixed(32);
+						data->useful_bytes_per_request[i] = bw_int_to_fixed(16);
+						break;
+					}
+				}
+				else {
+					data->bytes_per_request[i] = bw_int_to_fixed(64);
+					data->useful_bytes_per_request[i] = bw_int_to_fixed(64);
+					if (data->orthogonal_rotation[i]) {
+						data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(8);
+						data->latency_hiding_lines[i] = bw_int_to_fixed(4);
+					}
+					else {
+						switch (data->bytes_per_pixel[i]) {
+						case 4:
+							data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(2);
+							data->latency_hiding_lines[i] = bw_int_to_fixed(2);
+							break;
+						case 2:
+							data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(4);
+							data->latency_hiding_lines[i] = bw_int_to_fixed(4);
+							break;
+						default:
+							data->lines_interleaved_in_mem_access[i] = bw_int_to_fixed(8);
+							data->latency_hiding_lines[i] = bw_int_to_fixed(4);
+							break;
+						}
+					}
+				}
+			}
+		}
+	}
+	/*requested peak bandwidth:*/
+	/*the peak request-per-second bandwidth is the product of the maximum source lines in per line out in the beginning*/
+	/*and in the middle of the frame, the ratio of the source width to the line time, the ratio of line interleaving*/
+	/*in memory to lines of latency hiding, and the ratio of bytes per pixel to useful bytes per request.*/
+	/**/
+	/*if the dmif data buffer size holds more than vta_ps worth of source lines, then only vsr is used.*/
+	/*the peak bandwidth is the peak request-per-second bandwidth times the request size.*/
+	/**/
+	/*the line buffer lines in per line out in the beginning of the frame is the vertical filter initialization value*/
+	/*rounded up to even and divided by the line times for initialization, which is normally three.*/
+	/*the line buffer lines in per line out in the middle of the frame is at least one, or the vertical scale ratio,*/
+	/*rounded up to line pairs if not doing line buffer prefetching.*/
+	/**/
+	/*the non-prefetching rounding up of the vertical scale ratio can also be done up to 1 (for a 0,2 pattern), 4/3 (for a 0,2,2 pattern),*/
+	/*6/4 (for a 0,2,2,2 pattern), or 3 (for a 2,4 pattern).*/
+	/**/
+	/*the scaler vertical filter initialization value is calculated by the hardware as the floor of the average of the*/
+	/*vertical scale ratio and the number of vertical taps increased by one.  add one more for possible odd line*/
+	/*panning/bezel adjustment mode.*/
+	/**/
+	/*for the bottom interlace field an extra 50% of the vertical scale ratio is considered for this calculation.*/
+	/*in top-bottom stereo mode software has to set the filter initialization value manually and explicitly limit it to 4.*/
+	/*furthermore, there is only one line time for initialization.*/
+	/**/
+	/*line buffer prefetching is done when the number of lines in the line buffer exceeds the number of taps plus*/
+	/*the ceiling of the vertical scale ratio.*/
+	/**/
+	/*multi-line buffer prefetching is only done in the graphics pipe when the scaler is disabled or when upscaling and the vsr <= 0.8.'*/
+	/**/
+	/*the horizontal blank and chunk granularity factor is indirectly used indicate the interval of time required to transfer the source pixels.*/
+	/*the denominator of this term represents the total number of destination output pixels required for the input source pixels.*/
+	/*it applies when the lines in per line out is not 2 or 4.  it does not apply when there is a line buffer between the scl and blnd.*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			data->v_filter_init[i] = bw_floor2(bw_div((bw_add(bw_add(bw_add(bw_int_to_fixed(1), data->v_taps[i]), data->vsr[i]), bw_mul(bw_mul(bw_int_to_fixed(data->interlace_mode[i]), bw_frc_to_fixed(5, 10)), data->vsr[i]))), bw_int_to_fixed(2)), bw_int_to_fixed(1));
+			if (data->panning_and_bezel_adjustment == bw_def_any_lines) {
+				data->v_filter_init[i] = bw_add(data->v_filter_init[i], bw_int_to_fixed(1));
+			}
+			if (data->stereo_mode[i] == bw_def_top_bottom) {
+				v_filter_init_mode[i] = bw_def_manual;
+				data->v_filter_init[i] = bw_min2(data->v_filter_init[i], bw_int_to_fixed(4));
+			}
+			else {
+				v_filter_init_mode[i] = bw_def_auto;
+			}
+			if (data->stereo_mode[i] == bw_def_top_bottom) {
+				data->num_lines_at_frame_start = bw_int_to_fixed(1);
+			}
+			else {
+				data->num_lines_at_frame_start = bw_int_to_fixed(3);
+			}
+			if ((bw_mtn(data->vsr[i], bw_int_to_fixed(1)) && surface_type[i] == bw_def_graphics) || data->panning_and_bezel_adjustment == bw_def_any_lines) {
+				data->line_buffer_prefetch[i] = 0;
+			}
+			else if ((((dceip->underlay_downscale_prefetch_enabled == 1 && surface_type[i] != bw_def_graphics) || surface_type[i] == bw_def_graphics) && (bw_mtn(data->lb_partitions[i], bw_add(data->v_taps[i], bw_ceil2(data->vsr[i], bw_int_to_fixed(1))))))) {
+				data->line_buffer_prefetch[i] = 1;
+			}
+			else {
+				data->line_buffer_prefetch[i] = 0;
+			}
+			data->lb_lines_in_per_line_out_in_beginning_of_frame[i] = bw_div(bw_ceil2(data->v_filter_init[i], bw_int_to_fixed(dceip->lines_interleaved_into_lb)), data->num_lines_at_frame_start);
+			if (data->line_buffer_prefetch[i] == 1) {
+				data->lb_lines_in_per_line_out_in_middle_of_frame[i] = bw_max2(bw_int_to_fixed(1), data->vsr[i]);
+			}
+			else if (bw_leq(data->vsr[i], bw_int_to_fixed(1))) {
+				data->lb_lines_in_per_line_out_in_middle_of_frame[i] = bw_int_to_fixed(1);
+			}
+			else if (bw_leq(data->vsr[i], bw_int_to_fixed(4 / 3))) {
+				data->lb_lines_in_per_line_out_in_middle_of_frame[i] = bw_div(bw_int_to_fixed(4), bw_int_to_fixed(3));
+			}
+			else if (bw_leq(data->vsr[i], bw_int_to_fixed(6 / 4))) {
+				data->lb_lines_in_per_line_out_in_middle_of_frame[i] = bw_div(bw_int_to_fixed(6), bw_int_to_fixed(4));
+			}
+			else if (bw_leq(data->vsr[i], bw_int_to_fixed(2))) {
+				data->lb_lines_in_per_line_out_in_middle_of_frame[i] = bw_int_to_fixed(2);
+			}
+			else if (bw_leq(data->vsr[i], bw_int_to_fixed(3))) {
+				data->lb_lines_in_per_line_out_in_middle_of_frame[i] = bw_int_to_fixed(3);
+			}
+			else {
+				data->lb_lines_in_per_line_out_in_middle_of_frame[i] = bw_int_to_fixed(4);
+			}
+			if (data->line_buffer_prefetch[i] == 1 || bw_equ(data->lb_lines_in_per_line_out_in_middle_of_frame[i], bw_int_to_fixed(2)) || bw_equ(data->lb_lines_in_per_line_out_in_middle_of_frame[i], bw_int_to_fixed(4))) {
+				data->horizontal_blank_and_chunk_granularity_factor[i] = bw_int_to_fixed(1);
+			}
+			else {
+				data->horizontal_blank_and_chunk_granularity_factor[i] = bw_div(data->h_total[i], (bw_div((bw_add(data->h_total[i], bw_div((bw_sub(data->source_width_pixels[i], bw_int_to_fixed(dceip->chunk_width))), data->hsr[i]))), bw_int_to_fixed(2))));
+			}
+			data->request_bandwidth[i] = bw_div(bw_mul(bw_div(bw_mul(bw_div(bw_mul(bw_max2(data->lb_lines_in_per_line_out_in_beginning_of_frame[i], data->lb_lines_in_per_line_out_in_middle_of_frame[i]), data->source_width_rounded_up_to_chunks[i]), (bw_div(data->h_total[i], data->pixel_rate[i]))), bw_int_to_fixed(data->bytes_per_pixel[i])), data->useful_bytes_per_request[i]), data->lines_interleaved_in_mem_access[i]), data->latency_hiding_lines[i]);
+			data->display_bandwidth[i] = bw_mul(data->request_bandwidth[i], data->bytes_per_request[i]);
+		}
+	}
+	/*outstanding chunk request limit*/
+	/*if underlay buffer sharing is enabled, the data buffer size for underlay in 422 or 444 is the sum of the luma and chroma data buffer sizes.*/
+	/*underlay buffer sharing mode is only permitted in orthogonal rotation modes.*/
+	/**/
+	/*if there is only one display enabled, the dmif data buffer size for the graphics surface is increased by concatenating the adjacent buffers.*/
+	/**/
+	/*the memory chunk size in bytes is 1024 for the writeback, and 256 times the memory line interleaving and the bytes per pixel for graphics*/
+	/*and underlay.*/
+	/**/
+	/*the pipe chunk size uses 2 for line interleaving, except for the write back, in which case it is 1.*/
+	/*graphics and underlay data buffer size is adjusted (limited) using the outstanding chunk request limit if there is more than one*/
+	/*display enabled or if the dmif request buffer is not large enough for the total data buffer size.*/
+	/*the outstanding chunk request limit is the ceiling of the adjusted data buffer size divided by the chunk size in bytes*/
+	/*the adjusted data buffer size is the product of the display bandwidth and the minimum effective data buffer size in terms of time,*/
+	/*rounded up to the chunk size in bytes, but should not exceed the original data buffer size*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if ((dceip->dmif_pipe_en_fbc_chunk_tracker + 3 == i && fbc_enabled == 0 && tiling_mode[i] != bw_def_linear)) {
+				data->max_chunks_non_fbc_mode[i] = 128 - dmif_chunk_buff_margin;
+			}
+			else {
+				data->max_chunks_non_fbc_mode[i] = 16 - dmif_chunk_buff_margin;
+			}
+		}
+		if (data->fbc_en[i] == 1) {
+			max_chunks_fbc_mode = 128 - dmif_chunk_buff_margin;
+		}
+	}
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			switch (surface_type[i]) {
+			case bw_def_display_write_back420_luma:
+				data->data_buffer_size[i] = bw_int_to_fixed(dceip->display_write_back420_luma_mcifwr_buffer_size);
+				break;
+			case bw_def_display_write_back420_chroma:
+				data->data_buffer_size[i] = bw_int_to_fixed(dceip->display_write_back420_chroma_mcifwr_buffer_size);
+				break;
+			case bw_def_underlay420_luma:
+				data->data_buffer_size[i] = bw_int_to_fixed(dceip->underlay_luma_dmif_size);
+				break;
+			case bw_def_underlay420_chroma:
+				data->data_buffer_size[i] = bw_div(bw_int_to_fixed(dceip->underlay_chroma_dmif_size), bw_int_to_fixed(2));
+				break;
+			case bw_def_underlay422:case bw_def_underlay444:
+				if (data->orthogonal_rotation[i] == 0) {
+					data->data_buffer_size[i] = bw_int_to_fixed(dceip->underlay_luma_dmif_size);
+				}
+				else {
+					data->data_buffer_size[i] = bw_add(bw_int_to_fixed(dceip->underlay_luma_dmif_size), bw_int_to_fixed(dceip->underlay_chroma_dmif_size));
+				}
+				break;
+			default:
+				if (data->fbc_en[i] == 1) {
+					/*data_buffer_size(i) = max_dmif_buffer_allocated * graphics_dmif_size*/
+					if (data->number_of_displays == 1) {
+						data->data_buffer_size[i] = bw_min2(bw_mul(bw_mul(bw_int_to_fixed(max_chunks_fbc_mode), bw_int_to_fixed(pixels_per_chunk)), bw_int_to_fixed(data->bytes_per_pixel[i])), bw_mul(bw_int_to_fixed(dceip->max_dmif_buffer_allocated), bw_int_to_fixed(dceip->graphics_dmif_size)));
+					}
+					else {
+						data->data_buffer_size[i] = bw_min2(bw_mul(bw_mul(bw_int_to_fixed(max_chunks_fbc_mode), bw_int_to_fixed(pixels_per_chunk)), bw_int_to_fixed(data->bytes_per_pixel[i])), bw_int_to_fixed(dceip->graphics_dmif_size));
+					}
+				}
+				else {
+					/*the effective dmif buffer size in non-fbc mode is limited by the 16 entry chunk tracker*/
+					if (data->number_of_displays == 1) {
+						data->data_buffer_size[i] = bw_min2(bw_mul(bw_mul(bw_int_to_fixed(data->max_chunks_non_fbc_mode[i]), bw_int_to_fixed(pixels_per_chunk)), bw_int_to_fixed(data->bytes_per_pixel[i])), bw_mul(bw_int_to_fixed(dceip->max_dmif_buffer_allocated), bw_int_to_fixed(dceip->graphics_dmif_size)));
+					}
+					else {
+						data->data_buffer_size[i] = bw_min2(bw_mul(bw_mul(bw_int_to_fixed(data->max_chunks_non_fbc_mode[i]), bw_int_to_fixed(pixels_per_chunk)), bw_int_to_fixed(data->bytes_per_pixel[i])), bw_int_to_fixed(dceip->graphics_dmif_size));
+					}
+				}
+				break;
+			}
+			if (surface_type[i] == bw_def_display_write_back420_luma || surface_type[i] == bw_def_display_write_back420_chroma) {
+				data->memory_chunk_size_in_bytes[i] = bw_int_to_fixed(1024);
+				data->pipe_chunk_size_in_bytes[i] = bw_int_to_fixed(1024);
+			}
+			else {
+				data->memory_chunk_size_in_bytes[i] = bw_mul(bw_mul(bw_int_to_fixed(dceip->chunk_width), data->lines_interleaved_in_mem_access[i]), bw_int_to_fixed(data->bytes_per_pixel[i]));
+				data->pipe_chunk_size_in_bytes[i] = bw_mul(bw_mul(bw_int_to_fixed(dceip->chunk_width), bw_int_to_fixed(dceip->lines_interleaved_into_lb)), bw_int_to_fixed(data->bytes_per_pixel[i]));
+			}
+		}
+	}
+	data->min_dmif_size_in_time = bw_int_to_fixed(9999);
+	data->min_mcifwr_size_in_time = bw_int_to_fixed(9999);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma) {
+				if (bw_ltn(bw_div(bw_div(bw_mul(data->data_buffer_size[i], data->bytes_per_request[i]), data->useful_bytes_per_request[i]), data->display_bandwidth[i]), data->min_dmif_size_in_time)) {
+					data->min_dmif_size_in_time = bw_div(bw_div(bw_mul(data->data_buffer_size[i], data->bytes_per_request[i]), data->useful_bytes_per_request[i]), data->display_bandwidth[i]);
+				}
+			}
+			else {
+				if (bw_ltn(bw_div(bw_div(bw_mul(data->data_buffer_size[i], data->bytes_per_request[i]), data->useful_bytes_per_request[i]), data->display_bandwidth[i]), data->min_mcifwr_size_in_time)) {
+					data->min_mcifwr_size_in_time = bw_div(bw_div(bw_mul(data->data_buffer_size[i], data->bytes_per_request[i]), data->useful_bytes_per_request[i]), data->display_bandwidth[i]);
+				}
+			}
+		}
+	}
+	data->total_requests_for_dmif_size = bw_int_to_fixed(0);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i] && surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma) {
+			data->total_requests_for_dmif_size = bw_add(data->total_requests_for_dmif_size, bw_div(data->data_buffer_size[i], data->useful_bytes_per_request[i]));
+		}
+	}
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma && dceip->limit_excessive_outstanding_dmif_requests && (data->number_of_displays > 1 || bw_mtn(data->total_requests_for_dmif_size, dceip->dmif_request_buffer_size))) {
+				data->adjusted_data_buffer_size[i] = bw_min2(data->data_buffer_size[i], bw_ceil2(bw_mul(data->min_dmif_size_in_time, data->display_bandwidth[i]), data->memory_chunk_size_in_bytes[i]));
+			}
+			else {
+				data->adjusted_data_buffer_size[i] = data->data_buffer_size[i];
+			}
+		}
+	}
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if ((data->number_of_displays == 1 && data->number_of_underlay_surfaces == 0)) {
+				/*set maximum chunk limit if only one graphic pipe is enabled*/
+				data->outstanding_chunk_request_limit[i] = bw_int_to_fixed(127);
+			}
+			else {
+				data->outstanding_chunk_request_limit[i] = bw_ceil2(bw_div(data->adjusted_data_buffer_size[i], data->pipe_chunk_size_in_bytes[i]), bw_int_to_fixed(1));
+				/*clamp maximum chunk limit in the graphic display pipe*/
+				if ((i >= 4)) {
+					data->outstanding_chunk_request_limit[i] = bw_max2(bw_int_to_fixed(127), data->outstanding_chunk_request_limit[i]);
+				}
+			}
+		}
+	}
+	/*outstanding pte request limit*/
+	/*in tiling mode with no rotation the sg pte requests are 8 useful pt_es, the sg row height is the page height and the sg page width x height is 64x64 for 8bpp, 64x32 for 16 bpp, 32x32 for 32 bpp*/
+	/*in tiling mode with rotation the sg pte requests are only one useful pte, and the sg row height is also the page height, but the sg page width and height are swapped*/
+	/*in linear mode the pte requests are 8 useful pt_es, the sg page width is 4096 divided by the bytes per pixel, the sg page height is 1, but there is just one row whose height is the lines of pte prefetching*/
+	/*the outstanding pte request limit is obtained by multiplying the outstanding chunk request limit by the peak pte request to eviction limiting ratio, rounding up to integer, multiplying by the pte requests per chunk, and rounding up to integer again*/
+	/*if not using peak pte request to eviction limiting, the outstanding pte request limit is the pte requests in the vblank*/
+	/*the pte requests in the vblank is the product of the number of pte request rows times the number of pte requests in a row*/
+	/*the number of pte requests in a row is the quotient of the source width divided by 256, multiplied by the pte requests per chunk, rounded up to even, multiplied by the scatter-gather row height and divided by the scatter-gather page height*/
+	/*the pte requests per chunk is 256 divided by the scatter-gather page width and the useful pt_es per pte request*/
+	if (data->number_of_displays > 1 || (bw_neq(data->rotation_angle[4], bw_int_to_fixed(0)) && bw_neq(data->rotation_angle[4], bw_int_to_fixed(180)))) {
+		data->peak_pte_request_to_eviction_ratio_limiting = dceip->peak_pte_request_to_eviction_ratio_limiting_multiple_displays_or_single_rotated_display;
+	}
+	else {
+		data->peak_pte_request_to_eviction_ratio_limiting = dceip->peak_pte_request_to_eviction_ratio_limiting_single_display_no_rotation;
+	}
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i] && data->scatter_gather_enable_for_pipe[i] == 1) {
+			if (tiling_mode[i] == bw_def_linear) {
+				data->useful_pte_per_pte_request = bw_int_to_fixed(8);
+				data->scatter_gather_page_width[i] = bw_div(bw_int_to_fixed(4096), bw_int_to_fixed(data->bytes_per_pixel[i]));
+				data->scatter_gather_page_height[i] = bw_int_to_fixed(1);
+				data->scatter_gather_pte_request_rows = bw_int_to_fixed(1);
+				data->scatter_gather_row_height = bw_int_to_fixed(dceip->scatter_gather_lines_of_pte_prefetching_in_linear_mode);
+			}
+			else if (bw_equ(data->rotation_angle[i], bw_int_to_fixed(0)) || bw_equ(data->rotation_angle[i], bw_int_to_fixed(180))) {
+				data->useful_pte_per_pte_request = bw_int_to_fixed(8);
+				switch (data->bytes_per_pixel[i]) {
+				case 4:
+					data->scatter_gather_page_width[i] = bw_int_to_fixed(32);
+					data->scatter_gather_page_height[i] = bw_int_to_fixed(32);
+					break;
+				case 2:
+					data->scatter_gather_page_width[i] = bw_int_to_fixed(64);
+					data->scatter_gather_page_height[i] = bw_int_to_fixed(32);
+					break;
+				default:
+					data->scatter_gather_page_width[i] = bw_int_to_fixed(64);
+					data->scatter_gather_page_height[i] = bw_int_to_fixed(64);
+					break;
+				}
+				data->scatter_gather_pte_request_rows = bw_int_to_fixed(dceip->scatter_gather_pte_request_rows_in_tiling_mode);
+				data->scatter_gather_row_height = data->scatter_gather_page_height[i];
+			}
+			else {
+				data->useful_pte_per_pte_request = bw_int_to_fixed(1);
+				switch (data->bytes_per_pixel[i]) {
+				case 4:
+					data->scatter_gather_page_width[i] = bw_int_to_fixed(32);
+					data->scatter_gather_page_height[i] = bw_int_to_fixed(32);
+					break;
+				case 2:
+					data->scatter_gather_page_width[i] = bw_int_to_fixed(32);
+					data->scatter_gather_page_height[i] = bw_int_to_fixed(64);
+					break;
+				default:
+					data->scatter_gather_page_width[i] = bw_int_to_fixed(64);
+					data->scatter_gather_page_height[i] = bw_int_to_fixed(64);
+					break;
+				}
+				data->scatter_gather_pte_request_rows = bw_int_to_fixed(dceip->scatter_gather_pte_request_rows_in_tiling_mode);
+				data->scatter_gather_row_height = data->scatter_gather_page_height[i];
+			}
+			data->pte_request_per_chunk[i] = bw_div(bw_div(bw_int_to_fixed(dceip->chunk_width), data->scatter_gather_page_width[i]), data->useful_pte_per_pte_request);
+			data->scatter_gather_pte_requests_in_row[i] = bw_div(bw_mul(bw_ceil2(bw_mul(bw_div(data->source_width_rounded_up_to_chunks[i], bw_int_to_fixed(dceip->chunk_width)), data->pte_request_per_chunk[i]), bw_int_to_fixed(1)), data->scatter_gather_row_height), data->scatter_gather_page_height[i]);
+			data->scatter_gather_pte_requests_in_vblank = bw_mul(data->scatter_gather_pte_request_rows, data->scatter_gather_pte_requests_in_row[i]);
+			if (bw_equ(data->peak_pte_request_to_eviction_ratio_limiting, bw_int_to_fixed(0))) {
+				data->scatter_gather_pte_request_limit[i] = data->scatter_gather_pte_requests_in_vblank;
+			}
+			else {
+				data->scatter_gather_pte_request_limit[i] = bw_max2(dceip->minimum_outstanding_pte_request_limit, bw_min2(data->scatter_gather_pte_requests_in_vblank, bw_ceil2(bw_mul(bw_mul(bw_div(bw_ceil2(data->adjusted_data_buffer_size[i], data->memory_chunk_size_in_bytes[i]), data->memory_chunk_size_in_bytes[i]), data->pte_request_per_chunk[i]), data->peak_pte_request_to_eviction_ratio_limiting), bw_int_to_fixed(1))));
+			}
+		}
+	}
+	/*pitch padding recommended for efficiency in linear mode*/
+	/*in linear mode graphics or underlay with scatter gather, a pitch that is a multiple of the channel interleave (256 bytes) times the channel-bank rotation is not efficient*/
+	/*if that is the case it is recommended to pad the pitch by at least 256 pixels*/
+	data->inefficient_linear_pitch_in_bytes = bw_mul(bw_mul(bw_int_to_fixed(256), bw_int_to_fixed(vbios->number_of_dram_banks)), bw_int_to_fixed(data->number_of_dram_channels));
+
+	/*pixel transfer time*/
+	/*the dmif and mcifwr yclk(pclk) required is the one that allows the transfer of all pipe's data buffer size in memory in the time for data transfer*/
+	/*for dmif, pte and cursor requests have to be included.*/
+	/*the dram data requirement is doubled when the data request size in bytes is less than the dram channel width times the burst size (8)*/
+	/*the dram data requirement is also multiplied by the number of channels in the case of low power tiling*/
+	/*the page close-open time is determined by trc and the number of page close-opens*/
+	/*in tiled mode graphics or underlay with scatter-gather enabled the bytes per page close-open is the product of the memory line interleave times the maximum of the scatter-gather page width and the product of the tile width (8 pixels) times the number of channels times the number of banks.*/
+	/*in linear mode graphics or underlay with scatter-gather enabled and inefficient pitch, the bytes per page close-open is the line request alternation slice, because different lines are in completely different 4k address bases.*/
+	/*otherwise, the bytes page close-open is the chunk size because that is the arbitration slice.*/
+	/*pte requests are grouped by pte requests per chunk if that is more than 1. each group costs a page close-open time for dmif reads*/
+	/*cursor requests outstanding are limited to a group of two source lines. each group costs a page close-open time for dmif reads*/
+	/*the display reads and writes time for data transfer is the minimum data or cursor buffer size in time minus the mc urgent latency*/
+	/*the mc urgent latency is experienced more than one time if the number of dmif requests in the data buffer exceeds the request buffer size plus the request slots reserved for dmif in the dram channel arbiter queues*/
+	/*the dispclk required is the maximum for all surfaces of the maximum of the source pixels for first output pixel times the throughput factor, divided by the pixels per dispclk, and divided by the minimum latency hiding minus the dram speed/p-state change latency minus the burst time, and the source pixels for last output pixel, times the throughput factor, divided by the pixels per dispclk, and divided by the minimum latency hiding minus the dram speed/p-state change latency minus the burst time, plus the active time.*/
+	/*the data burst time is the maximum of the total page close-open time, total dmif/mcifwr buffer size in memory divided by the dram bandwidth, and the total dmif/mcifwr buffer size in memory divided by the 32 byte sclk data bus bandwidth, each multiplied by its efficiency.*/
+	/*the source line transfer time is the maximum for all surfaces of the maximum of the burst time plus the urgent latency times the floor of the data required divided by the buffer size for the fist pixel, and the burst time plus the urgent latency times the floor of the data required divided by the buffer size for the last pixel plus the active time.*/
+	/*the source pixels for the first output pixel is 512 if the scaler vertical filter initialization value is greater than 2, and it is 4 times the source width if it is greater than 4.*/
+	/*the source pixels for the last output pixel is the source width times the scaler vertical filter initialization value rounded up to even*/
+	/*the source data for these pixels is the number of pixels times the bytes per pixel times the bytes per request divided by the useful bytes per request.*/
+	data->cursor_total_data = bw_int_to_fixed(0);
+	data->cursor_total_request_groups = bw_int_to_fixed(0);
+	data->scatter_gather_total_pte_requests = bw_int_to_fixed(0);
+	data->scatter_gather_total_pte_request_groups = bw_int_to_fixed(0);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			data->cursor_total_data = bw_add(data->cursor_total_data, bw_mul(bw_mul(bw_int_to_fixed(2), data->cursor_width_pixels[i]), bw_int_to_fixed(4)));
+			if (dceip->large_cursor == 1) {
+				data->cursor_total_request_groups = bw_add(data->cursor_total_request_groups, bw_int_to_fixed((dceip->cursor_max_outstanding_group_num + 1)));
+			}
+			else {
+				data->cursor_total_request_groups = bw_add(data->cursor_total_request_groups, bw_ceil2(bw_div(data->cursor_width_pixels[i], dceip->cursor_chunk_width), bw_int_to_fixed(1)));
+			}
+			if (data->scatter_gather_enable_for_pipe[i]) {
+				data->scatter_gather_total_pte_requests = bw_add(data->scatter_gather_total_pte_requests, data->scatter_gather_pte_request_limit[i]);
+				data->scatter_gather_total_pte_request_groups = bw_add(data->scatter_gather_total_pte_request_groups, bw_ceil2(bw_div(data->scatter_gather_pte_request_limit[i], bw_ceil2(data->pte_request_per_chunk[i], bw_int_to_fixed(1))), bw_int_to_fixed(1)));
+			}
+		}
+	}
+	data->tile_width_in_pixels = bw_int_to_fixed(8);
+	data->dmif_total_number_of_data_request_page_close_open = bw_int_to_fixed(0);
+	data->mcifwr_total_number_of_data_request_page_close_open = bw_int_to_fixed(0);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (data->scatter_gather_enable_for_pipe[i] == 1 && tiling_mode[i] != bw_def_linear) {
+				data->bytes_per_page_close_open = bw_mul(data->lines_interleaved_in_mem_access[i], bw_max2(bw_mul(bw_mul(bw_mul(bw_int_to_fixed(data->bytes_per_pixel[i]), data->tile_width_in_pixels), bw_int_to_fixed(vbios->number_of_dram_banks)), bw_int_to_fixed(data->number_of_dram_channels)), bw_mul(bw_int_to_fixed(data->bytes_per_pixel[i]), data->scatter_gather_page_width[i])));
+			}
+			else if (data->scatter_gather_enable_for_pipe[i] == 1 && tiling_mode[i] == bw_def_linear && bw_equ(bw_mod((bw_mul(data->pitch_in_pixels_after_surface_type[i], bw_int_to_fixed(data->bytes_per_pixel[i]))), data->inefficient_linear_pitch_in_bytes), bw_int_to_fixed(0))) {
+				data->bytes_per_page_close_open = dceip->linear_mode_line_request_alternation_slice;
+			}
+			else {
+				data->bytes_per_page_close_open = data->memory_chunk_size_in_bytes[i];
+			}
+			if (surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma) {
+				data->dmif_total_number_of_data_request_page_close_open = bw_add(data->dmif_total_number_of_data_request_page_close_open, bw_div(bw_ceil2(data->adjusted_data_buffer_size[i], data->memory_chunk_size_in_bytes[i]), data->bytes_per_page_close_open));
+			}
+			else {
+				data->mcifwr_total_number_of_data_request_page_close_open = bw_add(data->mcifwr_total_number_of_data_request_page_close_open, bw_div(bw_ceil2(data->adjusted_data_buffer_size[i], data->memory_chunk_size_in_bytes[i]), data->bytes_per_page_close_open));
+			}
+		}
+	}
+	data->dmif_total_page_close_open_time = bw_div(bw_mul((bw_add(bw_add(data->dmif_total_number_of_data_request_page_close_open, data->scatter_gather_total_pte_request_groups), data->cursor_total_request_groups)), vbios->trc), bw_int_to_fixed(1000));
+	data->mcifwr_total_page_close_open_time = bw_div(bw_mul(data->mcifwr_total_number_of_data_request_page_close_open, vbios->trc), bw_int_to_fixed(1000));
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			data->adjusted_data_buffer_size_in_memory[i] = bw_div(bw_mul(data->adjusted_data_buffer_size[i], data->bytes_per_request[i]), data->useful_bytes_per_request[i]);
+		}
+	}
+	data->total_requests_for_adjusted_dmif_size = bw_int_to_fixed(0);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma) {
+				data->total_requests_for_adjusted_dmif_size = bw_add(data->total_requests_for_adjusted_dmif_size, bw_div(data->adjusted_data_buffer_size[i], data->useful_bytes_per_request[i]));
+			}
+		}
+	}
+	data->total_dmifmc_urgent_trips = bw_ceil2(bw_div(data->total_requests_for_adjusted_dmif_size, (bw_add(dceip->dmif_request_buffer_size, bw_int_to_fixed(vbios->number_of_request_slots_gmc_reserves_for_dmif_per_channel * data->number_of_dram_channels)))), bw_int_to_fixed(1));
+	data->total_dmifmc_urgent_latency = bw_mul(vbios->dmifmc_urgent_latency, data->total_dmifmc_urgent_trips);
+	data->total_display_reads_required_data = bw_int_to_fixed(0);
+	data->total_display_reads_required_dram_access_data = bw_int_to_fixed(0);
+	data->total_display_writes_required_data = bw_int_to_fixed(0);
+	data->total_display_writes_required_dram_access_data = bw_int_to_fixed(0);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma) {
+				data->display_reads_required_data = data->adjusted_data_buffer_size_in_memory[i];
+				/*for hbm memories, each channel is split into 2 pseudo-channels that are each 64 bits in width.  each*/
+				/*pseudo-channel may be read independently of one another.*/
+				/*the read burst length (bl) for hbm memories is 4, so each read command will access 32 bytes of data.*/
+				/*the 64 or 32 byte sized data is stored in one pseudo-channel.*/
+				/*it will take 4 memclk cycles or 8 yclk cycles to fetch 64 bytes of data from the hbm memory (2 read commands).*/
+				/*it will take 2 memclk cycles or 4 yclk cycles to fetch 32 bytes of data from the hbm memory (1 read command).*/
+				/*for gddr5/ddr4 memories, there is additional overhead if the size of the request is smaller than 64 bytes.*/
+				/*the read burst length (bl) for gddr5/ddr4 memories is 8, regardless of the size of the data request.*/
+				/*therefore it will require 8 cycles to fetch 64 or 32 bytes of data from the memory.*/
+				/*the memory efficiency will be 50% for the 32 byte sized data.*/
+				if (vbios->memory_type == bw_def_hbm) {
+					data->display_reads_required_dram_access_data = data->adjusted_data_buffer_size_in_memory[i];
+				}
+				else {
+					data->display_reads_required_dram_access_data = bw_mul(data->adjusted_data_buffer_size_in_memory[i], bw_ceil2(bw_div(bw_int_to_fixed((8 * vbios->dram_channel_width_in_bits / 8)), data->bytes_per_request[i]), bw_int_to_fixed(1)));
+				}
+				data->total_display_reads_required_data = bw_add(data->total_display_reads_required_data, data->display_reads_required_data);
+				data->total_display_reads_required_dram_access_data = bw_add(data->total_display_reads_required_dram_access_data, data->display_reads_required_dram_access_data);
+			}
+			else {
+				data->total_display_writes_required_data = bw_add(data->total_display_writes_required_data, data->adjusted_data_buffer_size_in_memory[i]);
+				data->total_display_writes_required_dram_access_data = bw_add(data->total_display_writes_required_dram_access_data, bw_mul(data->adjusted_data_buffer_size_in_memory[i], bw_ceil2(bw_div(bw_int_to_fixed(vbios->dram_channel_width_in_bits), data->bytes_per_request[i]), bw_int_to_fixed(1))));
+			}
+		}
+	}
+	data->total_display_reads_required_data = bw_add(bw_add(data->total_display_reads_required_data, data->cursor_total_data), bw_mul(data->scatter_gather_total_pte_requests, bw_int_to_fixed(64)));
+	data->total_display_reads_required_dram_access_data = bw_add(bw_add(data->total_display_reads_required_dram_access_data, data->cursor_total_data), bw_mul(data->scatter_gather_total_pte_requests, bw_int_to_fixed(64)));
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (bw_mtn(data->v_filter_init[i], bw_int_to_fixed(4))) {
+				data->src_pixels_for_first_output_pixel[i] = bw_mul(bw_int_to_fixed(4), data->source_width_rounded_up_to_chunks[i]);
+			}
+			else {
+				if (bw_mtn(data->v_filter_init[i], bw_int_to_fixed(2))) {
+					data->src_pixels_for_first_output_pixel[i] = bw_int_to_fixed(512);
+				}
+				else {
+					data->src_pixels_for_first_output_pixel[i] = bw_int_to_fixed(0);
+				}
+			}
+			data->src_data_for_first_output_pixel[i] = bw_div(bw_mul(bw_mul(data->src_pixels_for_first_output_pixel[i], bw_int_to_fixed(data->bytes_per_pixel[i])), data->bytes_per_request[i]), data->useful_bytes_per_request[i]);
+			data->src_pixels_for_last_output_pixel[i] = bw_mul(data->source_width_rounded_up_to_chunks[i], bw_max2(bw_ceil2(data->v_filter_init[i], bw_int_to_fixed(dceip->lines_interleaved_into_lb)), bw_mul(bw_ceil2(data->vsr[i], bw_int_to_fixed(dceip->lines_interleaved_into_lb)), data->horizontal_blank_and_chunk_granularity_factor[i])));
+			data->src_data_for_last_output_pixel[i] = bw_div(bw_mul(bw_mul(bw_mul(data->source_width_rounded_up_to_chunks[i], bw_max2(bw_ceil2(data->v_filter_init[i], bw_int_to_fixed(dceip->lines_interleaved_into_lb)), data->lines_interleaved_in_mem_access[i])), bw_int_to_fixed(data->bytes_per_pixel[i])), data->bytes_per_request[i]), data->useful_bytes_per_request[i]);
+			data->active_time[i] = bw_div(bw_div(data->source_width_rounded_up_to_chunks[i], data->hsr[i]), data->pixel_rate[i]);
+		}
+	}
+	for (i = 0; i <= 2; i++) {
+		for (j = 0; j <= 7; j++) {
+			data->dmif_burst_time[i][j] = bw_max3(data->dmif_total_page_close_open_time, bw_div(data->total_display_reads_required_dram_access_data, (bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[i]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels)))), bw_div(data->total_display_reads_required_data, (bw_mul(bw_mul(sclk[j], vbios->data_return_bus_width), bw_int_to_fixed(bus_efficiency)))));
+			if (data->d1_display_write_back_dwb_enable == 1) {
+				data->mcifwr_burst_time[i][j] = bw_max3(data->mcifwr_total_page_close_open_time, bw_div(data->total_display_writes_required_dram_access_data, (bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[i]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_wrchannels)))), bw_div(data->total_display_writes_required_data, (bw_mul(bw_mul(sclk[j], vbios->data_return_bus_width), bw_int_to_fixed(bus_efficiency)))));
+			}
+		}
+	}
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		for (j = 0; j <= 2; j++) {
+			for (k = 0; k <= 7; k++) {
+				if (data->enable[i]) {
+					if (surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma) {
+						/*time to transfer data from the dmif buffer to the lb.  since the mc to dmif transfer time overlaps*/
+						/*with the dmif to lb transfer time, only time to transfer the last chunk  is considered.*/
+						data->dmif_buffer_transfer_time[i] = bw_mul(data->source_width_rounded_up_to_chunks[i], (bw_div(dceip->lb_write_pixels_per_dispclk, (bw_div(vbios->low_voltage_max_dispclk, dceip->display_pipe_throughput_factor)))));
+						data->line_source_transfer_time[i][j][k] = bw_max2(bw_mul((bw_add(data->total_dmifmc_urgent_latency, data->dmif_burst_time[j][k])), bw_floor2(bw_div(data->src_data_for_first_output_pixel[i], data->adjusted_data_buffer_size_in_memory[i]), bw_int_to_fixed(1))), bw_sub(bw_add(bw_mul((bw_add(data->total_dmifmc_urgent_latency, data->dmif_burst_time[j][k])), bw_floor2(bw_div(data->src_data_for_last_output_pixel[i], data->adjusted_data_buffer_size_in_memory[i]), bw_int_to_fixed(1))), data->dmif_buffer_transfer_time[i]), data->active_time[i]));
+						/*during an mclk switch the requests from the dce ip are stored in the gmc/arb.  these requests should be serviced immediately*/
+						/*after the mclk switch sequence and not incur an urgent latency penalty.  it is assumed that the gmc/arb can hold up to 256 requests*/
+						/*per memory channel.  if the dce ip is urgent after the mclk switch sequence, all pending requests and subsequent requests should be*/
+						/*immediately serviced without a gap in the urgent requests.*/
+						/*the latency incurred would be the time to issue the requests and return the data for the first or last output pixel.*/
+						if (surface_type[i] == bw_def_graphics) {
+							switch (data->lb_bpc[i]) {
+							case 6:
+								data->v_scaler_efficiency = dceip->graphics_vscaler_efficiency6_bit_per_component;
+								break;
+							case 8:
+								data->v_scaler_efficiency = dceip->graphics_vscaler_efficiency8_bit_per_component;
+								break;
+							case 10:
+								data->v_scaler_efficiency = dceip->graphics_vscaler_efficiency10_bit_per_component;
+								break;
+							default:
+								data->v_scaler_efficiency = dceip->graphics_vscaler_efficiency12_bit_per_component;
+								break;
+							}
+							if (data->use_alpha[i] == 1) {
+								data->v_scaler_efficiency = bw_min2(data->v_scaler_efficiency, dceip->alpha_vscaler_efficiency);
+							}
+						}
+						else {
+							switch (data->lb_bpc[i]) {
+							case 6:
+								data->v_scaler_efficiency = dceip->underlay_vscaler_efficiency6_bit_per_component;
+								break;
+							case 8:
+								data->v_scaler_efficiency = dceip->underlay_vscaler_efficiency8_bit_per_component;
+								break;
+							case 10:
+								data->v_scaler_efficiency = dceip->underlay_vscaler_efficiency10_bit_per_component;
+								break;
+							default:
+								data->v_scaler_efficiency = bw_int_to_fixed(3);
+								break;
+							}
+						}
+						if (dceip->pre_downscaler_enabled && bw_mtn(data->hsr[i], bw_int_to_fixed(1))) {
+							data->scaler_limits_factor = bw_max2(bw_div(data->v_taps[i], data->v_scaler_efficiency), bw_div(data->source_width_rounded_up_to_chunks[i], data->h_total[i]));
+						}
+						else {
+							data->scaler_limits_factor = bw_max3(bw_int_to_fixed(1), bw_ceil2(bw_div(data->h_taps[i], bw_int_to_fixed(4)), bw_int_to_fixed(1)), bw_mul(data->hsr[i], bw_max2(bw_div(data->v_taps[i], data->v_scaler_efficiency), bw_int_to_fixed(1))));
+						}
+						data->dram_speed_change_line_source_transfer_time[i][j][k] = bw_mul(bw_int_to_fixed(2), bw_max2((bw_add((bw_div(data->src_data_for_first_output_pixel[i], bw_min2(bw_mul(data->bytes_per_request[i], sclk[k]), bw_div(bw_mul(bw_mul(data->bytes_per_request[i], data->pixel_rate[i]), data->scaler_limits_factor), bw_int_to_fixed(2))))), (bw_mul(data->dmif_burst_time[j][k], bw_floor2(bw_div(data->src_data_for_first_output_pixel[i], data->adjusted_data_buffer_size_in_memory[i]), bw_int_to_fixed(1)))))), (bw_add((bw_div(data->src_data_for_last_output_pixel[i], bw_min2(bw_mul(data->bytes_per_request[i], sclk[k]), bw_div(bw_mul(bw_mul(data->bytes_per_request[i], data->pixel_rate[i]), data->scaler_limits_factor), bw_int_to_fixed(2))))), (bw_sub(bw_mul(data->dmif_burst_time[j][k], bw_floor2(bw_div(data->src_data_for_last_output_pixel[i], data->adjusted_data_buffer_size_in_memory[i]), bw_int_to_fixed(1))), data->active_time[i]))))));
+					}
+					else {
+						data->line_source_transfer_time[i][j][k] = bw_max2(bw_mul((bw_add(vbios->mcifwrmc_urgent_latency, data->mcifwr_burst_time[j][k])), bw_floor2(bw_div(data->src_data_for_first_output_pixel[i], data->adjusted_data_buffer_size_in_memory[i]), bw_int_to_fixed(1))), bw_sub(bw_mul((bw_add(vbios->mcifwrmc_urgent_latency, data->mcifwr_burst_time[j][k])), bw_floor2(bw_div(data->src_data_for_last_output_pixel[i], data->adjusted_data_buffer_size_in_memory[i]), bw_int_to_fixed(1))), data->active_time[i]));
+						/*during an mclk switch the requests from the dce ip are stored in the gmc/arb.  these requests should be serviced immediately*/
+						/*after the mclk switch sequence and not incur an urgent latency penalty.  it is assumed that the gmc/arb can hold up to 256 requests*/
+						/*per memory channel.  if the dce ip is urgent after the mclk switch sequence, all pending requests and subsequent requests should be*/
+						/*immediately serviced without a gap in the urgent requests.*/
+						/*the latency incurred would be the time to issue the requests and return the data for the first or last output pixel.*/
+						data->dram_speed_change_line_source_transfer_time[i][j][k] = bw_max2((bw_add((bw_div(data->src_data_for_first_output_pixel[i], bw_min2(bw_mul(data->bytes_per_request[i], sclk[k]), bw_div(bw_mul(data->bytes_per_request[i], vbios->low_voltage_max_dispclk), bw_int_to_fixed(2))))), (bw_mul(data->mcifwr_burst_time[j][k], bw_floor2(bw_div(data->src_data_for_first_output_pixel[i], data->adjusted_data_buffer_size_in_memory[i]), bw_int_to_fixed(1)))))), (bw_add((bw_div(data->src_data_for_last_output_pixel[i], bw_min2(bw_mul(data->bytes_per_request[i], sclk[k]), bw_div(bw_mul(data->bytes_per_request[i], vbios->low_voltage_max_dispclk), bw_int_to_fixed(2))))), (bw_sub(bw_mul(data->mcifwr_burst_time[j][k], bw_floor2(bw_div(data->src_data_for_last_output_pixel[i], data->adjusted_data_buffer_size_in_memory[i]), bw_int_to_fixed(1))), data->active_time[i])))));
+					}
+				}
+			}
+		}
+	}
+	/*cpu c-state and p-state change enable*/
+	/*for cpu p-state change to be possible for a yclk(pclk) and sclk level the dispclk required has to be enough for the blackout duration*/
+	/*for cpu c-state change to be possible for a yclk(pclk) and sclk level the dispclk required has to be enough for the blackout duration and recovery*/
+	/*condition for the blackout duration:*/
+	/* minimum latency hiding > blackout duration + dmif burst time + line source transfer time*/
+	/*condition for the blackout recovery:*/
+	/* recovery time >  dmif burst time + 2 * urgent latency*/
+	/* recovery time > (display bw * blackout duration  + (2 * urgent latency + dmif burst time)*dispclk - dmif size )*/
+	/*                  / (dispclk - display bw)*/
+	/*the minimum latency hiding is the minimum for all pipes of one screen line time, plus one more line time if doing lb prefetch, plus the dmif data buffer size equivalent in time, minus the urgent latency.*/
+	/*the minimum latency hiding is  further limited by the cursor.  the cursor latency hiding is the number of lines of the cursor buffer, minus one if the downscaling is less than two, or minus three if it is more*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if ((bw_equ(dceip->stutter_and_dram_clock_state_change_gated_before_cursor, bw_int_to_fixed(0)) && bw_mtn(data->cursor_width_pixels[i], bw_int_to_fixed(0)))) {
+				if (bw_ltn(data->vsr[i], bw_int_to_fixed(2))) {
+					data->cursor_latency_hiding[i] = bw_div(bw_div(bw_mul((bw_sub(dceip->cursor_dcp_buffer_lines, bw_int_to_fixed(1))), data->h_total[i]), data->vsr[i]), data->pixel_rate[i]);
+				}
+				else {
+					data->cursor_latency_hiding[i] = bw_div(bw_div(bw_mul((bw_sub(dceip->cursor_dcp_buffer_lines, bw_int_to_fixed(3))), data->h_total[i]), data->vsr[i]), data->pixel_rate[i]);
+				}
+			}
+			else {
+				data->cursor_latency_hiding[i] = bw_int_to_fixed(9999);
+			}
+		}
+	}
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (dceip->graphics_lb_nodownscaling_multi_line_prefetching == 1 && (bw_equ(data->vsr[i], bw_int_to_fixed(1)) || (bw_leq(data->vsr[i], bw_frc_to_fixed(8, 10)) && bw_leq(data->v_taps[i], bw_int_to_fixed(2)) && data->lb_bpc[i] == 8)) && surface_type[i] == bw_def_graphics) {
+				data->minimum_latency_hiding[i] = bw_sub(bw_div(bw_mul((bw_div((bw_add(bw_sub(data->lb_partitions[i], bw_int_to_fixed(1)), bw_div(bw_div(data->data_buffer_size[i], bw_int_to_fixed(data->bytes_per_pixel[i])), data->source_width_pixels[i]))), data->vsr[i])), data->h_total[i]), data->pixel_rate[i]), data->total_dmifmc_urgent_latency);
+			}
+			else {
+				data->minimum_latency_hiding[i] = bw_sub(bw_div(bw_mul((bw_div((bw_add(bw_int_to_fixed(1 + data->line_buffer_prefetch[i]), bw_div(bw_div(data->data_buffer_size[i], bw_int_to_fixed(data->bytes_per_pixel[i])), data->source_width_pixels[i]))), data->vsr[i])), data->h_total[i]), data->pixel_rate[i]), data->total_dmifmc_urgent_latency);
+			}
+			data->minimum_latency_hiding_with_cursor[i] = bw_min2(data->minimum_latency_hiding[i], data->cursor_latency_hiding[i]);
+		}
+	}
+	for (i = 0; i <= 2; i++) {
+		for (j = 0; j <= 7; j++) {
+			data->blackout_duration_margin[i][j] = bw_int_to_fixed(9999);
+			data->dispclk_required_for_blackout_duration[i][j] = bw_int_to_fixed(0);
+			data->dispclk_required_for_blackout_recovery[i][j] = bw_int_to_fixed(0);
+			for (k = 0; k <= maximum_number_of_surfaces - 1; k++) {
+				if (data->enable[k] && bw_mtn(vbios->blackout_duration, bw_int_to_fixed(0))) {
+					if (surface_type[k] != bw_def_display_write_back420_luma && surface_type[k] != bw_def_display_write_back420_chroma) {
+						data->blackout_duration_margin[i][j] = bw_min2(data->blackout_duration_margin[i][j], bw_sub(bw_sub(bw_sub(data->minimum_latency_hiding_with_cursor[k], vbios->blackout_duration), data->dmif_burst_time[i][j]), data->line_source_transfer_time[k][i][j]));
+						data->dispclk_required_for_blackout_duration[i][j] = bw_max3(data->dispclk_required_for_blackout_duration[i][j], bw_div(bw_div(bw_mul(data->src_pixels_for_first_output_pixel[k], dceip->display_pipe_throughput_factor), dceip->lb_write_pixels_per_dispclk), (bw_sub(bw_sub(data->minimum_latency_hiding_with_cursor[k], vbios->blackout_duration), data->dmif_burst_time[i][j]))), bw_div(bw_div(bw_mul(data->src_pixels_for_last_output_pixel[k], dceip->display_pipe_throughput_factor), dceip->lb_write_pixels_per_dispclk), (bw_add(bw_sub(bw_sub(data->minimum_latency_hiding_with_cursor[k], vbios->blackout_duration), data->dmif_burst_time[i][j]), data->active_time[k]))));
+						if (bw_leq(vbios->maximum_blackout_recovery_time, bw_add(bw_mul(bw_int_to_fixed(2), data->total_dmifmc_urgent_latency), data->dmif_burst_time[i][j]))) {
+							data->dispclk_required_for_blackout_recovery[i][j] = bw_int_to_fixed(9999);
+						}
+						else if (bw_ltn(data->adjusted_data_buffer_size[k], bw_mul(bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k]), (bw_add(vbios->blackout_duration, bw_add(bw_mul(bw_int_to_fixed(2), data->total_dmifmc_urgent_latency), data->dmif_burst_time[i][j])))))) {
+							data->dispclk_required_for_blackout_recovery[i][j] = bw_max2(data->dispclk_required_for_blackout_recovery[i][j], bw_div(bw_mul(bw_div(bw_div((bw_sub(bw_mul(bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k]), (bw_add(vbios->blackout_duration, vbios->maximum_blackout_recovery_time))), data->adjusted_data_buffer_size[k])), bw_int_to_fixed(data->bytes_per_pixel[k])), (bw_sub(vbios->maximum_blackout_recovery_time, bw_sub(bw_mul(bw_int_to_fixed(2), data->total_dmifmc_urgent_latency), data->dmif_burst_time[i][j])))), data->latency_hiding_lines[k]), data->lines_interleaved_in_mem_access[k]));
+						}
+					}
+					else {
+						data->blackout_duration_margin[i][j] = bw_min2(data->blackout_duration_margin[i][j], bw_sub(bw_sub(bw_sub(bw_sub(data->minimum_latency_hiding_with_cursor[k], vbios->blackout_duration), data->dmif_burst_time[i][j]), data->mcifwr_burst_time[i][j]), data->line_source_transfer_time[k][i][j]));
+						data->dispclk_required_for_blackout_duration[i][j] = bw_max3(data->dispclk_required_for_blackout_duration[i][j], bw_div(bw_div(bw_mul(data->src_pixels_for_first_output_pixel[k], dceip->display_pipe_throughput_factor), dceip->lb_write_pixels_per_dispclk), (bw_sub(bw_sub(bw_sub(data->minimum_latency_hiding_with_cursor[k], vbios->blackout_duration), data->dmif_burst_time[i][j]), data->mcifwr_burst_time[i][j]))), bw_div(bw_div(bw_mul(data->src_pixels_for_last_output_pixel[k], dceip->display_pipe_throughput_factor), dceip->lb_write_pixels_per_dispclk), (bw_add(bw_sub(bw_sub(bw_sub(data->minimum_latency_hiding_with_cursor[k], vbios->blackout_duration), data->dmif_burst_time[i][j]), data->mcifwr_burst_time[i][j]), data->active_time[k]))));
+						if (bw_ltn(vbios->maximum_blackout_recovery_time, bw_add(bw_add(bw_mul(bw_int_to_fixed(2), vbios->mcifwrmc_urgent_latency), data->dmif_burst_time[i][j]), data->mcifwr_burst_time[i][j]))) {
+							data->dispclk_required_for_blackout_recovery[i][j] = bw_int_to_fixed(9999);
+						}
+						else if (bw_ltn(data->adjusted_data_buffer_size[k], bw_mul(bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k]), (bw_add(vbios->blackout_duration, bw_add(bw_mul(bw_int_to_fixed(2), data->total_dmifmc_urgent_latency), data->dmif_burst_time[i][j])))))) {
+							data->dispclk_required_for_blackout_recovery[i][j] = bw_max2(data->dispclk_required_for_blackout_recovery[i][j], bw_div(bw_mul(bw_div(bw_div((bw_sub(bw_mul(bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k]), (bw_add(vbios->blackout_duration, vbios->maximum_blackout_recovery_time))), data->adjusted_data_buffer_size[k])), bw_int_to_fixed(data->bytes_per_pixel[k])), (bw_sub(vbios->maximum_blackout_recovery_time, (bw_add(bw_mul(bw_int_to_fixed(2), data->total_dmifmc_urgent_latency), data->dmif_burst_time[i][j]))))), data->latency_hiding_lines[k]), data->lines_interleaved_in_mem_access[k]));
+						}
+					}
+				}
+			}
+		}
+	}
+	if (bw_mtn(data->blackout_duration_margin[high][s_high], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[high][s_high], vbios->high_voltage_max_dispclk)) {
+		data->cpup_state_change_enable = bw_def_yes;
+		if (bw_ltn(data->dispclk_required_for_blackout_recovery[high][s_high], vbios->high_voltage_max_dispclk)) {
+			data->cpuc_state_change_enable = bw_def_yes;
+		}
+		else {
+			data->cpuc_state_change_enable = bw_def_no;
+		}
+	}
+	else {
+		data->cpup_state_change_enable = bw_def_no;
+		data->cpuc_state_change_enable = bw_def_no;
+	}
+	/*nb p-state change enable*/
+	/*for dram speed/p-state change to be possible for a yclk(pclk) and sclk level there has to be positive margin and the dispclk required has to be*/
+	/*below the maximum.*/
+	/*the dram speed/p-state change margin is the minimum for all surfaces of the maximum latency hiding minus the dram speed/p-state change latency,*/
+	/*minus the dmif burst time, minus the source line transfer time*/
+	/*the maximum latency hiding is the minimum latency hiding plus one source line used for de-tiling in the line buffer, plus half the urgent latency*/
+	/*if stutter and dram clock state change are gated before cursor then the cursor latency hiding does not limit stutter or dram clock state change*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if ((dceip->graphics_lb_nodownscaling_multi_line_prefetching == 1)) {
+				data->maximum_latency_hiding[i] = bw_add(data->minimum_latency_hiding[i], bw_mul(bw_frc_to_fixed(8, 10), data->total_dmifmc_urgent_latency));
+			}
+			else {
+				/*maximum_latency_hiding(i) = minimum_latency_hiding(i) + 1 / vsr(i) * h_total(i) / pixel_rate(i) + 0.5 * total_dmifmc_urgent_latency*/
+				data->maximum_latency_hiding[i] = bw_add(data->minimum_latency_hiding[i], bw_mul(bw_frc_to_fixed(8, 10), data->total_dmifmc_urgent_latency));
+			}
+			data->maximum_latency_hiding_with_cursor[i] = bw_min2(data->maximum_latency_hiding[i], data->cursor_latency_hiding[i]);
+		}
+	}
+	/*initialize variables*/
+	number_of_displays_enabled = 0;
+	number_of_displays_enabled_with_margin = 0;
+	for (k = 0; k < maximum_number_of_surfaces; k++) {
+		if (data->enable[k]) {
+			number_of_displays_enabled = number_of_displays_enabled + 1;
+		}
+	}
+	data->display_pstate_change_enable[maximum_number_of_surfaces - 1] = 0;
+	for (i = 0; i <= 2; i++) {
+		for (j = 0; j <= 7; j++) {
+			data->min_dram_speed_change_margin[i][j] = bw_int_to_fixed(9999);
+			data->dram_speed_change_margin = bw_int_to_fixed(9999);
+			data->dispclk_required_for_dram_speed_change[i][j] = bw_int_to_fixed(0);
+			data->num_displays_with_margin[i][j] = 0;
+			for (k = 0; k <= maximum_number_of_surfaces - 1; k++) {
+				if (data->enable[k]) {
+					if (surface_type[k] != bw_def_display_write_back420_luma && surface_type[k] != bw_def_display_write_back420_chroma) {
+						data->dram_speed_change_margin = bw_sub(bw_sub(bw_sub(data->maximum_latency_hiding_with_cursor[k], vbios->nbp_state_change_latency), data->dmif_burst_time[i][j]), data->dram_speed_change_line_source_transfer_time[k][i][j]);
+						if ((bw_mtn(data->dram_speed_change_margin, bw_int_to_fixed(0)) && bw_ltn(data->dram_speed_change_margin, bw_int_to_fixed(9999)))) {
+							/*determine the minimum dram clock change margin for each set of clock frequencies*/
+							data->min_dram_speed_change_margin[i][j] = bw_min2(data->min_dram_speed_change_margin[i][j], data->dram_speed_change_margin);
+							/*compute the maximum clock frequuency required for the dram clock change at each set of clock frequencies*/
+							data->dispclk_required_for_dram_speed_change[i][j] = bw_max3(data->dispclk_required_for_dram_speed_change[i][j], bw_div(bw_div(bw_mul(data->src_pixels_for_first_output_pixel[k], dceip->display_pipe_throughput_factor), dceip->lb_write_pixels_per_dispclk), (bw_sub(bw_sub(bw_sub(data->maximum_latency_hiding_with_cursor[k], vbios->nbp_state_change_latency), data->dmif_burst_time[i][j]), data->dram_speed_change_line_source_transfer_time[k][i][j]))), bw_div(bw_div(bw_mul(data->src_pixels_for_last_output_pixel[k], dceip->display_pipe_throughput_factor), dceip->lb_write_pixels_per_dispclk), (bw_add(bw_sub(bw_sub(bw_sub(data->maximum_latency_hiding_with_cursor[k], vbios->nbp_state_change_latency), data->dmif_burst_time[i][j]), data->dram_speed_change_line_source_transfer_time[k][i][j]), data->active_time[k]))));
+							if ((bw_ltn(data->dispclk_required_for_dram_speed_change[i][j], vbios->high_voltage_max_dispclk))) {
+								data->display_pstate_change_enable[k] = 1;
+								data->num_displays_with_margin[i][j] = data->num_displays_with_margin[i][j] + 1;
+							}
+						}
+					}
+					else {
+						data->dram_speed_change_margin = bw_sub(bw_sub(bw_sub(bw_sub(data->maximum_latency_hiding_with_cursor[k], vbios->nbp_state_change_latency), data->dmif_burst_time[i][j]), data->mcifwr_burst_time[i][j]), data->dram_speed_change_line_source_transfer_time[k][i][j]);
+						if ((bw_mtn(data->dram_speed_change_margin, bw_int_to_fixed(0)) && bw_ltn(data->dram_speed_change_margin, bw_int_to_fixed(9999)))) {
+							/*determine the minimum dram clock change margin for each display pipe*/
+							data->min_dram_speed_change_margin[i][j] = bw_min2(data->min_dram_speed_change_margin[i][j], data->dram_speed_change_margin);
+							/*compute the maximum clock frequuency required for the dram clock change at each set of clock frequencies*/
+							data->dispclk_required_for_dram_speed_change[i][j] = bw_max3(data->dispclk_required_for_dram_speed_change[i][j], bw_div(bw_div(bw_mul(data->src_pixels_for_first_output_pixel[k], dceip->display_pipe_throughput_factor), dceip->lb_write_pixels_per_dispclk), (bw_sub(bw_sub(bw_sub(bw_sub(data->maximum_latency_hiding_with_cursor[k], vbios->nbp_state_change_latency), data->dmif_burst_time[i][j]), data->dram_speed_change_line_source_transfer_time[k][i][j]), data->mcifwr_burst_time[i][j]))), bw_div(bw_div(bw_mul(data->src_pixels_for_last_output_pixel[k], dceip->display_pipe_throughput_factor), dceip->lb_write_pixels_per_dispclk), (bw_add(bw_sub(bw_sub(bw_sub(bw_sub(data->maximum_latency_hiding_with_cursor[k], vbios->nbp_state_change_latency), data->dmif_burst_time[i][j]), data->dram_speed_change_line_source_transfer_time[k][i][j]), data->mcifwr_burst_time[i][j]), data->active_time[k]))));
+							if ((bw_ltn(data->dispclk_required_for_dram_speed_change[i][j], vbios->high_voltage_max_dispclk))) {
+								data->display_pstate_change_enable[k] = 1;
+								data->num_displays_with_margin[i][j] = data->num_displays_with_margin[i][j] + 1;
+							}
+						}
+					}
+				}
+			}
+		}
+	}
+	/*determine the number of displays with margin to switch in the v_active region*/
+	for (k = 0; k <= maximum_number_of_surfaces - 1; k++) {
+		if ((data->enable[k] == 1 && data->display_pstate_change_enable[k] == 1)) {
+			number_of_displays_enabled_with_margin = number_of_displays_enabled_with_margin + 1;
+		}
+	}
+	/*determine the number of displays that don't have any dram clock change margin, but*/
+	/*have the same resolution.  these displays can switch in a common vblank region if*/
+	/*their frames are aligned.*/
+	data->min_vblank_dram_speed_change_margin = bw_int_to_fixed(9999);
+	for (k = 0; k <= maximum_number_of_surfaces - 1; k++) {
+		if (data->enable[k]) {
+			if (surface_type[k] != bw_def_display_write_back420_luma && surface_type[k] != bw_def_display_write_back420_chroma) {
+				data->v_blank_dram_speed_change_margin[k] = bw_sub(bw_sub(bw_sub(bw_div(bw_mul((bw_sub(data->v_total[k], bw_sub(bw_div(data->src_height[k], data->v_scale_ratio[k]), bw_int_to_fixed(4)))), data->h_total[k]), data->pixel_rate[k]), vbios->nbp_state_change_latency), data->dmif_burst_time[low][s_low]), data->dram_speed_change_line_source_transfer_time[k][low][s_low]);
+				data->min_vblank_dram_speed_change_margin = bw_min2(data->min_vblank_dram_speed_change_margin, data->v_blank_dram_speed_change_margin[k]);
+			}
+			else {
+				data->v_blank_dram_speed_change_margin[k] = bw_sub(bw_sub(bw_sub(bw_sub(bw_div(bw_mul((bw_sub(data->v_total[k], bw_sub(bw_div(data->src_height[k], data->v_scale_ratio[k]), bw_int_to_fixed(4)))), data->h_total[k]), data->pixel_rate[k]), vbios->nbp_state_change_latency), data->dmif_burst_time[low][s_low]), data->mcifwr_burst_time[low][s_low]), data->dram_speed_change_line_source_transfer_time[k][low][s_low]);
+				data->min_vblank_dram_speed_change_margin = bw_min2(data->min_vblank_dram_speed_change_margin, data->v_blank_dram_speed_change_margin[k]);
+			}
+		}
+	}
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		data->displays_with_same_mode[i] = bw_int_to_fixed(0);
+		if (data->enable[i] == 1 && data->display_pstate_change_enable[i] == 0 && bw_mtn(data->v_blank_dram_speed_change_margin[i], bw_int_to_fixed(0))) {
+			for (j = 0; j <= maximum_number_of_surfaces - 1; j++) {
+				if ((data->enable[j] == 1 && bw_equ(data->source_width_rounded_up_to_chunks[i], data->source_width_rounded_up_to_chunks[j]) && bw_equ(data->source_height_rounded_up_to_chunks[i], data->source_height_rounded_up_to_chunks[j]) && bw_equ(data->vsr[i], data->vsr[j]) && bw_equ(data->hsr[i], data->hsr[j]) && bw_equ(data->pixel_rate[i], data->pixel_rate[j]))) {
+					data->displays_with_same_mode[i] = bw_add(data->displays_with_same_mode[i], bw_int_to_fixed(1));
+				}
+			}
+		}
+	}
+	/*compute the maximum number of aligned displays with no margin*/
+	number_of_aligned_displays_with_no_margin = 0;
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		number_of_aligned_displays_with_no_margin = bw_fixed_to_int(bw_max2(bw_int_to_fixed(number_of_aligned_displays_with_no_margin), data->displays_with_same_mode[i]));
+	}
+	/*dram clock change is possible, if all displays have positive margin except for one display or a group of*/
+	/*aligned displays with the same timing.*/
+	/*the display(s) with the negative margin can be switched in the v_blank region while the other*/
+	/*displays are in v_blank or v_active.*/
+	if ((number_of_displays_enabled_with_margin + number_of_aligned_displays_with_no_margin == number_of_displays_enabled && bw_mtn(data->min_dram_speed_change_margin[high][s_high], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[high][s_high], bw_int_to_fixed(9999)) && bw_ltn(data->dispclk_required_for_dram_speed_change[high][s_high], vbios->high_voltage_max_dispclk))) {
+		data->nbp_state_change_enable = bw_def_yes;
+	}
+	else {
+		data->nbp_state_change_enable = bw_def_no;
+	}
+	/*dram clock change is possible only in vblank if all displays are aligned and have no margin*/
+	if ((number_of_aligned_displays_with_no_margin == number_of_displays_enabled)) {
+		nbp_state_change_enable_blank = bw_def_yes;
+	}
+	else {
+		nbp_state_change_enable_blank = bw_def_no;
+	}
+	/*required yclk(pclk)*/
+	/*yclk requirement only makes sense if the dmif and mcifwr data total page close-open time is less than the time for data transfer and the total pte requests fit in the scatter-gather saw queque size*/
+	/*if that is the case, the yclk requirement is the maximum of the ones required by dmif and mcifwr, and the high/low yclk(pclk) is chosen accordingly*/
+	/*high yclk(pclk) has to be selected when dram speed/p-state change is not possible.*/
+	data->min_cursor_memory_interface_buffer_size_in_time = bw_int_to_fixed(9999);
+	/* number of cursor lines stored in the cursor data return buffer*/
+	num_cursor_lines = 0;
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (bw_mtn(data->cursor_width_pixels[i], bw_int_to_fixed(0))) {
+				/*compute number of cursor lines stored in data return buffer*/
+				if (bw_leq(data->cursor_width_pixels[i], bw_int_to_fixed(64)) && dceip->large_cursor == 1) {
+					num_cursor_lines = 4;
+				}
+				else {
+					num_cursor_lines = 2;
+				}
+				data->min_cursor_memory_interface_buffer_size_in_time = bw_min2(data->min_cursor_memory_interface_buffer_size_in_time, bw_div(bw_mul(bw_div(bw_int_to_fixed(num_cursor_lines), data->vsr[i]), data->h_total[i]), data->pixel_rate[i]));
+			}
+		}
+	}
+	/*compute minimum time to read one chunk from the dmif buffer*/
+	if ((number_of_displays_enabled > 2)) {
+		data->chunk_request_delay = 0;
+	}
+	else {
+		data->chunk_request_delay = bw_fixed_to_int(bw_div(bw_int_to_fixed(512), vbios->high_voltage_max_dispclk));
+	}
+	data->min_read_buffer_size_in_time = bw_min2(data->min_cursor_memory_interface_buffer_size_in_time, data->min_dmif_size_in_time);
+	data->display_reads_time_for_data_transfer = bw_sub(bw_sub(data->min_read_buffer_size_in_time, data->total_dmifmc_urgent_latency), bw_int_to_fixed(data->chunk_request_delay));
+	data->display_writes_time_for_data_transfer = bw_sub(data->min_mcifwr_size_in_time, vbios->mcifwrmc_urgent_latency);
+	data->dmif_required_dram_bandwidth = bw_div(data->total_display_reads_required_dram_access_data, data->display_reads_time_for_data_transfer);
+	data->mcifwr_required_dram_bandwidth = bw_div(data->total_display_writes_required_dram_access_data, data->display_writes_time_for_data_transfer);
+	data->required_dmifmc_urgent_latency_for_page_close_open = bw_div((bw_sub(data->min_read_buffer_size_in_time, data->dmif_total_page_close_open_time)), data->total_dmifmc_urgent_trips);
+	data->required_mcifmcwr_urgent_latency = bw_sub(data->min_mcifwr_size_in_time, data->mcifwr_total_page_close_open_time);
+	if (bw_mtn(data->scatter_gather_total_pte_requests, dceip->maximum_total_outstanding_pte_requests_allowed_by_saw)) {
+		data->required_dram_bandwidth_gbyte_per_second = bw_int_to_fixed(9999);
+		yclk_message = bw_def_exceeded_allowed_outstanding_pte_req_queue_size;
+		data->y_clk_level = high;
+		data->dram_bandwidth = bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[high]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels));
+	}
+	else if (bw_mtn(vbios->dmifmc_urgent_latency, data->required_dmifmc_urgent_latency_for_page_close_open) || bw_mtn(vbios->mcifwrmc_urgent_latency, data->required_mcifmcwr_urgent_latency)) {
+		data->required_dram_bandwidth_gbyte_per_second = bw_int_to_fixed(9999);
+		yclk_message = bw_def_exceeded_allowed_page_close_open;
+		data->y_clk_level = high;
+		data->dram_bandwidth = bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[high]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels));
+	}
+	else {
+		data->required_dram_bandwidth_gbyte_per_second = bw_div(bw_max2(data->dmif_required_dram_bandwidth, data->mcifwr_required_dram_bandwidth), bw_int_to_fixed(1000));
+		if (bw_ltn(bw_mul(data->required_dram_bandwidth_gbyte_per_second, bw_int_to_fixed(1000)), bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[low]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels))) && (data->cpup_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[low][s_high], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[low][s_high], vbios->high_voltage_max_dispclk))) && (data->cpuc_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[low][s_high], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[low][s_high], vbios->high_voltage_max_dispclk) && bw_ltn(data->dispclk_required_for_blackout_recovery[low][s_high], vbios->high_voltage_max_dispclk))) && (data->nbp_state_change_enable == bw_def_no || (bw_mtn(data->min_dram_speed_change_margin[low][s_high], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[low][s_high], bw_int_to_fixed(9999)) && bw_leq(data->dispclk_required_for_dram_speed_change[low][s_high], vbios->high_voltage_max_dispclk) && data->num_displays_with_margin[low][s_high] == number_of_displays_enabled_with_margin))) {
+			yclk_message = bw_fixed_to_int(vbios->low_yclk);
+			data->y_clk_level = low;
+			data->dram_bandwidth = bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[low]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels));
+		}
+		else if (bw_ltn(bw_mul(data->required_dram_bandwidth_gbyte_per_second, bw_int_to_fixed(1000)), bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[mid]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels))) && (data->cpup_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[mid][s_high], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[mid][s_high], vbios->high_voltage_max_dispclk))) && (data->cpuc_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[mid][s_high], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[mid][s_high], vbios->high_voltage_max_dispclk) && bw_ltn(data->dispclk_required_for_blackout_recovery[mid][s_high], vbios->high_voltage_max_dispclk))) && (data->nbp_state_change_enable == bw_def_no || (bw_mtn(data->min_dram_speed_change_margin[mid][s_high], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[mid][s_high], bw_int_to_fixed(9999)) && bw_leq(data->dispclk_required_for_dram_speed_change[mid][s_high], vbios->high_voltage_max_dispclk) && data->num_displays_with_margin[mid][s_high] == number_of_displays_enabled_with_margin))) {
+			yclk_message = bw_fixed_to_int(vbios->mid_yclk);
+			data->y_clk_level = mid;
+			data->dram_bandwidth = bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[mid]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels));
+		}
+		else if (bw_ltn(bw_mul(data->required_dram_bandwidth_gbyte_per_second, bw_int_to_fixed(1000)), bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[high]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels)))) {
+			yclk_message = bw_fixed_to_int(vbios->high_yclk);
+			data->y_clk_level = high;
+			data->dram_bandwidth = bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[high]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels));
+		}
+		else {
+			yclk_message = bw_def_exceeded_allowed_maximum_bw;
+			data->y_clk_level = high;
+			data->dram_bandwidth = bw_mul(bw_div(bw_mul(bw_mul(data->dram_efficiency, yclk[high]), bw_int_to_fixed(vbios->dram_channel_width_in_bits)), bw_int_to_fixed(8)), bw_int_to_fixed(data->number_of_dram_channels));
+		}
+	}
+	/*required sclk*/
+	/*sclk requirement only makes sense if the total pte requests fit in the scatter-gather saw queque size*/
+	/*if that is the case, the sclk requirement is the maximum of the ones required by dmif and mcifwr, and the high/mid/low sclk is chosen accordingly, unless that choice results in foresaking dram speed/nb p-state change.*/
+	/*the dmif and mcifwr sclk required is the one that allows the transfer of all pipe's data buffer size through the sclk bus in the time for data transfer*/
+	/*for dmif, pte and cursor requests have to be included.*/
+	data->dmif_required_sclk = bw_div(bw_div(data->total_display_reads_required_data, data->display_reads_time_for_data_transfer), (bw_mul(vbios->data_return_bus_width, bw_int_to_fixed(bus_efficiency))));
+	data->mcifwr_required_sclk = bw_div(bw_div(data->total_display_writes_required_data, data->display_writes_time_for_data_transfer), (bw_mul(vbios->data_return_bus_width, bw_int_to_fixed(bus_efficiency))));
+	if (bw_mtn(data->scatter_gather_total_pte_requests, dceip->maximum_total_outstanding_pte_requests_allowed_by_saw)) {
+		data->required_sclk = bw_int_to_fixed(9999);
+		sclk_message = bw_def_exceeded_allowed_outstanding_pte_req_queue_size;
+		data->sclk_level = s_high;
+	}
+	else if (bw_mtn(vbios->dmifmc_urgent_latency, data->required_dmifmc_urgent_latency_for_page_close_open) || bw_mtn(vbios->mcifwrmc_urgent_latency, data->required_mcifmcwr_urgent_latency)) {
+		data->required_sclk = bw_int_to_fixed(9999);
+		sclk_message = bw_def_exceeded_allowed_page_close_open;
+		data->sclk_level = s_high;
+	}
+	else {
+		data->required_sclk = bw_max2(data->dmif_required_sclk, data->mcifwr_required_sclk);
+		if (bw_ltn(data->required_sclk, sclk[s_low]) && (data->cpup_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_low], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_low], vbios->high_voltage_max_dispclk))) && (data->cpuc_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_low], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_low], vbios->high_voltage_max_dispclk) && bw_ltn(data->dispclk_required_for_blackout_recovery[data->y_clk_level][s_low], vbios->high_voltage_max_dispclk))) && (data->nbp_state_change_enable == bw_def_no || (bw_mtn(data->min_dram_speed_change_margin[data->y_clk_level][s_low], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[data->y_clk_level][s_low], bw_int_to_fixed(9999)) && bw_leq(data->dispclk_required_for_dram_speed_change[data->y_clk_level][s_low], vbios->low_voltage_max_dispclk) && data->num_displays_with_margin[data->y_clk_level][s_low] == number_of_displays_enabled_with_margin))) {
+			sclk_message = bw_def_low;
+			data->sclk_level = s_low;
+			data->required_sclk = vbios->low_sclk;
+		}
+		else if (bw_ltn(data->required_sclk, sclk[s_mid1]) && (data->cpup_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid1], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid1], vbios->high_voltage_max_dispclk))) && (data->cpuc_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid1], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid1], vbios->high_voltage_max_dispclk) && bw_ltn(data->dispclk_required_for_blackout_recovery[data->y_clk_level][s_mid1], vbios->high_voltage_max_dispclk))) && (data->nbp_state_change_enable == bw_def_no || (bw_mtn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid1], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid1], bw_int_to_fixed(9999)) && bw_leq(data->dispclk_required_for_dram_speed_change[data->y_clk_level][s_mid1], vbios->mid_voltage_max_dispclk) && data->num_displays_with_margin[data->y_clk_level][s_mid1] == number_of_displays_enabled_with_margin))) {
+			sclk_message = bw_def_mid;
+			data->sclk_level = s_mid1;
+			data->required_sclk = vbios->mid1_sclk;
+		}
+		else if (bw_ltn(data->required_sclk, sclk[s_mid2]) && (data->cpup_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid2], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid2], vbios->high_voltage_max_dispclk))) && (data->cpuc_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid2], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid2], vbios->high_voltage_max_dispclk) && bw_ltn(data->dispclk_required_for_blackout_recovery[data->y_clk_level][s_mid2], vbios->high_voltage_max_dispclk))) && (data->nbp_state_change_enable == bw_def_no || (bw_mtn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid2], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid2], bw_int_to_fixed(9999)) && bw_leq(data->dispclk_required_for_dram_speed_change[data->y_clk_level][s_mid2], vbios->mid_voltage_max_dispclk) && data->num_displays_with_margin[data->y_clk_level][s_mid2] == number_of_displays_enabled_with_margin))) {
+			sclk_message = bw_def_mid;
+			data->sclk_level = s_mid2;
+			data->required_sclk = vbios->mid2_sclk;
+		}
+		else if (bw_ltn(data->required_sclk, sclk[s_mid3]) && (data->cpup_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid3], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid3], vbios->high_voltage_max_dispclk))) && (data->cpuc_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid3], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid3], vbios->high_voltage_max_dispclk) && bw_ltn(data->dispclk_required_for_blackout_recovery[data->y_clk_level][s_mid3], vbios->high_voltage_max_dispclk))) && (data->nbp_state_change_enable == bw_def_no || (bw_mtn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid3], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid3], bw_int_to_fixed(9999)) && bw_leq(data->dispclk_required_for_dram_speed_change[data->y_clk_level][s_mid3], vbios->mid_voltage_max_dispclk) && data->num_displays_with_margin[data->y_clk_level][s_mid3] == number_of_displays_enabled_with_margin))) {
+			sclk_message = bw_def_mid;
+			data->sclk_level = s_mid3;
+			data->required_sclk = vbios->mid3_sclk;
+		}
+		else if (bw_ltn(data->required_sclk, sclk[s_mid4]) && (data->cpup_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid4], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid4], vbios->high_voltage_max_dispclk))) && (data->cpuc_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid4], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid4], vbios->high_voltage_max_dispclk) && bw_ltn(data->dispclk_required_for_blackout_recovery[data->y_clk_level][s_mid4], vbios->high_voltage_max_dispclk))) && (data->nbp_state_change_enable == bw_def_no || (bw_mtn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid4], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid4], bw_int_to_fixed(9999)) && bw_leq(data->dispclk_required_for_dram_speed_change[data->y_clk_level][s_mid4], vbios->mid_voltage_max_dispclk) && data->num_displays_with_margin[data->y_clk_level][s_mid4] == number_of_displays_enabled_with_margin))) {
+			sclk_message = bw_def_mid;
+			data->sclk_level = s_mid4;
+			data->required_sclk = vbios->mid4_sclk;
+		}
+		else if (bw_ltn(data->required_sclk, sclk[s_mid5]) && (data->cpup_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid5], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid5], vbios->high_voltage_max_dispclk))) && (data->cpuc_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid5], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid5], vbios->high_voltage_max_dispclk) && bw_ltn(data->dispclk_required_for_blackout_recovery[data->y_clk_level][s_mid5], vbios->high_voltage_max_dispclk))) && (data->nbp_state_change_enable == bw_def_no || (bw_mtn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid5], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid5], bw_int_to_fixed(9999)) && bw_leq(data->dispclk_required_for_dram_speed_change[data->y_clk_level][s_mid5], vbios->mid_voltage_max_dispclk) && data->num_displays_with_margin[data->y_clk_level][s_mid5] == number_of_displays_enabled_with_margin))) {
+			sclk_message = bw_def_mid;
+			data->sclk_level = s_mid5;
+			data->required_sclk = vbios->mid5_sclk;
+		}
+		else if (bw_ltn(data->required_sclk, sclk[s_mid6]) && (data->cpup_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid6], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid6], vbios->high_voltage_max_dispclk))) && (data->cpuc_state_change_enable == bw_def_no || (bw_mtn(data->blackout_duration_margin[data->y_clk_level][s_mid6], bw_int_to_fixed(0)) && bw_ltn(data->dispclk_required_for_blackout_duration[data->y_clk_level][s_mid6], vbios->high_voltage_max_dispclk) && bw_ltn(data->dispclk_required_for_blackout_recovery[data->y_clk_level][s_mid6], vbios->high_voltage_max_dispclk))) && (data->nbp_state_change_enable == bw_def_no || (bw_mtn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid6], bw_int_to_fixed(0)) && bw_ltn(data->min_dram_speed_change_margin[data->y_clk_level][s_mid6], bw_int_to_fixed(9999)) && bw_leq(data->dispclk_required_for_dram_speed_change[data->y_clk_level][s_mid6], vbios->high_voltage_max_dispclk) && data->num_displays_with_margin[data->y_clk_level][s_mid6] == number_of_displays_enabled_with_margin))) {
+			sclk_message = bw_def_mid;
+			data->sclk_level = s_mid6;
+			data->required_sclk = vbios->mid6_sclk;
+		}
+		else if (bw_ltn(data->required_sclk, sclk[s_high])) {
+			sclk_message = bw_def_high;
+			data->sclk_level = s_high;
+			data->required_sclk = vbios->high_sclk;
+		}
+		else {
+			sclk_message = bw_def_exceeded_allowed_maximum_sclk;
+			data->sclk_level = s_high;
+			/*required_sclk = high_sclk*/
+		}
+	}
+	/*dispclk*/
+	/*if dispclk is set to the maximum, ramping is not required.  dispclk required without ramping is less than the dispclk required with ramping.*/
+	/*if dispclk required without ramping is more than the maximum dispclk, that is the dispclk required, and the mode is not supported*/
+	/*if that does not happen, but dispclk required with ramping is more than the maximum dispclk, dispclk required is just the maximum dispclk*/
+	/*if that does not happen either, dispclk required is the dispclk required with ramping.*/
+	/*dispclk required without ramping is the maximum of the one required for display pipe pixel throughput, for scaler throughput, for total read request thrrougput and for dram/np p-state change if enabled.*/
+	/*the display pipe pixel throughput is the maximum of lines in per line out in the beginning of the frame and lines in per line out in the middle of the frame multiplied by the horizontal blank and chunk granularity factor, altogether multiplied by the ratio of the source width to the line time, divided by the line buffer pixels per dispclk throughput, and multiplied by the display pipe throughput factor.*/
+	/*the horizontal blank and chunk granularity factor is the ratio of the line time divided by the line time minus half the horizontal blank and chunk time.  it applies when the lines in per line out is not 2 or 4.*/
+	/*the dispclk required for scaler throughput is the product of the pixel rate and the scaling limits factor.*/
+	/*the dispclk required for total read request throughput is the product of the peak request-per-second bandwidth and the dispclk cycles per request, divided by the request efficiency.*/
+	/*for the dispclk required with ramping, instead of multiplying just the pipe throughput by the display pipe throughput factor, we multiply the scaler and pipe throughput by the ramping factor.*/
+	/*the scaling limits factor is the product of the horizontal scale ratio, and the ratio of the vertical taps divided by the scaler efficiency clamped to at least 1.*/
+	/*the scaling limits factor itself it also clamped to at least 1*/
+	/*if doing downscaling with the pre-downscaler enabled, the horizontal scale ratio should not be considered above (use "1")*/
+	data->downspread_factor = bw_add(bw_int_to_fixed(1), bw_div(vbios->down_spread_percentage, bw_int_to_fixed(100)));
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (surface_type[i] == bw_def_graphics) {
+				switch (data->lb_bpc[i]) {
+				case 6:
+					data->v_scaler_efficiency = dceip->graphics_vscaler_efficiency6_bit_per_component;
+					break;
+				case 8:
+					data->v_scaler_efficiency = dceip->graphics_vscaler_efficiency8_bit_per_component;
+					break;
+				case 10:
+					data->v_scaler_efficiency = dceip->graphics_vscaler_efficiency10_bit_per_component;
+					break;
+				default:
+					data->v_scaler_efficiency = dceip->graphics_vscaler_efficiency12_bit_per_component;
+					break;
+				}
+				if (data->use_alpha[i] == 1) {
+					data->v_scaler_efficiency = bw_min2(data->v_scaler_efficiency, dceip->alpha_vscaler_efficiency);
+				}
+			}
+			else {
+				switch (data->lb_bpc[i]) {
+				case 6:
+					data->v_scaler_efficiency = dceip->underlay_vscaler_efficiency6_bit_per_component;
+					break;
+				case 8:
+					data->v_scaler_efficiency = dceip->underlay_vscaler_efficiency8_bit_per_component;
+					break;
+				case 10:
+					data->v_scaler_efficiency = dceip->underlay_vscaler_efficiency10_bit_per_component;
+					break;
+				default:
+					data->v_scaler_efficiency = dceip->underlay_vscaler_efficiency12_bit_per_component;
+					break;
+				}
+			}
+			if (dceip->pre_downscaler_enabled && bw_mtn(data->hsr[i], bw_int_to_fixed(1))) {
+				data->scaler_limits_factor = bw_max2(bw_div(data->v_taps[i], data->v_scaler_efficiency), bw_div(data->source_width_rounded_up_to_chunks[i], data->h_total[i]));
+			}
+			else {
+				data->scaler_limits_factor = bw_max3(bw_int_to_fixed(1), bw_ceil2(bw_div(data->h_taps[i], bw_int_to_fixed(4)), bw_int_to_fixed(1)), bw_mul(data->hsr[i], bw_max2(bw_div(data->v_taps[i], data->v_scaler_efficiency), bw_int_to_fixed(1))));
+			}
+			data->display_pipe_pixel_throughput = bw_div(bw_div(bw_mul(bw_max2(data->lb_lines_in_per_line_out_in_beginning_of_frame[i], bw_mul(data->lb_lines_in_per_line_out_in_middle_of_frame[i], data->horizontal_blank_and_chunk_granularity_factor[i])), data->source_width_rounded_up_to_chunks[i]), (bw_div(data->h_total[i], data->pixel_rate[i]))), dceip->lb_write_pixels_per_dispclk);
+			data->dispclk_required_without_ramping[i] = bw_mul(data->downspread_factor, bw_max2(bw_mul(data->pixel_rate[i], data->scaler_limits_factor), bw_mul(dceip->display_pipe_throughput_factor, data->display_pipe_pixel_throughput)));
+			data->dispclk_required_with_ramping[i] = bw_mul(dceip->dispclk_ramping_factor, bw_max2(bw_mul(data->pixel_rate[i], data->scaler_limits_factor), data->display_pipe_pixel_throughput));
+		}
+	}
+	data->total_dispclk_required_with_ramping = bw_int_to_fixed(0);
+	data->total_dispclk_required_without_ramping = bw_int_to_fixed(0);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (bw_ltn(data->total_dispclk_required_with_ramping, data->dispclk_required_with_ramping[i])) {
+				data->total_dispclk_required_with_ramping = data->dispclk_required_with_ramping[i];
+			}
+			if (bw_ltn(data->total_dispclk_required_without_ramping, data->dispclk_required_without_ramping[i])) {
+				data->total_dispclk_required_without_ramping = data->dispclk_required_without_ramping[i];
+			}
+		}
+	}
+	data->total_read_request_bandwidth = bw_int_to_fixed(0);
+	data->total_write_request_bandwidth = bw_int_to_fixed(0);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma) {
+				data->total_read_request_bandwidth = bw_add(data->total_read_request_bandwidth, data->request_bandwidth[i]);
+			}
+			else {
+				data->total_write_request_bandwidth = bw_add(data->total_write_request_bandwidth, data->request_bandwidth[i]);
+			}
+		}
+	}
+	data->dispclk_required_for_total_read_request_bandwidth = bw_div(bw_mul(data->total_read_request_bandwidth, dceip->dispclk_per_request), dceip->request_efficiency);
+	data->total_dispclk_required_with_ramping_with_request_bandwidth = bw_max2(data->total_dispclk_required_with_ramping, data->dispclk_required_for_total_read_request_bandwidth);
+	data->total_dispclk_required_without_ramping_with_request_bandwidth = bw_max2(data->total_dispclk_required_without_ramping, data->dispclk_required_for_total_read_request_bandwidth);
+	if (data->cpuc_state_change_enable == bw_def_yes) {
+		data->total_dispclk_required_with_ramping_with_request_bandwidth = bw_max3(data->total_dispclk_required_with_ramping_with_request_bandwidth, data->dispclk_required_for_blackout_duration[data->y_clk_level][data->sclk_level], data->dispclk_required_for_blackout_recovery[data->y_clk_level][data->sclk_level]);
+		data->total_dispclk_required_without_ramping_with_request_bandwidth = bw_max3(data->total_dispclk_required_without_ramping_with_request_bandwidth, data->dispclk_required_for_blackout_duration[data->y_clk_level][data->sclk_level], data->dispclk_required_for_blackout_recovery[data->y_clk_level][data->sclk_level]);
+	}
+	if (data->cpup_state_change_enable == bw_def_yes) {
+		data->total_dispclk_required_with_ramping_with_request_bandwidth = bw_max2(data->total_dispclk_required_with_ramping_with_request_bandwidth, data->dispclk_required_for_blackout_duration[data->y_clk_level][data->sclk_level]);
+		data->total_dispclk_required_without_ramping_with_request_bandwidth = bw_max2(data->total_dispclk_required_without_ramping_with_request_bandwidth, data->dispclk_required_for_blackout_duration[data->y_clk_level][data->sclk_level]);
+	}
+	if (data->nbp_state_change_enable == bw_def_yes) {
+		data->total_dispclk_required_with_ramping_with_request_bandwidth = bw_max2(data->total_dispclk_required_with_ramping_with_request_bandwidth, data->dispclk_required_for_dram_speed_change[data->y_clk_level][data->sclk_level]);
+		data->total_dispclk_required_without_ramping_with_request_bandwidth = bw_max2(data->total_dispclk_required_without_ramping_with_request_bandwidth, data->dispclk_required_for_dram_speed_change[data->y_clk_level][data->sclk_level]);
+	}
+	if (bw_ltn(data->total_dispclk_required_with_ramping_with_request_bandwidth, vbios->high_voltage_max_dispclk)) {
+		data->dispclk = data->total_dispclk_required_with_ramping_with_request_bandwidth;
+	}
+	else if (bw_ltn(data->total_dispclk_required_without_ramping_with_request_bandwidth, vbios->high_voltage_max_dispclk)) {
+		data->dispclk = vbios->high_voltage_max_dispclk;
+	}
+	else {
+		data->dispclk = data->total_dispclk_required_without_ramping_with_request_bandwidth;
+	}
+	/* required core voltage*/
+	/* the core voltage required is low if sclk, yclk(pclk)and dispclk are within the low limits*/
+	/* otherwise, the core voltage required is medium if yclk (pclk) is within the low limit and sclk and dispclk are within the medium limit*/
+	/* otherwise, the core voltage required is high if the three clocks are within the high limits*/
+	/* otherwise, or if the mode is not supported, core voltage requirement is not applicable*/
+	if (pipe_check == bw_def_notok) {
+		voltage = bw_def_na;
+	}
+	else if (mode_check == bw_def_notok) {
+		voltage = bw_def_notok;
+	}
+	else if (bw_equ(bw_int_to_fixed(yclk_message), vbios->low_yclk) && sclk_message == bw_def_low && bw_ltn(data->dispclk, vbios->low_voltage_max_dispclk)) {
+		voltage = bw_def_0_72;
+	}
+	else if ((bw_equ(bw_int_to_fixed(yclk_message), vbios->low_yclk) || bw_equ(bw_int_to_fixed(yclk_message), vbios->mid_yclk)) && (sclk_message == bw_def_low || sclk_message == bw_def_mid) && bw_ltn(data->dispclk, vbios->mid_voltage_max_dispclk)) {
+		voltage = bw_def_0_8;
+	}
+	else if ((bw_equ(bw_int_to_fixed(yclk_message), vbios->low_yclk) || bw_equ(bw_int_to_fixed(yclk_message), vbios->mid_yclk) || bw_equ(bw_int_to_fixed(yclk_message), vbios->high_yclk)) && (sclk_message == bw_def_low || sclk_message == bw_def_mid || sclk_message == bw_def_high) && bw_leq(data->dispclk, vbios->high_voltage_max_dispclk)) {
+		if ((data->nbp_state_change_enable == bw_def_no && nbp_state_change_enable_blank == bw_def_no)) {
+			voltage = bw_def_high_no_nbp_state_change;
+		}
+		else {
+			voltage = bw_def_0_9;
+		}
+	}
+	else {
+		voltage = bw_def_notok;
+	}
+	if (voltage == bw_def_0_72) {
+		data->max_phyclk = vbios->low_voltage_max_phyclk;
+	}
+	else if (voltage == bw_def_0_8) {
+		data->max_phyclk = vbios->mid_voltage_max_phyclk;
+	}
+	else {
+		data->max_phyclk = vbios->high_voltage_max_phyclk;
+	}
+	/*required blackout recovery time*/
+	data->blackout_recovery_time = bw_int_to_fixed(0);
+	for (k = 0; k <= maximum_number_of_surfaces - 1; k++) {
+		if (data->enable[k] && bw_mtn(vbios->blackout_duration, bw_int_to_fixed(0)) && data->cpup_state_change_enable == bw_def_yes) {
+			if (surface_type[k] != bw_def_display_write_back420_luma && surface_type[k] != bw_def_display_write_back420_chroma) {
+				data->blackout_recovery_time = bw_max2(data->blackout_recovery_time, bw_add(bw_mul(bw_int_to_fixed(2), data->total_dmifmc_urgent_latency), data->dmif_burst_time[data->y_clk_level][data->sclk_level]));
+				if (bw_ltn(data->adjusted_data_buffer_size[k], bw_mul(bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k]), (bw_add(vbios->blackout_duration, bw_add(bw_mul(bw_int_to_fixed(2), data->total_dmifmc_urgent_latency), data->dmif_burst_time[data->y_clk_level][data->sclk_level])))))) {
+					data->blackout_recovery_time = bw_max2(data->blackout_recovery_time, bw_div((bw_add(bw_mul(bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k]), vbios->blackout_duration), bw_sub(bw_div(bw_mul(bw_mul(bw_mul((bw_add(bw_mul(bw_int_to_fixed(2), data->total_dmifmc_urgent_latency), data->dmif_burst_time[data->y_clk_level][data->sclk_level])), data->dispclk), bw_int_to_fixed(data->bytes_per_pixel[k])), data->lines_interleaved_in_mem_access[k]), data->latency_hiding_lines[k]), data->adjusted_data_buffer_size[k]))), (bw_sub(bw_div(bw_mul(bw_mul(data->dispclk, bw_int_to_fixed(data->bytes_per_pixel[k])), data->lines_interleaved_in_mem_access[k]), data->latency_hiding_lines[k]), bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k])))));
+				}
+			}
+			else {
+				data->blackout_recovery_time = bw_max2(data->blackout_recovery_time, bw_add(bw_mul(bw_int_to_fixed(2), vbios->mcifwrmc_urgent_latency), data->mcifwr_burst_time[data->y_clk_level][data->sclk_level]));
+				if (bw_ltn(data->adjusted_data_buffer_size[k], bw_mul(bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k]), (bw_add(vbios->blackout_duration, bw_add(bw_mul(bw_int_to_fixed(2), vbios->mcifwrmc_urgent_latency), data->mcifwr_burst_time[data->y_clk_level][data->sclk_level])))))) {
+					data->blackout_recovery_time = bw_max2(data->blackout_recovery_time, bw_div((bw_add(bw_mul(bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k]), vbios->blackout_duration), bw_sub(bw_div(bw_mul(bw_mul(bw_mul((bw_add(bw_add(bw_mul(bw_int_to_fixed(2), vbios->mcifwrmc_urgent_latency), data->dmif_burst_time[i][j]), data->mcifwr_burst_time[data->y_clk_level][data->sclk_level])), data->dispclk), bw_int_to_fixed(data->bytes_per_pixel[k])), data->lines_interleaved_in_mem_access[k]), data->latency_hiding_lines[k]), data->adjusted_data_buffer_size[k]))), (bw_sub(bw_div(bw_mul(bw_mul(data->dispclk, bw_int_to_fixed(data->bytes_per_pixel[k])), data->lines_interleaved_in_mem_access[k]), data->latency_hiding_lines[k]), bw_div(bw_mul(data->display_bandwidth[k], data->useful_bytes_per_request[k]), data->bytes_per_request[k])))));
+				}
+			}
+		}
+	}
+	/*sclk deep sleep*/
+	/*during self-refresh, sclk can be reduced to dispclk divided by the minimum pixels in the data fifo entry, with 15% margin, but shoudl not be set to less than the request bandwidth.*/
+	/*the data fifo entry is 16 pixels for the writeback, 64 bytes/bytes_per_pixel for the graphics, 16 pixels for the parallel rotation underlay,*/
+	/*and 16 bytes/bytes_per_pixel for the orthogonal rotation underlay.*/
+	/*in parallel mode (underlay pipe), the data read from the dmifv buffer is variable and based on the pixel depth (8bbp - 16 bytes, 16 bpp - 32 bytes, 32 bpp - 64 bytes)*/
+	/*in orthogonal mode (underlay pipe), the data read from the dmifv buffer is fixed at 16 bytes.*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (surface_type[i] == bw_def_display_write_back420_luma || surface_type[i] == bw_def_display_write_back420_chroma) {
+				data->pixels_per_data_fifo_entry[i] = bw_int_to_fixed(16);
+			}
+			else if (surface_type[i] == bw_def_graphics) {
+				data->pixels_per_data_fifo_entry[i] = bw_div(bw_int_to_fixed(64), bw_int_to_fixed(data->bytes_per_pixel[i]));
+			}
+			else if (data->orthogonal_rotation[i] == 0) {
+				data->pixels_per_data_fifo_entry[i] = bw_int_to_fixed(16);
+			}
+			else {
+				data->pixels_per_data_fifo_entry[i] = bw_div(bw_int_to_fixed(16), bw_int_to_fixed(data->bytes_per_pixel[i]));
+			}
+		}
+	}
+	data->min_pixels_per_data_fifo_entry = bw_int_to_fixed(9999);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (bw_mtn(data->min_pixels_per_data_fifo_entry, data->pixels_per_data_fifo_entry[i])) {
+				data->min_pixels_per_data_fifo_entry = data->pixels_per_data_fifo_entry[i];
+			}
+		}
+	}
+	data->sclk_deep_sleep = bw_max2(bw_div(bw_mul(data->dispclk, bw_frc_to_fixed(115, 100)), data->min_pixels_per_data_fifo_entry), data->total_read_request_bandwidth);
+	/*urgent, stutter and nb-p_state watermark*/
+	/*the urgent watermark is the maximum of the urgent trip time plus the pixel transfer time, the urgent trip times to get data for the first pixel, and the urgent trip times to get data for the last pixel.*/
+	/*the stutter exit watermark is the self refresh exit time plus the maximum of the data burst time plus the pixel transfer time, the data burst times to get data for the first pixel, and the data burst times to get data for the last pixel.  it does not apply to the writeback.*/
+	/*the nb p-state change watermark is the dram speed/p-state change time plus the maximum of the data burst time plus the pixel transfer time, the data burst times to get data for the first pixel, and the data burst times to get data for the last pixel.*/
+	/*the pixel transfer time is the maximum of the time to transfer the source pixels required for the first output pixel, and the time to transfer the pixels for the last output pixel minus the active line time.*/
+	/*blackout_duration is added to the urgent watermark*/
+	data->chunk_request_time = bw_int_to_fixed(0);
+	data->cursor_request_time = bw_int_to_fixed(0);
+	/*compute total time to request one chunk from each active display pipe*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			data->chunk_request_time = bw_add(data->chunk_request_time, (bw_div((bw_div(bw_int_to_fixed(pixels_per_chunk * data->bytes_per_pixel[i]), data->useful_bytes_per_request[i])), bw_min2(sclk[data->sclk_level], bw_div(data->dispclk, bw_int_to_fixed(2))))));
+		}
+	}
+	/*compute total time to request cursor data*/
+	data->cursor_request_time = (bw_div(data->cursor_total_data, (bw_mul(bw_int_to_fixed(32), sclk[data->sclk_level]))));
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			data->line_source_pixels_transfer_time = bw_max2(bw_div(bw_div(data->src_pixels_for_first_output_pixel[i], dceip->lb_write_pixels_per_dispclk), (bw_div(data->dispclk, dceip->display_pipe_throughput_factor))), bw_sub(bw_div(bw_div(data->src_pixels_for_last_output_pixel[i], dceip->lb_write_pixels_per_dispclk), (bw_div(data->dispclk, dceip->display_pipe_throughput_factor))), data->active_time[i]));
+			if (surface_type[i] != bw_def_display_write_back420_luma && surface_type[i] != bw_def_display_write_back420_chroma) {
+				data->urgent_watermark[i] = bw_add(bw_add(bw_add(bw_add(bw_add(data->total_dmifmc_urgent_latency, data->dmif_burst_time[data->y_clk_level][data->sclk_level]), bw_max2(data->line_source_pixels_transfer_time, data->line_source_transfer_time[i][data->y_clk_level][data->sclk_level])), vbios->blackout_duration), data->chunk_request_time), data->cursor_request_time);
+				data->stutter_exit_watermark[i] = bw_add(bw_sub(vbios->stutter_self_refresh_exit_latency, data->total_dmifmc_urgent_latency), data->urgent_watermark[i]);
+				data->stutter_entry_watermark[i] = bw_add(bw_sub(bw_add(vbios->stutter_self_refresh_exit_latency, vbios->stutter_self_refresh_entry_latency), data->total_dmifmc_urgent_latency), data->urgent_watermark[i]);
+				/*unconditionally remove black out time from the nb p_state watermark*/
+				if ((data->display_pstate_change_enable[i] == 1)) {
+					data->nbp_state_change_watermark[i] = bw_add(bw_add(vbios->nbp_state_change_latency, data->dmif_burst_time[data->y_clk_level][data->sclk_level]), bw_max2(data->line_source_pixels_transfer_time, data->dram_speed_change_line_source_transfer_time[i][data->y_clk_level][data->sclk_level]));
+				}
+				else {
+					/*maximize the watermark to force the switch in the vb_lank region of the frame*/
+					data->nbp_state_change_watermark[i] = bw_int_to_fixed(131000);
+				}
+			}
+			else {
+				data->urgent_watermark[i] = bw_add(bw_add(bw_add(bw_add(bw_add(vbios->mcifwrmc_urgent_latency, data->mcifwr_burst_time[data->y_clk_level][data->sclk_level]), bw_max2(data->line_source_pixels_transfer_time, data->line_source_transfer_time[i][data->y_clk_level][data->sclk_level])), vbios->blackout_duration), data->chunk_request_time), data->cursor_request_time);
+				data->stutter_exit_watermark[i] = bw_int_to_fixed(0);
+				data->stutter_entry_watermark[i] = bw_int_to_fixed(0);
+				if ((data->display_pstate_change_enable[i] == 1)) {
+					data->nbp_state_change_watermark[i] = bw_add(bw_add(vbios->nbp_state_change_latency, data->mcifwr_burst_time[data->y_clk_level][data->sclk_level]), bw_max2(data->line_source_pixels_transfer_time, data->dram_speed_change_line_source_transfer_time[i][data->y_clk_level][data->sclk_level]));
+				}
+				else {
+					/*maximize the watermark to force the switch in the vb_lank region of the frame*/
+					data->nbp_state_change_watermark[i] = bw_int_to_fixed(131000);
+				}
+			}
+		}
+	}
+	/*stutter mode enable*/
+	/*in the multi-display case the stutter exit or entry watermark cannot exceed the minimum latency hiding capabilities of the*/
+	/*display pipe.*/
+	data->stutter_mode_enable = data->cpuc_state_change_enable;
+	if (data->number_of_displays > 1) {
+		for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+			if (data->enable[i]) {
+				if ((bw_mtn(data->stutter_exit_watermark[i], data->minimum_latency_hiding[i]) || bw_mtn(data->stutter_entry_watermark[i], data->minimum_latency_hiding[i]))) {
+					data->stutter_mode_enable = bw_def_no;
+				}
+			}
+		}
+	}
+	/*performance metrics*/
+	/* display read access efficiency (%)*/
+	/* display write back access efficiency (%)*/
+	/* stutter efficiency (%)*/
+	/* extra underlay pitch recommended for efficiency (pixels)*/
+	/* immediate flip time (us)*/
+	/* latency for other clients due to urgent display read (us)*/
+	/* latency for other clients due to urgent display write (us)*/
+	/* average bandwidth consumed by display (no compression) (gb/s)*/
+	/* required dram  bandwidth (gb/s)*/
+	/* required sclk (m_hz)*/
+	/* required rd urgent latency (us)*/
+	/* nb p-state change margin (us)*/
+	/*dmif and mcifwr dram access efficiency*/
+	/*is the ratio between the ideal dram access time (which is the data buffer size in memory divided by the dram bandwidth), and the actual time which is the total page close-open time.  but it cannot exceed the dram efficiency provided by the memory subsystem*/
+	data->dmifdram_access_efficiency = bw_min2(bw_div(bw_div(data->total_display_reads_required_dram_access_data, data->dram_bandwidth), data->dmif_total_page_close_open_time), bw_int_to_fixed(1));
+	if (bw_mtn(data->total_display_writes_required_dram_access_data, bw_int_to_fixed(0))) {
+		data->mcifwrdram_access_efficiency = bw_min2(bw_div(bw_div(data->total_display_writes_required_dram_access_data, data->dram_bandwidth), data->mcifwr_total_page_close_open_time), bw_int_to_fixed(1));
+	}
+	else {
+		data->mcifwrdram_access_efficiency = bw_int_to_fixed(0);
+	}
+	/*average bandwidth*/
+	/*the average bandwidth with no compression is the vertical active time is the source width times the bytes per pixel divided by the line time, multiplied by the vertical scale ratio and the ratio of bytes per request divided by the useful bytes per request.*/
+	/*the average bandwidth with compression is the same, divided by the compression ratio*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			data->average_bandwidth_no_compression[i] = bw_div(bw_mul(bw_mul(bw_div(bw_mul(data->source_width_rounded_up_to_chunks[i], bw_int_to_fixed(data->bytes_per_pixel[i])), (bw_div(data->h_total[i], data->pixel_rate[i]))), data->vsr[i]), data->bytes_per_request[i]), data->useful_bytes_per_request[i]);
+			data->average_bandwidth[i] = bw_div(data->average_bandwidth_no_compression[i], data->compression_rate[i]);
+		}
+	}
+	data->total_average_bandwidth_no_compression = bw_int_to_fixed(0);
+	data->total_average_bandwidth = bw_int_to_fixed(0);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			data->total_average_bandwidth_no_compression = bw_add(data->total_average_bandwidth_no_compression, data->average_bandwidth_no_compression[i]);
+			data->total_average_bandwidth = bw_add(data->total_average_bandwidth, data->average_bandwidth[i]);
+		}
+	}
+	/*stutter efficiency*/
+	/*the stutter efficiency is the frame-average time in self-refresh divided by the frame-average stutter cycle duration.  only applies if the display write-back is not enabled.*/
+	/*the frame-average stutter cycle used is the minimum for all pipes of the frame-average data buffer size in time, times the compression rate*/
+	/*the frame-average time in self-refresh is the stutter cycle minus the self refresh exit latency and the burst time*/
+	/*the stutter cycle is the dmif buffer size reduced by the excess of the stutter exit watermark over the lb size in time.*/
+	/*the burst time is the data needed during the stutter cycle divided by the available bandwidth*/
+	/*compute the time read all the data from the dmif buffer to the lb (dram refresh period)*/
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			data->stutter_refresh_duration[i] = bw_sub(bw_mul(bw_div(bw_div(bw_mul(bw_div(bw_div(data->adjusted_data_buffer_size[i], bw_int_to_fixed(data->bytes_per_pixel[i])), data->source_width_rounded_up_to_chunks[i]), data->h_total[i]), data->vsr[i]), data->pixel_rate[i]), data->compression_rate[i]), bw_max2(bw_int_to_fixed(0), bw_sub(data->stutter_exit_watermark[i], bw_div(bw_mul((bw_sub(data->lb_partitions[i], bw_int_to_fixed(1))), data->h_total[i]), data->pixel_rate[i]))));
+			data->stutter_dmif_buffer_size[i] = bw_div(bw_mul(bw_mul(bw_div(bw_mul(bw_mul(data->stutter_refresh_duration[i], bw_int_to_fixed(data->bytes_per_pixel[i])), data->source_width_rounded_up_to_chunks[i]), data->h_total[i]), data->vsr[i]), data->pixel_rate[i]), data->compression_rate[i]);
+		}
+	}
+	data->min_stutter_refresh_duration = bw_int_to_fixed(9999);
+	data->total_stutter_dmif_buffer_size = 0;
+	data->total_bytes_requested = 0;
+	data->min_stutter_dmif_buffer_size = 9999;
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			if (bw_mtn(data->min_stutter_refresh_duration, data->stutter_refresh_duration[i])) {
+				data->min_stutter_refresh_duration = data->stutter_refresh_duration[i];
+				data->total_bytes_requested = bw_fixed_to_int(bw_add(bw_int_to_fixed(data->total_bytes_requested), (bw_mul(bw_mul(data->source_height_rounded_up_to_chunks[i], data->source_width_rounded_up_to_chunks[i]), bw_int_to_fixed(data->bytes_per_pixel[i])))));
+				data->min_stutter_dmif_buffer_size = bw_fixed_to_int(data->stutter_dmif_buffer_size[i]);
+			}
+			data->total_stutter_dmif_buffer_size = bw_fixed_to_int(bw_add(data->stutter_dmif_buffer_size[i], bw_int_to_fixed(data->total_stutter_dmif_buffer_size)));
+		}
+	}
+	data->stutter_burst_time = bw_div(bw_int_to_fixed(data->total_stutter_dmif_buffer_size), bw_min2(bw_mul(data->dram_bandwidth, data->dmifdram_access_efficiency), bw_mul(sclk[data->sclk_level], bw_int_to_fixed(32))));
+	data->num_stutter_bursts = data->total_bytes_requested / data->min_stutter_dmif_buffer_size;
+	data->total_stutter_cycle_duration = bw_add(bw_add(data->min_stutter_refresh_duration, vbios->stutter_self_refresh_exit_latency), data->stutter_burst_time);
+	data->time_in_self_refresh = data->min_stutter_refresh_duration;
+	if (data->d1_display_write_back_dwb_enable == 1) {
+		data->stutter_efficiency = bw_int_to_fixed(0);
+	}
+	else if (bw_ltn(data->time_in_self_refresh, bw_int_to_fixed(0))) {
+		data->stutter_efficiency = bw_int_to_fixed(0);
+	}
+	else {
+		/*compute stutter efficiency assuming 60 hz refresh rate*/
+		data->stutter_efficiency = bw_max2(bw_int_to_fixed(0), bw_mul((bw_sub(bw_int_to_fixed(1), (bw_div(bw_mul((bw_add(vbios->stutter_self_refresh_exit_latency, data->stutter_burst_time)), bw_int_to_fixed(data->num_stutter_bursts)), bw_frc_to_fixed(166666667, 10000))))), bw_int_to_fixed(100)));
+	}
+	/*immediate flip time*/
+	/*if scatter gather is enabled, the immediate flip takes a number of urgent memory trips equivalent to the pte requests in a row divided by the pte request limit.*/
+	/*otherwise, it may take just one urgenr memory trip*/
+	data->worst_number_of_trips_to_memory = bw_int_to_fixed(1);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i] && data->scatter_gather_enable_for_pipe[i] == 1) {
+			data->number_of_trips_to_memory_for_getting_apte_row[i] = bw_ceil2(bw_div(data->scatter_gather_pte_requests_in_row[i], data->scatter_gather_pte_request_limit[i]), bw_int_to_fixed(1));
+			if (bw_ltn(data->worst_number_of_trips_to_memory, data->number_of_trips_to_memory_for_getting_apte_row[i])) {
+				data->worst_number_of_trips_to_memory = data->number_of_trips_to_memory_for_getting_apte_row[i];
+			}
+		}
+	}
+	data->immediate_flip_time = bw_mul(data->worst_number_of_trips_to_memory, data->total_dmifmc_urgent_latency);
+	/*worst latency for other clients*/
+	/*it is the urgent latency plus the urgent burst time*/
+	data->latency_for_non_dmif_clients = bw_add(data->total_dmifmc_urgent_latency, data->dmif_burst_time[data->y_clk_level][data->sclk_level]);
+	if (data->d1_display_write_back_dwb_enable == 1) {
+		data->latency_for_non_mcifwr_clients = bw_add(vbios->mcifwrmc_urgent_latency, dceip->mcifwr_all_surfaces_burst_time);
+	}
+	else {
+		data->latency_for_non_mcifwr_clients = bw_int_to_fixed(0);
+	}
+	/*dmif mc urgent latency suppported in high sclk and yclk*/
+	data->dmifmc_urgent_latency_supported_in_high_sclk_and_yclk = bw_div((bw_sub(data->min_read_buffer_size_in_time, data->dmif_burst_time[high][s_high])), data->total_dmifmc_urgent_trips);
+	/*dram speed/p-state change margin*/
+	/*in the multi-display case the nb p-state change watermark cannot exceed the average lb size plus the dmif size or the cursor dcp buffer size*/
+	data->v_blank_nbp_state_dram_speed_change_latency_supported = bw_int_to_fixed(99999);
+	data->nbp_state_dram_speed_change_latency_supported = bw_int_to_fixed(99999);
+	for (i = 0; i <= maximum_number_of_surfaces - 1; i++) {
+		if (data->enable[i]) {
+			data->nbp_state_dram_speed_change_latency_supported = bw_min2(data->nbp_state_dram_speed_change_latency_supported, bw_add(bw_sub(data->maximum_latency_hiding_with_cursor[i], data->nbp_state_change_watermark[i]), vbios->nbp_state_change_latency));
+			data->v_blank_nbp_state_dram_speed_change_latency_supported = bw_min2(data->v_blank_nbp_state_dram_speed_change_latency_supported, bw_add(bw_sub(bw_div(bw_mul((bw_sub(data->v_total[i], bw_sub(bw_div(data->src_height[i], data->v_scale_ratio[i]), bw_int_to_fixed(4)))), data->h_total[i]), data->pixel_rate[i]), data->nbp_state_change_watermark[i]), vbios->nbp_state_change_latency));
+		}
+	}
+	/*sclk required vs urgent latency*/
+	for (i = 1; i <= 5; i++) {
+		data->display_reads_time_for_data_transfer_and_urgent_latency = bw_sub(data->min_read_buffer_size_in_time, bw_mul(data->total_dmifmc_urgent_trips, bw_int_to_fixed(i)));
+		if (pipe_check == bw_def_ok && (bw_mtn(data->display_reads_time_for_data_transfer_and_urgent_latency, data->dmif_total_page_close_open_time))) {
+			data->dmif_required_sclk_for_urgent_latency[i] = bw_div(bw_div(data->total_display_reads_required_data, data->display_reads_time_for_data_transfer_and_urgent_latency), (bw_mul(vbios->data_return_bus_width, bw_int_to_fixed(bus_efficiency))));
+		}
+		else {
+			data->dmif_required_sclk_for_urgent_latency[i] = bw_int_to_fixed(bw_def_na);
+		}
+	}
+	/*output link bit per pixel supported*/
+	for (k = 0; k <= maximum_number_of_surfaces - 1; k++) {
+		data->output_bpphdmi[k] = bw_def_na;
+		data->output_bppdp4_lane_hbr[k] = bw_def_na;
+		data->output_bppdp4_lane_hbr2[k] = bw_def_na;
+		data->output_bppdp4_lane_hbr3[k] = bw_def_na;
+		if (data->enable[k]) {
+			data->output_bpphdmi[k] = bw_fixed_to_int(bw_mul(bw_div(bw_min2(bw_int_to_fixed(600), data->max_phyclk), data->pixel_rate[k]), bw_int_to_fixed(24)));
+			if (bw_meq(data->max_phyclk, bw_int_to_fixed(270))) {
+				data->output_bppdp4_lane_hbr[k] = bw_fixed_to_int(bw_mul(bw_div(bw_mul(bw_int_to_fixed(270), bw_int_to_fixed(4)), data->pixel_rate[k]), bw_int_to_fixed(8)));
+			}
+			if (bw_meq(data->max_phyclk, bw_int_to_fixed(540))) {
+				data->output_bppdp4_lane_hbr2[k] = bw_fixed_to_int(bw_mul(bw_div(bw_mul(bw_int_to_fixed(540), bw_int_to_fixed(4)), data->pixel_rate[k]), bw_int_to_fixed(8)));
+			}
+			if (bw_meq(data->max_phyclk, bw_int_to_fixed(810))) {
+				data->output_bppdp4_lane_hbr3[k] = bw_fixed_to_int(bw_mul(bw_div(bw_mul(bw_int_to_fixed(810), bw_int_to_fixed(4)), data->pixel_rate[k]), bw_int_to_fixed(8)));
+			}
+		}
+	}
+}
+
+/*******************************************************************************
+ * Public functions
+ ******************************************************************************/
+void bw_calcs_init(struct bw_calcs_dceip *bw_dceip,
+	struct bw_calcs_vbios *bw_vbios,
+	enum bw_calcs_version version)
+{
+	struct bw_calcs_dceip dceip = { 0 };
+	struct bw_calcs_vbios vbios = { 0 };
+
+	dceip.version = version;
+
+	switch (version) {
+	case BW_CALCS_VERSION_CARRIZO:
+		vbios.memory_type = bw_def_gddr5;
+		vbios.dram_channel_width_in_bits = 64;
+		vbios.number_of_dram_channels = 2;
+		vbios.number_of_dram_banks = 8;
+		vbios.high_yclk = bw_int_to_fixed(1600);
+		vbios.mid_yclk = bw_int_to_fixed(1600);
+		vbios.low_yclk = bw_frc_to_fixed(66666, 100);
+		vbios.low_sclk = bw_int_to_fixed(200);
+		vbios.mid1_sclk = bw_int_to_fixed(300);
+		vbios.mid2_sclk = bw_int_to_fixed(300);
+		vbios.mid3_sclk = bw_int_to_fixed(300);
+		vbios.mid4_sclk = bw_int_to_fixed(300);
+		vbios.mid5_sclk = bw_int_to_fixed(300);
+		vbios.mid6_sclk = bw_int_to_fixed(300);
+		vbios.high_sclk = bw_frc_to_fixed(62609, 100);
+		vbios.low_voltage_max_dispclk = bw_int_to_fixed(352);
+		vbios.mid_voltage_max_dispclk = bw_int_to_fixed(467);
+		vbios.high_voltage_max_dispclk = bw_int_to_fixed(643);
+		vbios.low_voltage_max_phyclk = bw_int_to_fixed(540);
+		vbios.mid_voltage_max_phyclk = bw_int_to_fixed(810);
+		vbios.high_voltage_max_phyclk = bw_int_to_fixed(810);
+		vbios.data_return_bus_width = bw_int_to_fixed(32);
+		vbios.trc = bw_int_to_fixed(50);
+		vbios.dmifmc_urgent_latency = bw_int_to_fixed(4);
+		vbios.stutter_self_refresh_exit_latency = bw_frc_to_fixed(153, 10);
+		vbios.stutter_self_refresh_entry_latency = bw_int_to_fixed(0);
+		vbios.nbp_state_change_latency = bw_frc_to_fixed(19649, 1000);
+		vbios.mcifwrmc_urgent_latency = bw_int_to_fixed(10);
+		vbios.scatter_gather_enable = true;
+		vbios.down_spread_percentage = bw_frc_to_fixed(5, 10);
+		vbios.cursor_width = 32;
+		vbios.average_compression_rate = 4;
+		vbios.number_of_request_slots_gmc_reserves_for_dmif_per_channel = 256;
+		vbios.blackout_duration = bw_int_to_fixed(18); /* us */
+		vbios.maximum_blackout_recovery_time = bw_int_to_fixed(20);
+
+		dceip.large_cursor = false;
+		dceip.dmif_request_buffer_size = bw_int_to_fixed(768);
+		dceip.dmif_pipe_en_fbc_chunk_tracker = false;
+		dceip.cursor_max_outstanding_group_num = 1;
+		dceip.lines_interleaved_into_lb = 2;
+		dceip.chunk_width = 256;
+		dceip.number_of_graphics_pipes = 3;
+		dceip.number_of_underlay_pipes = 1;
+		dceip.low_power_tiling_mode = 0;
+		dceip.display_write_back_supported = false;
+		dceip.argb_compression_support = false;
+		dceip.underlay_vscaler_efficiency6_bit_per_component =
+			bw_frc_to_fixed(35556, 10000);
+		dceip.underlay_vscaler_efficiency8_bit_per_component =
+			bw_frc_to_fixed(34286, 10000);
+		dceip.underlay_vscaler_efficiency10_bit_per_component =
+			bw_frc_to_fixed(32, 10);
+		dceip.underlay_vscaler_efficiency12_bit_per_component =
+			bw_int_to_fixed(3);
+		dceip.graphics_vscaler_efficiency6_bit_per_component =
+			bw_frc_to_fixed(35, 10);
+		dceip.graphics_vscaler_efficiency8_bit_per_component =
+			bw_frc_to_fixed(34286, 10000);
+		dceip.graphics_vscaler_efficiency10_bit_per_component =
+			bw_frc_to_fixed(32, 10);
+		dceip.graphics_vscaler_efficiency12_bit_per_component =
+			bw_int_to_fixed(3);
+		dceip.alpha_vscaler_efficiency = bw_int_to_fixed(3);
+		dceip.max_dmif_buffer_allocated = 2;
+		dceip.graphics_dmif_size = 12288;
+		dceip.underlay_luma_dmif_size = 19456;
+		dceip.underlay_chroma_dmif_size = 23552;
+		dceip.pre_downscaler_enabled = true;
+		dceip.underlay_downscale_prefetch_enabled = true;
+		dceip.lb_write_pixels_per_dispclk = bw_int_to_fixed(1);
+		dceip.lb_size_per_component444 = bw_int_to_fixed(82176);
+		dceip.graphics_lb_nodownscaling_multi_line_prefetching = false;
+		dceip.stutter_and_dram_clock_state_change_gated_before_cursor =
+			bw_int_to_fixed(0);
+		dceip.underlay420_luma_lb_size_per_component = bw_int_to_fixed(
+			82176);
+		dceip.underlay420_chroma_lb_size_per_component =
+			bw_int_to_fixed(164352);
+		dceip.underlay422_lb_size_per_component = bw_int_to_fixed(
+			82176);
+		dceip.cursor_chunk_width = bw_int_to_fixed(64);
+		dceip.cursor_dcp_buffer_lines = bw_int_to_fixed(4);
+		dceip.underlay_maximum_width_efficient_for_tiling =
+			bw_int_to_fixed(1920);
+		dceip.underlay_maximum_height_efficient_for_tiling =
+			bw_int_to_fixed(1080);
+		dceip.peak_pte_request_to_eviction_ratio_limiting_multiple_displays_or_single_rotated_display =
+			bw_frc_to_fixed(3, 10);
+		dceip.peak_pte_request_to_eviction_ratio_limiting_single_display_no_rotation =
+			bw_int_to_fixed(25);
+		dceip.minimum_outstanding_pte_request_limit = bw_int_to_fixed(
+			2);
+		dceip.maximum_total_outstanding_pte_requests_allowed_by_saw =
+			bw_int_to_fixed(128);
+		dceip.limit_excessive_outstanding_dmif_requests = true;
+		dceip.linear_mode_line_request_alternation_slice =
+			bw_int_to_fixed(64);
+		dceip.scatter_gather_lines_of_pte_prefetching_in_linear_mode =
+			32;
+		dceip.display_write_back420_luma_mcifwr_buffer_size = 12288;
+		dceip.display_write_back420_chroma_mcifwr_buffer_size = 8192;
+		dceip.request_efficiency = bw_frc_to_fixed(8, 10);
+		dceip.dispclk_per_request = bw_int_to_fixed(2);
+		dceip.dispclk_ramping_factor = bw_frc_to_fixed(105, 100);
+		dceip.display_pipe_throughput_factor = bw_frc_to_fixed(105, 100);
+		dceip.scatter_gather_pte_request_rows_in_tiling_mode = 2;
+		dceip.mcifwr_all_surfaces_burst_time = bw_int_to_fixed(0); /* todo: this is a bug*/
+		break;
+	case BW_CALCS_VERSION_POLARIS10:
+		vbios.memory_type = bw_def_gddr5;
+		vbios.dram_channel_width_in_bits = 32;
+		vbios.number_of_dram_channels = 8;
+		vbios.number_of_dram_banks = 8;
+		vbios.high_yclk = bw_int_to_fixed(6000);
+		vbios.mid_yclk = bw_int_to_fixed(3200);
+		vbios.low_yclk = bw_int_to_fixed(1000);
+		vbios.low_sclk = bw_int_to_fixed(300);
+		vbios.mid1_sclk = bw_int_to_fixed(400);
+		vbios.mid2_sclk = bw_int_to_fixed(500);
+		vbios.mid3_sclk = bw_int_to_fixed(600);
+		vbios.mid4_sclk = bw_int_to_fixed(700);
+		vbios.mid5_sclk = bw_int_to_fixed(800);
+		vbios.mid6_sclk = bw_int_to_fixed(974);
+		vbios.high_sclk = bw_int_to_fixed(1154);
+		vbios.low_voltage_max_dispclk = bw_int_to_fixed(459);
+		vbios.mid_voltage_max_dispclk = bw_int_to_fixed(654);
+		vbios.high_voltage_max_dispclk = bw_int_to_fixed(1108);
+		vbios.low_voltage_max_phyclk = bw_int_to_fixed(540);
+		vbios.mid_voltage_max_phyclk = bw_int_to_fixed(810);
+		vbios.high_voltage_max_phyclk = bw_int_to_fixed(810);
+		vbios.data_return_bus_width = bw_int_to_fixed(32);
+		vbios.trc = bw_int_to_fixed(48);
+		vbios.dmifmc_urgent_latency = bw_int_to_fixed(3);
+		vbios.stutter_self_refresh_exit_latency = bw_int_to_fixed(5);
+		vbios.stutter_self_refresh_entry_latency = bw_int_to_fixed(0);
+		vbios.nbp_state_change_latency = bw_int_to_fixed(45);
+		vbios.mcifwrmc_urgent_latency = bw_int_to_fixed(10);
+		vbios.scatter_gather_enable = true;
+		vbios.down_spread_percentage = bw_frc_to_fixed(5, 10);
+		vbios.cursor_width = 32;
+		vbios.average_compression_rate = 4;
+		vbios.number_of_request_slots_gmc_reserves_for_dmif_per_channel = 256;
+		vbios.blackout_duration = bw_int_to_fixed(0); /* us */
+		vbios.maximum_blackout_recovery_time = bw_int_to_fixed(0);
+
+		dceip.large_cursor = false;
+		dceip.dmif_request_buffer_size = bw_int_to_fixed(768);
+		dceip.dmif_pipe_en_fbc_chunk_tracker = false;
+		dceip.cursor_max_outstanding_group_num = 1;
+		dceip.lines_interleaved_into_lb = 2;
+		dceip.chunk_width = 256;
+		dceip.number_of_graphics_pipes = 6;
+		dceip.number_of_underlay_pipes = 0;
+		dceip.low_power_tiling_mode = 0;
+		dceip.display_write_back_supported = false;
+		dceip.argb_compression_support = true;
+		dceip.underlay_vscaler_efficiency6_bit_per_component =
+			bw_frc_to_fixed(35556, 10000);
+		dceip.underlay_vscaler_efficiency8_bit_per_component =
+			bw_frc_to_fixed(34286, 10000);
+		dceip.underlay_vscaler_efficiency10_bit_per_component =
+			bw_frc_to_fixed(32, 10);
+		dceip.underlay_vscaler_efficiency12_bit_per_component =
+			bw_int_to_fixed(3);
+		dceip.graphics_vscaler_efficiency6_bit_per_component =
+			bw_frc_to_fixed(35, 10);
+		dceip.graphics_vscaler_efficiency8_bit_per_component =
+			bw_frc_to_fixed(34286, 10000);
+		dceip.graphics_vscaler_efficiency10_bit_per_component =
+			bw_frc_to_fixed(32, 10);
+		dceip.graphics_vscaler_efficiency12_bit_per_component =
+			bw_int_to_fixed(3);
+		dceip.alpha_vscaler_efficiency = bw_int_to_fixed(3);
+		dceip.max_dmif_buffer_allocated = 4;
+		dceip.graphics_dmif_size = 12288;
+		dceip.underlay_luma_dmif_size = 19456;
+		dceip.underlay_chroma_dmif_size = 23552;
+		dceip.pre_downscaler_enabled = true;
+		dceip.underlay_downscale_prefetch_enabled = true;
+		dceip.lb_write_pixels_per_dispclk = bw_int_to_fixed(1);
+		dceip.lb_size_per_component444 = bw_int_to_fixed(245952);
+		dceip.graphics_lb_nodownscaling_multi_line_prefetching = true;
+		dceip.stutter_and_dram_clock_state_change_gated_before_cursor =
+			bw_int_to_fixed(1);
+		dceip.underlay420_luma_lb_size_per_component = bw_int_to_fixed(
+			82176);
+		dceip.underlay420_chroma_lb_size_per_component =
+			bw_int_to_fixed(164352);
+		dceip.underlay422_lb_size_per_component = bw_int_to_fixed(
+			82176);
+		dceip.cursor_chunk_width = bw_int_to_fixed(64);
+		dceip.cursor_dcp_buffer_lines = bw_int_to_fixed(4);
+		dceip.underlay_maximum_width_efficient_for_tiling =
+			bw_int_to_fixed(1920);
+		dceip.underlay_maximum_height_efficient_for_tiling =
+			bw_int_to_fixed(1080);
+		dceip.peak_pte_request_to_eviction_ratio_limiting_multiple_displays_or_single_rotated_display =
+			bw_frc_to_fixed(3, 10);
+		dceip.peak_pte_request_to_eviction_ratio_limiting_single_display_no_rotation =
+			bw_int_to_fixed(25);
+		dceip.minimum_outstanding_pte_request_limit = bw_int_to_fixed(
+			2);
+		dceip.maximum_total_outstanding_pte_requests_allowed_by_saw =
+			bw_int_to_fixed(128);
+		dceip.limit_excessive_outstanding_dmif_requests = true;
+		dceip.linear_mode_line_request_alternation_slice =
+			bw_int_to_fixed(64);
+		dceip.scatter_gather_lines_of_pte_prefetching_in_linear_mode =
+			32;
+		dceip.display_write_back420_luma_mcifwr_buffer_size = 12288;
+		dceip.display_write_back420_chroma_mcifwr_buffer_size = 8192;
+		dceip.request_efficiency = bw_frc_to_fixed(8, 10);
+		dceip.dispclk_per_request = bw_int_to_fixed(2);
+		dceip.dispclk_ramping_factor = bw_frc_to_fixed(105, 100);
+		dceip.display_pipe_throughput_factor = bw_frc_to_fixed(105, 100);
+		dceip.scatter_gather_pte_request_rows_in_tiling_mode = 2;
+		dceip.mcifwr_all_surfaces_burst_time = bw_int_to_fixed(0);
+		break;
+	case BW_CALCS_VERSION_POLARIS11:
+		vbios.memory_type = bw_def_gddr5;
+		vbios.dram_channel_width_in_bits = 32;
+		vbios.number_of_dram_channels = 4;
+		vbios.number_of_dram_banks = 8;
+		vbios.high_yclk = bw_int_to_fixed(6000);
+		vbios.mid_yclk = bw_int_to_fixed(3200);
+		vbios.low_yclk = bw_int_to_fixed(1000);
+		vbios.low_sclk = bw_int_to_fixed(300);
+		vbios.mid1_sclk = bw_int_to_fixed(400);
+		vbios.mid2_sclk = bw_int_to_fixed(500);
+		vbios.mid3_sclk = bw_int_to_fixed(600);
+		vbios.mid4_sclk = bw_int_to_fixed(700);
+		vbios.mid5_sclk = bw_int_to_fixed(800);
+		vbios.mid6_sclk = bw_int_to_fixed(974);
+		vbios.high_sclk = bw_int_to_fixed(1154);
+		vbios.low_voltage_max_dispclk = bw_int_to_fixed(459);
+		vbios.mid_voltage_max_dispclk = bw_int_to_fixed(654);
+		vbios.high_voltage_max_dispclk = bw_int_to_fixed(1108);
+		vbios.low_voltage_max_phyclk = bw_int_to_fixed(540);
+		vbios.mid_voltage_max_phyclk = bw_int_to_fixed(810);
+		vbios.high_voltage_max_phyclk = bw_int_to_fixed(810);
+		vbios.data_return_bus_width = bw_int_to_fixed(32);
+		vbios.trc = bw_int_to_fixed(48);
+		vbios.dmifmc_urgent_latency = bw_int_to_fixed(3);
+		vbios.stutter_self_refresh_exit_latency = bw_int_to_fixed(5);
+		vbios.stutter_self_refresh_entry_latency = bw_int_to_fixed(0);
+		vbios.nbp_state_change_latency = bw_int_to_fixed(45);
+		vbios.mcifwrmc_urgent_latency = bw_int_to_fixed(10);
+		vbios.scatter_gather_enable = true;
+		vbios.down_spread_percentage = bw_frc_to_fixed(5, 10);
+		vbios.cursor_width = 32;
+		vbios.average_compression_rate = 4;
+		vbios.number_of_request_slots_gmc_reserves_for_dmif_per_channel = 256;
+		vbios.blackout_duration = bw_int_to_fixed(0); /* us */
+		vbios.maximum_blackout_recovery_time = bw_int_to_fixed(0);
+
+		dceip.large_cursor = false;
+		dceip.dmif_request_buffer_size = bw_int_to_fixed(768);
+		dceip.dmif_pipe_en_fbc_chunk_tracker = false;
+		dceip.cursor_max_outstanding_group_num = 1;
+		dceip.lines_interleaved_into_lb = 2;
+		dceip.chunk_width = 256;
+		dceip.number_of_graphics_pipes = 5;
+		dceip.number_of_underlay_pipes = 0;
+		dceip.low_power_tiling_mode = 0;
+		dceip.display_write_back_supported = false;
+		dceip.argb_compression_support = true;
+		dceip.underlay_vscaler_efficiency6_bit_per_component =
+			bw_frc_to_fixed(35556, 10000);
+		dceip.underlay_vscaler_efficiency8_bit_per_component =
+			bw_frc_to_fixed(34286, 10000);
+		dceip.underlay_vscaler_efficiency10_bit_per_component =
+			bw_frc_to_fixed(32, 10);
+		dceip.underlay_vscaler_efficiency12_bit_per_component =
+			bw_int_to_fixed(3);
+		dceip.graphics_vscaler_efficiency6_bit_per_component =
+			bw_frc_to_fixed(35, 10);
+		dceip.graphics_vscaler_efficiency8_bit_per_component =
+			bw_frc_to_fixed(34286, 10000);
+		dceip.graphics_vscaler_efficiency10_bit_per_component =
+			bw_frc_to_fixed(32, 10);
+		dceip.graphics_vscaler_efficiency12_bit_per_component =
+			bw_int_to_fixed(3);
+		dceip.alpha_vscaler_efficiency = bw_int_to_fixed(3);
+		dceip.max_dmif_buffer_allocated = 4;
+		dceip.graphics_dmif_size = 12288;
+		dceip.underlay_luma_dmif_size = 19456;
+		dceip.underlay_chroma_dmif_size = 23552;
+		dceip.pre_downscaler_enabled = true;
+		dceip.underlay_downscale_prefetch_enabled = true;
+		dceip.lb_write_pixels_per_dispclk = bw_int_to_fixed(1);
+		dceip.lb_size_per_component444 = bw_int_to_fixed(245952);
+		dceip.graphics_lb_nodownscaling_multi_line_prefetching = true;
+		dceip.stutter_and_dram_clock_state_change_gated_before_cursor =
+			bw_int_to_fixed(1);
+		dceip.underlay420_luma_lb_size_per_component = bw_int_to_fixed(
+			82176);
+		dceip.underlay420_chroma_lb_size_per_component =
+			bw_int_to_fixed(164352);
+		dceip.underlay422_lb_size_per_component = bw_int_to_fixed(
+			82176);
+		dceip.cursor_chunk_width = bw_int_to_fixed(64);
+		dceip.cursor_dcp_buffer_lines = bw_int_to_fixed(4);
+		dceip.underlay_maximum_width_efficient_for_tiling =
+			bw_int_to_fixed(1920);
+		dceip.underlay_maximum_height_efficient_for_tiling =
+			bw_int_to_fixed(1080);
+		dceip.peak_pte_request_to_eviction_ratio_limiting_multiple_displays_or_single_rotated_display =
+			bw_frc_to_fixed(3, 10);
+		dceip.peak_pte_request_to_eviction_ratio_limiting_single_display_no_rotation =
+			bw_int_to_fixed(25);
+		dceip.minimum_outstanding_pte_request_limit = bw_int_to_fixed(
+			2);
+		dceip.maximum_total_outstanding_pte_requests_allowed_by_saw =
+			bw_int_to_fixed(128);
+		dceip.limit_excessive_outstanding_dmif_requests = true;
+		dceip.linear_mode_line_request_alternation_slice =
+			bw_int_to_fixed(64);
+		dceip.scatter_gather_lines_of_pte_prefetching_in_linear_mode =
+			32;
+		dceip.display_write_back420_luma_mcifwr_buffer_size = 12288;
+		dceip.display_write_back420_chroma_mcifwr_buffer_size = 8192;
+		dceip.request_efficiency = bw_frc_to_fixed(8, 10);
+		dceip.dispclk_per_request = bw_int_to_fixed(2);
+		dceip.dispclk_ramping_factor = bw_frc_to_fixed(105, 100);
+		dceip.display_pipe_throughput_factor = bw_frc_to_fixed(105, 100);
+		dceip.scatter_gather_pte_request_rows_in_tiling_mode = 2;
+		dceip.mcifwr_all_surfaces_burst_time = bw_int_to_fixed(0);
+		break;
+	case BW_CALCS_VERSION_STONEY:
+		vbios.memory_type = bw_def_gddr5;
+		vbios.dram_channel_width_in_bits = 64;
+		vbios.number_of_dram_channels = 1;
+		vbios.number_of_dram_banks = 8;
+		vbios.high_yclk = bw_int_to_fixed(1866);
+		vbios.mid_yclk = bw_int_to_fixed(1866);
+		vbios.low_yclk = bw_int_to_fixed(1333);
+		vbios.low_sclk = bw_int_to_fixed(200);
+		vbios.mid1_sclk = bw_int_to_fixed(600);
+		vbios.mid2_sclk = bw_int_to_fixed(600);
+		vbios.mid3_sclk = bw_int_to_fixed(600);
+		vbios.mid4_sclk = bw_int_to_fixed(600);
+		vbios.mid5_sclk = bw_int_to_fixed(600);
+		vbios.mid6_sclk = bw_int_to_fixed(600);
+		vbios.high_sclk = bw_int_to_fixed(800);
+		vbios.low_voltage_max_dispclk = bw_int_to_fixed(352);
+		vbios.mid_voltage_max_dispclk = bw_int_to_fixed(467);
+		vbios.high_voltage_max_dispclk = bw_int_to_fixed(643);
+		vbios.low_voltage_max_phyclk = bw_int_to_fixed(540);
+		vbios.mid_voltage_max_phyclk = bw_int_to_fixed(810);
+		vbios.high_voltage_max_phyclk = bw_int_to_fixed(810);
+		vbios.data_return_bus_width = bw_int_to_fixed(32);
+		vbios.trc = bw_int_to_fixed(50);
+		vbios.dmifmc_urgent_latency = bw_int_to_fixed(4);
+		vbios.stutter_self_refresh_exit_latency = bw_frc_to_fixed(158, 10);
+		vbios.stutter_self_refresh_entry_latency = bw_int_to_fixed(0);
+		vbios.nbp_state_change_latency = bw_frc_to_fixed(2008, 100);
+		vbios.mcifwrmc_urgent_latency = bw_int_to_fixed(10);
+		vbios.scatter_gather_enable = true;
+		vbios.down_spread_percentage = bw_frc_to_fixed(5, 10);
+		vbios.cursor_width = 32;
+		vbios.average_compression_rate = 4;
+		vbios.number_of_request_slots_gmc_reserves_for_dmif_per_channel = 256;
+		vbios.blackout_duration = bw_int_to_fixed(18); /* us */
+		vbios.maximum_blackout_recovery_time = bw_int_to_fixed(20);
+
+		dceip.large_cursor = false;
+		dceip.dmif_request_buffer_size = bw_int_to_fixed(768);
+		dceip.dmif_pipe_en_fbc_chunk_tracker = false;
+		dceip.cursor_max_outstanding_group_num = 1;
+		dceip.lines_interleaved_into_lb = 2;
+		dceip.chunk_width = 256;
+		dceip.number_of_graphics_pipes = 2;
+		dceip.number_of_underlay_pipes = 1;
+		dceip.low_power_tiling_mode = 0;
+		dceip.display_write_back_supported = false;
+		dceip.argb_compression_support = true;
+		dceip.underlay_vscaler_efficiency6_bit_per_component =
+			bw_frc_to_fixed(35556, 10000);
+		dceip.underlay_vscaler_efficiency8_bit_per_component =
+			bw_frc_to_fixed(34286, 10000);
+		dceip.underlay_vscaler_efficiency10_bit_per_component =
+			bw_frc_to_fixed(32, 10);
+		dceip.underlay_vscaler_efficiency12_bit_per_component =
+			bw_int_to_fixed(3);
+		dceip.graphics_vscaler_efficiency6_bit_per_component =
+			bw_frc_to_fixed(35, 10);
+		dceip.graphics_vscaler_efficiency8_bit_per_component =
+			bw_frc_to_fixed(34286, 10000);
+		dceip.graphics_vscaler_efficiency10_bit_per_component =
+			bw_frc_to_fixed(32, 10);
+		dceip.graphics_vscaler_efficiency12_bit_per_component =
+			bw_int_to_fixed(3);
+		dceip.alpha_vscaler_efficiency = bw_int_to_fixed(3);
+		dceip.max_dmif_buffer_allocated = 2;
+		dceip.graphics_dmif_size = 12288;
+		dceip.underlay_luma_dmif_size = 19456;
+		dceip.underlay_chroma_dmif_size = 23552;
+		dceip.pre_downscaler_enabled = true;
+		dceip.underlay_downscale_prefetch_enabled = true;
+		dceip.lb_write_pixels_per_dispclk = bw_int_to_fixed(1);
+		dceip.lb_size_per_component444 = bw_int_to_fixed(82176);
+		dceip.graphics_lb_nodownscaling_multi_line_prefetching = false;
+		dceip.stutter_and_dram_clock_state_change_gated_before_cursor =
+			bw_int_to_fixed(0);
+		dceip.underlay420_luma_lb_size_per_component = bw_int_to_fixed(
+			82176);
+		dceip.underlay420_chroma_lb_size_per_component =
+			bw_int_to_fixed(164352);
+		dceip.underlay422_lb_size_per_component = bw_int_to_fixed(
+			82176);
+		dceip.cursor_chunk_width = bw_int_to_fixed(64);
+		dceip.cursor_dcp_buffer_lines = bw_int_to_fixed(4);
+		dceip.underlay_maximum_width_efficient_for_tiling =
+			bw_int_to_fixed(1920);
+		dceip.underlay_maximum_height_efficient_for_tiling =
+			bw_int_to_fixed(1080);
+		dceip.peak_pte_request_to_eviction_ratio_limiting_multiple_displays_or_single_rotated_display =
+			bw_frc_to_fixed(3, 10);
+		dceip.peak_pte_request_to_eviction_ratio_limiting_single_display_no_rotation =
+			bw_int_to_fixed(25);
+		dceip.minimum_outstanding_pte_request_limit = bw_int_to_fixed(
+			2);
+		dceip.maximum_total_outstanding_pte_requests_allowed_by_saw =
+			bw_int_to_fixed(128);
+		dceip.limit_excessive_outstanding_dmif_requests = true;
+		dceip.linear_mode_line_request_alternation_slice =
+			bw_int_to_fixed(64);
+		dceip.scatter_gather_lines_of_pte_prefetching_in_linear_mode =
+			32;
+		dceip.display_write_back420_luma_mcifwr_buffer_size = 12288;
+		dceip.display_write_back420_chroma_mcifwr_buffer_size = 8192;
+		dceip.request_efficiency = bw_frc_to_fixed(8, 10);
+		dceip.dispclk_per_request = bw_int_to_fixed(2);
+		dceip.dispclk_ramping_factor = bw_frc_to_fixed(105, 100);
+		dceip.display_pipe_throughput_factor = bw_frc_to_fixed(105, 100);
+		dceip.scatter_gather_pte_request_rows_in_tiling_mode = 2;
+		dceip.mcifwr_all_surfaces_burst_time = bw_int_to_fixed(0);
+		break;
+	default:
+		break;
+	}
+	*bw_dceip = dceip;
+	*bw_vbios = vbios;
+
+}
+
+/**
+ * Compare calculated (required) clocks against the clocks available at
+ * maximum voltage (max Performance Level).
+ */
+static bool is_display_configuration_supported(
+	const struct bw_calcs_vbios *vbios,
+	const struct bw_calcs_output *calcs_output)
+{
+	uint32_t int_max_clk;
+
+	int_max_clk = bw_fixed_to_int(vbios->high_voltage_max_dispclk);
+	int_max_clk *= 1000; /* MHz to kHz */
+	if (calcs_output->dispclk_khz > int_max_clk)
+		return false;
+
+	int_max_clk = bw_fixed_to_int(vbios->high_sclk);
+	int_max_clk *= 1000; /* MHz to kHz */
+	if (calcs_output->required_sclk > int_max_clk)
+		return false;
+
+	return true;
+}
+
+static void populate_initial_data(
+	const struct pipe_ctx pipe[], int pipe_count, struct bw_calcs_data *data)
+{
+	int i, j;
+	int num_displays = 0;
+
+	data->underlay_surface_type = bw_def_420;
+	data->panning_and_bezel_adjustment = bw_def_none;
+	data->graphics_lb_bpc = 10;
+	data->underlay_lb_bpc = 8;
+	data->underlay_tiling_mode = bw_def_tiled;
+	data->graphics_tiling_mode = bw_def_tiled;
+	data->underlay_micro_tile_mode = bw_def_display_micro_tiling;
+	data->graphics_micro_tile_mode = bw_def_display_micro_tiling;
+
+	/* Pipes with underlay first */
+	for (i = 0; i < pipe_count; i++) {
+		if (!pipe[i].stream || !pipe[i].bottom_pipe)
+			continue;
+
+		ASSERT(pipe[i].surface);
+
+		if (num_displays == 0) {
+			if (!pipe[i].surface->public.visible)
+				data->d0_underlay_mode = bw_def_underlay_only;
+			else
+				data->d0_underlay_mode = bw_def_blend;
+		} else {
+			if (!pipe[i].surface->public.visible)
+				data->d1_underlay_mode = bw_def_underlay_only;
+			else
+				data->d1_underlay_mode = bw_def_blend;
+		}
+
+		data->fbc_en[num_displays + 4] = false;
+		data->lpt_en[num_displays + 4] = false;
+		data->h_total[num_displays + 4] = bw_int_to_fixed(pipe[i].stream->public.timing.h_total);
+		data->v_total[num_displays + 4] = bw_int_to_fixed(pipe[i].stream->public.timing.v_total);
+		data->pixel_rate[num_displays + 4] = bw_frc_to_fixed(pipe[i].stream->public.timing.pix_clk_khz, 1000);
+		data->src_width[num_displays + 4] = bw_int_to_fixed(pipe[i].scl_data.viewport.width);
+		data->pitch_in_pixels[num_displays + 4] = bw_int_to_fixed(pipe[i].surface->public.plane_size.grph.surface_pitch);
+		data->src_height[num_displays + 4] = bw_int_to_fixed(pipe[i].scl_data.viewport.height);
+		data->h_taps[num_displays + 4] = bw_int_to_fixed(pipe[i].scl_data.taps.h_taps);
+		data->v_taps[num_displays + 4] = bw_int_to_fixed(pipe[i].scl_data.taps.v_taps);
+		data->h_scale_ratio[num_displays + 4] = fixed31_32_to_bw_fixed(pipe[i].scl_data.ratios.horz.value);
+		data->v_scale_ratio[num_displays + 4] = fixed31_32_to_bw_fixed(pipe[i].scl_data.ratios.vert.value);
+		switch (pipe[i].surface->public.rotation) {
+		case ROTATION_ANGLE_0:
+			data->rotation_angle[num_displays + 4] = bw_int_to_fixed(0);
+			break;
+		case ROTATION_ANGLE_90:
+			data->rotation_angle[num_displays + 4] = bw_int_to_fixed(90);
+			break;
+		case ROTATION_ANGLE_180:
+			data->rotation_angle[num_displays + 4] = bw_int_to_fixed(180);
+			break;
+		case ROTATION_ANGLE_270:
+			data->rotation_angle[num_displays + 4] = bw_int_to_fixed(270);
+			break;
+		default:
+			break;
+		}
+		switch (pipe[i].surface->public.format) {
+		case SURFACE_PIXEL_FORMAT_VIDEO_420_YCbCr:
+		case SURFACE_PIXEL_FORMAT_GRPH_ARGB1555:
+		case SURFACE_PIXEL_FORMAT_GRPH_RGB565:
+			data->bytes_per_pixel[num_displays + 4] = 2;
+			break;
+		case SURFACE_PIXEL_FORMAT_GRPH_ARGB8888:
+		case SURFACE_PIXEL_FORMAT_GRPH_BGRA8888:
+		case SURFACE_PIXEL_FORMAT_GRPH_ARGB2101010:
+		case SURFACE_PIXEL_FORMAT_GRPH_ABGR2101010:
+		case SURFACE_PIXEL_FORMAT_GRPH_ABGR2101010_XR_BIAS:
+			data->bytes_per_pixel[num_displays + 4] = 4;
+			break;
+		case SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616:
+		case SURFACE_PIXEL_FORMAT_GRPH_ABGR16161616F:
+			data->bytes_per_pixel[num_displays + 4] = 8;
+			break;
+		default:
+			data->bytes_per_pixel[num_displays + 4] = 4;
+			break;
+		}
+		data->interlace_mode[num_displays + 4] = false;
+		data->stereo_mode[num_displays + 4] = bw_def_mono;
+
+
+		for (j = 0; j < 2; j++) {
+			data->fbc_en[num_displays * 2 + j] = false;
+			data->lpt_en[num_displays * 2 + j] = false;
+
+			data->src_height[num_displays * 2 + j] = bw_int_to_fixed(pipe[i].bottom_pipe->scl_data.viewport.height);
+			data->src_width[num_displays * 2 + j] = bw_int_to_fixed(pipe[i].bottom_pipe->scl_data.viewport.width);
+			data->pitch_in_pixels[num_displays * 2 + j] = bw_int_to_fixed(
+					pipe[i].bottom_pipe->surface->public.plane_size.grph.surface_pitch);
+			data->h_taps[num_displays * 2 + j] = bw_int_to_fixed(pipe[i].bottom_pipe->scl_data.taps.h_taps);
+			data->v_taps[num_displays * 2 + j] = bw_int_to_fixed(pipe[i].bottom_pipe->scl_data.taps.v_taps);
+			data->h_scale_ratio[num_displays * 2 + j] = fixed31_32_to_bw_fixed(
+					pipe[i].bottom_pipe->scl_data.ratios.horz.value);
+			data->v_scale_ratio[num_displays * 2 + j] = fixed31_32_to_bw_fixed(
+					pipe[i].bottom_pipe->scl_data.ratios.vert.value);
+			switch (pipe[i].bottom_pipe->surface->public.rotation) {
+			case ROTATION_ANGLE_0:
+				data->rotation_angle[num_displays * 2 + j] = bw_int_to_fixed(0);
+				break;
+			case ROTATION_ANGLE_90:
+				data->rotation_angle[num_displays * 2 + j] = bw_int_to_fixed(90);
+				break;
+			case ROTATION_ANGLE_180:
+				data->rotation_angle[num_displays * 2 + j] = bw_int_to_fixed(180);
+				break;
+			case ROTATION_ANGLE_270:
+				data->rotation_angle[num_displays * 2 + j] = bw_int_to_fixed(270);
+				break;
+			default:
+				break;
+			}
+			data->stereo_mode[num_displays * 2 + j] = bw_def_mono;
+		}
+
+		num_displays++;
+	}
+
+	/* Pipes without underlay after */
+	for (i = 0; i < pipe_count; i++) {
+		if (!pipe[i].stream || pipe[i].bottom_pipe)
+			continue;
+
+
+		data->fbc_en[num_displays + 4] = false;
+		data->lpt_en[num_displays + 4] = false;
+		data->h_total[num_displays + 4] = bw_int_to_fixed(pipe[i].stream->public.timing.h_total);
+		data->v_total[num_displays + 4] = bw_int_to_fixed(pipe[i].stream->public.timing.v_total);
+		data->pixel_rate[num_displays + 4] = bw_frc_to_fixed(pipe[i].stream->public.timing.pix_clk_khz, 1000);
+		if (pipe[i].surface) {
+			data->src_width[num_displays + 4] = bw_int_to_fixed(pipe[i].scl_data.viewport.width);
+			data->pitch_in_pixels[num_displays + 4] = bw_int_to_fixed(pipe[i].surface->public.plane_size.grph.surface_pitch);
+			data->src_height[num_displays + 4] = bw_int_to_fixed(pipe[i].scl_data.viewport.height);
+			data->h_taps[num_displays + 4] = bw_int_to_fixed(pipe[i].scl_data.taps.h_taps);
+			data->v_taps[num_displays + 4] = bw_int_to_fixed(pipe[i].scl_data.taps.v_taps);
+			data->h_scale_ratio[num_displays + 4] = fixed31_32_to_bw_fixed(pipe[i].scl_data.ratios.horz.value);
+			data->v_scale_ratio[num_displays + 4] = fixed31_32_to_bw_fixed(pipe[i].scl_data.ratios.vert.value);
+			switch (pipe[i].surface->public.rotation) {
+			case ROTATION_ANGLE_0:
+				data->rotation_angle[num_displays + 4] = bw_int_to_fixed(0);
+				break;
+			case ROTATION_ANGLE_90:
+				data->rotation_angle[num_displays + 4] = bw_int_to_fixed(90);
+				break;
+			case ROTATION_ANGLE_180:
+				data->rotation_angle[num_displays + 4] = bw_int_to_fixed(180);
+				break;
+			case ROTATION_ANGLE_270:
+				data->rotation_angle[num_displays + 4] = bw_int_to_fixed(270);
+				break;
+			default:
+				break;
+			}
+			switch (pipe[i].surface->public.format) {
+			case SURFACE_PIXEL_FORMAT_VIDEO_420_YCbCr:
+			case SURFACE_PIXEL_FORMAT_GRPH_ARGB1555:
+			case SURFACE_PIXEL_FORMAT_GRPH_RGB565:
+				data->bytes_per_pixel[num_displays + 4] = 2;
+				break;
+			case SURFACE_PIXEL_FORMAT_GRPH_ARGB8888:
+			case SURFACE_PIXEL_FORMAT_GRPH_BGRA8888:
+			case SURFACE_PIXEL_FORMAT_GRPH_ARGB2101010:
+			case SURFACE_PIXEL_FORMAT_GRPH_ABGR2101010:
+			case SURFACE_PIXEL_FORMAT_GRPH_ABGR2101010_XR_BIAS:
+				data->bytes_per_pixel[num_displays + 4] = 4;
+				break;
+			case SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616:
+			case SURFACE_PIXEL_FORMAT_GRPH_ABGR16161616F:
+				data->bytes_per_pixel[num_displays + 4] = 8;
+				break;
+			default:
+				data->bytes_per_pixel[num_displays + 4] = 4;
+				break;
+			}
+		} else {
+			data->src_width[num_displays + 4] = bw_int_to_fixed(pipe[i].stream->public.timing.h_addressable);
+			data->pitch_in_pixels[num_displays + 4] = data->src_width[num_displays + 4];
+			data->src_height[num_displays + 4] = bw_int_to_fixed(pipe[i].stream->public.timing.v_addressable);
+			data->h_taps[num_displays + 4] = bw_int_to_fixed(1);
+			data->v_taps[num_displays + 4] = bw_int_to_fixed(1);
+			data->h_scale_ratio[num_displays + 4] = bw_int_to_fixed(1);
+			data->v_scale_ratio[num_displays + 4] = bw_int_to_fixed(1);
+			data->rotation_angle[num_displays + 4] = bw_int_to_fixed(0);
+			data->bytes_per_pixel[num_displays + 4] = 4;
+		}
+
+		data->interlace_mode[num_displays + 4] = false;
+		data->stereo_mode[num_displays + 4] = bw_def_mono;
+		num_displays++;
+	}
+
+	data->number_of_displays = num_displays;
+}
+
+/**
+ * Return:
+ *	true -	Display(s) configuration supported.
+ *		In this case 'calcs_output' contains data for HW programming
+ *	false - Display(s) configuration not supported (not enough bandwidth).
+ */
+
+bool bw_calcs(struct dc_context *ctx,
+	const struct bw_calcs_dceip *dceip,
+	const struct bw_calcs_vbios *vbios,
+	const struct pipe_ctx pipe[],
+	int pipe_count,
+	struct bw_calcs_output *calcs_output)
+{
+	struct bw_calcs_data *data = dm_alloc(sizeof(struct bw_calcs_data));
+
+	populate_initial_data(pipe, pipe_count, data);
+
+	/*TODO: this should be taken out calcs output and assigned during timing sync for pplib use*/
+	calcs_output->all_displays_in_sync = false;
+
+	if (data->number_of_displays != 0) {
+		uint8_t yclk_lvl, sclk_lvl;
+		struct bw_fixed high_sclk = vbios->high_sclk;
+		struct bw_fixed mid1_sclk = vbios->mid1_sclk;
+		struct bw_fixed mid2_sclk = vbios->mid2_sclk;
+		struct bw_fixed mid3_sclk = vbios->mid3_sclk;
+		struct bw_fixed mid4_sclk = vbios->mid4_sclk;
+		struct bw_fixed mid5_sclk = vbios->mid5_sclk;
+		struct bw_fixed mid6_sclk = vbios->mid6_sclk;
+		struct bw_fixed low_sclk = vbios->low_sclk;
+		struct bw_fixed high_yclk = vbios->high_yclk;
+		struct bw_fixed mid_yclk = vbios->mid_yclk;
+		struct bw_fixed low_yclk = vbios->low_yclk;
+
+		calculate_bandwidth(dceip, vbios, data);
+
+		yclk_lvl = data->y_clk_level;
+		sclk_lvl = data->sclk_level;
+
+		calcs_output->nbp_state_change_enable =
+			data->nbp_state_change_enable;
+		calcs_output->cpuc_state_change_enable =
+				data->cpuc_state_change_enable;
+		calcs_output->cpup_state_change_enable =
+				data->cpup_state_change_enable;
+		calcs_output->stutter_mode_enable =
+				data->stutter_mode_enable;
+		calcs_output->dispclk_khz =
+			bw_fixed_to_int(bw_mul(data->dispclk,
+					bw_int_to_fixed(1000)));
+		calcs_output->blackout_recovery_time_us =
+			bw_fixed_to_int(data->blackout_recovery_time);
+		calcs_output->required_sclk =
+			bw_fixed_to_int(bw_mul(data->required_sclk,
+					bw_int_to_fixed(1000)));
+		calcs_output->required_sclk_deep_sleep =
+			bw_fixed_to_int(bw_mul(data->sclk_deep_sleep,
+					bw_int_to_fixed(1000)));
+		if (yclk_lvl == 0)
+			calcs_output->required_yclk = bw_fixed_to_int(
+				bw_mul(low_yclk, bw_int_to_fixed(1000)));
+		else if (yclk_lvl == 1)
+			calcs_output->required_yclk = bw_fixed_to_int(
+				bw_mul(mid_yclk, bw_int_to_fixed(1000)));
+		else
+			calcs_output->required_yclk = bw_fixed_to_int(
+				bw_mul(high_yclk, bw_int_to_fixed(1000)));
+
+		/* units: nanosecond, 16bit storage. */
+
+		calcs_output->nbp_state_change_wm_ns[0].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				nbp_state_change_watermark[4], bw_int_to_fixed(1000)));
+		calcs_output->nbp_state_change_wm_ns[1].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				nbp_state_change_watermark[5], bw_int_to_fixed(1000)));
+		calcs_output->nbp_state_change_wm_ns[2].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				nbp_state_change_watermark[6], bw_int_to_fixed(1000)));
+
+		if (ctx->dc->caps.max_slave_planes) {
+			calcs_output->nbp_state_change_wm_ns[3].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[0], bw_int_to_fixed(1000)));
+			calcs_output->nbp_state_change_wm_ns[4].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+							nbp_state_change_watermark[1], bw_int_to_fixed(1000)));
+		} else {
+			calcs_output->nbp_state_change_wm_ns[3].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[7], bw_int_to_fixed(1000)));
+			calcs_output->nbp_state_change_wm_ns[4].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[8], bw_int_to_fixed(1000)));
+		}
+		calcs_output->nbp_state_change_wm_ns[5].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				nbp_state_change_watermark[9], bw_int_to_fixed(1000)));
+
+
+
+		calcs_output->stutter_exit_wm_ns[0].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				stutter_exit_watermark[4], bw_int_to_fixed(1000)));
+		calcs_output->stutter_exit_wm_ns[1].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				stutter_exit_watermark[5], bw_int_to_fixed(1000)));
+		calcs_output->stutter_exit_wm_ns[2].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				stutter_exit_watermark[6], bw_int_to_fixed(1000)));
+		if (ctx->dc->caps.max_slave_planes) {
+			calcs_output->stutter_exit_wm_ns[3].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[0], bw_int_to_fixed(1000)));
+			calcs_output->stutter_exit_wm_ns[4].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[1], bw_int_to_fixed(1000)));
+		} else {
+			calcs_output->stutter_exit_wm_ns[3].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[7], bw_int_to_fixed(1000)));
+			calcs_output->stutter_exit_wm_ns[4].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[8], bw_int_to_fixed(1000)));
+		}
+		calcs_output->stutter_exit_wm_ns[5].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				stutter_exit_watermark[9], bw_int_to_fixed(1000)));
+
+
+
+		calcs_output->urgent_wm_ns[0].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				urgent_watermark[4], bw_int_to_fixed(1000)));
+		calcs_output->urgent_wm_ns[1].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				urgent_watermark[5], bw_int_to_fixed(1000)));
+		calcs_output->urgent_wm_ns[2].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				urgent_watermark[6], bw_int_to_fixed(1000)));
+		if (ctx->dc->caps.max_slave_planes) {
+			calcs_output->urgent_wm_ns[3].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[0], bw_int_to_fixed(1000)));
+			calcs_output->urgent_wm_ns[4].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[1], bw_int_to_fixed(1000)));
+		} else {
+			calcs_output->urgent_wm_ns[3].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[7], bw_int_to_fixed(1000)));
+			calcs_output->urgent_wm_ns[4].a_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[8], bw_int_to_fixed(1000)));
+		}
+		calcs_output->urgent_wm_ns[5].a_mark =
+			bw_fixed_to_int(bw_mul(data->
+				urgent_watermark[9], bw_int_to_fixed(1000)));
+
+		if (dceip->version != BW_CALCS_VERSION_CARRIZO) {
+			((struct bw_calcs_vbios *)vbios)->low_sclk = mid3_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid1_sclk = mid3_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid2_sclk = mid3_sclk;
+			calculate_bandwidth(dceip, vbios, data);
+
+			calcs_output->nbp_state_change_wm_ns[0].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[4],bw_int_to_fixed(1000)));
+			calcs_output->nbp_state_change_wm_ns[1].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[5], bw_int_to_fixed(1000)));
+			calcs_output->nbp_state_change_wm_ns[2].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[6], bw_int_to_fixed(1000)));
+
+			if (ctx->dc->caps.max_slave_planes) {
+				calcs_output->nbp_state_change_wm_ns[3].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						nbp_state_change_watermark[0], bw_int_to_fixed(1000)));
+				calcs_output->nbp_state_change_wm_ns[4].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						nbp_state_change_watermark[1], bw_int_to_fixed(1000)));
+			} else {
+				calcs_output->nbp_state_change_wm_ns[3].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						nbp_state_change_watermark[7], bw_int_to_fixed(1000)));
+				calcs_output->nbp_state_change_wm_ns[4].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						nbp_state_change_watermark[8], bw_int_to_fixed(1000)));
+			}
+			calcs_output->nbp_state_change_wm_ns[5].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[9], bw_int_to_fixed(1000)));
+
+
+
+			calcs_output->stutter_exit_wm_ns[0].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[4], bw_int_to_fixed(1000)));
+			calcs_output->stutter_exit_wm_ns[1].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[5], bw_int_to_fixed(1000)));
+			calcs_output->stutter_exit_wm_ns[2].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[6], bw_int_to_fixed(1000)));
+			if (ctx->dc->caps.max_slave_planes) {
+				calcs_output->stutter_exit_wm_ns[3].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						stutter_exit_watermark[0], bw_int_to_fixed(1000)));
+				calcs_output->stutter_exit_wm_ns[4].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						stutter_exit_watermark[1], bw_int_to_fixed(1000)));
+			} else {
+				calcs_output->stutter_exit_wm_ns[3].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						stutter_exit_watermark[7], bw_int_to_fixed(1000)));
+				calcs_output->stutter_exit_wm_ns[4].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						stutter_exit_watermark[8], bw_int_to_fixed(1000)));
+			}
+			calcs_output->stutter_exit_wm_ns[5].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[9], bw_int_to_fixed(1000)));
+
+
+
+			calcs_output->urgent_wm_ns[0].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[4], bw_int_to_fixed(1000)));
+			calcs_output->urgent_wm_ns[1].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[5], bw_int_to_fixed(1000)));
+			calcs_output->urgent_wm_ns[2].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[6], bw_int_to_fixed(1000)));
+			if (ctx->dc->caps.max_slave_planes) {
+				calcs_output->urgent_wm_ns[3].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						urgent_watermark[0], bw_int_to_fixed(1000)));
+				calcs_output->urgent_wm_ns[4].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						urgent_watermark[1], bw_int_to_fixed(1000)));
+			} else {
+				calcs_output->urgent_wm_ns[3].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						urgent_watermark[7], bw_int_to_fixed(1000)));
+				calcs_output->urgent_wm_ns[4].b_mark =
+					bw_fixed_to_int(bw_mul(data->
+						urgent_watermark[8], bw_int_to_fixed(1000)));
+			}
+			calcs_output->urgent_wm_ns[5].b_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[9], bw_int_to_fixed(1000)));
+
+			((struct bw_calcs_vbios *)vbios)->low_sclk = low_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid1_sclk = mid1_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid2_sclk = mid2_sclk;
+			((struct bw_calcs_vbios *)vbios)->low_yclk = mid_yclk;
+			calculate_bandwidth(dceip, vbios, data);
+
+			calcs_output->nbp_state_change_wm_ns[0].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[4], bw_int_to_fixed(1000)));
+			calcs_output->nbp_state_change_wm_ns[1].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[5], bw_int_to_fixed(1000)));
+			calcs_output->nbp_state_change_wm_ns[2].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[6], bw_int_to_fixed(1000)));
+			if (ctx->dc->caps.max_slave_planes) {
+				calcs_output->nbp_state_change_wm_ns[3].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						nbp_state_change_watermark[0], bw_int_to_fixed(1000)));
+				calcs_output->nbp_state_change_wm_ns[4].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						nbp_state_change_watermark[1], bw_int_to_fixed(1000)));
+			} else {
+				calcs_output->nbp_state_change_wm_ns[3].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						nbp_state_change_watermark[7], bw_int_to_fixed(1000)));
+				calcs_output->nbp_state_change_wm_ns[4].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						nbp_state_change_watermark[8], bw_int_to_fixed(1000)));
+			}
+			calcs_output->nbp_state_change_wm_ns[5].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[9], bw_int_to_fixed(1000)));
+
+
+			calcs_output->stutter_exit_wm_ns[0].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[4], bw_int_to_fixed(1000)));
+			calcs_output->stutter_exit_wm_ns[1].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[5], bw_int_to_fixed(1000)));
+			calcs_output->stutter_exit_wm_ns[2].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[6], bw_int_to_fixed(1000)));
+			if (ctx->dc->caps.max_slave_planes) {
+				calcs_output->stutter_exit_wm_ns[3].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						stutter_exit_watermark[0], bw_int_to_fixed(1000)));
+				calcs_output->stutter_exit_wm_ns[4].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						stutter_exit_watermark[1], bw_int_to_fixed(1000)));
+			} else {
+				calcs_output->stutter_exit_wm_ns[3].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						stutter_exit_watermark[7], bw_int_to_fixed(1000)));
+				calcs_output->stutter_exit_wm_ns[4].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						stutter_exit_watermark[8], bw_int_to_fixed(1000)));
+			}
+			calcs_output->stutter_exit_wm_ns[5].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[9], bw_int_to_fixed(1000)));
+
+			calcs_output->urgent_wm_ns[0].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[4], bw_int_to_fixed(1000)));
+			calcs_output->urgent_wm_ns[1].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[5], bw_int_to_fixed(1000)));
+			calcs_output->urgent_wm_ns[2].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[6], bw_int_to_fixed(1000)));
+			if (ctx->dc->caps.max_slave_planes) {
+				calcs_output->urgent_wm_ns[3].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						urgent_watermark[0], bw_int_to_fixed(1000)));
+				calcs_output->urgent_wm_ns[4].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						urgent_watermark[1], bw_int_to_fixed(1000)));
+			} else {
+				calcs_output->urgent_wm_ns[3].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						urgent_watermark[7], bw_int_to_fixed(1000)));
+				calcs_output->urgent_wm_ns[4].c_mark =
+					bw_fixed_to_int(bw_mul(data->
+						urgent_watermark[8], bw_int_to_fixed(1000)));
+			}
+			calcs_output->urgent_wm_ns[5].c_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[9], bw_int_to_fixed(1000)));
+		}
+
+		if (dceip->version == BW_CALCS_VERSION_CARRIZO) {
+			((struct bw_calcs_vbios *)vbios)->low_yclk = high_yclk;
+			((struct bw_calcs_vbios *)vbios)->mid_yclk = high_yclk;
+			((struct bw_calcs_vbios *)vbios)->low_sclk = high_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid1_sclk = high_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid2_sclk = high_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid3_sclk = high_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid4_sclk = high_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid5_sclk = high_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid6_sclk = high_sclk;
+		} else {
+			((struct bw_calcs_vbios *)vbios)->low_yclk = mid_yclk;
+			((struct bw_calcs_vbios *)vbios)->low_sclk = mid3_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid1_sclk = mid3_sclk;
+			((struct bw_calcs_vbios *)vbios)->mid2_sclk = mid3_sclk;
+		}
+
+		calculate_bandwidth(dceip, vbios, data);
+
+		calcs_output->nbp_state_change_wm_ns[0].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				nbp_state_change_watermark[4], bw_int_to_fixed(1000)));
+		calcs_output->nbp_state_change_wm_ns[1].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				nbp_state_change_watermark[5], bw_int_to_fixed(1000)));
+		calcs_output->nbp_state_change_wm_ns[2].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				nbp_state_change_watermark[6], bw_int_to_fixed(1000)));
+		if (ctx->dc->caps.max_slave_planes) {
+			calcs_output->nbp_state_change_wm_ns[3].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[0], bw_int_to_fixed(1000)));
+			calcs_output->nbp_state_change_wm_ns[4].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[1], bw_int_to_fixed(1000)));
+		} else {
+			calcs_output->nbp_state_change_wm_ns[3].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[7], bw_int_to_fixed(1000)));
+			calcs_output->nbp_state_change_wm_ns[4].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					nbp_state_change_watermark[8], bw_int_to_fixed(1000)));
+		}
+		calcs_output->nbp_state_change_wm_ns[5].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				nbp_state_change_watermark[9], bw_int_to_fixed(1000)));
+
+		calcs_output->stutter_exit_wm_ns[0].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				stutter_exit_watermark[4], bw_int_to_fixed(1000)));
+		calcs_output->stutter_exit_wm_ns[1].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				stutter_exit_watermark[5], bw_int_to_fixed(1000)));
+		calcs_output->stutter_exit_wm_ns[2].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				stutter_exit_watermark[6], bw_int_to_fixed(1000)));
+		if (ctx->dc->caps.max_slave_planes) {
+			calcs_output->stutter_exit_wm_ns[3].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[0], bw_int_to_fixed(1000)));
+			calcs_output->stutter_exit_wm_ns[4].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[1], bw_int_to_fixed(1000)));
+		} else {
+			calcs_output->stutter_exit_wm_ns[3].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[7], bw_int_to_fixed(1000)));
+			calcs_output->stutter_exit_wm_ns[4].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					stutter_exit_watermark[8], bw_int_to_fixed(1000)));
+		}
+		calcs_output->stutter_exit_wm_ns[5].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				stutter_exit_watermark[9], bw_int_to_fixed(1000)));
+
+
+		calcs_output->urgent_wm_ns[0].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				urgent_watermark[4], bw_int_to_fixed(1000)));
+		calcs_output->urgent_wm_ns[1].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				urgent_watermark[5], bw_int_to_fixed(1000)));
+		calcs_output->urgent_wm_ns[2].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				urgent_watermark[6], bw_int_to_fixed(1000)));
+		if (ctx->dc->caps.max_slave_planes) {
+			calcs_output->urgent_wm_ns[3].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[0], bw_int_to_fixed(1000)));
+			calcs_output->urgent_wm_ns[4].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[1], bw_int_to_fixed(1000)));
+		} else {
+			calcs_output->urgent_wm_ns[3].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[7], bw_int_to_fixed(1000)));
+			calcs_output->urgent_wm_ns[4].d_mark =
+				bw_fixed_to_int(bw_mul(data->
+					urgent_watermark[8], bw_int_to_fixed(1000)));
+		}
+		calcs_output->urgent_wm_ns[5].d_mark =
+			bw_fixed_to_int(bw_mul(data->
+				urgent_watermark[9], bw_int_to_fixed(1000)));
+
+		((struct bw_calcs_vbios *)vbios)->low_yclk = low_yclk;
+		((struct bw_calcs_vbios *)vbios)->mid_yclk = mid_yclk;
+		((struct bw_calcs_vbios *)vbios)->low_sclk = low_sclk;
+		((struct bw_calcs_vbios *)vbios)->mid1_sclk = mid1_sclk;
+		((struct bw_calcs_vbios *)vbios)->mid2_sclk = mid2_sclk;
+		((struct bw_calcs_vbios *)vbios)->mid3_sclk = mid3_sclk;
+		((struct bw_calcs_vbios *)vbios)->mid4_sclk = mid4_sclk;
+		((struct bw_calcs_vbios *)vbios)->mid5_sclk = mid5_sclk;
+		((struct bw_calcs_vbios *)vbios)->mid6_sclk = mid6_sclk;
+		((struct bw_calcs_vbios *)vbios)->high_sclk = high_sclk;
+	} else {
+		calcs_output->nbp_state_change_enable = true;
+		calcs_output->cpuc_state_change_enable = true;
+		calcs_output->cpup_state_change_enable = true;
+		calcs_output->stutter_mode_enable = true;
+		calcs_output->dispclk_khz = 0;
+		calcs_output->required_sclk = 0;
+	}
+
+	dm_free(data);
+
+	return is_display_configuration_supported(vbios, calcs_output);
+}
diff --git a/drivers/gpu/drm/amd/display/dc/calcs/gamma_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/gamma_calcs.c
new file mode 100644
index 0000000..854796a
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/calcs/gamma_calcs.c
@@ -0,0 +1,1382 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "gamma_calcs.h"
+
+struct curve_config {
+	uint32_t offset;
+	int8_t segments[16];
+	int8_t begin;
+};
+
+static bool build_custom_float(
+	struct fixed31_32 value,
+	const struct custom_float_format *format,
+	bool *negative,
+	uint32_t *mantissa,
+	uint32_t *exponenta)
+{
+	uint32_t exp_offset = (1 << (format->exponenta_bits - 1)) - 1;
+
+	const struct fixed31_32 mantissa_constant_plus_max_fraction =
+		dal_fixed31_32_from_fraction(
+			(1LL << (format->mantissa_bits + 1)) - 1,
+			1LL << format->mantissa_bits);
+
+	struct fixed31_32 mantiss;
+
+	if (dal_fixed31_32_eq(
+		value,
+		dal_fixed31_32_zero)) {
+		*negative = false;
+		*mantissa = 0;
+		*exponenta = 0;
+		return true;
+	}
+
+	if (dal_fixed31_32_lt(
+		value,
+		dal_fixed31_32_zero)) {
+		*negative = format->sign;
+		value = dal_fixed31_32_neg(value);
+	} else {
+		*negative = false;
+	}
+
+	if (dal_fixed31_32_lt(
+		value,
+		dal_fixed31_32_one)) {
+		uint32_t i = 1;
+
+		do {
+			value = dal_fixed31_32_shl(value, 1);
+			++i;
+		} while (dal_fixed31_32_lt(
+			value,
+			dal_fixed31_32_one));
+
+		--i;
+
+		if (exp_offset <= i) {
+			*mantissa = 0;
+			*exponenta = 0;
+			return true;
+		}
+
+		*exponenta = exp_offset - i;
+	} else if (dal_fixed31_32_le(
+		mantissa_constant_plus_max_fraction,
+		value)) {
+		uint32_t i = 1;
+
+		do {
+			value = dal_fixed31_32_shr(value, 1);
+			++i;
+		} while (dal_fixed31_32_lt(
+			mantissa_constant_plus_max_fraction,
+			value));
+
+		*exponenta = exp_offset + i - 1;
+	} else {
+		*exponenta = exp_offset;
+	}
+
+	mantiss = dal_fixed31_32_sub(
+		value,
+		dal_fixed31_32_one);
+
+	if (dal_fixed31_32_lt(
+			mantiss,
+			dal_fixed31_32_zero) ||
+		dal_fixed31_32_lt(
+			dal_fixed31_32_one,
+			mantiss))
+		mantiss = dal_fixed31_32_zero;
+	else
+		mantiss = dal_fixed31_32_shl(
+			mantiss,
+			format->mantissa_bits);
+
+	*mantissa = dal_fixed31_32_floor(mantiss);
+
+	return true;
+}
+
+static bool setup_custom_float(
+	const struct custom_float_format *format,
+	bool negative,
+	uint32_t mantissa,
+	uint32_t exponenta,
+	uint32_t *result)
+{
+	uint32_t i = 0;
+	uint32_t j = 0;
+
+	uint32_t value = 0;
+
+	/* verification code:
+	 * once calculation is ok we can remove it
+	 */
+
+	const uint32_t mantissa_mask =
+		(1 << (format->mantissa_bits + 1)) - 1;
+
+	const uint32_t exponenta_mask =
+		(1 << (format->exponenta_bits + 1)) - 1;
+
+	if (mantissa & ~mantissa_mask) {
+		BREAK_TO_DEBUGGER();
+		mantissa = mantissa_mask;
+	}
+
+	if (exponenta & ~exponenta_mask) {
+		BREAK_TO_DEBUGGER();
+		exponenta = exponenta_mask;
+	}
+
+	/* end of verification code */
+
+	while (i < format->mantissa_bits) {
+		uint32_t mask = 1 << i;
+
+		if (mantissa & mask)
+			value |= mask;
+
+		++i;
+	}
+
+	while (j < format->exponenta_bits) {
+		uint32_t mask = 1 << j;
+
+		if (exponenta & mask)
+			value |= mask << i;
+
+		++j;
+	}
+
+	if (negative && format->sign)
+		value |= 1 << (i + j);
+
+	*result = value;
+
+	return true;
+}
+
+static bool convert_to_custom_float_format_ex(
+	struct fixed31_32 value,
+	const struct custom_float_format *format,
+	struct custom_float_value *result)
+{
+	return build_custom_float(
+		value, format,
+		&result->negative, &result->mantissa, &result->exponenta) &&
+	setup_custom_float(
+		format, result->negative, result->mantissa, result->exponenta,
+		&result->value);
+}
+
+static bool round_custom_float_6_12(
+	struct hw_x_point *x)
+{
+	struct custom_float_format fmt;
+
+	struct custom_float_value value;
+
+	fmt.exponenta_bits = 6;
+	fmt.mantissa_bits = 12;
+	fmt.sign = true;
+
+	if (!convert_to_custom_float_format_ex(
+		x->x, &fmt, &value))
+		return false;
+
+	x->adjusted_x = x->x;
+
+	if (value.mantissa) {
+		BREAK_TO_DEBUGGER();
+
+		return false;
+	}
+
+	return true;
+}
+
+static bool build_hw_curve_configuration(
+	const struct curve_config *curve_config,
+	struct gamma_curve *gamma_curve,
+	struct curve_points *curve_points,
+	struct hw_x_point *points,
+	uint32_t *number_of_points)
+{
+	const int8_t max_regions_number = ARRAY_SIZE(curve_config->segments);
+
+	int8_t i;
+
+	uint8_t segments_calculation[8] = { 0 };
+
+	struct fixed31_32 region1 = dal_fixed31_32_zero;
+	struct fixed31_32 region2;
+	struct fixed31_32 increment;
+
+	uint32_t index = 0;
+	uint32_t segments = 0;
+	uint32_t max_number;
+
+	bool result = false;
+
+	if (!number_of_points) {
+		BREAK_TO_DEBUGGER();
+		return false;
+	}
+
+	max_number = *number_of_points;
+
+	i = 0;
+
+	while (i != max_regions_number) {
+		gamma_curve[i].offset = 0;
+		gamma_curve[i].segments_num = 0;
+
+		++i;
+	}
+
+	i = 0;
+
+	while (i != max_regions_number) {
+		/* number should go in uninterruptible sequence */
+		if (curve_config->segments[i] == -1)
+			break;
+
+		ASSERT(curve_config->segments[i] >= 0);
+
+		segments += (1 << curve_config->segments[i]);
+
+		++i;
+	}
+
+	if (segments > max_number) {
+		BREAK_TO_DEBUGGER();
+	} else {
+		int32_t divisor;
+		uint32_t offset = 0;
+		int8_t begin = curve_config->begin;
+		int32_t region_number = 0;
+
+		i = begin;
+
+		while ((index < max_number) &&
+			(region_number < max_regions_number) &&
+			(i <= 1)) {
+			int32_t j = 0;
+
+			segments = curve_config->segments[region_number];
+			divisor = 1 << segments;
+
+			if (segments == -1) {
+				if (i > 0) {
+					region1 = dal_fixed31_32_shl(
+						dal_fixed31_32_one,
+						i - 1);
+					region2 = dal_fixed31_32_shl(
+						dal_fixed31_32_one,
+						i);
+				} else {
+					region1 = dal_fixed31_32_shr(
+						dal_fixed31_32_one,
+						-(i - 1));
+					region2 = dal_fixed31_32_shr(
+						dal_fixed31_32_one,
+						-i);
+				}
+
+				break;
+			}
+
+			if (i > -1) {
+				region1 = dal_fixed31_32_shl(
+					dal_fixed31_32_one,
+					i);
+				region2 = dal_fixed31_32_shl(
+					dal_fixed31_32_one,
+					i + 1);
+			} else {
+				region1 = dal_fixed31_32_shr(
+					dal_fixed31_32_one,
+					-i);
+				region2 = dal_fixed31_32_shr(
+					dal_fixed31_32_one,
+					-(i + 1));
+			}
+
+			gamma_curve[region_number].offset = offset;
+			gamma_curve[region_number].segments_num = segments;
+
+			offset += divisor;
+
+			++segments_calculation[segments];
+
+			increment = dal_fixed31_32_div_int(
+				dal_fixed31_32_sub(
+					region2,
+					region1),
+				divisor);
+
+			points[index].x = region1;
+
+			round_custom_float_6_12(points + index);
+
+			++index;
+			++region_number;
+
+			while ((index < max_number) && (j < divisor - 1)) {
+				region1 = dal_fixed31_32_add(
+					region1,
+					increment);
+
+				points[index].x = region1;
+				points[index].adjusted_x = region1;
+
+				++index;
+				++j;
+			}
+
+			++i;
+		}
+
+		points[index].x = region1;
+
+		round_custom_float_6_12(points + index);
+
+		*number_of_points = index;
+
+		result = true;
+	}
+
+	curve_points[0].x = points[0].adjusted_x;
+	curve_points[0].offset = dal_fixed31_32_zero;
+
+	curve_points[1].x = points[index - 1].adjusted_x;
+	curve_points[1].offset = dal_fixed31_32_zero;
+
+	curve_points[2].x = points[index].adjusted_x;
+	curve_points[2].offset = dal_fixed31_32_zero;
+
+	return result;
+}
+
+static bool setup_distribution_points(
+		struct gamma_curve *arr_curve_points,
+		struct curve_points *arr_points,
+		uint32_t *hw_points_num,
+		struct hw_x_point *coordinates_x)
+{
+	struct curve_config cfg;
+
+	cfg.offset = 0;
+	cfg.segments[0] = 3;
+	cfg.segments[1] = 4;
+	cfg.segments[2] = 4;
+	cfg.segments[3] = 4;
+	cfg.segments[4] = 4;
+	cfg.segments[5] = 4;
+	cfg.segments[6] = 4;
+	cfg.segments[7] = 4;
+	cfg.segments[8] = 5;
+	cfg.segments[9] = 5;
+	cfg.segments[10] = 0;
+	cfg.segments[11] = -1;
+	cfg.segments[12] = -1;
+	cfg.segments[13] = -1;
+	cfg.segments[14] = -1;
+	cfg.segments[15] = -1;
+
+	cfg.begin = -10;
+
+	if (!build_hw_curve_configuration(
+		&cfg, arr_curve_points,
+		arr_points,
+		coordinates_x, hw_points_num)) {
+		ASSERT_CRITICAL(false);
+		return false;
+	}
+	return true;
+}
+
+struct dividers {
+	struct fixed31_32 divider1;
+	struct fixed31_32 divider2;
+	struct fixed31_32 divider3;
+};
+
+static void build_regamma_coefficients(struct gamma_coefficients *coefficients)
+{
+	/* sRGB should apply 2.4 */
+	static const int32_t numerator01[3] = { 31308, 31308, 31308 };
+	static const int32_t numerator02[3] = { 12920, 12920, 12920 };
+	static const int32_t numerator03[3] = { 55, 55, 55 };
+	static const int32_t numerator04[3] = { 55, 55, 55 };
+	static const int32_t numerator05[3] = { 2400, 2400, 2400 };
+
+	const int32_t *numerator1;
+	const int32_t *numerator2;
+	const int32_t *numerator3;
+	const int32_t *numerator4;
+	const int32_t *numerator5;
+
+	uint32_t i = 0;
+
+	numerator1 = numerator01;
+	numerator2 = numerator02;
+	numerator3 = numerator03;
+	numerator4 = numerator04;
+	numerator5 = numerator05;
+
+	do {
+		coefficients->a0[i] = dal_fixed31_32_from_fraction(
+			numerator1[i], 10000000);
+		coefficients->a1[i] = dal_fixed31_32_from_fraction(
+			numerator2[i], 1000);
+		coefficients->a2[i] = dal_fixed31_32_from_fraction(
+			numerator3[i], 1000);
+		coefficients->a3[i] = dal_fixed31_32_from_fraction(
+			numerator4[i], 1000);
+		coefficients->user_gamma[i] = dal_fixed31_32_from_fraction(
+			numerator5[i], 1000);
+
+		++i;
+	} while (i != ARRAY_SIZE(coefficients->a0));
+}
+
+static struct fixed31_32 translate_from_linear_space(
+	struct fixed31_32 arg,
+	struct fixed31_32 a0,
+	struct fixed31_32 a1,
+	struct fixed31_32 a2,
+	struct fixed31_32 a3,
+	struct fixed31_32 gamma)
+{
+	const struct fixed31_32 one = dal_fixed31_32_from_int(1);
+
+	if (dal_fixed31_32_le(arg, dal_fixed31_32_neg(a0)))
+		return dal_fixed31_32_sub(
+			a2,
+			dal_fixed31_32_mul(
+				dal_fixed31_32_add(
+					one,
+					a3),
+				dal_fixed31_32_pow(
+					dal_fixed31_32_neg(arg),
+					dal_fixed31_32_recip(gamma))));
+	else if (dal_fixed31_32_le(a0, arg))
+		return dal_fixed31_32_sub(
+			dal_fixed31_32_mul(
+				dal_fixed31_32_add(
+					one,
+					a3),
+				dal_fixed31_32_pow(
+					arg,
+					dal_fixed31_32_recip(gamma))),
+			a2);
+	else
+		return dal_fixed31_32_mul(
+			arg,
+			a1);
+}
+
+static inline struct fixed31_32 translate_from_linear_space_ex(
+	struct fixed31_32 arg,
+	struct gamma_coefficients *coeff,
+	uint32_t color_index)
+{
+	return translate_from_linear_space(
+		arg,
+		coeff->a0[color_index],
+		coeff->a1[color_index],
+		coeff->a2[color_index],
+		coeff->a3[color_index],
+		coeff->user_gamma[color_index]);
+}
+
+static bool find_software_points(
+	const struct gamma_pixel *axis_x_256,
+	struct fixed31_32 hw_point,
+	enum channel_name channel,
+	uint32_t *index_to_start,
+	uint32_t *index_left,
+	uint32_t *index_right,
+	enum hw_point_position *pos)
+{
+	const uint32_t max_number = RGB_256X3X16 + 3;
+
+	struct fixed31_32 left, right;
+
+	uint32_t i = *index_to_start;
+
+	while (i < max_number) {
+		if (channel == CHANNEL_NAME_RED) {
+			left = axis_x_256[i].r;
+
+			if (i < max_number - 1)
+				right = axis_x_256[i + 1].r;
+			else
+				right = axis_x_256[max_number - 1].r;
+		} else if (channel == CHANNEL_NAME_GREEN) {
+			left = axis_x_256[i].g;
+
+			if (i < max_number - 1)
+				right = axis_x_256[i + 1].g;
+			else
+				right = axis_x_256[max_number - 1].g;
+		} else {
+			left = axis_x_256[i].b;
+
+			if (i < max_number - 1)
+				right = axis_x_256[i + 1].b;
+			else
+				right = axis_x_256[max_number - 1].b;
+		}
+
+		if (dal_fixed31_32_le(left, hw_point) &&
+			dal_fixed31_32_le(hw_point, right)) {
+			*index_to_start = i;
+			*index_left = i;
+
+			if (i < max_number - 1)
+				*index_right = i + 1;
+			else
+				*index_right = max_number - 1;
+
+			*pos = HW_POINT_POSITION_MIDDLE;
+
+			return true;
+		} else if ((i == *index_to_start) &&
+			dal_fixed31_32_le(hw_point, left)) {
+			*index_to_start = i;
+			*index_left = i;
+			*index_right = i;
+
+			*pos = HW_POINT_POSITION_LEFT;
+
+			return true;
+		} else if ((i == max_number - 1) &&
+			dal_fixed31_32_le(right, hw_point)) {
+			*index_to_start = i;
+			*index_left = i;
+			*index_right = i;
+
+			*pos = HW_POINT_POSITION_RIGHT;
+
+			return true;
+		}
+
+		++i;
+	}
+
+	return false;
+}
+
+static bool build_custom_gamma_mapping_coefficients_worker(
+	struct pixel_gamma_point *coeff,
+	const struct hw_x_point *coordinates_x,
+	const struct gamma_pixel *axis_x_256,
+	enum channel_name channel,
+	uint32_t number_of_points,
+	enum surface_pixel_format pixel_format)
+{
+	uint32_t i = 0;
+
+	while (i <= number_of_points) {
+		struct fixed31_32 coord_x;
+
+		uint32_t index_to_start = 0;
+		uint32_t index_left = 0;
+		uint32_t index_right = 0;
+
+		enum hw_point_position hw_pos;
+
+		struct gamma_point *point;
+
+		struct fixed31_32 left_pos;
+		struct fixed31_32 right_pos;
+
+		/*
+		 * TODO: confirm enum in surface_pixel_format
+		 * if (pixel_format == PIXEL_FORMAT_FP16)
+		 *coord_x = coordinates_x[i].adjusted_x;
+		 *else
+		 */
+		if (channel == CHANNEL_NAME_RED)
+			coord_x = coordinates_x[i].regamma_y_red;
+		else if (channel == CHANNEL_NAME_GREEN)
+			coord_x = coordinates_x[i].regamma_y_green;
+		else
+			coord_x = coordinates_x[i].regamma_y_blue;
+
+		if (!find_software_points(
+			axis_x_256, coord_x, channel,
+			&index_to_start, &index_left, &index_right, &hw_pos)) {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		if (index_left >= RGB_256X3X16 + 3) {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		if (index_right >= RGB_256X3X16 + 3) {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		if (channel == CHANNEL_NAME_RED) {
+			point = &coeff[i].r;
+
+			left_pos = axis_x_256[index_left].r;
+			right_pos = axis_x_256[index_right].r;
+		} else if (channel == CHANNEL_NAME_GREEN) {
+			point = &coeff[i].g;
+
+			left_pos = axis_x_256[index_left].g;
+			right_pos = axis_x_256[index_right].g;
+		} else {
+			point = &coeff[i].b;
+
+			left_pos = axis_x_256[index_left].b;
+			right_pos = axis_x_256[index_right].b;
+		}
+
+		if (hw_pos == HW_POINT_POSITION_MIDDLE)
+			point->coeff = dal_fixed31_32_div(
+				dal_fixed31_32_sub(
+					coord_x,
+					left_pos),
+				dal_fixed31_32_sub(
+					right_pos,
+					left_pos));
+		else if (hw_pos == HW_POINT_POSITION_LEFT)
+			point->coeff = dal_fixed31_32_zero;
+		else if (hw_pos == HW_POINT_POSITION_RIGHT)
+			point->coeff = dal_fixed31_32_from_int(2);
+		else {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		point->left_index = index_left;
+		point->right_index = index_right;
+		point->pos = hw_pos;
+
+		++i;
+	}
+
+	return true;
+}
+
+static inline bool build_oem_custom_gamma_mapping_coefficients(
+	struct pixel_gamma_point *coeff128_oem,
+	const struct hw_x_point *coordinates_x,
+	const struct gamma_pixel *axis_x_256,
+	uint32_t number_of_points,
+	enum surface_pixel_format pixel_format)
+{
+	int i;
+
+	for (i = 0; i < 3; i++) {
+		if (!build_custom_gamma_mapping_coefficients_worker(
+				coeff128_oem, coordinates_x, axis_x_256, i,
+				number_of_points, pixel_format))
+			return false;
+	}
+	return true;
+}
+
+static struct fixed31_32 calculate_mapped_value(
+	struct pwl_float_data *rgb,
+	const struct pixel_gamma_point *coeff,
+	enum channel_name channel,
+	uint32_t max_index)
+{
+	const struct gamma_point *point;
+
+	struct fixed31_32 result;
+
+	if (channel == CHANNEL_NAME_RED)
+		point = &coeff->r;
+	else if (channel == CHANNEL_NAME_GREEN)
+		point = &coeff->g;
+	else
+		point = &coeff->b;
+
+	if ((point->left_index < 0) || (point->left_index > max_index)) {
+		BREAK_TO_DEBUGGER();
+		return dal_fixed31_32_zero;
+	}
+
+	if ((point->right_index < 0) || (point->right_index > max_index)) {
+		BREAK_TO_DEBUGGER();
+		return dal_fixed31_32_zero;
+	}
+
+	if (point->pos == HW_POINT_POSITION_MIDDLE)
+		if (channel == CHANNEL_NAME_RED)
+			result = dal_fixed31_32_add(
+				dal_fixed31_32_mul(
+					point->coeff,
+					dal_fixed31_32_sub(
+						rgb[point->right_index].r,
+						rgb[point->left_index].r)),
+				rgb[point->left_index].r);
+		else if (channel == CHANNEL_NAME_GREEN)
+			result = dal_fixed31_32_add(
+				dal_fixed31_32_mul(
+					point->coeff,
+					dal_fixed31_32_sub(
+						rgb[point->right_index].g,
+						rgb[point->left_index].g)),
+				rgb[point->left_index].g);
+		else
+			result = dal_fixed31_32_add(
+				dal_fixed31_32_mul(
+					point->coeff,
+					dal_fixed31_32_sub(
+						rgb[point->right_index].b,
+						rgb[point->left_index].b)),
+				rgb[point->left_index].b);
+	else if (point->pos == HW_POINT_POSITION_LEFT) {
+		BREAK_TO_DEBUGGER();
+		result = dal_fixed31_32_zero;
+	} else {
+		BREAK_TO_DEBUGGER();
+		result = dal_fixed31_32_one;
+	}
+
+	return result;
+}
+
+static inline struct fixed31_32 calculate_oem_mapped_value(
+	struct pwl_float_data *rgb_oem,
+	const struct pixel_gamma_point *coeff,
+	uint32_t index,
+	enum channel_name channel,
+	uint32_t max_index)
+{
+	return calculate_mapped_value(
+			rgb_oem,
+			coeff + index,
+			channel,
+			max_index);
+}
+
+static void build_regamma_curve(struct pwl_float_data_ex *rgb_regamma,
+		struct pwl_float_data *rgb_oem,
+		struct pixel_gamma_point *coeff128_oem,
+		const struct core_gamma *ramp,
+		const struct core_surface *surface,
+		uint32_t hw_points_num,
+		const struct hw_x_point *coordinate_x,
+		const struct gamma_pixel *axis_x,
+		struct dividers dividers)
+{
+	uint32_t i;
+
+	struct gamma_coefficients coeff;
+	struct pwl_float_data_ex *rgb = rgb_regamma;
+	const struct hw_x_point *coord_x = coordinate_x;
+
+	build_regamma_coefficients(&coeff);
+
+	/* Use opp110->regamma.coordinates_x to retrieve
+	 * coordinates chosen base on given user curve (future task).
+	 * The x values are exponentially distributed and currently
+	 * it is hard-coded, the user curve shape is ignored.
+	 * The future task is to recalculate opp110-
+	 * regamma.coordinates_x based on input/user curve,
+	 * translation from 256/1025 to 128 pwl points.
+	 */
+
+	i = 0;
+
+	while (i != hw_points_num + 1) {
+		rgb->r = translate_from_linear_space_ex(
+			coord_x->adjusted_x, &coeff, 0);
+		rgb->g = translate_from_linear_space_ex(
+			coord_x->adjusted_x, &coeff, 1);
+		rgb->b = translate_from_linear_space_ex(
+			coord_x->adjusted_x, &coeff, 2);
+
+		++coord_x;
+		++rgb;
+		++i;
+	}
+}
+
+static bool scale_gamma(struct pwl_float_data *pwl_rgb,
+		const struct core_gamma *ramp,
+		struct dividers dividers)
+{
+	const struct dc_gamma_ramp_rgb256x3x16 *gamma;
+	const uint16_t max_driver = 0xFFFF;
+	const uint16_t max_os = 0xFF00;
+	uint16_t scaler = max_os;
+	uint32_t i;
+	struct pwl_float_data *rgb = pwl_rgb;
+	struct pwl_float_data *rgb_last = rgb + RGB_256X3X16 - 1;
+
+	if (ramp->public.type == GAMMA_RAMP_RBG256X3X16)
+		gamma = &ramp->public.gamma_ramp_rgb256x3x16;
+	else
+		return false; /* invalid option */
+
+	i = 0;
+
+	do {
+		if ((gamma->red[i] > max_os) ||
+			(gamma->green[i] > max_os) ||
+			(gamma->blue[i] > max_os)) {
+			scaler = max_driver;
+			break;
+		}
+		++i;
+	} while (i != RGB_256X3X16);
+
+	i = 0;
+
+	do {
+		rgb->r = dal_fixed31_32_from_fraction(
+			gamma->red[i], scaler);
+		rgb->g = dal_fixed31_32_from_fraction(
+			gamma->green[i], scaler);
+		rgb->b = dal_fixed31_32_from_fraction(
+			gamma->blue[i], scaler);
+
+		++rgb;
+		++i;
+	} while (i != RGB_256X3X16);
+
+	rgb->r = dal_fixed31_32_mul(rgb_last->r,
+			dividers.divider1);
+	rgb->g = dal_fixed31_32_mul(rgb_last->g,
+			dividers.divider1);
+	rgb->b = dal_fixed31_32_mul(rgb_last->b,
+			dividers.divider1);
+
+	++rgb;
+
+	rgb->r = dal_fixed31_32_mul(rgb_last->r,
+			dividers.divider2);
+	rgb->g = dal_fixed31_32_mul(rgb_last->g,
+			dividers.divider2);
+	rgb->b = dal_fixed31_32_mul(rgb_last->b,
+			dividers.divider2);
+
+	++rgb;
+
+	rgb->r = dal_fixed31_32_mul(rgb_last->r,
+			dividers.divider3);
+	rgb->g = dal_fixed31_32_mul(rgb_last->g,
+			dividers.divider3);
+	rgb->b = dal_fixed31_32_mul(rgb_last->b,
+			dividers.divider3);
+
+	return true;
+}
+
+static void build_evenly_distributed_points(
+	struct gamma_pixel *points,
+	uint32_t numberof_points,
+	struct fixed31_32 max_value,
+	struct dividers dividers)
+{
+	struct gamma_pixel *p = points;
+	struct gamma_pixel *p_last = p + numberof_points - 1;
+
+	uint32_t i = 0;
+
+	do {
+		struct fixed31_32 value = dal_fixed31_32_div_int(
+			dal_fixed31_32_mul_int(max_value, i),
+			numberof_points - 1);
+
+		p->r = value;
+		p->g = value;
+		p->b = value;
+
+		++p;
+		++i;
+	} while (i != numberof_points);
+
+	p->r = dal_fixed31_32_div(p_last->r, dividers.divider1);
+	p->g = dal_fixed31_32_div(p_last->g, dividers.divider1);
+	p->b = dal_fixed31_32_div(p_last->b, dividers.divider1);
+
+	++p;
+
+	p->r = dal_fixed31_32_div(p_last->r, dividers.divider2);
+	p->g = dal_fixed31_32_div(p_last->g, dividers.divider2);
+	p->b = dal_fixed31_32_div(p_last->b, dividers.divider2);
+
+	++p;
+
+	p->r = dal_fixed31_32_div(p_last->r, dividers.divider3);
+	p->g = dal_fixed31_32_div(p_last->g, dividers.divider3);
+	p->b = dal_fixed31_32_div(p_last->b, dividers.divider3);
+}
+
+static inline void copy_rgb_regamma_to_coordinates_x(
+		struct hw_x_point *coordinates_x,
+		uint32_t hw_points_num,
+		const struct pwl_float_data_ex *rgb_ex)
+{
+	struct hw_x_point *coords = coordinates_x;
+	uint32_t i = 0;
+	const struct pwl_float_data_ex *rgb_regamma = rgb_ex;
+
+	while (i <= hw_points_num) {
+		coords->regamma_y_red = rgb_regamma->r;
+		coords->regamma_y_green = rgb_regamma->g;
+		coords->regamma_y_blue = rgb_regamma->b;
+
+		++coords;
+		++rgb_regamma;
+		++i;
+	}
+}
+
+static bool calculate_interpolated_hardware_curve(
+	struct pwl_result_data *rgb,
+	struct pixel_gamma_point *coeff128,
+	struct pwl_float_data *rgb_user,
+	const struct hw_x_point *coordinates_x,
+	const struct gamma_pixel *axis_x_256,
+	uint32_t number_of_points,
+	enum surface_pixel_format pixel_format)
+{
+
+	const struct pixel_gamma_point *coeff;
+	struct pixel_gamma_point *coeff_128 = coeff128;
+	uint32_t max_entries = 3 - 1;
+	struct pwl_result_data *rgb_resulted = rgb;
+
+	uint32_t i = 0;
+
+	if (!build_oem_custom_gamma_mapping_coefficients(
+			coeff_128, coordinates_x, axis_x_256,
+			number_of_points,
+			pixel_format))
+		return false;
+
+	coeff = coeff128;
+	max_entries += RGB_256X3X16;
+
+	/* TODO: float point case */
+
+	while (i <= number_of_points) {
+		rgb_resulted->red = calculate_mapped_value(
+			rgb_user, coeff, CHANNEL_NAME_RED, max_entries);
+		rgb_resulted->green = calculate_mapped_value(
+			rgb_user, coeff, CHANNEL_NAME_GREEN, max_entries);
+		rgb_resulted->blue = calculate_mapped_value(
+			rgb_user, coeff, CHANNEL_NAME_BLUE, max_entries);
+
+		++coeff;
+		++rgb_resulted;
+		++i;
+	}
+
+	return true;
+}
+
+static bool map_regamma_hw_to_x_user(
+	struct pixel_gamma_point *coeff128,
+	struct pwl_float_data *rgb_oem,
+	struct pwl_result_data *rgb_resulted,
+	struct pwl_float_data *rgb_user,
+	struct hw_x_point *coords_x,
+	const struct gamma_pixel *axis_x,
+	const struct dc_gamma *gamma,
+	const struct pwl_float_data_ex *rgb_regamma,
+	struct dividers dividers,
+	uint32_t hw_points_num,
+	const struct core_surface *surface)
+{
+	/* setup to spare calculated ideal regamma values */
+
+	struct pixel_gamma_point *coeff = coeff128;
+
+	struct hw_x_point *coords = coords_x;
+
+	copy_rgb_regamma_to_coordinates_x(coords, hw_points_num, rgb_regamma);
+
+	return calculate_interpolated_hardware_curve(
+			rgb_resulted, coeff, rgb_user, coords, axis_x,
+			hw_points_num, surface->public.format);
+}
+
+static void build_new_custom_resulted_curve(
+	struct pwl_result_data *rgb_resulted,
+	uint32_t hw_points_num)
+{
+	struct pwl_result_data *rgb = rgb_resulted;
+	struct pwl_result_data *rgb_plus_1 = rgb + 1;
+
+	uint32_t i;
+
+	i = 0;
+
+	while (i != hw_points_num + 1) {
+		rgb->red = dal_fixed31_32_clamp(
+			rgb->red, dal_fixed31_32_zero,
+			dal_fixed31_32_one);
+		rgb->green = dal_fixed31_32_clamp(
+			rgb->green, dal_fixed31_32_zero,
+			dal_fixed31_32_one);
+		rgb->blue = dal_fixed31_32_clamp(
+			rgb->blue, dal_fixed31_32_zero,
+			dal_fixed31_32_one);
+
+		++rgb;
+		++i;
+	}
+
+	rgb = rgb_resulted;
+
+	i = 1;
+
+	while (i != hw_points_num + 1) {
+		if (dal_fixed31_32_lt(rgb_plus_1->red, rgb->red))
+			rgb_plus_1->red = rgb->red;
+		if (dal_fixed31_32_lt(rgb_plus_1->green, rgb->green))
+			rgb_plus_1->green = rgb->green;
+		if (dal_fixed31_32_lt(rgb_plus_1->blue, rgb->blue))
+			rgb_plus_1->blue = rgb->blue;
+
+		rgb->delta_red = dal_fixed31_32_sub(
+			rgb_plus_1->red,
+			rgb->red);
+		rgb->delta_green = dal_fixed31_32_sub(
+			rgb_plus_1->green,
+			rgb->green);
+		rgb->delta_blue = dal_fixed31_32_sub(
+			rgb_plus_1->blue,
+			rgb->blue);
+
+		++rgb_plus_1;
+		++rgb;
+		++i;
+	}
+}
+
+static void rebuild_curve_configuration_magic(
+		struct curve_points *arr_points,
+		struct pwl_result_data *rgb_resulted,
+		const struct hw_x_point *coordinates_x,
+		uint32_t hw_points_num)
+{
+	const struct fixed31_32 magic_number =
+		dal_fixed31_32_from_fraction(249, 1000);
+
+	struct fixed31_32 y_r;
+	struct fixed31_32 y_g;
+	struct fixed31_32 y_b;
+
+	struct fixed31_32 y1_min;
+	struct fixed31_32 y2_max;
+	struct fixed31_32 y3_max;
+
+	y_r = rgb_resulted[0].red;
+	y_g = rgb_resulted[0].green;
+	y_b = rgb_resulted[0].blue;
+
+	y1_min = dal_fixed31_32_min(y_r, dal_fixed31_32_min(y_g, y_b));
+
+	arr_points[0].x = coordinates_x[0].adjusted_x;
+	arr_points[0].y = y1_min;
+	arr_points[0].slope = dal_fixed31_32_div(
+					arr_points[0].y,
+					arr_points[0].x);
+
+	arr_points[1].x = dal_fixed31_32_add(
+			coordinates_x[hw_points_num - 1].adjusted_x,
+			magic_number);
+
+	arr_points[2].x = arr_points[1].x;
+
+	y_r = rgb_resulted[hw_points_num - 1].red;
+	y_g = rgb_resulted[hw_points_num - 1].green;
+	y_b = rgb_resulted[hw_points_num - 1].blue;
+
+	y2_max = dal_fixed31_32_max(y_r, dal_fixed31_32_max(y_g, y_b));
+
+	arr_points[1].y = y2_max;
+
+	y_r = rgb_resulted[hw_points_num].red;
+	y_g = rgb_resulted[hw_points_num].green;
+	y_b = rgb_resulted[hw_points_num].blue;
+
+	y3_max = dal_fixed31_32_max(y_r, dal_fixed31_32_max(y_g, y_b));
+
+	arr_points[2].y = y3_max;
+
+	arr_points[2].slope = dal_fixed31_32_one;
+}
+
+static bool convert_to_custom_float_format(
+	struct fixed31_32 value,
+	const struct custom_float_format *format,
+	uint32_t *result)
+{
+	uint32_t mantissa;
+	uint32_t exponenta;
+	bool negative;
+
+	return build_custom_float(
+		value, format, &negative, &mantissa, &exponenta) &&
+	setup_custom_float(
+		format, negative, mantissa, exponenta, result);
+}
+
+static bool convert_to_custom_float(
+		struct pwl_result_data *rgb_resulted,
+		struct curve_points *arr_points,
+		uint32_t hw_points_num)
+{
+	struct custom_float_format fmt;
+
+	struct pwl_result_data *rgb = rgb_resulted;
+
+	uint32_t i = 0;
+
+	fmt.exponenta_bits = 6;
+	fmt.mantissa_bits = 12;
+	fmt.sign = true;
+
+	if (!convert_to_custom_float_format(
+		arr_points[0].x,
+		&fmt,
+		&arr_points[0].custom_float_x)) {
+		BREAK_TO_DEBUGGER();
+		return false;
+	}
+
+	if (!convert_to_custom_float_format(
+		arr_points[0].offset,
+		&fmt,
+		&arr_points[0].custom_float_offset)) {
+		BREAK_TO_DEBUGGER();
+		return false;
+	}
+
+	if (!convert_to_custom_float_format(
+		arr_points[0].slope,
+		&fmt,
+		&arr_points[0].custom_float_slope)) {
+		BREAK_TO_DEBUGGER();
+		return false;
+	}
+
+	fmt.mantissa_bits = 10;
+	fmt.sign = false;
+
+	if (!convert_to_custom_float_format(
+		arr_points[1].x,
+		&fmt,
+		&arr_points[1].custom_float_x)) {
+		BREAK_TO_DEBUGGER();
+		return false;
+	}
+
+	if (!convert_to_custom_float_format(
+		arr_points[1].y,
+		&fmt,
+		&arr_points[1].custom_float_y)) {
+		BREAK_TO_DEBUGGER();
+		return false;
+	}
+
+	if (!convert_to_custom_float_format(
+		arr_points[2].slope,
+		&fmt,
+		&arr_points[2].custom_float_slope)) {
+		BREAK_TO_DEBUGGER();
+		return false;
+	}
+
+	fmt.mantissa_bits = 12;
+	fmt.sign = true;
+
+	while (i != hw_points_num) {
+		if (!convert_to_custom_float_format(
+			rgb->red,
+			&fmt,
+			&rgb->red_reg)) {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		if (!convert_to_custom_float_format(
+			rgb->green,
+			&fmt,
+			&rgb->green_reg)) {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		if (!convert_to_custom_float_format(
+			rgb->blue,
+			&fmt,
+			&rgb->blue_reg)) {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		if (!convert_to_custom_float_format(
+			rgb->delta_red,
+			&fmt,
+			&rgb->delta_red_reg)) {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		if (!convert_to_custom_float_format(
+			rgb->delta_green,
+			&fmt,
+			&rgb->delta_green_reg)) {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		if (!convert_to_custom_float_format(
+			rgb->delta_blue,
+			&fmt,
+			&rgb->delta_blue_reg)) {
+			BREAK_TO_DEBUGGER();
+			return false;
+		}
+
+		++rgb;
+		++i;
+	}
+
+	return true;
+}
+
+bool calculate_regamma_params(struct pwl_params *params,
+		const struct core_gamma *ramp,
+		const struct core_surface *surface)
+{
+	struct gamma_curve *arr_curve_points = params->arr_curve_points;
+	struct curve_points *arr_points = params->arr_points;
+	struct pwl_result_data *rgb_resulted = params->rgb_resulted;
+	struct dividers dividers;
+
+	struct hw_x_point *coordinates_x = NULL;
+	struct pwl_float_data *rgb_user = NULL ;
+	struct pwl_float_data_ex *rgb_regamma = NULL;
+	struct pwl_float_data *rgb_oem = NULL;
+	struct gamma_pixel *axix_x_256 = NULL;
+	struct pixel_gamma_point *coeff128_oem = NULL;
+	struct pixel_gamma_point *coeff128 = NULL;
+
+
+	bool ret = false;
+
+	coordinates_x = dm_alloc(sizeof(*coordinates_x)*(256 + 3));
+	if (!coordinates_x)
+		goto coordinates_x_alloc_fail;
+	rgb_user = dm_alloc(sizeof(*rgb_user) * (FLOAT_GAMMA_RAMP_MAX + 3));
+	if (!rgb_user)
+		goto rgb_user_alloc_fail;
+	rgb_regamma = dm_alloc(sizeof(*rgb_regamma) * (256 + 3));
+	if (!rgb_regamma)
+		goto rgb_regamma_alloc_fail;
+	rgb_oem = dm_alloc(sizeof(*rgb_oem) * (FLOAT_GAMMA_RAMP_MAX + 3));
+	if (!rgb_oem)
+		goto rgb_oem_alloc_fail;
+	axix_x_256 = dm_alloc(sizeof(*axix_x_256) * (256 + 3));
+	if (!axix_x_256)
+		goto axix_x_256_alloc_fail;
+	coeff128_oem = dm_alloc(sizeof(*coeff128_oem) * (256 + 3));
+	if (!coeff128_oem)
+		goto coeff128_oem_alloc_fail;
+	coeff128 = dm_alloc(sizeof(*coeff128) * (256 + 3));
+	if (!coeff128)
+		goto coeff128_alloc_fail;
+
+	dividers.divider1 = dal_fixed31_32_from_fraction(3, 2);
+	dividers.divider2 = dal_fixed31_32_from_int(2);
+	dividers.divider3 = dal_fixed31_32_from_fraction(5, 2);
+
+	build_evenly_distributed_points(
+			axix_x_256,
+			256,
+			dal_fixed31_32_one,
+			dividers);
+
+	scale_gamma(rgb_user, ramp, dividers);
+
+	setup_distribution_points(arr_curve_points, arr_points,
+			&params->hw_points_num, coordinates_x);
+
+	build_regamma_curve(rgb_regamma, rgb_oem, coeff128_oem,
+			ramp, surface, params->hw_points_num,
+			coordinates_x, axix_x_256, dividers);
+
+	map_regamma_hw_to_x_user(coeff128, rgb_oem, rgb_resulted, rgb_user,
+			coordinates_x, axix_x_256, &ramp->public, rgb_regamma,
+			dividers, params->hw_points_num, surface);
+
+	build_new_custom_resulted_curve(rgb_resulted, params->hw_points_num);
+
+	rebuild_curve_configuration_magic(
+			arr_points,
+			rgb_resulted,
+			coordinates_x,
+			params->hw_points_num);
+
+	convert_to_custom_float(rgb_resulted, arr_points,
+			params->hw_points_num);
+
+	ret = true;
+
+	dm_free(coeff128);
+coeff128_alloc_fail:
+	dm_free(coeff128_oem);
+coeff128_oem_alloc_fail:
+	dm_free(axix_x_256);
+axix_x_256_alloc_fail:
+	dm_free(rgb_oem);
+rgb_oem_alloc_fail:
+	dm_free(rgb_regamma);
+rgb_regamma_alloc_fail:
+	dm_free(rgb_user);
+rgb_user_alloc_fail:
+	dm_free(coordinates_x);
+coordinates_x_alloc_fail:
+	return ret;
+
+}
+
diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_target.c b/drivers/gpu/drm/amd/display/dc/core/dc_target.c
new file mode 100644
index 0000000..48eb7b0
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_target.c
@@ -0,0 +1,334 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "core_types.h"
+#include "hw_sequencer.h"
+#include "resource.h"
+#include "ipp.h"
+#include "timing_generator.h"
+
+struct target {
+	struct core_target protected;
+	int ref_count;
+};
+
+#define DC_TARGET_TO_TARGET(dc_target) \
+	container_of(dc_target, struct target, protected.public)
+#define CORE_TARGET_TO_TARGET(core_target) \
+	container_of(core_target, struct target, protected)
+
+static void construct(
+	struct core_target *target,
+	struct dc_context *ctx,
+	struct dc_stream *dc_streams[],
+	uint8_t stream_count)
+{
+	uint8_t i;
+	for (i = 0; i < stream_count; i++) {
+		target->public.streams[i] = dc_streams[i];
+		dc_stream_retain(dc_streams[i]);
+	}
+
+	target->ctx = ctx;
+	target->public.stream_count = stream_count;
+}
+
+static void destruct(struct core_target *core_target)
+{
+	int i;
+
+	for (i = 0; i < core_target->public.stream_count; i++) {
+		dc_stream_release(
+			(struct dc_stream *)core_target->public.streams[i]);
+		core_target->public.streams[i] = NULL;
+	}
+}
+
+void dc_target_retain(const struct dc_target *dc_target)
+{
+	struct target *target = DC_TARGET_TO_TARGET(dc_target);
+
+	target->ref_count++;
+}
+
+void dc_target_release(const struct dc_target *dc_target)
+{
+	struct target *target = DC_TARGET_TO_TARGET(dc_target);
+	struct core_target *protected = DC_TARGET_TO_CORE(dc_target);
+
+	ASSERT(target->ref_count > 0);
+	target->ref_count--;
+	if (target->ref_count == 0) {
+		destruct(protected);
+		dm_free(target);
+	}
+}
+
+const struct dc_target_status *dc_target_get_status(
+					const struct dc_target* dc_target)
+{
+	uint8_t i;
+	struct core_target* target = DC_TARGET_TO_CORE(dc_target);
+	struct core_dc *dc = DC_TO_CORE(target->ctx->dc);
+
+	for (i = 0; i < dc->current_context->target_count; i++)
+		if (target == dc->current_context->targets[i])
+			return &dc->current_context->target_status[i];
+
+	return NULL;
+}
+
+struct dc_target *dc_create_target_for_streams(
+		struct dc_stream *dc_streams[],
+		uint8_t stream_count)
+{
+	struct core_stream *stream;
+	struct target *target;
+
+	if (0 == stream_count)
+		goto target_alloc_fail;
+
+	stream = DC_STREAM_TO_CORE(dc_streams[0]);
+
+	target = dm_alloc(sizeof(struct target));
+
+	if (NULL == target)
+		goto target_alloc_fail;
+
+	construct(&target->protected, stream->ctx, dc_streams, stream_count);
+
+	dc_target_retain(&target->protected.public);
+
+	return &target->protected.public;
+
+target_alloc_fail:
+	return NULL;
+}
+
+bool dc_target_is_connected_to_sink(
+		const struct dc_target * dc_target,
+		const struct dc_sink *dc_sink)
+{
+	struct core_target *target = DC_TARGET_TO_CORE(dc_target);
+	uint8_t i;
+	for (i = 0; i < target->public.stream_count; i++) {
+		if (target->public.streams[i]->sink == dc_sink)
+			return true;
+	}
+	return false;
+}
+
+/**
+ * Update the cursor attributes and set cursor surface address
+ */
+bool dc_target_set_cursor_attributes(
+	struct dc_target *dc_target,
+	const struct dc_cursor_attributes *attributes)
+{
+	uint8_t i, j;
+	struct core_target *target;
+	struct core_dc *core_dc;
+	struct resource_context *res_ctx;
+
+	if (NULL == dc_target) {
+		dm_error("DC: dc_target is NULL!\n");
+			return false;
+
+	}
+	if (NULL == attributes) {
+		dm_error("DC: attributes is NULL!\n");
+			return false;
+
+	}
+
+	target = DC_TARGET_TO_CORE(dc_target);
+	core_dc = DC_TO_CORE(target->ctx->dc);
+	res_ctx = &core_dc->current_context->res_ctx;
+
+	for (i = 0; i < target->public.stream_count; i++) {
+		for (j = 0; j < MAX_PIPES; j++) {
+			struct input_pixel_processor *ipp =
+						res_ctx->pipe_ctx[j].ipp;
+
+			if (res_ctx->pipe_ctx[j].stream !=
+				DC_STREAM_TO_CORE(target->public.streams[i]))
+				continue;
+
+			/* As of writing of this code cursor is on the top
+			 * plane so we only need to set it on first pipe we
+			 * find. May need to make this code dce specific later.
+			 */
+			if (ipp->funcs->ipp_cursor_set_attributes(
+							ipp, attributes))
+				return true;
+		}
+	}
+
+	return false;
+}
+
+bool dc_target_set_cursor_position(
+	struct dc_target *dc_target,
+	const struct dc_cursor_position *position)
+{
+	uint8_t i, j;
+	struct core_target *target;
+	struct core_dc *core_dc;
+	struct resource_context *res_ctx;
+
+	if (NULL == dc_target) {
+		dm_error("DC: dc_target is NULL!\n");
+		return false;
+	}
+
+	if (NULL == position) {
+		dm_error("DC: cursor position is NULL!\n");
+		return false;
+	}
+
+	target = DC_TARGET_TO_CORE(dc_target);
+	core_dc = DC_TO_CORE(target->ctx->dc);
+	res_ctx = &core_dc->current_context->res_ctx;
+
+	for (i = 0; i < target->public.stream_count; i++) {
+		for (j = 0; j < MAX_PIPES; j++) {
+			struct input_pixel_processor *ipp =
+						res_ctx->pipe_ctx[j].ipp;
+
+			if (res_ctx->pipe_ctx[j].stream !=
+				DC_STREAM_TO_CORE(target->public.streams[i]))
+				continue;
+
+			/* As of writing of this code cursor is on the top
+			 * plane so we only need to set it on first pipe we
+			 * find. May need to make this code dce specific later.
+			 */
+			ipp->funcs->ipp_cursor_set_position(ipp, position);
+			return true;
+		}
+	}
+
+	return false;
+}
+
+uint32_t dc_target_get_vblank_counter(const struct dc_target *dc_target)
+{
+	uint8_t i, j;
+	struct core_target *target = DC_TARGET_TO_CORE(dc_target);
+	struct core_dc *core_dc = DC_TO_CORE(target->ctx->dc);
+	struct resource_context *res_ctx =
+		&core_dc->current_context->res_ctx;
+
+	for (i = 0; i < target->public.stream_count; i++) {
+		for (j = 0; j < MAX_PIPES; j++) {
+			struct timing_generator *tg = res_ctx->pipe_ctx[j].tg;
+
+			if (res_ctx->pipe_ctx[j].stream !=
+				DC_STREAM_TO_CORE(target->public.streams[i]))
+				continue;
+
+			return tg->funcs->get_frame_count(tg);
+		}
+	}
+
+	return 0;
+}
+
+uint32_t dc_target_get_scanoutpos(
+		const struct dc_target *dc_target,
+		uint32_t *vbl,
+		uint32_t *position)
+{
+	uint8_t i, j;
+	struct core_target *target = DC_TARGET_TO_CORE(dc_target);
+	struct core_dc *core_dc = DC_TO_CORE(target->ctx->dc);
+	struct resource_context *res_ctx =
+		&core_dc->current_context->res_ctx;
+
+	for (i = 0; i < target->public.stream_count; i++) {
+		for (j = 0; j < MAX_PIPES; j++) {
+			struct timing_generator *tg = res_ctx->pipe_ctx[j].tg;
+
+			if (res_ctx->pipe_ctx[j].stream !=
+				DC_STREAM_TO_CORE(target->public.streams[i]))
+				continue;
+
+			return tg->funcs->get_scanoutpos(tg, vbl, position);
+		}
+	}
+
+	return 0;
+}
+
+void dc_target_log(
+	const struct dc_target *dc_target,
+	struct dal_logger *dm_logger,
+	enum dc_log_type log_type)
+{
+	int i;
+
+	const struct core_target *core_target =
+			CONST_DC_TARGET_TO_CORE(dc_target);
+
+	dm_logger_write(dm_logger,
+			log_type,
+			"core_target 0x%x: stream_count=%d\n",
+			core_target,
+			core_target->public.stream_count);
+
+	for (i = 0; i < core_target->public.stream_count; i++) {
+		const struct core_stream *core_stream =
+			DC_STREAM_TO_CORE(core_target->public.streams[i]);
+
+		dm_logger_write(dm_logger,
+			log_type,
+			"core_stream 0x%x: src: %d, %d, %d, %d; dst: %d, %d, %d, %d;\n",
+			core_stream,
+			core_stream->public.src.x,
+			core_stream->public.src.y,
+			core_stream->public.src.width,
+			core_stream->public.src.height,
+			core_stream->public.dst.x,
+			core_stream->public.dst.y,
+			core_stream->public.dst.width,
+			core_stream->public.dst.height);
+		dm_logger_write(dm_logger,
+			log_type,
+			"\tpix_clk_khz: %d, h_total: %d, v_total: %d\n",
+			core_stream->public.timing.pix_clk_khz,
+			core_stream->public.timing.h_total,
+			core_stream->public.timing.v_total);
+		dm_logger_write(dm_logger,
+			log_type,
+			"\tsink name: %s, serial: %d\n",
+			core_stream->sink->public.edid_caps.display_name,
+			core_stream->sink->public.edid_caps.serial_number);
+		dm_logger_write(dm_logger,
+			log_type,
+			"\tlink: %d\n",
+			core_stream->sink->link->public.link_index);
+	}
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.c b/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.c
new file mode 100644
index 0000000..dd69f60
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.c
@@ -0,0 +1,62 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "include/logger_interface.h"
+
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+
+#include "dce110_ipp.h"
+
+static const struct ipp_funcs funcs = {
+		.ipp_cursor_set_attributes = dce110_ipp_cursor_set_attributes,
+		.ipp_cursor_set_position = dce110_ipp_cursor_set_position,
+		.ipp_program_prescale = dce110_ipp_program_prescale,
+		.ipp_set_degamma = dce110_ipp_set_degamma,
+};
+
+bool dce110_ipp_construct(
+	struct dce110_ipp* ipp,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_ipp_reg_offsets *offset)
+{
+	ipp->base.ctx = ctx;
+
+	ipp->base.inst = inst;
+
+	ipp->offsets = *offset;
+
+	ipp->base.funcs = &funcs;
+
+	return true;
+}
+
+void dce110_ipp_destroy(struct input_pixel_processor **ipp)
+{
+	dm_free(TO_DCE110_IPP(*ipp));
+	*ipp = NULL;
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.h b/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.h
new file mode 100644
index 0000000..60eebde
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp.h
@@ -0,0 +1,76 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DC_IPP_DCE110_H__
+#define __DC_IPP_DCE110_H__
+
+#include "ipp.h"
+
+struct gamma_parameters;
+struct dev_c_lut;
+
+#define TO_DCE110_IPP(input_pixel_processor)\
+	container_of(input_pixel_processor, struct dce110_ipp, base)
+
+struct dce110_ipp_reg_offsets {
+	uint32_t dcp_offset;
+};
+
+struct dce110_ipp {
+	struct input_pixel_processor base;
+	struct dce110_ipp_reg_offsets offsets;
+};
+
+bool dce110_ipp_construct(
+	struct dce110_ipp* ipp,
+	struct dc_context *ctx,
+	enum controller_id id,
+	const struct dce110_ipp_reg_offsets *offset);
+
+void dce110_ipp_destroy(struct input_pixel_processor **ipp);
+
+/* CURSOR RELATED */
+void dce110_ipp_cursor_set_position(
+	struct input_pixel_processor *ipp,
+	const struct dc_cursor_position *position);
+
+bool dce110_ipp_cursor_set_attributes(
+	struct input_pixel_processor *ipp,
+	const struct dc_cursor_attributes *attributes);
+
+/* DEGAMMA RELATED */
+bool dce110_ipp_set_degamma(
+	struct input_pixel_processor *ipp,
+	enum ipp_degamma_mode mode);
+
+void dce110_ipp_program_prescale(
+	struct input_pixel_processor *ipp,
+	struct ipp_prescale_params *params);
+/*
+ * Helper functions to be resused in other ASICs
+ */
+void dce110_helper_select_lut(struct dce110_ipp *ipp110);
+
+#endif /*__DC_IPP_DCE110_H__*/
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp_cursor.c b/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp_cursor.c
new file mode 100644
index 0000000..95f6ca3
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp_cursor.c
@@ -0,0 +1,253 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "include/logger_interface.h"
+
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+
+#include "dce110_ipp.h"
+
+#define CURSOR_COLOR_BLACK 0x00000000
+#define CURSOR_COLOR_WHITE 0xFFFFFFFF
+
+#define DCP_REG(reg)\
+	(reg + ipp110->offsets.dcp_offset)
+
+static void enable(
+	struct dce110_ipp *ipp110,
+	bool enable);
+
+static void lock(
+	struct dce110_ipp *ipp110,
+	bool enable);
+
+static void program_position(
+	struct dce110_ipp *ipp110,
+	uint32_t x,
+	uint32_t y);
+
+static bool program_control(
+	struct dce110_ipp *ipp110,
+	enum dc_cursor_color_format color_format,
+	bool enable_magnification,
+	bool inverse_transparent_clamping);
+
+static void program_hotspot(
+	struct dce110_ipp *ipp110,
+	uint32_t x,
+	uint32_t y);
+
+static void program_size(
+	struct dce110_ipp *ipp110,
+	uint32_t width,
+	uint32_t height);
+
+static void program_address(
+	struct dce110_ipp *ipp110,
+	PHYSICAL_ADDRESS_LOC address);
+
+void dce110_ipp_cursor_set_position(
+	struct input_pixel_processor *ipp,
+	const struct dc_cursor_position *position)
+{
+	struct dce110_ipp *ipp110 = TO_DCE110_IPP(ipp);
+
+	/* lock cursor registers */
+	lock(ipp110, true);
+
+	/* Flag passed in structure differentiates cursor enable/disable. */
+	/* Update if it differs from cached state. */
+	enable(ipp110, position->enable);
+
+	program_position(ipp110, position->x, position->y);
+
+	if (position->hot_spot_enable)
+		program_hotspot(
+				ipp110,
+				position->x_hotspot,
+				position->y_hotspot);
+
+	/* unlock cursor registers */
+	lock(ipp110, false);
+}
+
+bool dce110_ipp_cursor_set_attributes(
+	struct input_pixel_processor *ipp,
+	const struct dc_cursor_attributes *attributes)
+{
+	struct dce110_ipp *ipp110 = TO_DCE110_IPP(ipp);
+	/* Lock cursor registers */
+	lock(ipp110, true);
+
+	/* Program cursor control */
+	program_control(
+		ipp110,
+		attributes->color_format,
+		attributes->attribute_flags.bits.ENABLE_MAGNIFICATION,
+		attributes->attribute_flags.bits.INVERSE_TRANSPARENT_CLAMPING);
+
+	/* Program hot spot coordinates */
+	program_hotspot(ipp110, attributes->x_hot, attributes->y_hot);
+
+	/*
+	 * Program cursor size -- NOTE: HW spec specifies that HW register
+	 * stores size as (height - 1, width - 1)
+	 */
+	program_size(ipp110, attributes->width-1, attributes->height-1);
+
+	/* Program cursor surface address */
+	program_address(ipp110, attributes->address);
+
+	/* Unlock Cursor registers. */
+	lock(ipp110, false);
+
+	return true;
+}
+
+static void enable(
+	struct dce110_ipp *ipp110, bool enable)
+{
+	uint32_t value = 0;
+	uint32_t addr = DCP_REG(mmCUR_CONTROL);
+
+	value = dm_read_reg(ipp110->base.ctx, addr);
+	set_reg_field_value(value, enable, CUR_CONTROL, CURSOR_EN);
+	dm_write_reg(ipp110->base.ctx, addr, value);
+}
+
+static void lock(
+	struct dce110_ipp *ipp110, bool lock)
+{
+	uint32_t value = 0;
+	uint32_t addr = DCP_REG(mmCUR_UPDATE);
+
+	value = dm_read_reg(ipp110->base.ctx, addr);
+	set_reg_field_value(value, lock, CUR_UPDATE, CURSOR_UPDATE_LOCK);
+	dm_write_reg(ipp110->base.ctx, addr, value);
+}
+
+static void program_position(
+	struct dce110_ipp *ipp110,
+	uint32_t x,
+	uint32_t y)
+{
+	uint32_t value = 0;
+	uint32_t addr = DCP_REG(mmCUR_POSITION);
+
+	value = dm_read_reg(ipp110->base.ctx, addr);
+	set_reg_field_value(value, x, CUR_POSITION, CURSOR_X_POSITION);
+	set_reg_field_value(value, y, CUR_POSITION, CURSOR_Y_POSITION);
+	dm_write_reg(ipp110->base.ctx, addr, value);
+}
+
+static bool program_control(
+	struct dce110_ipp *ipp110,
+	enum dc_cursor_color_format color_format,
+	bool enable_magnification,
+	bool inverse_transparent_clamping)
+{
+	uint32_t value = 0;
+	uint32_t addr = DCP_REG(mmCUR_CONTROL);
+	uint32_t mode = 0;
+
+	switch (color_format) {
+	case CURSOR_MODE_MONO:
+		mode = 0;
+		break;
+	case CURSOR_MODE_COLOR_1BIT_AND:
+		mode = 1;
+		break;
+	case CURSOR_MODE_COLOR_PRE_MULTIPLIED_ALPHA:
+		mode = 2;
+		break;
+	case CURSOR_MODE_COLOR_UN_PRE_MULTIPLIED_ALPHA:
+		mode = 3;
+		break;
+	default:
+		return false;
+	}
+
+	set_reg_field_value(value, mode, CUR_CONTROL, CURSOR_MODE);
+	set_reg_field_value(value, enable_magnification,
+			CUR_CONTROL, CURSOR_2X_MAGNIFY);
+	set_reg_field_value(value, inverse_transparent_clamping,
+			CUR_CONTROL, CUR_INV_TRANS_CLAMP);
+	dm_write_reg(ipp110->base.ctx, addr, value);
+
+	if (color_format == CURSOR_MODE_MONO) {
+		addr = DCP_REG(mmCUR_COLOR1);
+		dm_write_reg(ipp110->base.ctx, addr, CURSOR_COLOR_BLACK);
+		addr = DCP_REG(mmCUR_COLOR2);
+		dm_write_reg(ipp110->base.ctx, addr, CURSOR_COLOR_WHITE);
+	}
+	return true;
+}
+
+static void program_hotspot(
+	struct dce110_ipp *ipp110,
+	uint32_t x,
+	uint32_t y)
+{
+	uint32_t value = 0;
+	uint32_t addr = DCP_REG(mmCUR_HOT_SPOT);
+
+	value = dm_read_reg(ipp110->base.ctx, addr);
+	set_reg_field_value(value, x, CUR_HOT_SPOT, CURSOR_HOT_SPOT_X);
+	set_reg_field_value(value, y, CUR_HOT_SPOT, CURSOR_HOT_SPOT_Y);
+	dm_write_reg(ipp110->base.ctx, addr, value);
+}
+
+static void program_size(
+	struct dce110_ipp *ipp110,
+	uint32_t width,
+	uint32_t height)
+{
+	uint32_t value = 0;
+	uint32_t addr = DCP_REG(mmCUR_SIZE);
+
+	value = dm_read_reg(ipp110->base.ctx, addr);
+	set_reg_field_value(value, width, CUR_SIZE, CURSOR_WIDTH);
+	set_reg_field_value(value, height, CUR_SIZE, CURSOR_HEIGHT);
+	dm_write_reg(ipp110->base.ctx, addr, value);
+}
+
+static void program_address(
+	struct dce110_ipp *ipp110,
+	PHYSICAL_ADDRESS_LOC address)
+{
+	uint32_t addr = DCP_REG(mmCUR_SURFACE_ADDRESS_HIGH);
+	/* SURFACE_ADDRESS_HIGH: Higher order bits (39:32) of hardware cursor
+	 * surface base address in byte. It is 4K byte aligned.
+	 * The correct way to program cursor surface address is to first write
+	 * to CUR_SURFACE_ADDRESS_HIGH, and then write to CUR_SURFACE_ADDRESS */
+
+	dm_write_reg(ipp110->base.ctx, addr, address.high_part);
+
+	addr = DCP_REG(mmCUR_SURFACE_ADDRESS);
+	dm_write_reg(ipp110->base.ctx, addr, address.low_part);
+}
+
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp_gamma.c b/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp_gamma.c
new file mode 100644
index 0000000..79a6a6d
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_ipp_gamma.c
@@ -0,0 +1,303 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "include/logger_interface.h"
+#include "include/fixed31_32.h"
+#include "basics/conversion.h"
+
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+
+#include "dce110_ipp.h"
+#include "gamma_types.h"
+
+#define DCP_REG(reg)\
+	(reg + ipp110->offsets.dcp_offset)
+
+enum {
+	MAX_INPUT_LUT_ENTRY = 256
+};
+
+/*PROTOTYPE DECLARATIONS*/
+static void set_lut_inc(
+	struct dce110_ipp *ipp110,
+	uint8_t inc,
+	bool is_float,
+	bool is_signed);
+
+bool dce110_ipp_set_degamma(
+	struct input_pixel_processor *ipp,
+	enum ipp_degamma_mode mode)
+{
+	struct dce110_ipp *ipp110 = TO_DCE110_IPP(ipp);
+
+	uint32_t value = 0;
+
+	uint32_t degamma_type = (mode == IPP_DEGAMMA_MODE_HW_sRGB) ? 1 : 0;
+
+	ASSERT(mode == IPP_DEGAMMA_MODE_BYPASS ||
+			mode == IPP_DEGAMMA_MODE_HW_sRGB);
+
+	set_reg_field_value(
+		value,
+		degamma_type,
+		DEGAMMA_CONTROL,
+		GRPH_DEGAMMA_MODE);
+
+	set_reg_field_value(
+		value,
+		degamma_type,
+		DEGAMMA_CONTROL,
+		CURSOR_DEGAMMA_MODE);
+
+	set_reg_field_value(
+		value,
+		degamma_type,
+		DEGAMMA_CONTROL,
+		CURSOR2_DEGAMMA_MODE);
+
+	dm_write_reg(ipp110->base.ctx, DCP_REG(mmDEGAMMA_CONTROL), value);
+
+	return true;
+}
+
+void dce110_ipp_program_prescale(
+	struct input_pixel_processor *ipp,
+	struct ipp_prescale_params *params)
+{
+	struct dce110_ipp *ipp110 = TO_DCE110_IPP(ipp);
+
+	uint32_t prescale_control = 0;
+	uint32_t prescale_value = 0;
+	uint32_t legacy_lut_control = 0;
+
+	prescale_control = dm_read_reg(ipp110->base.ctx,
+			DCP_REG(mmPRESCALE_GRPH_CONTROL));
+
+	if (params->mode != IPP_PRESCALE_MODE_BYPASS) {
+
+		set_reg_field_value(
+			prescale_control,
+			0,
+			PRESCALE_GRPH_CONTROL,
+			GRPH_PRESCALE_BYPASS);
+
+		/*
+		 * If prescale is in use, then legacy lut should
+		 * be bypassed
+		 */
+		legacy_lut_control = dm_read_reg(ipp110->base.ctx,
+			DCP_REG(mmINPUT_GAMMA_CONTROL));
+
+		set_reg_field_value(
+			legacy_lut_control,
+			1,
+			INPUT_GAMMA_CONTROL,
+			GRPH_INPUT_GAMMA_MODE);
+
+		dm_write_reg(ipp110->base.ctx,
+			DCP_REG(mmINPUT_GAMMA_CONTROL),
+			legacy_lut_control);
+	} else {
+		set_reg_field_value(
+			prescale_control,
+			1,
+			PRESCALE_GRPH_CONTROL,
+			GRPH_PRESCALE_BYPASS);
+	}
+
+	set_reg_field_value(
+		prescale_value,
+		params->scale,
+		PRESCALE_VALUES_GRPH_R,
+		GRPH_PRESCALE_SCALE_R);
+
+	set_reg_field_value(
+		prescale_value,
+		params->bias,
+		PRESCALE_VALUES_GRPH_R,
+		GRPH_PRESCALE_BIAS_R);
+
+	dm_write_reg(ipp110->base.ctx,
+		DCP_REG(mmPRESCALE_GRPH_CONTROL),
+		prescale_control);
+
+	dm_write_reg(ipp110->base.ctx,
+		DCP_REG(mmPRESCALE_VALUES_GRPH_R),
+		prescale_value);
+
+	dm_write_reg(ipp110->base.ctx,
+		DCP_REG(mmPRESCALE_VALUES_GRPH_G),
+		prescale_value);
+
+	dm_write_reg(ipp110->base.ctx,
+		DCP_REG(mmPRESCALE_VALUES_GRPH_B),
+		prescale_value);
+}
+
+static void set_lut_inc(
+	struct dce110_ipp *ipp110,
+	uint8_t inc,
+	bool is_float,
+	bool is_signed)
+{
+	const uint32_t addr = DCP_REG(mmDC_LUT_CONTROL);
+
+	uint32_t value = dm_read_reg(ipp110->base.ctx, addr);
+
+	set_reg_field_value(
+		value,
+		inc,
+		DC_LUT_CONTROL,
+		DC_LUT_INC_R);
+
+	set_reg_field_value(
+		value,
+		inc,
+		DC_LUT_CONTROL,
+		DC_LUT_INC_G);
+
+	set_reg_field_value(
+		value,
+		inc,
+		DC_LUT_CONTROL,
+		DC_LUT_INC_B);
+
+	set_reg_field_value(
+		value,
+		is_float,
+		DC_LUT_CONTROL,
+		DC_LUT_DATA_R_FLOAT_POINT_EN);
+
+	set_reg_field_value(
+		value,
+		is_float,
+		DC_LUT_CONTROL,
+		DC_LUT_DATA_G_FLOAT_POINT_EN);
+
+	set_reg_field_value(
+		value,
+		is_float,
+		DC_LUT_CONTROL,
+		DC_LUT_DATA_B_FLOAT_POINT_EN);
+
+	set_reg_field_value(
+		value,
+		is_signed,
+		DC_LUT_CONTROL,
+		DC_LUT_DATA_R_SIGNED_EN);
+
+	set_reg_field_value(
+		value,
+		is_signed,
+		DC_LUT_CONTROL,
+		DC_LUT_DATA_G_SIGNED_EN);
+
+	set_reg_field_value(
+		value,
+		is_signed,
+		DC_LUT_CONTROL,
+		DC_LUT_DATA_B_SIGNED_EN);
+
+	dm_write_reg(ipp110->base.ctx, addr, value);
+}
+
+void dce110_helper_select_lut(struct dce110_ipp *ipp110)
+{
+	uint32_t value = 0;
+
+	set_lut_inc(ipp110, 0, false, false);
+
+	{
+		const uint32_t addr = DCP_REG(mmDC_LUT_WRITE_EN_MASK);
+
+		value = dm_read_reg(ipp110->base.ctx, addr);
+
+		/* enable all */
+		set_reg_field_value(
+			value,
+			0x7,
+			DC_LUT_WRITE_EN_MASK,
+			DC_LUT_WRITE_EN_MASK);
+
+		dm_write_reg(ipp110->base.ctx, addr, value);
+	}
+
+	{
+		const uint32_t addr = DCP_REG(mmDC_LUT_RW_MODE);
+
+		value = dm_read_reg(ipp110->base.ctx, addr);
+
+		set_reg_field_value(
+			value,
+			0,
+			DC_LUT_RW_MODE,
+			DC_LUT_RW_MODE);
+
+		dm_write_reg(ipp110->base.ctx, addr, value);
+	}
+
+	{
+		const uint32_t addr = DCP_REG(mmDC_LUT_CONTROL);
+
+		value = dm_read_reg(ipp110->base.ctx, addr);
+
+		/* 00 - new u0.12 */
+		set_reg_field_value(
+			value,
+			3,
+			DC_LUT_CONTROL,
+			DC_LUT_DATA_R_FORMAT);
+
+		set_reg_field_value(
+			value,
+			3,
+			DC_LUT_CONTROL,
+			DC_LUT_DATA_G_FORMAT);
+
+		set_reg_field_value(
+			value,
+			3,
+			DC_LUT_CONTROL,
+			DC_LUT_DATA_B_FORMAT);
+
+		dm_write_reg(ipp110->base.ctx, addr, value);
+	}
+
+	{
+		const uint32_t addr = DCP_REG(mmDC_LUT_RW_INDEX);
+
+		value = dm_read_reg(ipp110->base.ctx, addr);
+
+		set_reg_field_value(
+			value,
+			0,
+			DC_LUT_RW_INDEX,
+			DC_LUT_RW_INDEX);
+
+		dm_write_reg(ipp110->base.ctx, addr, value);
+	}
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_mem_input.c b/drivers/gpu/drm/amd/display/dc/dce110/dce110_mem_input.c
new file mode 100644
index 0000000..c0a68c6
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_mem_input.c
@@ -0,0 +1,535 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+#include "dm_services.h"
+
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+/* TODO: this needs to be looked at, used by Stella's workaround*/
+#include "gmc/gmc_8_2_d.h"
+#include "gmc/gmc_8_2_sh_mask.h"
+
+#include "include/logger_interface.h"
+
+#include "dce110_mem_input.h"
+
+#define DCP_REG(reg) (reg + mem_input110->offsets.dcp)
+#define DMIF_REG(reg) (reg + mem_input110->offsets.dmif)
+#define PIPE_REG(reg) (reg + mem_input110->offsets.pipe)
+
+static void program_sec_addr(
+	struct dce110_mem_input *mem_input110,
+	PHYSICAL_ADDRESS_LOC address)
+{
+	uint32_t value = 0;
+	uint32_t temp;
+
+	/*high register MUST be programmed first*/
+	temp = address.high_part &
+		GRPH_SECONDARY_SURFACE_ADDRESS_HIGH__GRPH_SECONDARY_SURFACE_ADDRESS_HIGH_MASK;
+	set_reg_field_value(value, temp,
+		GRPH_SECONDARY_SURFACE_ADDRESS_HIGH,
+		GRPH_SECONDARY_SURFACE_ADDRESS_HIGH);
+	dm_write_reg(mem_input110->base.ctx,
+				 DCP_REG(mmGRPH_SECONDARY_SURFACE_ADDRESS_HIGH), value);
+
+	value = 0;
+	temp = address.low_part >>
+		GRPH_SECONDARY_SURFACE_ADDRESS__GRPH_SECONDARY_SURFACE_ADDRESS__SHIFT;
+	set_reg_field_value(value, temp,
+		GRPH_SECONDARY_SURFACE_ADDRESS,
+		GRPH_SECONDARY_SURFACE_ADDRESS);
+	dm_write_reg(mem_input110->base.ctx,
+				 DCP_REG(mmGRPH_SECONDARY_SURFACE_ADDRESS), value);
+}
+
+static void program_pri_addr(
+	struct dce110_mem_input *mem_input110,
+	PHYSICAL_ADDRESS_LOC address)
+{
+	uint32_t value = 0;
+	uint32_t temp;
+
+	/*high register MUST be programmed first*/
+	temp = address.high_part &
+		GRPH_PRIMARY_SURFACE_ADDRESS_HIGH__GRPH_PRIMARY_SURFACE_ADDRESS_HIGH_MASK;
+	set_reg_field_value(value, temp,
+		GRPH_PRIMARY_SURFACE_ADDRESS_HIGH,
+		GRPH_PRIMARY_SURFACE_ADDRESS_HIGH);
+	dm_write_reg(mem_input110->base.ctx,
+				 DCP_REG(mmGRPH_PRIMARY_SURFACE_ADDRESS_HIGH), value);
+
+	value = 0;
+	temp = address.low_part >>
+		GRPH_PRIMARY_SURFACE_ADDRESS__GRPH_PRIMARY_SURFACE_ADDRESS__SHIFT;
+	set_reg_field_value(value, temp,
+		GRPH_PRIMARY_SURFACE_ADDRESS,
+		GRPH_PRIMARY_SURFACE_ADDRESS);
+	dm_write_reg(mem_input110->base.ctx,
+				 DCP_REG(mmGRPH_PRIMARY_SURFACE_ADDRESS), value);
+}
+
+bool dce110_mem_input_is_flip_pending(struct mem_input *mem_input)
+{
+	struct dce110_mem_input *mem_input110 = TO_DCE110_MEM_INPUT(mem_input);
+	uint32_t value;
+
+	value = dm_read_reg(mem_input110->base.ctx, DCP_REG(mmGRPH_UPDATE));
+
+	if (get_reg_field_value(value, GRPH_UPDATE,
+			GRPH_SURFACE_UPDATE_PENDING))
+		return true;
+
+	mem_input->current_address = mem_input->request_address;
+	return false;
+}
+
+bool dce110_mem_input_program_surface_flip_and_addr(
+	struct mem_input *mem_input,
+	const struct dc_plane_address *address,
+	bool flip_immediate)
+{
+	struct dce110_mem_input *mem_input110 = TO_DCE110_MEM_INPUT(mem_input);
+
+	uint32_t value = 0;
+
+	value = dm_read_reg(mem_input110->base.ctx, DCP_REG(mmGRPH_FLIP_CONTROL));
+	if (flip_immediate) {
+		set_reg_field_value(value, 1, GRPH_FLIP_CONTROL, GRPH_SURFACE_UPDATE_IMMEDIATE_EN);
+		set_reg_field_value(value, 1, GRPH_FLIP_CONTROL, GRPH_SURFACE_UPDATE_H_RETRACE_EN);
+	} else {
+		set_reg_field_value(value, 0, GRPH_FLIP_CONTROL, GRPH_SURFACE_UPDATE_IMMEDIATE_EN);
+		set_reg_field_value(value, 0, GRPH_FLIP_CONTROL, GRPH_SURFACE_UPDATE_H_RETRACE_EN);
+	}
+	dm_write_reg(mem_input110->base.ctx, DCP_REG(mmGRPH_FLIP_CONTROL), value);
+
+	switch (address->type) {
+	case PLN_ADDR_TYPE_GRAPHICS:
+		if (address->grph.addr.quad_part == 0)
+			break;
+		program_pri_addr(mem_input110, address->grph.addr);
+		break;
+	case PLN_ADDR_TYPE_GRPH_STEREO:
+		if (address->grph_stereo.left_addr.quad_part == 0
+			|| address->grph_stereo.right_addr.quad_part == 0)
+			break;
+		program_pri_addr(mem_input110, address->grph_stereo.left_addr);
+		program_sec_addr(mem_input110, address->grph_stereo.right_addr);
+		break;
+	default:
+		/* not supported */
+		BREAK_TO_DEBUGGER();
+		break;
+	}
+
+	mem_input->request_address = *address;
+	if (flip_immediate)
+		mem_input->current_address = *address;
+
+	return true;
+}
+
+/* Scatter Gather param tables */
+static const unsigned int dvmm_Hw_Setting_2DTiling[4][9] = {
+		{  8, 64, 64,  8,  8, 1, 4, 0, 0},
+		{ 16, 64, 32,  8, 16, 1, 8, 0, 0},
+		{ 32, 32, 32, 16, 16, 1, 8, 0, 0},
+		{ 64,  8, 32, 16, 16, 1, 8, 0, 0}, /* fake */
+};
+
+static const unsigned int dvmm_Hw_Setting_1DTiling[4][9] = {
+		{  8, 512, 8, 1, 0, 1, 0, 0, 0},  /* 0 for invalid */
+		{ 16, 256, 8, 2, 0, 1, 0, 0, 0},
+		{ 32, 128, 8, 4, 0, 1, 0, 0, 0},
+		{ 64,  64, 8, 4, 0, 1, 0, 0, 0}, /* fake */
+};
+
+static const unsigned int dvmm_Hw_Setting_Linear[4][9] = {
+		{  8, 4096, 1, 8, 0, 1, 0, 0, 0},
+		{ 16, 2048, 1, 8, 0, 1, 0, 0, 0},
+		{ 32, 1024, 1, 8, 0, 1, 0, 0, 0},
+		{ 64,  512, 1, 8, 0, 1, 0, 0, 0}, /* new for 64bpp from HW */
+};
+
+/* Helper to get table entry from surface info */
+static const unsigned int *get_dvmm_hw_setting(
+		union dc_tiling_info *tiling_info,
+		enum surface_pixel_format format)
+{
+	enum bits_per_pixel {
+		bpp_8 = 0,
+		bpp_16,
+		bpp_32,
+		bpp_64
+	} bpp;
+
+	if (format >= SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616)
+		bpp = bpp_64;
+	else if (format >= SURFACE_PIXEL_FORMAT_GRPH_ARGB8888)
+		bpp = bpp_32;
+	else if (format >= SURFACE_PIXEL_FORMAT_GRPH_ARGB1555)
+		bpp = bpp_16;
+	else
+		bpp = bpp_8;
+
+	switch (tiling_info->gfx8.array_mode) {
+	case DC_ARRAY_1D_TILED_THIN1:
+	case DC_ARRAY_1D_TILED_THICK:
+	case DC_ARRAY_PRT_TILED_THIN1:
+		return dvmm_Hw_Setting_1DTiling[bpp];
+	case DC_ARRAY_2D_TILED_THIN1:
+	case DC_ARRAY_2D_TILED_THICK:
+	case DC_ARRAY_2D_TILED_X_THICK:
+	case DC_ARRAY_PRT_2D_TILED_THIN1:
+	case DC_ARRAY_PRT_2D_TILED_THICK:
+		return dvmm_Hw_Setting_2DTiling[bpp];
+	case DC_ARRAY_LINEAR_GENERAL:
+	case DC_ARRAY_LINEAR_ALLIGNED:
+		return dvmm_Hw_Setting_Linear[bpp];
+	default:
+		return dvmm_Hw_Setting_2DTiling[bpp];
+	}
+}
+
+bool dce110_mem_input_program_pte_vm(
+		struct mem_input *mem_input,
+		enum surface_pixel_format format,
+		union dc_tiling_info *tiling_info,
+		enum dc_rotation_angle rotation)
+{
+	struct dce110_mem_input *mem_input110 = TO_DCE110_MEM_INPUT(mem_input);
+	const unsigned int *pte = get_dvmm_hw_setting(tiling_info, format);
+
+	unsigned int page_width = 0;
+	unsigned int page_height = 0;
+	unsigned int temp_page_width = pte[1];
+	unsigned int temp_page_height = pte[2];
+	unsigned int min_pte_before_flip = 0;
+	uint32_t value = 0;
+
+	while ((temp_page_width >>= 1) != 0)
+		page_width++;
+	while ((temp_page_height >>= 1) != 0)
+		page_height++;
+
+	switch (rotation) {
+	case ROTATION_ANGLE_90:
+	case ROTATION_ANGLE_270:
+		min_pte_before_flip = pte[4];
+		break;
+	default:
+		min_pte_before_flip = pte[3];
+		break;
+	}
+
+	value = dm_read_reg(mem_input110->base.ctx, DCP_REG(mmGRPH_PIPE_OUTSTANDING_REQUEST_LIMIT));
+	set_reg_field_value(value, 0xff, GRPH_PIPE_OUTSTANDING_REQUEST_LIMIT, GRPH_PIPE_OUTSTANDING_REQUEST_LIMIT);
+	dm_write_reg(mem_input110->base.ctx, DCP_REG(mmGRPH_PIPE_OUTSTANDING_REQUEST_LIMIT), value);
+
+	value = dm_read_reg(mem_input110->base.ctx, DCP_REG(mmDVMM_PTE_CONTROL));
+	set_reg_field_value(value, page_width, DVMM_PTE_CONTROL, DVMM_PAGE_WIDTH);
+	set_reg_field_value(value, page_height, DVMM_PTE_CONTROL, DVMM_PAGE_HEIGHT);
+	set_reg_field_value(value, min_pte_before_flip, DVMM_PTE_CONTROL, DVMM_MIN_PTE_BEFORE_FLIP);
+	dm_write_reg(mem_input110->base.ctx, DCP_REG(mmDVMM_PTE_CONTROL), value);
+
+	value = dm_read_reg(mem_input110->base.ctx, DCP_REG(mmDVMM_PTE_ARB_CONTROL));
+	set_reg_field_value(value, pte[5], DVMM_PTE_ARB_CONTROL, DVMM_PTE_REQ_PER_CHUNK);
+	set_reg_field_value(value, 0xff, DVMM_PTE_ARB_CONTROL, DVMM_MAX_PTE_REQ_OUTSTANDING);
+	dm_write_reg(mem_input110->base.ctx, DCP_REG(mmDVMM_PTE_ARB_CONTROL), value);
+
+	return true;
+}
+
+static void program_urgency_watermark(
+	const struct dc_context *ctx,
+	const uint32_t offset,
+	struct bw_watermarks marks_low,
+	uint32_t total_dest_line_time_ns)
+{
+	/* register value */
+	uint32_t urgency_cntl = 0;
+	uint32_t wm_mask_cntl = 0;
+
+	uint32_t urgency_addr = offset + mmDPG_PIPE_URGENCY_CONTROL;
+	uint32_t wm_addr = offset + mmDPG_WATERMARK_MASK_CONTROL;
+
+	/*Write mask to enable reading/writing of watermark set A*/
+	wm_mask_cntl = dm_read_reg(ctx, wm_addr);
+	set_reg_field_value(wm_mask_cntl,
+			1,
+			DPG_WATERMARK_MASK_CONTROL,
+			URGENCY_WATERMARK_MASK);
+	dm_write_reg(ctx, wm_addr, wm_mask_cntl);
+
+	urgency_cntl = dm_read_reg(ctx, urgency_addr);
+
+	set_reg_field_value(
+		urgency_cntl,
+		marks_low.d_mark,
+		DPG_PIPE_URGENCY_CONTROL,
+		URGENCY_LOW_WATERMARK);
+
+	set_reg_field_value(
+		urgency_cntl,
+		total_dest_line_time_ns,
+		DPG_PIPE_URGENCY_CONTROL,
+		URGENCY_HIGH_WATERMARK);
+	dm_write_reg(ctx, urgency_addr, urgency_cntl);
+
+	/*Write mask to enable reading/writing of watermark set B*/
+	wm_mask_cntl = dm_read_reg(ctx, wm_addr);
+	set_reg_field_value(wm_mask_cntl,
+			2,
+			DPG_WATERMARK_MASK_CONTROL,
+			URGENCY_WATERMARK_MASK);
+	dm_write_reg(ctx, wm_addr, wm_mask_cntl);
+
+	urgency_cntl = dm_read_reg(ctx, urgency_addr);
+
+	set_reg_field_value(urgency_cntl,
+		marks_low.a_mark,
+		DPG_PIPE_URGENCY_CONTROL,
+		URGENCY_LOW_WATERMARK);
+
+	set_reg_field_value(urgency_cntl,
+		total_dest_line_time_ns,
+		DPG_PIPE_URGENCY_CONTROL,
+		URGENCY_HIGH_WATERMARK);
+	dm_write_reg(ctx, urgency_addr, urgency_cntl);
+}
+
+static void program_stutter_watermark(
+	const struct dc_context *ctx,
+	const uint32_t offset,
+	struct bw_watermarks marks)
+{
+	/* register value */
+	uint32_t stutter_cntl = 0;
+	uint32_t wm_mask_cntl = 0;
+
+	uint32_t stutter_addr = offset + mmDPG_PIPE_STUTTER_CONTROL;
+	uint32_t wm_addr = offset + mmDPG_WATERMARK_MASK_CONTROL;
+
+	/*Write mask to enable reading/writing of watermark set A*/
+
+	wm_mask_cntl = dm_read_reg(ctx, wm_addr);
+	set_reg_field_value(wm_mask_cntl,
+		1,
+		DPG_WATERMARK_MASK_CONTROL,
+		STUTTER_EXIT_SELF_REFRESH_WATERMARK_MASK);
+	dm_write_reg(ctx, wm_addr, wm_mask_cntl);
+
+	stutter_cntl = dm_read_reg(ctx, stutter_addr);
+
+	if (ctx->dc->debug.disable_stutter) {
+		set_reg_field_value(stutter_cntl,
+			0,
+			DPG_PIPE_STUTTER_CONTROL,
+			STUTTER_ENABLE);
+	} else {
+		set_reg_field_value(stutter_cntl,
+			1,
+			DPG_PIPE_STUTTER_CONTROL,
+			STUTTER_ENABLE);
+	}
+
+	set_reg_field_value(stutter_cntl,
+		1,
+		DPG_PIPE_STUTTER_CONTROL,
+		STUTTER_IGNORE_FBC);
+
+	/*Write watermark set A*/
+	set_reg_field_value(stutter_cntl,
+		marks.d_mark,
+		DPG_PIPE_STUTTER_CONTROL,
+		STUTTER_EXIT_SELF_REFRESH_WATERMARK);
+	dm_write_reg(ctx, stutter_addr, stutter_cntl);
+
+	/*Write mask to enable reading/writing of watermark set B*/
+	wm_mask_cntl = dm_read_reg(ctx, wm_addr);
+	set_reg_field_value(wm_mask_cntl,
+		2,
+		DPG_WATERMARK_MASK_CONTROL,
+		STUTTER_EXIT_SELF_REFRESH_WATERMARK_MASK);
+	dm_write_reg(ctx, wm_addr, wm_mask_cntl);
+
+	stutter_cntl = dm_read_reg(ctx, stutter_addr);
+
+	/*Write watermark set B*/
+	set_reg_field_value(stutter_cntl,
+		marks.a_mark,
+		DPG_PIPE_STUTTER_CONTROL,
+		STUTTER_EXIT_SELF_REFRESH_WATERMARK);
+	dm_write_reg(ctx, stutter_addr, stutter_cntl);
+}
+
+static void program_nbp_watermark(
+	const struct dc_context *ctx,
+	const uint32_t offset,
+	struct bw_watermarks marks)
+{
+	uint32_t value;
+	uint32_t addr;
+	/* Write mask to enable reading/writing of watermark set A */
+	addr = offset + mmDPG_WATERMARK_MASK_CONTROL;
+	value = dm_read_reg(ctx, addr);
+	set_reg_field_value(
+		value,
+		1,
+		DPG_WATERMARK_MASK_CONTROL,
+		NB_PSTATE_CHANGE_WATERMARK_MASK);
+	dm_write_reg(ctx, addr, value);
+
+	addr = offset + mmDPG_PIPE_NB_PSTATE_CHANGE_CONTROL;
+	value = dm_read_reg(ctx, addr);
+	set_reg_field_value(
+		value,
+		1,
+		DPG_PIPE_NB_PSTATE_CHANGE_CONTROL,
+		NB_PSTATE_CHANGE_ENABLE);
+	set_reg_field_value(
+		value,
+		1,
+		DPG_PIPE_NB_PSTATE_CHANGE_CONTROL,
+		NB_PSTATE_CHANGE_URGENT_DURING_REQUEST);
+	set_reg_field_value(
+		value,
+		1,
+		DPG_PIPE_NB_PSTATE_CHANGE_CONTROL,
+		NB_PSTATE_CHANGE_NOT_SELF_REFRESH_DURING_REQUEST);
+	dm_write_reg(ctx, addr, value);
+
+	/* Write watermark set A */
+	value = dm_read_reg(ctx, addr);
+	set_reg_field_value(
+		value,
+		marks.d_mark,
+		DPG_PIPE_NB_PSTATE_CHANGE_CONTROL,
+		NB_PSTATE_CHANGE_WATERMARK);
+	dm_write_reg(ctx, addr, value);
+
+	/* Write mask to enable reading/writing of watermark set B */
+	addr = offset + mmDPG_WATERMARK_MASK_CONTROL;
+	value = dm_read_reg(ctx, addr);
+	set_reg_field_value(
+		value,
+		2,
+		DPG_WATERMARK_MASK_CONTROL,
+		NB_PSTATE_CHANGE_WATERMARK_MASK);
+	dm_write_reg(ctx, addr, value);
+
+	addr = offset + mmDPG_PIPE_NB_PSTATE_CHANGE_CONTROL;
+	value = dm_read_reg(ctx, addr);
+	set_reg_field_value(
+		value,
+		1,
+		DPG_PIPE_NB_PSTATE_CHANGE_CONTROL,
+		NB_PSTATE_CHANGE_ENABLE);
+	set_reg_field_value(
+		value,
+		1,
+		DPG_PIPE_NB_PSTATE_CHANGE_CONTROL,
+		NB_PSTATE_CHANGE_URGENT_DURING_REQUEST);
+	set_reg_field_value(
+		value,
+		1,
+		DPG_PIPE_NB_PSTATE_CHANGE_CONTROL,
+		NB_PSTATE_CHANGE_NOT_SELF_REFRESH_DURING_REQUEST);
+	dm_write_reg(ctx, addr, value);
+
+	/* Write watermark set B */
+	value = dm_read_reg(ctx, addr);
+	set_reg_field_value(
+		value,
+		marks.a_mark,
+		DPG_PIPE_NB_PSTATE_CHANGE_CONTROL,
+		NB_PSTATE_CHANGE_WATERMARK);
+	dm_write_reg(ctx, addr, value);
+}
+
+void dce110_mem_input_program_display_marks(
+	struct mem_input *mem_input,
+	struct bw_watermarks nbp,
+	struct bw_watermarks stutter,
+	struct bw_watermarks urgent,
+	uint32_t total_dest_line_time_ns)
+{
+	struct dce110_mem_input *bm_dce110 = TO_DCE110_MEM_INPUT(mem_input);
+
+	program_urgency_watermark(
+		mem_input->ctx,
+		bm_dce110->offsets.dmif,
+		urgent,
+		total_dest_line_time_ns);
+
+	program_nbp_watermark(
+		mem_input->ctx,
+		bm_dce110->offsets.dmif,
+		nbp);
+
+	program_stutter_watermark(
+		mem_input->ctx,
+		bm_dce110->offsets.dmif,
+		stutter);
+}
+
+static struct mem_input_funcs dce110_mem_input_funcs = {
+	.mem_input_program_display_marks =
+			dce110_mem_input_program_display_marks,
+	.allocate_mem_input = dce_mem_input_allocate_dmif,
+	.free_mem_input = dce_mem_input_free_dmif,
+	.mem_input_program_surface_flip_and_addr =
+			dce110_mem_input_program_surface_flip_and_addr,
+	.mem_input_program_pte_vm =
+			dce110_mem_input_program_pte_vm,
+	.mem_input_program_surface_config =
+			dce_mem_input_program_surface_config,
+	.mem_input_is_flip_pending =
+			dce110_mem_input_is_flip_pending,
+	.mem_input_update_dchub = NULL
+};
+/*****************************************/
+/* Constructor, Destructor               */
+/*****************************************/
+
+bool dce110_mem_input_construct(
+	struct dce110_mem_input *mem_input110,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_mem_input_reg_offsets *offsets)
+{
+	/* supported stutter method
+	 * STUTTER_MODE_ENHANCED
+	 * STUTTER_MODE_QUAD_DMIF_BUFFER
+	 * STUTTER_MODE_WATERMARK_NBP_STATE
+	 */
+	mem_input110->base.funcs = &dce110_mem_input_funcs;
+	mem_input110->base.ctx = ctx;
+
+	mem_input110->base.inst = inst;
+
+	mem_input110->offsets = *offsets;
+
+	return true;
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_mem_input.h b/drivers/gpu/drm/amd/display/dc/dce110/dce110_mem_input.h
new file mode 100644
index 0000000..83b2df9
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_mem_input.h
@@ -0,0 +1,131 @@
+/* Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DC_MEM_INPUT_DCE110_H__
+#define __DC_MEM_INPUT_DCE110_H__
+
+#include "mem_input.h"
+
+#define TO_DCE110_MEM_INPUT(mi)\
+	container_of(mi, struct dce110_mem_input, base)
+
+struct dce110_mem_input_reg_offsets {
+	uint32_t dcp;
+	uint32_t dmif;
+	uint32_t pipe;
+};
+
+struct dce110_mem_input {
+	struct mem_input base;
+	struct dce110_mem_input_reg_offsets offsets;
+};
+
+bool dce110_mem_input_construct(
+	struct dce110_mem_input *mem_input110,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_mem_input_reg_offsets *offsets);
+
+/*
+ * dce110_mem_input_program_display_marks
+ *
+ * This function will program nbp stutter and urgency watermarks to minimum
+ * allowable values
+ */
+void dce110_mem_input_program_display_marks(
+	struct mem_input *mem_input,
+	struct bw_watermarks nbp,
+	struct bw_watermarks stutter,
+	struct bw_watermarks urgent,
+	uint32_t total_dest_line_time_ns);
+
+/*
+ * dce110_allocate_mem_input
+ *
+ * This function will allocate a dmif buffer and program required
+ * pixel duration for pipe
+ */
+void dce110_allocate_mem_input(
+	struct mem_input *mem_input,
+	uint32_t h_total,/* for current stream */
+	uint32_t v_total,/* for current stream */
+	uint32_t pix_clk_khz,/* for current stream */
+	uint32_t total_stream_num);
+
+/*
+ * dce110_free_mem_input
+ *
+ * This function will deallocate a dmif buffer from pipe
+ */
+void dce110_free_mem_input(
+	struct mem_input *mem_input,
+	uint32_t total_stream_num);
+
+/*
+ * dce110_mem_input_program_surface_flip_and_addr
+ *
+ * This function programs hsync/vsync mode and surface address
+ */
+bool dce110_mem_input_program_surface_flip_and_addr(
+	struct mem_input *mem_input,
+	const struct dc_plane_address *address,
+	bool flip_immediate);
+
+/*
+ * dce110_mem_input_program_surface_config
+ *
+ * This function will program surface tiling, size, rotation and pixel format
+ * to corresponding dcp registers.
+ */
+bool  dce110_mem_input_program_surface_config(
+	struct mem_input *mem_input,
+	enum surface_pixel_format format,
+	union dc_tiling_info *tiling_info,
+	union plane_size *plane_size,
+	enum dc_rotation_angle rotation,
+	struct dc_plane_dcc_param *dcc,
+	bool horizontal_mirror);
+
+/*
+ * dce110_mem_input_program_pte_vm
+ *
+ * This function will program pte vm registers.
+ */
+bool  dce110_mem_input_program_pte_vm(
+	struct mem_input *mem_input,
+	enum surface_pixel_format format,
+	union dc_tiling_info *tiling_info,
+	enum dc_rotation_angle rotation);
+
+/*
+ * dce110_mem_input_is_flip_pending
+ *
+ * This function will wait until the surface update-pending bit is cleared.
+ * This is necessary when a flip immediate call is requested as we shouldn't
+ * return until the flip has actually occurred.
+ */
+bool dce110_mem_input_is_flip_pending(
+	struct mem_input *mem_input);
+
+#endif
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.c b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.c
new file mode 100644
index 0000000..698ec2f
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.c
@@ -0,0 +1,77 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+/* include DCE11 register header files */
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+
+#include "dce110_opp.h"
+
+#include "gamma_types.h"
+
+enum {
+	MAX_LUT_ENTRY = 256,
+	MAX_NUMBER_OF_ENTRIES = 256
+};
+
+/*****************************************/
+/* Constructor, Destructor               */
+/*****************************************/
+
+static const struct opp_funcs funcs = {
+		.opp_power_on_regamma_lut = dce110_opp_power_on_regamma_lut,
+		.opp_set_csc_adjustment = dce110_opp_set_csc_adjustment,
+		.opp_set_csc_default = dce110_opp_set_csc_default,
+		.opp_set_dyn_expansion = dce110_opp_set_dyn_expansion,
+		.opp_program_regamma_pwl = dce110_opp_program_regamma_pwl,
+		.opp_set_regamma_mode = dce110_opp_set_regamma_mode,
+		.opp_destroy = dce110_opp_destroy,
+		.opp_program_fmt = dce110_opp_program_fmt,
+};
+
+bool dce110_opp_construct(struct dce110_opp *opp110,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_opp_reg_offsets *offsets)
+{
+	opp110->base.funcs = &funcs;
+
+	opp110->base.ctx = ctx;
+
+	opp110->base.inst = inst;
+
+	opp110->offsets = *offsets;
+
+	return true;
+}
+
+void dce110_opp_destroy(struct output_pixel_processor **opp)
+{
+	dm_free(FROM_DCE11_OPP(*opp));
+	*opp = NULL;
+}
+
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.h b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.h
new file mode 100644
index 0000000..2fbb241
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp.h
@@ -0,0 +1,149 @@
+/* Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DC_OPP_DCE110_H__
+#define __DC_OPP_DCE110_H__
+
+#include "dc_types.h"
+#include "opp.h"
+#include "core_types.h"
+
+#include "gamma_types.h" /* decprecated */
+
+struct gamma_parameters;
+
+#define FROM_DCE11_OPP(opp)\
+	container_of(opp, struct dce110_opp, base)
+
+enum dce110_opp_reg_type {
+	DCE110_OPP_REG_DCP = 0,
+	DCE110_OPP_REG_DCFE,
+	DCE110_OPP_REG_FMT,
+
+	DCE110_OPP_REG_MAX
+};
+
+struct dce110_regamma {
+	struct gamma_curve arr_curve_points[16];
+	struct curve_points arr_points[3];
+	uint32_t hw_points_num;
+	struct hw_x_point *coordinates_x;
+	struct pwl_result_data *rgb_resulted;
+
+	/* re-gamma curve */
+	struct pwl_float_data_ex *rgb_regamma;
+	/* coeff used to map user evenly distributed points
+	 * to our hardware points (predefined) for gamma 256 */
+	struct pixel_gamma_point *coeff128;
+	struct pixel_gamma_point *coeff128_oem;
+	/* coeff used to map user evenly distributed points
+	 * to our hardware points (predefined) for gamma 1025 */
+	struct pixel_gamma_point *coeff128_dx;
+	/* evenly distributed points, gamma 256 software points 0-255 */
+	struct gamma_pixel *axis_x_256;
+	/* evenly distributed points, gamma 1025 software points 0-1025 */
+	struct gamma_pixel *axis_x_1025;
+	/* OEM supplied gamma for regamma LUT */
+	struct pwl_float_data *rgb_oem;
+	/* user supplied gamma */
+	struct pwl_float_data *rgb_user;
+	uint32_t extra_points;
+	bool use_half_points;
+	struct fixed31_32 x_max1;
+	struct fixed31_32 x_max2;
+	struct fixed31_32 x_min;
+	struct fixed31_32 divider1;
+	struct fixed31_32 divider2;
+	struct fixed31_32 divider3;
+};
+
+/* OPP RELATED */
+#define TO_DCE110_OPP(opp)\
+	container_of(opp, struct dce110_opp, base)
+
+struct dce110_opp_reg_offsets {
+	uint32_t fmt_offset;
+	uint32_t fmt_mem_offset;
+	uint32_t dcp_offset;
+	uint32_t dcfe_offset;
+};
+
+struct dce110_opp {
+	struct output_pixel_processor base;
+	struct dce110_opp_reg_offsets offsets;
+	struct dce110_regamma regamma;
+};
+
+bool dce110_opp_construct(struct dce110_opp *opp110,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_opp_reg_offsets *offsets);
+
+void dce110_opp_destroy(struct output_pixel_processor **opp);
+
+/* REGAMMA RELATED */
+void dce110_opp_power_on_regamma_lut(
+	struct output_pixel_processor *opp,
+	bool power_on);
+
+bool dce110_opp_program_regamma_pwl(
+	struct output_pixel_processor *opp,
+	const struct pwl_params *params);
+
+void dce110_opp_set_regamma_mode(struct output_pixel_processor *opp,
+		enum opp_regamma mode);
+
+void dce110_opp_set_csc_adjustment(
+	struct output_pixel_processor *opp,
+	const struct out_csc_color_matrix *tbl_entry);
+
+void dce110_opp_set_csc_default(
+	struct output_pixel_processor *opp,
+	const struct default_adjustment *default_adjust);
+
+/* FORMATTER RELATED */
+void dce110_opp_program_bit_depth_reduction(
+	struct output_pixel_processor *opp,
+	const struct bit_depth_reduction_params *params);
+
+void dce110_opp_program_clamping_and_pixel_encoding(
+	struct output_pixel_processor *opp,
+	const struct clamping_and_pixel_encoding_params *params);
+
+void dce110_opp_set_dyn_expansion(
+	struct output_pixel_processor *opp,
+	enum dc_color_space color_sp,
+	enum dc_color_depth color_dpth,
+	enum signal_type signal);
+
+void dce110_opp_program_fmt(
+	struct output_pixel_processor *opp,
+	struct bit_depth_reduction_params *fmt_bit_depth,
+	struct clamping_and_pixel_encoding_params *clamping);
+
+void dce110_opp_set_clamping(
+	struct dce110_opp *opp110,
+	const struct clamping_and_pixel_encoding_params *params);
+
+#endif
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_csc.c b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_csc.c
new file mode 100644
index 0000000..b46db20
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_csc.c
@@ -0,0 +1,363 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ *  and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "dce110_opp.h"
+#include "basics/conversion.h"
+
+/* include DCE11 register header files */
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+
+#define DCP_REG(reg)\
+	(reg + opp110->offsets.dcp_offset)
+
+enum {
+	OUTPUT_CSC_MATRIX_SIZE = 12
+};
+
+static const struct out_csc_color_matrix global_color_matrix[] = {
+{ COLOR_SPACE_SRGB,
+	{ 0x2000, 0, 0, 0, 0, 0x2000, 0, 0, 0, 0, 0x2000, 0} },
+{ COLOR_SPACE_SRGB_LIMITED,
+	{ 0x1B60, 0, 0, 0x200, 0, 0x1B60, 0, 0x200, 0, 0, 0x1B60, 0x200} },
+{ COLOR_SPACE_YCBCR601,
+	{ 0xE00, 0xF447, 0xFDB9, 0x1000, 0x82F, 0x1012, 0x31F, 0x200, 0xFB47,
+		0xF6B9, 0xE00, 0x1000} },
+{ COLOR_SPACE_YCBCR709, { 0xE00, 0xF349, 0xFEB7, 0x1000, 0x5D2, 0x1394, 0x1FA,
+	0x200, 0xFCCB, 0xF535, 0xE00, 0x1000} },
+/* TODO: correct values below */
+{ COLOR_SPACE_YCBCR601_LIMITED, { 0xE00, 0xF447, 0xFDB9, 0x1000, 0x991,
+	0x12C9, 0x3A6, 0x200, 0xFB47, 0xF6B9, 0xE00, 0x1000} },
+{ COLOR_SPACE_YCBCR709_LIMITED, { 0xE00, 0xF349, 0xFEB7, 0x1000, 0x6CE, 0x16E3,
+	0x24F, 0x200, 0xFCCB, 0xF535, 0xE00, 0x1000} }
+};
+
+enum csc_color_mode {
+	/* 00 - BITS2:0 Bypass */
+	CSC_COLOR_MODE_GRAPHICS_BYPASS,
+	/* 01 - hard coded coefficient TV RGB */
+	CSC_COLOR_MODE_GRAPHICS_PREDEFINED,
+	/* 04 - programmable OUTPUT CSC coefficient */
+	CSC_COLOR_MODE_GRAPHICS_OUTPUT_CSC,
+};
+
+static void program_color_matrix(
+	struct dce110_opp *opp110,
+	const struct out_csc_color_matrix *tbl_entry,
+	enum grph_color_adjust_option options)
+{
+	struct dc_context *ctx = opp110->base.ctx;
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C11_C12);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[0],
+			OUTPUT_CSC_C11_C12,
+			OUTPUT_CSC_C11);
+
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[1],
+			OUTPUT_CSC_C11_C12,
+			OUTPUT_CSC_C12);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C13_C14);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[2],
+			OUTPUT_CSC_C13_C14,
+			OUTPUT_CSC_C13);
+		/* fixed S0.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[3],
+			OUTPUT_CSC_C13_C14,
+			OUTPUT_CSC_C14);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C21_C22);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[4],
+			OUTPUT_CSC_C21_C22,
+			OUTPUT_CSC_C21);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[5],
+			OUTPUT_CSC_C21_C22,
+			OUTPUT_CSC_C22);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C23_C24);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[6],
+			OUTPUT_CSC_C23_C24,
+			OUTPUT_CSC_C23);
+		/* fixed S0.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[7],
+			OUTPUT_CSC_C23_C24,
+			OUTPUT_CSC_C24);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C31_C32);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[8],
+			OUTPUT_CSC_C31_C32,
+			OUTPUT_CSC_C31);
+		/* fixed S0.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[9],
+			OUTPUT_CSC_C31_C32,
+			OUTPUT_CSC_C32);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C33_C34);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[10],
+			OUTPUT_CSC_C33_C34,
+			OUTPUT_CSC_C33);
+		/* fixed S0.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[11],
+			OUTPUT_CSC_C33_C34,
+			OUTPUT_CSC_C34);
+
+		dm_write_reg(ctx, addr, value);
+	}
+}
+
+static bool configure_graphics_mode(
+	struct dce110_opp *opp110,
+	enum csc_color_mode config,
+	enum graphics_csc_adjust_type csc_adjust_type,
+	enum dc_color_space color_space)
+{
+	struct dc_context *ctx = opp110->base.ctx;
+	uint32_t addr = DCP_REG(mmOUTPUT_CSC_CONTROL);
+	uint32_t value = dm_read_reg(ctx, addr);
+
+	set_reg_field_value(
+		value,
+		0,
+		OUTPUT_CSC_CONTROL,
+		OUTPUT_CSC_GRPH_MODE);
+
+	if (csc_adjust_type == GRAPHICS_CSC_ADJUST_TYPE_SW) {
+		if (config == CSC_COLOR_MODE_GRAPHICS_OUTPUT_CSC) {
+			set_reg_field_value(
+				value,
+				4,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+		} else {
+
+			switch (color_space) {
+			case COLOR_SPACE_SRGB:
+				/* by pass */
+				set_reg_field_value(
+					value,
+					0,
+					OUTPUT_CSC_CONTROL,
+					OUTPUT_CSC_GRPH_MODE);
+				break;
+			case COLOR_SPACE_SRGB_LIMITED:
+				/* TV RGB */
+				set_reg_field_value(
+					value,
+					1,
+					OUTPUT_CSC_CONTROL,
+					OUTPUT_CSC_GRPH_MODE);
+				break;
+			case COLOR_SPACE_YCBCR601:
+			case COLOR_SPACE_YPBPR601:
+			case COLOR_SPACE_YCBCR601_LIMITED:
+				/* YCbCr601 */
+				set_reg_field_value(
+					value,
+					2,
+					OUTPUT_CSC_CONTROL,
+					OUTPUT_CSC_GRPH_MODE);
+				break;
+			case COLOR_SPACE_YCBCR709:
+			case COLOR_SPACE_YPBPR709:
+			case COLOR_SPACE_YCBCR709_LIMITED:
+				/* YCbCr709 */
+				set_reg_field_value(
+					value,
+					3,
+					OUTPUT_CSC_CONTROL,
+					OUTPUT_CSC_GRPH_MODE);
+				break;
+			default:
+				return false;
+			}
+		}
+	} else if (csc_adjust_type == GRAPHICS_CSC_ADJUST_TYPE_HW) {
+		switch (color_space) {
+		case COLOR_SPACE_SRGB:
+			/* by pass */
+			set_reg_field_value(
+				value,
+				0,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+			break;
+		case COLOR_SPACE_SRGB_LIMITED:
+			/* TV RGB */
+			set_reg_field_value(
+				value,
+				1,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+			break;
+		case COLOR_SPACE_YCBCR601:
+		case COLOR_SPACE_YPBPR601:
+		case COLOR_SPACE_YCBCR601_LIMITED:
+			/* YCbCr601 */
+			set_reg_field_value(
+				value,
+				2,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+			break;
+		case COLOR_SPACE_YCBCR709:
+		case COLOR_SPACE_YPBPR709:
+		case COLOR_SPACE_YCBCR709_LIMITED:
+			 /* YCbCr709 */
+			set_reg_field_value(
+				value,
+				3,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+			break;
+		default:
+			return false;
+		}
+
+	} else
+		/* by pass */
+		set_reg_field_value(
+			value,
+			0,
+			OUTPUT_CSC_CONTROL,
+			OUTPUT_CSC_GRPH_MODE);
+
+	addr = DCP_REG(mmOUTPUT_CSC_CONTROL);
+	dm_write_reg(ctx, addr, value);
+
+	return true;
+}
+
+void dce110_opp_set_csc_adjustment(
+	struct output_pixel_processor *opp,
+	const struct out_csc_color_matrix *tbl_entry)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+	enum csc_color_mode config =
+			CSC_COLOR_MODE_GRAPHICS_OUTPUT_CSC;
+
+	program_color_matrix(
+			opp110, tbl_entry, GRAPHICS_CSC_ADJUST_TYPE_SW);
+
+	/*  We did everything ,now program DxOUTPUT_CSC_CONTROL */
+	configure_graphics_mode(opp110, config, GRAPHICS_CSC_ADJUST_TYPE_SW,
+			tbl_entry->color_space);
+}
+
+void dce110_opp_set_csc_default(
+	struct output_pixel_processor *opp,
+	const struct default_adjustment *default_adjust)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+	enum csc_color_mode config =
+			CSC_COLOR_MODE_GRAPHICS_PREDEFINED;
+
+	if (default_adjust->force_hw_default == false) {
+		const struct out_csc_color_matrix *elm;
+		/* currently parameter not in use */
+		enum grph_color_adjust_option option =
+			GRPH_COLOR_MATRIX_HW_DEFAULT;
+		uint32_t i;
+		/*
+		 * HW default false we program locally defined matrix
+		 * HW default true  we use predefined hw matrix and we
+		 * do not need to program matrix
+		 * OEM wants the HW default via runtime parameter.
+		 */
+		option = GRPH_COLOR_MATRIX_SW;
+
+		for (i = 0; i < ARRAY_SIZE(global_color_matrix); ++i) {
+			elm = &global_color_matrix[i];
+			if (elm->color_space != default_adjust->out_color_space)
+				continue;
+			/* program the matrix with default values from this
+			 * file */
+			program_color_matrix(opp110, elm, option);
+			config = CSC_COLOR_MODE_GRAPHICS_OUTPUT_CSC;
+			break;
+		}
+	}
+
+	/* configure the what we programmed :
+	 * 1. Default values from this file
+	 * 2. Use hardware default from ROM_A and we do not need to program
+	 * matrix */
+
+	configure_graphics_mode(opp110, config,
+		default_adjust->csc_adjust_type,
+		default_adjust->out_color_space);
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_formatter.c b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_formatter.c
new file mode 100644
index 0000000..eac2077
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_formatter.c
@@ -0,0 +1,627 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ *  and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+
+#include "dce110_opp.h"
+
+#define FMT_REG(reg)\
+	(reg + opp110->offsets.fmt_offset)
+/**
+ *	set_truncation
+ *	1) set truncation depth: 0 for 18 bpp or 1 for 24 bpp
+ *	2) enable truncation
+ *	3) HW remove 12bit FMT support for DCE11 power saving reason.
+ */
+static void set_truncation(
+		struct dce110_opp *opp110,
+		const struct bit_depth_reduction_params *params)
+{
+	uint32_t value = 0;
+	uint32_t addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+
+	/*Disable truncation*/
+	value = dm_read_reg(opp110->base.ctx, addr);
+	set_reg_field_value(value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_EN);
+	set_reg_field_value(value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_DEPTH);
+	set_reg_field_value(value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_MODE);
+
+	dm_write_reg(opp110->base.ctx, addr, value);
+
+	/* no 10bpc trunc on DCE11*/
+	if (params->flags.TRUNCATE_ENABLED == 0 ||
+		params->flags.TRUNCATE_DEPTH == 2)
+		return;
+
+	/*Set truncation depth and Enable truncation*/
+	set_reg_field_value(value, 1,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_EN);
+	set_reg_field_value(value, params->flags.TRUNCATE_MODE,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_MODE);
+	set_reg_field_value(value, params->flags.TRUNCATE_DEPTH,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_DEPTH);
+
+	dm_write_reg(opp110->base.ctx, addr, value);
+
+}
+
+/**
+ *	set_spatial_dither
+ *	1) set spatial dithering mode: pattern of seed
+ *	2) set spatical dithering depth: 0 for 18bpp or 1 for 24bpp
+ *	3) set random seed
+ *	4) set random mode
+ *		lfsr is reset every frame or not reset
+ *		RGB dithering method
+ *		0: RGB data are all dithered with x^28+x^3+1
+ *		1: R data is dithered with x^28+x^3+1
+ *		G data is dithered with x^28+X^9+1
+ *		B data is dithered with x^28+x^13+1
+ *		enable high pass filter or not
+ *	5) enable spatical dithering
+ */
+static void set_spatial_dither(
+	struct dce110_opp *opp110,
+	const struct bit_depth_reduction_params *params)
+{
+	uint32_t addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+	uint32_t depth_cntl_value = 0;
+	uint32_t fmt_cntl_value = 0;
+	uint32_t dither_r_value = 0;
+	uint32_t dither_g_value = 0;
+	uint32_t dither_b_value = 0;
+
+	/*Disable spatial (random) dithering*/
+	depth_cntl_value = dm_read_reg(opp110->base.ctx, addr);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_SPATIAL_DITHER_EN);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_SPATIAL_DITHER_MODE);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_SPATIAL_DITHER_DEPTH);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_TEMPORAL_DITHER_EN);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_HIGHPASS_RANDOM_ENABLE);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_FRAME_RANDOM_ENABLE);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_RGB_RANDOM_ENABLE);
+
+	dm_write_reg(opp110->base.ctx, addr, depth_cntl_value);
+
+	/* no 10bpc on DCE11*/
+	if (params->flags.SPATIAL_DITHER_ENABLED == 0 ||
+		params->flags.SPATIAL_DITHER_DEPTH == 2)
+		return;
+
+	addr = FMT_REG(mmFMT_CONTROL);
+	fmt_cntl_value = dm_read_reg(opp110->base.ctx, addr);
+	/* only use FRAME_COUNTER_MAX if frameRandom == 1*/
+	if (params->flags.FRAME_RANDOM == 1) {
+		if (params->flags.SPATIAL_DITHER_DEPTH == 0 ||
+		params->flags.SPATIAL_DITHER_DEPTH == 1) {
+			set_reg_field_value(fmt_cntl_value, 15,
+				FMT_CONTROL,
+				FMT_SPATIAL_DITHER_FRAME_COUNTER_MAX);
+			set_reg_field_value(fmt_cntl_value, 2,
+				FMT_CONTROL,
+				FMT_SPATIAL_DITHER_FRAME_COUNTER_BIT_SWAP);
+		} else if (params->flags.SPATIAL_DITHER_DEPTH == 2) {
+			set_reg_field_value(fmt_cntl_value, 3,
+				FMT_CONTROL,
+				FMT_SPATIAL_DITHER_FRAME_COUNTER_MAX);
+			set_reg_field_value(fmt_cntl_value, 1,
+				FMT_CONTROL,
+				FMT_SPATIAL_DITHER_FRAME_COUNTER_BIT_SWAP);
+		} else
+			return;
+	} else {
+		set_reg_field_value(fmt_cntl_value, 0,
+			FMT_CONTROL,
+			FMT_SPATIAL_DITHER_FRAME_COUNTER_MAX);
+		set_reg_field_value(fmt_cntl_value, 0,
+			FMT_CONTROL,
+			FMT_SPATIAL_DITHER_FRAME_COUNTER_BIT_SWAP);
+	}
+
+	dm_write_reg(opp110->base.ctx, addr, fmt_cntl_value);
+
+	/*Set seed for random values for
+	 * spatial dithering for R,G,B channels*/
+	addr = FMT_REG(mmFMT_DITHER_RAND_R_SEED);
+	set_reg_field_value(dither_r_value, params->r_seed_value,
+		FMT_DITHER_RAND_R_SEED,
+		FMT_RAND_R_SEED);
+	dm_write_reg(opp110->base.ctx, addr, dither_r_value);
+
+	addr = FMT_REG(mmFMT_DITHER_RAND_G_SEED);
+	set_reg_field_value(dither_g_value,
+		params->g_seed_value,
+		FMT_DITHER_RAND_G_SEED,
+		FMT_RAND_G_SEED);
+	dm_write_reg(opp110->base.ctx, addr, dither_g_value);
+
+	addr = FMT_REG(mmFMT_DITHER_RAND_B_SEED);
+	set_reg_field_value(dither_b_value, params->b_seed_value,
+		FMT_DITHER_RAND_B_SEED,
+		FMT_RAND_B_SEED);
+	dm_write_reg(opp110->base.ctx, addr, dither_b_value);
+
+	/* FMT_OFFSET_R_Cr  31:16 0x0 Setting the zero
+	 * offset for the R/Cr channel, lower 4LSB
+	 * is forced to zeros. Typically set to 0
+	 * RGB and 0x80000 YCbCr.
+	 */
+	/* FMT_OFFSET_G_Y   31:16 0x0 Setting the zero
+	 * offset for the G/Y  channel, lower 4LSB is
+	 * forced to zeros. Typically set to 0 RGB
+	 * and 0x80000 YCbCr.
+	 */
+	/* FMT_OFFSET_B_Cb  31:16 0x0 Setting the zero
+	 * offset for the B/Cb channel, lower 4LSB is
+	 * forced to zeros. Typically set to 0 RGB and
+	 * 0x80000 YCbCr.
+	 */
+
+	/*Set spatial dithering bit depth*/
+	set_reg_field_value(depth_cntl_value,
+		params->flags.SPATIAL_DITHER_DEPTH,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_SPATIAL_DITHER_DEPTH);
+
+	/* Set spatial dithering mode
+	 * (default is Seed patterrn AAAA...)
+	 */
+	set_reg_field_value(depth_cntl_value,
+		params->flags.SPATIAL_DITHER_MODE,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_SPATIAL_DITHER_MODE);
+
+	/*Reset only at startup*/
+	set_reg_field_value(depth_cntl_value,
+		params->flags.FRAME_RANDOM,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_FRAME_RANDOM_ENABLE);
+
+	/*Set RGB data dithered with x^28+x^3+1*/
+	set_reg_field_value(depth_cntl_value,
+		params->flags.RGB_RANDOM,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_RGB_RANDOM_ENABLE);
+
+	/*Disable High pass filter*/
+	set_reg_field_value(depth_cntl_value,
+		params->flags.HIGHPASS_RANDOM,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_HIGHPASS_RANDOM_ENABLE);
+
+	/*Enable spatial dithering*/
+	set_reg_field_value(depth_cntl_value,
+		1,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_SPATIAL_DITHER_EN);
+
+	addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+	dm_write_reg(opp110->base.ctx, addr, depth_cntl_value);
+
+}
+
+/**
+ *	SetTemporalDither (Frame Modulation)
+ *	1) set temporal dither depth
+ *	2) select pattern: from hard-coded pattern or programmable pattern
+ *	3) select optimized strips for BGR or RGB LCD sub-pixel
+ *	4) set s matrix
+ *	5) set t matrix
+ *	6) set grey level for 0.25, 0.5, 0.75
+ *	7) enable temporal dithering
+ */
+static void set_temporal_dither(
+	struct dce110_opp *opp110,
+	const struct bit_depth_reduction_params *params)
+{
+	uint32_t addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+	uint32_t value;
+
+	/*Disable temporal (frame modulation) dithering first*/
+	value = dm_read_reg(opp110->base.ctx, addr);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_EN);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_RESET);
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_OFFSET);
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_DEPTH);
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_LEVEL);
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_25FRC_SEL);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_50FRC_SEL);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_75FRC_SEL);
+
+	dm_write_reg(opp110->base.ctx, addr, value);
+
+	/* no 10bpc dither on DCE11*/
+	if (params->flags.FRAME_MODULATION_ENABLED == 0 ||
+		params->flags.FRAME_MODULATION_DEPTH == 2)
+		return;
+
+	/* Set temporal dithering depth*/
+	set_reg_field_value(value,
+		params->flags.FRAME_MODULATION_DEPTH,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_DEPTH);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_RESET);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_OFFSET);
+
+	/*Select legacy pattern based on FRC and Temporal level*/
+	addr = FMT_REG(mmFMT_TEMPORAL_DITHER_PATTERN_CONTROL);
+	dm_write_reg(opp110->base.ctx, addr, 0);
+	/*Set s matrix*/
+	addr = FMT_REG(
+		mmFMT_TEMPORAL_DITHER_PROGRAMMABLE_PATTERN_S_MATRIX);
+	dm_write_reg(opp110->base.ctx, addr, 0);
+	/*Set t matrix*/
+	addr = FMT_REG(
+		mmFMT_TEMPORAL_DITHER_PROGRAMMABLE_PATTERN_T_MATRIX);
+	dm_write_reg(opp110->base.ctx, addr, 0);
+
+	/*Select patterns for 0.25, 0.5 and 0.75 grey level*/
+	set_reg_field_value(value,
+		params->flags.TEMPORAL_LEVEL,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_LEVEL);
+
+	set_reg_field_value(value,
+		params->flags.FRC25,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_25FRC_SEL);
+
+	set_reg_field_value(value,
+		params->flags.FRC50,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_50FRC_SEL);
+
+	set_reg_field_value(value,
+		params->flags.FRC75,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_75FRC_SEL);
+
+	/*Enable bit reduction by temporal (frame modulation) dithering*/
+	set_reg_field_value(value,
+		1,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_EN);
+
+	addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+	dm_write_reg(opp110->base.ctx, addr, value);
+
+}
+
+/**
+ *	Set Clamping
+ *	1) Set clamping format based on bpc - 0 for 6bpc (No clamping)
+ *		1 for 8 bpc
+ *		2 for 10 bpc
+ *		3 for 12 bpc
+ *		7 for programable
+ *	2) Enable clamp if Limited range requested
+ */
+void dce110_opp_set_clamping(
+	struct dce110_opp *opp110,
+	const struct clamping_and_pixel_encoding_params *params)
+{
+	uint32_t clamp_cntl_value = 0;
+	uint32_t red_clamp_value = 0;
+	uint32_t green_clamp_value = 0;
+	uint32_t blue_clamp_value = 0;
+	uint32_t addr = FMT_REG(mmFMT_CLAMP_CNTL);
+
+	clamp_cntl_value = dm_read_reg(opp110->base.ctx, addr);
+
+	set_reg_field_value(clamp_cntl_value,
+		0,
+		FMT_CLAMP_CNTL,
+		FMT_CLAMP_DATA_EN);
+
+	set_reg_field_value(clamp_cntl_value,
+		0,
+		FMT_CLAMP_CNTL,
+		FMT_CLAMP_COLOR_FORMAT);
+
+	switch (params->clamping_level) {
+	case CLAMPING_FULL_RANGE:
+		break;
+
+	case CLAMPING_LIMITED_RANGE_8BPC:
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_DATA_EN);
+
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_COLOR_FORMAT);
+
+		break;
+
+	case CLAMPING_LIMITED_RANGE_10BPC:
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_DATA_EN);
+
+		set_reg_field_value(clamp_cntl_value,
+			2,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_COLOR_FORMAT);
+
+		break;
+	case CLAMPING_LIMITED_RANGE_12BPC:
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_DATA_EN);
+
+		set_reg_field_value(clamp_cntl_value,
+			3,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_COLOR_FORMAT);
+
+		break;
+	case CLAMPING_LIMITED_RANGE_PROGRAMMABLE:
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_DATA_EN);
+
+		set_reg_field_value(clamp_cntl_value,
+			7,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_COLOR_FORMAT);
+
+		/*set the defaults*/
+		set_reg_field_value(red_clamp_value,
+			0x10,
+			FMT_CLAMP_COMPONENT_R,
+			FMT_CLAMP_LOWER_R);
+
+		set_reg_field_value(red_clamp_value,
+			0xFEF,
+			FMT_CLAMP_COMPONENT_R,
+			FMT_CLAMP_UPPER_R);
+
+		addr = FMT_REG(mmFMT_CLAMP_COMPONENT_R);
+		dm_write_reg(opp110->base.ctx, addr, red_clamp_value);
+
+		set_reg_field_value(green_clamp_value,
+			0x10,
+			FMT_CLAMP_COMPONENT_G,
+			FMT_CLAMP_LOWER_G);
+
+		set_reg_field_value(green_clamp_value,
+			0xFEF,
+			FMT_CLAMP_COMPONENT_G,
+			FMT_CLAMP_UPPER_G);
+
+		addr = FMT_REG(mmFMT_CLAMP_COMPONENT_G);
+		dm_write_reg(opp110->base.ctx, addr, green_clamp_value);
+
+		set_reg_field_value(blue_clamp_value,
+			0x10,
+			FMT_CLAMP_COMPONENT_B,
+			FMT_CLAMP_LOWER_B);
+
+		set_reg_field_value(blue_clamp_value,
+			0xFEF,
+			FMT_CLAMP_COMPONENT_B,
+			FMT_CLAMP_UPPER_B);
+
+		addr = FMT_REG(mmFMT_CLAMP_COMPONENT_B);
+		dm_write_reg(opp110->base.ctx, addr, blue_clamp_value);
+
+		break;
+
+	default:
+		break;
+	}
+
+	addr = FMT_REG(mmFMT_CLAMP_CNTL);
+	/*Set clamp control*/
+	dm_write_reg(opp110->base.ctx, addr, clamp_cntl_value);
+
+}
+
+/**
+ *	set_pixel_encoding
+ *
+ *	Set Pixel Encoding
+ *		0: RGB 4:4:4 or YCbCr 4:4:4 or YOnly
+ *		1: YCbCr 4:2:2
+ */
+static void set_pixel_encoding(
+	struct dce110_opp *opp110,
+	const struct clamping_and_pixel_encoding_params *params)
+{
+	uint32_t fmt_cntl_value;
+	uint32_t addr = FMT_REG(mmFMT_CONTROL);
+
+	/*RGB 4:4:4 or YCbCr 4:4:4 - 0; YCbCr 4:2:2 -1.*/
+	fmt_cntl_value = dm_read_reg(opp110->base.ctx, addr);
+
+	set_reg_field_value(fmt_cntl_value,
+		0,
+		FMT_CONTROL,
+		FMT_PIXEL_ENCODING);
+
+	if (params->pixel_encoding == PIXEL_ENCODING_YCBCR422) {
+		set_reg_field_value(fmt_cntl_value,
+			1,
+			FMT_CONTROL,
+			FMT_PIXEL_ENCODING);
+
+		/*00 - Pixels drop mode ,01 - Pixels average mode*/
+		set_reg_field_value(fmt_cntl_value,
+			0,
+			FMT_CONTROL,
+			FMT_SUBSAMPLING_MODE);
+
+		/*00 - Cb before Cr ,01 - Cr before Cb*/
+		set_reg_field_value(fmt_cntl_value,
+			0,
+			FMT_CONTROL,
+			FMT_SUBSAMPLING_ORDER);
+	}
+	dm_write_reg(opp110->base.ctx, addr, fmt_cntl_value);
+
+}
+
+void dce110_opp_program_bit_depth_reduction(
+	struct output_pixel_processor *opp,
+	const struct bit_depth_reduction_params *params)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+
+	set_truncation(opp110, params);
+	set_spatial_dither(opp110, params);
+	set_temporal_dither(opp110, params);
+}
+
+void dce110_opp_program_clamping_and_pixel_encoding(
+	struct output_pixel_processor *opp,
+	const struct clamping_and_pixel_encoding_params *params)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+
+	dce110_opp_set_clamping(opp110, params);
+	set_pixel_encoding(opp110, params);
+}
+
+void dce110_opp_set_dyn_expansion(
+	struct output_pixel_processor *opp,
+	enum dc_color_space color_sp,
+	enum dc_color_depth color_dpth,
+	enum signal_type signal)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+	uint32_t value;
+	bool enable_dyn_exp = false;
+	uint32_t addr = FMT_REG(mmFMT_DYNAMIC_EXP_CNTL);
+
+	value = dm_read_reg(opp->ctx, addr);
+
+	set_reg_field_value(value, 0,
+		FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_EN);
+	set_reg_field_value(value, 0,
+		FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_MODE);
+
+	/* From HW programming guide:
+		FMT_DYNAMIC_EXP_EN = 0 for limited RGB or YCbCr output
+		FMT_DYNAMIC_EXP_EN = 1 for RGB full range only*/
+	if (color_sp == COLOR_SPACE_SRGB)
+		enable_dyn_exp = true;
+
+	/*00 - 10-bit -> 12-bit dynamic expansion*/
+	/*01 - 8-bit  -> 12-bit dynamic expansion*/
+	if (signal == SIGNAL_TYPE_HDMI_TYPE_A) {
+		switch (color_dpth) {
+		case COLOR_DEPTH_888:
+			set_reg_field_value(value, enable_dyn_exp ? 1:0,
+				FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_EN);
+			set_reg_field_value(value, 1,
+				FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_MODE);
+			break;
+		case COLOR_DEPTH_101010:
+			set_reg_field_value(value, enable_dyn_exp ? 1:0,
+				FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_EN);
+			set_reg_field_value(value, 0,
+				FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_MODE);
+			break;
+		case COLOR_DEPTH_121212:
+			break;
+		default:
+			break;
+		}
+	}
+
+	dm_write_reg(opp->ctx, addr, value);
+}
+
+void dce110_opp_program_fmt(
+	struct output_pixel_processor *opp,
+	struct bit_depth_reduction_params *fmt_bit_depth,
+	struct clamping_and_pixel_encoding_params *clamping)
+{
+	/* dithering is affected by <CrtcSourceSelect>, hence should be
+	 * programmed afterwards */
+	dce110_opp_program_bit_depth_reduction(
+		opp,
+		fmt_bit_depth);
+
+	dce110_opp_program_clamping_and_pixel_encoding(
+		opp,
+		clamping);
+
+	return;
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_regamma.c b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_regamma.c
new file mode 100644
index 0000000..62051ab
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_opp_regamma.c
@@ -0,0 +1,537 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+/* include DCE11 register header files */
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+
+#include "dce110_opp.h"
+#include "gamma_types.h"
+
+#define DCP_REG(reg)\
+	(reg + opp110->offsets.dcp_offset)
+
+#define DCFE_REG(reg)\
+	(reg + opp110->offsets.dcfe_offset)
+
+enum {
+	MAX_PWL_ENTRY = 128,
+	MAX_REGIONS_NUMBER = 16
+
+};
+
+/*
+ *****************************************************************************
+ *  Function: regamma_config_regions_and_segments
+ *
+ *     build regamma curve by using predefined hw points
+ *     uses interface parameters ,like EDID coeff.
+ *
+ * @param   : parameters   interface parameters
+ *  @return void
+ *
+ *  @note
+ *
+ *  @see
+ *
+ *****************************************************************************
+ */
+static void regamma_config_regions_and_segments(
+	struct dce110_opp *opp110,
+	const struct pwl_params *params)
+{
+	const struct gamma_curve *curve;
+	uint32_t value = 0;
+
+	{
+		set_reg_field_value(
+			value,
+			params->arr_points[0].custom_float_x,
+			REGAMMA_CNTLA_START_CNTL,
+			REGAMMA_CNTLA_EXP_REGION_START);
+
+		set_reg_field_value(
+			value,
+			0,
+			REGAMMA_CNTLA_START_CNTL,
+			REGAMMA_CNTLA_EXP_REGION_START_SEGMENT);
+
+		dm_write_reg(opp110->base.ctx,
+				DCP_REG(mmREGAMMA_CNTLA_START_CNTL),
+				value);
+	}
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			params->arr_points[0].custom_float_slope,
+			REGAMMA_CNTLA_SLOPE_CNTL,
+			REGAMMA_CNTLA_EXP_REGION_LINEAR_SLOPE);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_SLOPE_CNTL), value);
+	}
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			params->arr_points[1].custom_float_x,
+			REGAMMA_CNTLA_END_CNTL1,
+			REGAMMA_CNTLA_EXP_REGION_END);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_END_CNTL1), value);
+	}
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			params->arr_points[2].custom_float_slope,
+			REGAMMA_CNTLA_END_CNTL2,
+			REGAMMA_CNTLA_EXP_REGION_END_BASE);
+
+		set_reg_field_value(
+			value,
+			params->arr_points[1].custom_float_y,
+			REGAMMA_CNTLA_END_CNTL2,
+			REGAMMA_CNTLA_EXP_REGION_END_SLOPE);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_END_CNTL2), value);
+	}
+
+	curve = params->arr_curve_points;
+
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_0_1,
+			REGAMMA_CNTLA_EXP_REGION0_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_0_1,
+			REGAMMA_CNTLA_EXP_REGION0_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_0_1,
+			REGAMMA_CNTLA_EXP_REGION1_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_0_1,
+			REGAMMA_CNTLA_EXP_REGION1_NUM_SEGMENTS);
+
+		dm_write_reg(
+			opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_0_1),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_2_3,
+			REGAMMA_CNTLA_EXP_REGION2_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_2_3,
+			REGAMMA_CNTLA_EXP_REGION2_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_2_3,
+			REGAMMA_CNTLA_EXP_REGION3_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_2_3,
+			REGAMMA_CNTLA_EXP_REGION3_NUM_SEGMENTS);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_2_3),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_4_5,
+			REGAMMA_CNTLA_EXP_REGION4_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_4_5,
+			REGAMMA_CNTLA_EXP_REGION4_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_4_5,
+			REGAMMA_CNTLA_EXP_REGION5_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_4_5,
+			REGAMMA_CNTLA_EXP_REGION5_NUM_SEGMENTS);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_4_5),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_6_7,
+			REGAMMA_CNTLA_EXP_REGION6_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_6_7,
+			REGAMMA_CNTLA_EXP_REGION6_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_6_7,
+			REGAMMA_CNTLA_EXP_REGION7_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_6_7,
+			REGAMMA_CNTLA_EXP_REGION7_NUM_SEGMENTS);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_6_7),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_8_9,
+			REGAMMA_CNTLA_EXP_REGION8_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_8_9,
+			REGAMMA_CNTLA_EXP_REGION8_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_8_9,
+			REGAMMA_CNTLA_EXP_REGION9_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_8_9,
+			REGAMMA_CNTLA_EXP_REGION9_NUM_SEGMENTS);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_8_9),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_10_11,
+			REGAMMA_CNTLA_EXP_REGION10_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_10_11,
+			REGAMMA_CNTLA_EXP_REGION10_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_10_11,
+			REGAMMA_CNTLA_EXP_REGION11_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_10_11,
+			REGAMMA_CNTLA_EXP_REGION11_NUM_SEGMENTS);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_10_11),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_12_13,
+			REGAMMA_CNTLA_EXP_REGION12_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_12_13,
+			REGAMMA_CNTLA_EXP_REGION12_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_12_13,
+			REGAMMA_CNTLA_EXP_REGION13_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_12_13,
+			REGAMMA_CNTLA_EXP_REGION13_NUM_SEGMENTS);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_12_13),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_14_15,
+			REGAMMA_CNTLA_EXP_REGION14_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_14_15,
+			REGAMMA_CNTLA_EXP_REGION14_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_14_15,
+			REGAMMA_CNTLA_EXP_REGION15_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_14_15,
+			REGAMMA_CNTLA_EXP_REGION15_NUM_SEGMENTS);
+
+		dm_write_reg(opp110->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_14_15),
+			value);
+	}
+}
+
+static void program_pwl(
+	struct dce110_opp *opp110,
+	const struct pwl_params *params)
+{
+	uint32_t value;
+
+	{
+		uint8_t max_tries = 10;
+		uint8_t counter = 0;
+
+		/* Power on LUT memory */
+		value = dm_read_reg(opp110->base.ctx,
+				DCFE_REG(mmDCFE_MEM_PWR_CTRL));
+
+		set_reg_field_value(
+			value,
+			1,
+			DCFE_MEM_PWR_CTRL,
+			DCP_REGAMMA_MEM_PWR_DIS);
+
+		dm_write_reg(opp110->base.ctx,
+				DCFE_REG(mmDCFE_MEM_PWR_CTRL), value);
+
+		while (counter < max_tries) {
+			value =
+				dm_read_reg(
+					opp110->base.ctx,
+					DCFE_REG(mmDCFE_MEM_PWR_STATUS));
+
+			if (get_reg_field_value(
+				value,
+				DCFE_MEM_PWR_STATUS,
+				DCP_REGAMMA_MEM_PWR_STATE) == 0)
+				break;
+
+			++counter;
+		}
+
+		if (counter == max_tries) {
+			dm_logger_write(opp110->base.ctx->logger, LOG_WARNING,
+				"%s: regamma lut was not powered on "
+				"in a timely manner,"
+				" programming still proceeds\n",
+				__func__);
+		}
+	}
+
+	value = 0;
+
+	set_reg_field_value(
+		value,
+		7,
+		REGAMMA_LUT_WRITE_EN_MASK,
+		REGAMMA_LUT_WRITE_EN_MASK);
+
+	dm_write_reg(opp110->base.ctx,
+		DCP_REG(mmREGAMMA_LUT_WRITE_EN_MASK), value);
+	dm_write_reg(opp110->base.ctx,
+		DCP_REG(mmREGAMMA_LUT_INDEX), 0);
+
+	/* Program REGAMMA_LUT_DATA */
+	{
+		const uint32_t addr = DCP_REG(mmREGAMMA_LUT_DATA);
+
+		uint32_t i = 0;
+
+		const struct pwl_result_data *rgb = params->rgb_resulted;
+
+		while (i != params->hw_points_num) {
+			dm_write_reg(opp110->base.ctx, addr, rgb->red_reg);
+			dm_write_reg(opp110->base.ctx, addr, rgb->green_reg);
+			dm_write_reg(opp110->base.ctx, addr, rgb->blue_reg);
+
+			dm_write_reg(opp110->base.ctx, addr,
+				rgb->delta_red_reg);
+			dm_write_reg(opp110->base.ctx, addr,
+				rgb->delta_green_reg);
+			dm_write_reg(opp110->base.ctx, addr,
+				rgb->delta_blue_reg);
+
+			++rgb;
+			++i;
+		}
+	}
+
+	/*  we are done with DCP LUT memory; re-enable low power mode */
+	value = dm_read_reg(opp110->base.ctx, DCFE_REG(mmDCFE_MEM_PWR_CTRL));
+
+	set_reg_field_value(
+		value,
+		0,
+		DCFE_MEM_PWR_CTRL,
+		DCP_REGAMMA_MEM_PWR_DIS);
+
+	dm_write_reg(opp110->base.ctx, DCFE_REG(mmDCFE_MEM_PWR_CTRL), value);
+}
+
+bool dce110_opp_program_regamma_pwl(
+	struct output_pixel_processor *opp,
+	const struct pwl_params *params)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+
+	/* Setup regions */
+	regamma_config_regions_and_segments(opp110, params);
+
+	/* Program PWL */
+	program_pwl(opp110, params);
+
+	return true;
+}
+
+void dce110_opp_power_on_regamma_lut(
+	struct output_pixel_processor *opp,
+	bool power_on)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+
+	uint32_t value =
+		dm_read_reg(opp->ctx, DCFE_REG(mmDCFE_MEM_PWR_CTRL));
+
+	set_reg_field_value(
+		value,
+		power_on,
+		DCFE_MEM_PWR_CTRL,
+		DCP_REGAMMA_MEM_PWR_DIS);
+
+	set_reg_field_value(
+		value,
+		power_on,
+		DCFE_MEM_PWR_CTRL,
+		DCP_LUT_MEM_PWR_DIS);
+
+	dm_write_reg(opp->ctx, DCFE_REG(mmDCFE_MEM_PWR_CTRL), value);
+}
+
+void dce110_opp_set_regamma_mode(struct output_pixel_processor *opp,
+		enum opp_regamma mode)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+	uint32_t value = dm_read_reg(opp110->base.ctx,
+				DCP_REG(mmREGAMMA_CONTROL));
+
+	set_reg_field_value(
+		value,
+		mode,
+		REGAMMA_CONTROL,
+		GRPH_REGAMMA_MODE);
+
+	dm_write_reg(opp110->base.ctx, DCP_REG(mmREGAMMA_CONTROL),
+			value);
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce112/dce112_mem_input.c b/drivers/gpu/drm/amd/display/dc/dce112/dce112_mem_input.c
new file mode 100644
index 0000000..c29007d
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce112/dce112_mem_input.c
@@ -0,0 +1,54 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+#include "dm_services.h"
+#include "dce112_mem_input.h"
+
+
+#include "dce/dce_11_2_d.h"
+#include "dce/dce_11_2_sh_mask.h"
+
+
+#define DCP_REG(reg) (reg + mem_input110->offsets.dcp)
+#define DMIF_REG(reg) (reg + mem_input110->offsets.dmif)
+#define PIPE_REG(reg) (reg + mem_input110->offsets.pipe)
+
+/*****************************************/
+/* Constructor, Destructor               */
+/*****************************************/
+
+bool dce112_mem_input_construct(
+	struct dce110_mem_input *mem_input110,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_mem_input_reg_offsets *offsets)
+{
+  if (!dce110_mem_input_construct(mem_input110, ctx, inst, offsets))
+		return false;
+
+	mem_input110->base.funcs->mem_input_program_display_marks =
+			dce_mem_input_program_display_marks;
+
+	return true;
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce112/dce112_mem_input.h b/drivers/gpu/drm/amd/display/dc/dce112/dce112_mem_input.h
new file mode 100644
index 0000000..de2aaf0
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce112/dce112_mem_input.h
@@ -0,0 +1,38 @@
+/* Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DC_MEM_INPUT_DCE112_H__
+#define __DC_MEM_INPUT_DCE112_H__
+
+#include "mem_input.h"
+#include "dce110/dce110_mem_input.h"
+
+bool dce112_mem_input_construct(
+	struct dce110_mem_input *mem_input110,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_mem_input_reg_offsets *offsets);
+
+
+#endif
diff --git a/drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.c b/drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.c
new file mode 100644
index 0000000..23c2d10
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.c
@@ -0,0 +1,72 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+/* include DCE11 register header files */
+#include "dce/dce_11_2_d.h"
+#include "dce/dce_11_2_sh_mask.h"
+
+#include "dce112_opp.h"
+
+#include "gamma_types.h"
+
+enum {
+	MAX_LUT_ENTRY = 256,
+	MAX_NUMBER_OF_ENTRIES = 256
+};
+
+/*****************************************/
+/* Constructor, Destructor               */
+/*****************************************/
+
+static struct opp_funcs funcs = {
+		.opp_power_on_regamma_lut = dce110_opp_power_on_regamma_lut,
+		.opp_set_csc_adjustment = dce110_opp_set_csc_adjustment,
+		.opp_set_csc_default = dce110_opp_set_csc_default,
+		.opp_set_dyn_expansion = dce110_opp_set_dyn_expansion,
+		.opp_program_regamma_pwl = dce110_opp_program_regamma_pwl,
+		.opp_set_regamma_mode = dce110_opp_set_regamma_mode,
+		.opp_destroy = dce110_opp_destroy,
+		.opp_program_fmt = dce112_opp_program_fmt,
+		.opp_program_bit_depth_reduction =
+				dce110_opp_program_bit_depth_reduction
+};
+
+bool dce112_opp_construct(struct dce110_opp *opp110,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_opp_reg_offsets *offsets)
+{
+	opp110->base.funcs = &funcs;
+
+	opp110->base.ctx = ctx;
+
+	opp110->base.inst = inst;
+
+	opp110->offsets = *offsets;
+
+	return true;
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.h b/drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.h
new file mode 100644
index 0000000..9443b87
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce112/dce112_opp.h
@@ -0,0 +1,48 @@
+/* Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DC_OPP_DCE112_H__
+#define __DC_OPP_DCE112_H__
+
+#include "dc_types.h"
+#include "opp.h"
+#include "../dce110/dce110_opp.h"
+#include "core_types.h"
+
+void dce112_opp_program_clamping_and_pixel_encoding(
+	struct output_pixel_processor *opp,
+	const struct clamping_and_pixel_encoding_params *params);
+
+void dce112_opp_program_fmt(
+		struct output_pixel_processor *opp,
+		struct bit_depth_reduction_params *fmt_bit_depth,
+		struct clamping_and_pixel_encoding_params *clamping);
+
+bool dce112_opp_construct(struct dce110_opp *opp110,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_opp_reg_offsets *offsets);
+
+
+#endif
diff --git a/drivers/gpu/drm/amd/display/dc/dce112/dce112_opp_formatter.c b/drivers/gpu/drm/amd/display/dc/dce112/dce112_opp_formatter.c
new file mode 100644
index 0000000..2d90721
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce112/dce112_opp_formatter.c
@@ -0,0 +1,215 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ *  and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+#include "dce/dce_11_2_d.h"
+#include "dce/dce_11_2_sh_mask.h"
+
+#include "dce112_opp.h"
+
+#define FMT_REG(reg)\
+	(reg + opp110->offsets.fmt_offset)
+#define FMT_MEM_REG(reg)\
+	(reg + opp110->offsets.fmt_mem_offset)
+
+/**
+ *	Set Clamping
+ *	1) Set clamping format based on bpc - 0 for 6bpc (No clamping)
+ *		1 for 8 bpc
+ *		2 for 10 bpc
+ *		3 for 12 bpc
+ *		7 for programable
+ *	2) Enable clamp if Limited range requested
+ */
+
+/**
+ *	set_pixel_encoding
+ *
+ *	Set Pixel Encoding
+ *		0: RGB 4:4:4 or YCbCr 4:4:4 or YOnly
+ *		1: YCbCr 4:2:2
+ *		2: YCbCr 4:2:0
+ */
+static void set_pixel_encoding(
+	struct dce110_opp *opp110,
+	const struct clamping_and_pixel_encoding_params *params)
+{
+	uint32_t fmt_cntl_value;
+	uint32_t addr = FMT_REG(mmFMT_CONTROL);
+
+	/*RGB 4:4:4 or YCbCr 4:4:4 - 0; YCbCr 4:2:2 -1.*/
+	fmt_cntl_value = dm_read_reg(opp110->base.ctx, addr);
+
+	set_reg_field_value(fmt_cntl_value,
+		0,
+		FMT_CONTROL,
+		FMT_PIXEL_ENCODING);
+
+	/*00 - Pixels drop mode HW default*/
+	set_reg_field_value(fmt_cntl_value,
+		0,
+		FMT_CONTROL,
+		FMT_SUBSAMPLING_MODE);
+
+	/* By default no bypass*/
+	set_reg_field_value(fmt_cntl_value,
+		0,
+		FMT_CONTROL,
+		FMT_CBCR_BIT_REDUCTION_BYPASS);
+
+	if (params->pixel_encoding == PIXEL_ENCODING_YCBCR422) {
+		set_reg_field_value(fmt_cntl_value,
+			1,
+			FMT_CONTROL,
+			FMT_PIXEL_ENCODING);
+
+		/*00 - Cb before Cr ,01 - Cr before Cb*/
+		set_reg_field_value(fmt_cntl_value,
+			0,
+			FMT_CONTROL,
+			FMT_SUBSAMPLING_ORDER);
+	}
+
+	if (params->pixel_encoding == PIXEL_ENCODING_YCBCR420) {
+		set_reg_field_value(fmt_cntl_value,
+			2,
+			FMT_CONTROL,
+			FMT_PIXEL_ENCODING);
+
+		/* 02 - Subsampling mode, 3 taps*/
+		set_reg_field_value(fmt_cntl_value,
+			2,
+			FMT_CONTROL,
+			FMT_SUBSAMPLING_MODE);
+
+		/* 00 - Enable CbCr bit reduction bypass to preserve precision*/
+		set_reg_field_value(fmt_cntl_value,
+			1,
+			FMT_CONTROL,
+			FMT_CBCR_BIT_REDUCTION_BYPASS);
+	}
+	dm_write_reg(opp110->base.ctx, addr, fmt_cntl_value);
+
+}
+
+void dce112_opp_program_clamping_and_pixel_encoding(
+	struct output_pixel_processor *opp,
+	const struct clamping_and_pixel_encoding_params *params)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+
+	dce110_opp_set_clamping(opp110, params);
+	set_pixel_encoding(opp110, params);
+}
+
+static void program_formatter_420_memory(struct output_pixel_processor *opp)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+	uint32_t fmt_cntl_value;
+	uint32_t fmt_mem_cntl_value;
+	uint32_t fmt_cntl_addr = FMT_REG(mmFMT_CONTROL);
+	uint32_t fmt_mem_cntl_addr = FMT_MEM_REG(mmFMT_MEMORY0_CONTROL);
+
+	fmt_mem_cntl_value = dm_read_reg(opp110->base.ctx, fmt_mem_cntl_addr);
+	fmt_cntl_value = dm_read_reg(opp110->base.ctx, fmt_cntl_addr);
+	/* Program source select*/
+	/* Use HW default source select for FMT_MEMORYx_CONTROL */
+	/* Use that value for FMT_SRC_SELECT as well*/
+	set_reg_field_value(fmt_cntl_value,
+		get_reg_field_value(fmt_mem_cntl_value, FMT_MEMORY0_CONTROL, FMT420_MEM0_SOURCE_SEL),
+		FMT_CONTROL,
+		FMT_SRC_SELECT);
+	dm_write_reg(opp110->base.ctx, fmt_cntl_addr, fmt_cntl_value);
+
+	/* Turn on the memory */
+	set_reg_field_value(fmt_mem_cntl_value,
+		0,
+		FMT_MEMORY0_CONTROL,
+		FMT420_MEM0_PWR_FORCE);
+	dm_write_reg(opp110->base.ctx, fmt_mem_cntl_addr, fmt_mem_cntl_value);
+}
+
+static void program_formatter_reset_dig_resync_fifo(struct output_pixel_processor *opp)
+{
+	struct dce110_opp *opp110 = TO_DCE110_OPP(opp);
+	uint32_t value;
+	uint32_t addr = FMT_REG(mmFMT_CONTROL);
+	uint8_t counter = 10;
+
+
+	value = dm_read_reg(opp110->base.ctx, addr);
+
+	/* clear previous phase lock status*/
+	set_reg_field_value(value,
+		1,
+		FMT_CONTROL,
+		FMT_420_PIXEL_PHASE_LOCKED_CLEAR);
+	dm_write_reg(opp110->base.ctx, addr, value);
+
+	/* poll until FMT_420_PIXEL_PHASE_LOCKED become 1*/
+	while (counter > 0) {
+		value = dm_read_reg(opp110->base.ctx, addr);
+
+		if (get_reg_field_value(
+			value,
+			FMT_CONTROL,
+			FMT_420_PIXEL_PHASE_LOCKED) == 1)
+			break;
+
+		msleep(10);
+		counter--;
+	}
+
+	if (counter == 0)
+		dm_logger_write(opp->ctx->logger, LOG_ERROR,
+				"%s:opp program formattter reset dig resync info time out.\n",
+				__func__);
+}
+
+void dce112_opp_program_fmt(
+		struct output_pixel_processor *opp,
+		struct bit_depth_reduction_params *fmt_bit_depth,
+		struct clamping_and_pixel_encoding_params *clamping)
+{
+	/* dithering is affected by <CrtcSourceSelect>, hence should be
+	 * programmed afterwards */
+
+	if (clamping->pixel_encoding == PIXEL_ENCODING_YCBCR420)
+		program_formatter_420_memory(opp);
+
+	dce110_opp_program_bit_depth_reduction(
+		opp,
+		fmt_bit_depth);
+
+	dce112_opp_program_clamping_and_pixel_encoding(
+		opp,
+		clamping);
+
+	if (clamping->pixel_encoding == PIXEL_ENCODING_YCBCR420)
+		program_formatter_reset_dig_resync_fifo(opp);
+
+	return;
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.c b/drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.c
new file mode 100644
index 0000000..86826c2
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.c
@@ -0,0 +1,64 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "include/logger_interface.h"
+
+#include "dce/dce_8_0_d.h"
+#include "dce/dce_8_0_sh_mask.h"
+
+#include "dce80_ipp.h"
+
+#include "dce110/dce110_ipp.h"
+
+static const struct ipp_funcs funcs = {
+		.ipp_cursor_set_attributes = dce110_ipp_cursor_set_attributes,
+		.ipp_cursor_set_position = dce110_ipp_cursor_set_position,
+		.ipp_program_prescale = dce110_ipp_program_prescale,
+		.ipp_set_degamma = dce110_ipp_set_degamma,
+};
+
+bool dce80_ipp_construct(
+	struct dce110_ipp *ipp,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_ipp_reg_offsets *offset)
+{
+	ipp->base.ctx = ctx;
+
+	ipp->base.inst = inst;
+
+	ipp->offsets = *offset;
+
+	ipp->base.funcs = &funcs;
+
+	return true;
+}
+
+void dce80_ipp_destroy(struct input_pixel_processor **ipp)
+{
+	dm_free(TO_DCE80_IPP(*ipp));
+	*ipp = NULL;
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.h b/drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.h
new file mode 100644
index 0000000..d350138
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp.h
@@ -0,0 +1,47 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DC_IPP_DCE80_H__
+#define __DC_IPP_DCE80_H__
+
+#include "ipp.h"
+
+#define TO_DCE80_IPP(input_pixel_processor)\
+		container_of(input_pixel_processor, struct dce110_ipp, base)
+
+struct dce110_ipp;
+struct dce110_ipp_reg_offsets;
+struct gamma_parameters;
+struct dev_c_lut;
+
+bool dce80_ipp_construct(
+	struct dce110_ipp *ipp,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_ipp_reg_offsets *offset);
+
+void dce80_ipp_destroy(struct input_pixel_processor **ipp);
+
+#endif /*__DC_IPP_DCE80_H__*/
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp_gamma.c b/drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp_gamma.c
new file mode 100644
index 0000000..eacb14e
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_ipp_gamma.c
@@ -0,0 +1,76 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+#include "include/logger_interface.h"
+#include "include/fixed31_32.h"
+#include "basics/conversion.h"
+
+#include "dce/dce_8_0_d.h"
+#include "dce/dce_8_0_sh_mask.h"
+
+#include "dce80_ipp.h"
+#include "dce110/dce110_ipp.h"
+#include "gamma_types.h"
+
+#define DCP_REG(reg)\
+	(reg + ipp80->offsets.dcp_offset)
+
+enum {
+	MAX_INPUT_LUT_ENTRY = 256
+};
+
+/*PROTOTYPE DECLARATIONS*/
+
+static void set_legacy_input_gamma_mode(
+	struct dce110_ipp *ipp80,
+	bool is_legacy);
+
+void dce80_ipp_set_legacy_input_gamma_mode(
+		struct input_pixel_processor *ipp,
+		bool is_legacy)
+{
+	struct dce110_ipp *ipp80 = TO_DCE80_IPP(ipp);
+
+	set_legacy_input_gamma_mode(ipp80, is_legacy);
+}
+
+static void set_legacy_input_gamma_mode(
+	struct dce110_ipp *ipp80,
+	bool is_legacy)
+{
+	const uint32_t addr = DCP_REG(mmINPUT_GAMMA_CONTROL);
+	uint32_t value = dm_read_reg(ipp80->base.ctx, addr);
+
+	set_reg_field_value(
+		value,
+		!is_legacy,
+		INPUT_GAMMA_CONTROL,
+		GRPH_INPUT_GAMMA_MODE);
+
+	dm_write_reg(ipp80->base.ctx, addr, value);
+}
+
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_mem_input.c b/drivers/gpu/drm/amd/display/dc/dce80/dce80_mem_input.c
new file mode 100644
index 0000000..ebb8df3
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_mem_input.c
@@ -0,0 +1,83 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+#include "dm_services.h"
+
+#include "dce/dce_8_0_d.h"
+#include "dce/dce_8_0_sh_mask.h"
+/* TODO: this needs to be looked at, used by Stella's workaround*/
+#include "gmc/gmc_7_1_d.h"
+#include "gmc/gmc_7_1_sh_mask.h"
+
+#include "include/logger_interface.h"
+#include "inc/bandwidth_calcs.h"
+
+#include "../dce110/dce110_mem_input.h"
+#include "dce80_mem_input.h"
+
+#define MAX_WATERMARK 0xFFFF
+#define SAFE_NBP_MARK 0x7FFF
+
+#define DCP_REG(reg) (reg + mem_input80->offsets.dcp)
+#define DMIF_REG(reg) (reg + mem_input80->offsets.dmif)
+#define PIPE_REG(reg) (reg + mem_input80->offsets.pipe)
+
+static struct mem_input_funcs dce80_mem_input_funcs = {
+	.mem_input_program_display_marks =
+			dce110_mem_input_program_display_marks,
+	.allocate_mem_input = dce_mem_input_allocate_dmif,
+	.free_mem_input = dce_mem_input_free_dmif,
+	.mem_input_program_surface_flip_and_addr =
+			dce110_mem_input_program_surface_flip_and_addr,
+	.mem_input_program_surface_config =
+			dce_mem_input_program_surface_config,
+	.mem_input_is_flip_pending =
+			dce110_mem_input_is_flip_pending,
+	.mem_input_update_dchub = NULL
+};
+
+/*****************************************/
+/* Constructor, Destructor               */
+/*****************************************/
+
+bool dce80_mem_input_construct(
+	struct dce110_mem_input *mem_input80,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_mem_input_reg_offsets *offsets)
+{
+	/* supported stutter method
+	 * STUTTER_MODE_ENHANCED
+	 * STUTTER_MODE_QUAD_DMIF_BUFFER
+	 */
+	mem_input80->base.funcs = &dce80_mem_input_funcs;
+	mem_input80->base.ctx = ctx;
+
+	mem_input80->base.inst = inst;
+
+	mem_input80->offsets = *offsets;
+
+	return true;
+}
+
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_mem_input.h b/drivers/gpu/drm/amd/display/dc/dce80/dce80_mem_input.h
new file mode 100644
index 0000000..357b9e2
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_mem_input.h
@@ -0,0 +1,36 @@
+/* Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DC_MEM_INPUT_DCE80_H__
+#define __DC_MEM_INPUT_DCE80_H__
+
+#include "mem_input.h"
+
+bool dce80_mem_input_construct(
+	struct dce110_mem_input *mem_input80,
+	struct dc_context *ctx,
+	uint32_t inst,
+	const struct dce110_mem_input_reg_offsets *offsets);
+
+#endif
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.c b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.c
new file mode 100644
index 0000000..b69e8a5
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.c
@@ -0,0 +1,136 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+/* include DCE8 register header files */
+#include "dce/dce_8_0_d.h"
+#include "dce/dce_8_0_sh_mask.h"
+
+#include "dce80_opp.h"
+
+#define FROM_OPP(opp)\
+	container_of(opp, struct dce80_opp, base)
+
+enum {
+	MAX_LUT_ENTRY = 256,
+	MAX_NUMBER_OF_ENTRIES = 256
+};
+
+static const struct dce80_opp_reg_offsets reg_offsets[] = {
+{
+	.fmt_offset = (mmFMT0_FMT_CONTROL - mmFMT0_FMT_CONTROL),
+	.crtc_offset = (mmCRTC0_DCFE_MEM_LIGHT_SLEEP_CNTL -
+					mmCRTC0_DCFE_MEM_LIGHT_SLEEP_CNTL),
+	.dcp_offset = (mmDCP0_GRPH_CONTROL - mmDCP0_GRPH_CONTROL),
+},
+{	.fmt_offset = (mmFMT1_FMT_CONTROL - mmFMT0_FMT_CONTROL),
+	.crtc_offset = (mmCRTC1_DCFE_MEM_LIGHT_SLEEP_CNTL -
+					mmCRTC0_DCFE_MEM_LIGHT_SLEEP_CNTL),
+	.dcp_offset = (mmDCP1_GRPH_CONTROL - mmDCP0_GRPH_CONTROL),
+},
+{	.fmt_offset = (mmFMT2_FMT_CONTROL - mmFMT0_FMT_CONTROL),
+	.crtc_offset = (mmCRTC2_DCFE_MEM_LIGHT_SLEEP_CNTL -
+					mmCRTC0_DCFE_MEM_LIGHT_SLEEP_CNTL),
+	.dcp_offset = (mmDCP2_GRPH_CONTROL - mmDCP0_GRPH_CONTROL),
+},
+{
+	.fmt_offset = (mmFMT3_FMT_CONTROL - mmFMT0_FMT_CONTROL),
+	.crtc_offset = (mmCRTC3_DCFE_MEM_LIGHT_SLEEP_CNTL -
+					mmCRTC0_DCFE_MEM_LIGHT_SLEEP_CNTL),
+	.dcp_offset = (mmDCP3_GRPH_CONTROL - mmDCP0_GRPH_CONTROL),
+},
+{
+	.fmt_offset = (mmFMT4_FMT_CONTROL - mmFMT0_FMT_CONTROL),
+	.crtc_offset = (mmCRTC4_DCFE_MEM_LIGHT_SLEEP_CNTL -
+					mmCRTC0_DCFE_MEM_LIGHT_SLEEP_CNTL),
+	.dcp_offset = (mmDCP4_GRPH_CONTROL - mmDCP0_GRPH_CONTROL),
+},
+{
+	.fmt_offset = (mmFMT5_FMT_CONTROL - mmFMT0_FMT_CONTROL),
+	.crtc_offset = (mmCRTC5_DCFE_MEM_LIGHT_SLEEP_CNTL -
+					mmCRTC0_DCFE_MEM_LIGHT_SLEEP_CNTL),
+	.dcp_offset = (mmDCP5_GRPH_CONTROL - mmDCP0_GRPH_CONTROL),
+}
+};
+
+static const struct opp_funcs funcs = {
+		.opp_power_on_regamma_lut = dce80_opp_power_on_regamma_lut,
+		.opp_set_csc_adjustment = dce80_opp_set_csc_adjustment,
+		.opp_set_csc_default = dce80_opp_set_csc_default,
+		.opp_set_dyn_expansion = dce80_opp_set_dyn_expansion,
+		.opp_program_regamma_pwl = dce80_opp_program_regamma_pwl,
+		.opp_set_regamma_mode = dce80_opp_set_regamma_mode,
+		.opp_destroy = dce80_opp_destroy,
+		.opp_program_fmt = dce110_opp_program_fmt,
+};
+
+/*****************************************/
+/* Constructor, Destructor               */
+/*****************************************/
+
+bool dce80_opp_construct(struct dce80_opp *opp80,
+	struct dc_context *ctx,
+	uint32_t inst)
+{
+	if (inst >= ARRAY_SIZE(reg_offsets))
+		return false;
+
+	opp80->base.funcs = &funcs;
+
+	opp80->base.ctx = ctx;
+
+	opp80->base.inst = inst;
+
+	opp80->offsets = reg_offsets[inst];
+
+	return true;
+}
+
+void dce80_opp_destroy(struct output_pixel_processor **opp)
+{
+	dm_free(FROM_OPP(*opp));
+	*opp = NULL;
+}
+
+struct output_pixel_processor *dce80_opp_create(
+	struct dc_context *ctx,
+	uint32_t inst)
+{
+	struct dce80_opp *opp =
+		dm_alloc(sizeof(struct dce80_opp));
+
+	if (!opp)
+		return NULL;
+
+	if (dce80_opp_construct(opp,
+			ctx, inst))
+		return &opp->base;
+
+	BREAK_TO_DEBUGGER();
+	dm_free(opp);
+	return NULL;
+}
+
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.h b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.h
new file mode 100644
index 0000000..965cce3
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp.h
@@ -0,0 +1,130 @@
+/* Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DC_OPP_DCE80_H__
+#define __DC_OPP_DCE80_H__
+
+#include "dc_types.h"
+#include "opp.h"
+#include "gamma_types.h"
+#include "../dce110/dce110_opp.h"
+
+struct gamma_parameters;
+
+struct dce80_regamma {
+	struct gamma_curve arr_curve_points[16];
+	struct curve_points arr_points[3];
+	uint32_t hw_points_num;
+	struct hw_x_point *coordinates_x;
+	struct pwl_result_data *rgb_resulted;
+
+	/* re-gamma curve */
+	struct pwl_float_data_ex *rgb_regamma;
+	/* coeff used to map user evenly distributed points
+	 * to our hardware points (predefined) for gamma 256 */
+	struct pixel_gamma_point *coeff128;
+	struct pixel_gamma_point *coeff128_oem;
+	/* coeff used to map user evenly distributed points
+	 * to our hardware points (predefined) for gamma 1025 */
+	struct pixel_gamma_point *coeff128_dx;
+	/* evenly distributed points, gamma 256 software points 0-255 */
+	struct gamma_pixel *axis_x_256;
+	/* evenly distributed points, gamma 1025 software points 0-1025 */
+	struct gamma_pixel *axis_x_1025;
+	/* OEM supplied gamma for regamma LUT */
+	struct pwl_float_data *rgb_oem;
+	/* user supplied gamma */
+	struct pwl_float_data *rgb_user;
+	uint32_t extra_points;
+	bool use_half_points;
+	struct fixed31_32 x_max1;
+	struct fixed31_32 x_max2;
+	struct fixed31_32 x_min;
+	struct fixed31_32 divider1;
+	struct fixed31_32 divider2;
+	struct fixed31_32 divider3;
+};
+
+/* OPP RELATED */
+#define TO_DCE80_OPP(opp)\
+	container_of(opp, struct dce80_opp, base)
+
+struct dce80_opp_reg_offsets {
+	uint32_t fmt_offset;
+	uint32_t dcp_offset;
+	uint32_t crtc_offset;
+};
+
+struct dce80_opp {
+	struct output_pixel_processor base;
+	struct dce80_opp_reg_offsets offsets;
+	struct dce80_regamma regamma;
+};
+
+bool dce80_opp_construct(struct dce80_opp *opp80,
+	struct dc_context *ctx,
+	uint32_t inst);
+
+void dce80_opp_destroy(struct output_pixel_processor **opp);
+
+struct output_pixel_processor *dce80_opp_create(
+	struct dc_context *ctx,
+	uint32_t inst);
+
+/* REGAMMA RELATED */
+void dce80_opp_power_on_regamma_lut(
+	struct output_pixel_processor *opp,
+	bool power_on);
+
+bool dce80_opp_program_regamma_pwl(
+	struct output_pixel_processor *opp,
+	const struct pwl_params *pamras);
+
+void dce80_opp_set_regamma_mode(struct output_pixel_processor *opp,
+		enum opp_regamma mode);
+
+void dce80_opp_set_csc_adjustment(
+	struct output_pixel_processor *opp,
+	const struct out_csc_color_matrix *tbl_entry);
+
+void dce80_opp_set_csc_default(
+	struct output_pixel_processor *opp,
+	const struct default_adjustment *default_adjust);
+
+/* FORMATTER RELATED */
+void dce80_opp_program_bit_depth_reduction(
+	struct output_pixel_processor *opp,
+	const struct bit_depth_reduction_params *params);
+
+void dce80_opp_program_clamping_and_pixel_encoding(
+	struct output_pixel_processor *opp,
+	const struct clamping_and_pixel_encoding_params *params);
+
+void dce80_opp_set_dyn_expansion(
+	struct output_pixel_processor *opp,
+	enum dc_color_space color_sp,
+	enum dc_color_depth color_dpth,
+	enum signal_type signal);
+
+#endif
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_csc.c b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_csc.c
new file mode 100644
index 0000000..bdb9e0a
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_csc.c
@@ -0,0 +1,363 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ *  and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+#include "dce80_opp.h"
+#include "basics/conversion.h"
+
+/* include DCE8 register header files */
+#include "dce/dce_8_0_d.h"
+#include "dce/dce_8_0_sh_mask.h"
+
+#define DCP_REG(reg)\
+	(reg + opp80->offsets.dcp_offset)
+
+enum {
+	OUTPUT_CSC_MATRIX_SIZE = 12
+};
+
+static const struct out_csc_color_matrix global_color_matrix[] = {
+{ COLOR_SPACE_SRGB,
+	{ 0x2000, 0, 0, 0, 0, 0x2000, 0, 0, 0, 0, 0x2000, 0} },
+{ COLOR_SPACE_SRGB_LIMITED,
+	{ 0x1B60, 0, 0, 0x200, 0, 0x1B60, 0, 0x200, 0, 0, 0x1B60, 0x200} },
+{ COLOR_SPACE_YCBCR601,
+	{ 0xE00, 0xF447, 0xFDB9, 0x1000, 0x82F, 0x1012, 0x31F, 0x200, 0xFB47,
+		0xF6B9, 0xE00, 0x1000} },
+{ COLOR_SPACE_YCBCR709, { 0xE00, 0xF349, 0xFEB7, 0x1000, 0x5D2, 0x1394, 0x1FA,
+	0x200, 0xFCCB, 0xF535, 0xE00, 0x1000} },
+/* TODO: correct values below */
+{ COLOR_SPACE_YCBCR601_LIMITED, { 0xE00, 0xF447, 0xFDB9, 0x1000, 0x991,
+	0x12C9, 0x3A6, 0x200, 0xFB47, 0xF6B9, 0xE00, 0x1000} },
+{ COLOR_SPACE_YCBCR709_LIMITED, { 0xE00, 0xF349, 0xFEB7, 0x1000, 0x6CE, 0x16E3,
+	0x24F, 0x200, 0xFCCB, 0xF535, 0xE00, 0x1000} }
+};
+
+enum csc_color_mode {
+	/* 00 - BITS2:0 Bypass */
+	CSC_COLOR_MODE_GRAPHICS_BYPASS,
+	/* 01 - hard coded coefficient TV RGB */
+	CSC_COLOR_MODE_GRAPHICS_PREDEFINED,
+	/* 04 - programmable OUTPUT CSC coefficient */
+	CSC_COLOR_MODE_GRAPHICS_OUTPUT_CSC,
+};
+
+static void program_color_matrix(
+	struct dce80_opp *opp80,
+	const struct out_csc_color_matrix *tbl_entry,
+	enum grph_color_adjust_option options)
+{
+	struct dc_context *ctx = opp80->base.ctx;
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C11_C12);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[0],
+			OUTPUT_CSC_C11_C12,
+			OUTPUT_CSC_C11);
+
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[1],
+			OUTPUT_CSC_C11_C12,
+			OUTPUT_CSC_C12);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C13_C14);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[2],
+			OUTPUT_CSC_C13_C14,
+			OUTPUT_CSC_C13);
+		/* fixed S0.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[3],
+			OUTPUT_CSC_C13_C14,
+			OUTPUT_CSC_C14);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C21_C22);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[4],
+			OUTPUT_CSC_C21_C22,
+			OUTPUT_CSC_C21);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[5],
+			OUTPUT_CSC_C21_C22,
+			OUTPUT_CSC_C22);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C23_C24);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[6],
+			OUTPUT_CSC_C23_C24,
+			OUTPUT_CSC_C23);
+		/* fixed S0.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[7],
+			OUTPUT_CSC_C23_C24,
+			OUTPUT_CSC_C24);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C31_C32);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[8],
+			OUTPUT_CSC_C31_C32,
+			OUTPUT_CSC_C31);
+		/* fixed S0.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[9],
+			OUTPUT_CSC_C31_C32,
+			OUTPUT_CSC_C32);
+
+		dm_write_reg(ctx, addr, value);
+	}
+	{
+		uint32_t value = 0;
+		uint32_t addr = DCP_REG(mmOUTPUT_CSC_C33_C34);
+		/* fixed S2.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[10],
+			OUTPUT_CSC_C33_C34,
+			OUTPUT_CSC_C33);
+		/* fixed S0.13 format */
+		set_reg_field_value(
+			value,
+			tbl_entry->regval[11],
+			OUTPUT_CSC_C33_C34,
+			OUTPUT_CSC_C34);
+
+		dm_write_reg(ctx, addr, value);
+	}
+}
+
+static bool configure_graphics_mode(
+	struct dce80_opp *opp80,
+	enum csc_color_mode config,
+	enum graphics_csc_adjust_type csc_adjust_type,
+	enum dc_color_space color_space)
+{
+	struct dc_context *ctx = opp80->base.ctx;
+	uint32_t addr = DCP_REG(mmOUTPUT_CSC_CONTROL);
+	uint32_t value = dm_read_reg(ctx, addr);
+
+	set_reg_field_value(
+		value,
+		0,
+		OUTPUT_CSC_CONTROL,
+		OUTPUT_CSC_GRPH_MODE);
+
+	if (csc_adjust_type == GRAPHICS_CSC_ADJUST_TYPE_SW) {
+		if (config == CSC_COLOR_MODE_GRAPHICS_OUTPUT_CSC) {
+			set_reg_field_value(
+				value,
+				4,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+		} else {
+
+			switch (color_space) {
+			case COLOR_SPACE_SRGB:
+				/* by pass */
+				set_reg_field_value(
+					value,
+					0,
+					OUTPUT_CSC_CONTROL,
+					OUTPUT_CSC_GRPH_MODE);
+				break;
+			case COLOR_SPACE_SRGB_LIMITED:
+				/* TV RGB */
+				set_reg_field_value(
+					value,
+					1,
+					OUTPUT_CSC_CONTROL,
+					OUTPUT_CSC_GRPH_MODE);
+				break;
+			case COLOR_SPACE_YCBCR601:
+			case COLOR_SPACE_YPBPR601:
+			case COLOR_SPACE_YCBCR601_LIMITED:
+				/* YCbCr601 */
+				set_reg_field_value(
+					value,
+					2,
+					OUTPUT_CSC_CONTROL,
+					OUTPUT_CSC_GRPH_MODE);
+				break;
+			case COLOR_SPACE_YCBCR709:
+			case COLOR_SPACE_YPBPR709:
+			case COLOR_SPACE_YCBCR709_LIMITED:
+				/* YCbCr709 */
+				set_reg_field_value(
+					value,
+					3,
+					OUTPUT_CSC_CONTROL,
+					OUTPUT_CSC_GRPH_MODE);
+				break;
+			default:
+				return false;
+			}
+		}
+	} else if (csc_adjust_type == GRAPHICS_CSC_ADJUST_TYPE_HW) {
+		switch (color_space) {
+		case COLOR_SPACE_SRGB:
+			/* by pass */
+			set_reg_field_value(
+				value,
+				0,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+			break;
+		case COLOR_SPACE_SRGB_LIMITED:
+			/* TV RGB */
+			set_reg_field_value(
+				value,
+				1,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+			break;
+		case COLOR_SPACE_YCBCR601:
+		case COLOR_SPACE_YPBPR601:
+		case COLOR_SPACE_YCBCR601_LIMITED:
+			/* YCbCr601 */
+			set_reg_field_value(
+				value,
+				2,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+			break;
+		case COLOR_SPACE_YCBCR709:
+		case COLOR_SPACE_YPBPR709:
+		case COLOR_SPACE_YCBCR709_LIMITED:
+			 /* YCbCr709 */
+			set_reg_field_value(
+				value,
+				3,
+				OUTPUT_CSC_CONTROL,
+				OUTPUT_CSC_GRPH_MODE);
+			break;
+		default:
+			return false;
+		}
+
+	} else
+		/* by pass */
+		set_reg_field_value(
+			value,
+			0,
+			OUTPUT_CSC_CONTROL,
+			OUTPUT_CSC_GRPH_MODE);
+
+	addr = DCP_REG(mmOUTPUT_CSC_CONTROL);
+	dm_write_reg(ctx, addr, value);
+
+	return true;
+}
+
+void dce80_opp_set_csc_adjustment(
+	struct output_pixel_processor *opp,
+	const struct out_csc_color_matrix *tbl_entry)
+{
+	struct dce80_opp *opp80 = TO_DCE80_OPP(opp);
+	enum csc_color_mode config =
+			CSC_COLOR_MODE_GRAPHICS_OUTPUT_CSC;
+
+	program_color_matrix(opp80, tbl_entry, GRAPHICS_CSC_ADJUST_TYPE_SW);
+
+	/*  We did everything ,now program DxOUTPUT_CSC_CONTROL */
+	configure_graphics_mode(opp80, config, GRAPHICS_CSC_ADJUST_TYPE_SW,
+				tbl_entry->color_space);
+}
+
+void dce80_opp_set_csc_default(
+	struct output_pixel_processor *opp,
+	const struct default_adjustment *default_adjust)
+{
+	struct dce80_opp *opp80 = TO_DCE80_OPP(opp);
+	enum csc_color_mode config =
+			CSC_COLOR_MODE_GRAPHICS_PREDEFINED;
+
+	if (default_adjust->force_hw_default == false) {
+		const struct out_csc_color_matrix *elm;
+		/* currently parameter not in use */
+		enum grph_color_adjust_option option =
+			GRPH_COLOR_MATRIX_HW_DEFAULT;
+		uint32_t i;
+		/*
+		 * HW default false we program locally defined matrix
+		 * HW default true  we use predefined hw matrix and we
+		 * do not need to program matrix
+		 * OEM wants the HW default via runtime parameter.
+		 */
+		option = GRPH_COLOR_MATRIX_SW;
+
+		for (i = 0; i < ARRAY_SIZE(global_color_matrix); ++i) {
+			elm = &global_color_matrix[i];
+			if (elm->color_space != default_adjust->out_color_space)
+				continue;
+			/* program the matrix with default values from this
+			 * file */
+			program_color_matrix(opp80, elm, option);
+			config = CSC_COLOR_MODE_GRAPHICS_OUTPUT_CSC;
+			break;
+		}
+	}
+
+	/* configure the what we programmed :
+	 * 1. Default values from this file
+	 * 2. Use hardware default from ROM_A and we do not need to program
+	 * matrix */
+
+	configure_graphics_mode(opp80, config,
+		default_adjust->csc_adjust_type,
+		default_adjust->out_color_space);
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_formatter.c b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_formatter.c
new file mode 100644
index 0000000..433296a
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_formatter.c
@@ -0,0 +1,577 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ *  and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+#include "dce/dce_8_0_d.h"
+#include "dce/dce_8_0_sh_mask.h"
+
+#include "dce80_opp.h"
+
+#define FMT_REG(reg)\
+	(reg + opp80->offsets.fmt_offset)
+
+/**
+ *	set_truncation
+ *	1) set truncation depth: 0 for 18 bpp or 1 for 24 bpp
+ *	2) enable truncation
+ *	3) HW remove 12bit FMT support for DCE8 power saving reason.
+ */
+static void set_truncation(
+		struct dce80_opp *opp80,
+		const struct bit_depth_reduction_params *params)
+{
+	uint32_t value = 0;
+	uint32_t addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+
+	/*Disable truncation*/
+	value = dm_read_reg(opp80->base.ctx, addr);
+	set_reg_field_value(value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_EN);
+	set_reg_field_value(value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_DEPTH);
+	set_reg_field_value(value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_MODE);
+
+	dm_write_reg(opp80->base.ctx, addr, value);
+
+	/* no 10bpc trunc on DCE8*/
+	if (params->flags.TRUNCATE_ENABLED == 0 ||
+		params->flags.TRUNCATE_DEPTH == 2)
+		return;
+
+	/*Set truncation depth and Enable truncation*/
+	set_reg_field_value(value, 1,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_EN);
+	set_reg_field_value(value, params->flags.TRUNCATE_MODE,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_MODE);
+	set_reg_field_value(value, params->flags.TRUNCATE_DEPTH,
+		FMT_BIT_DEPTH_CONTROL, FMT_TRUNCATE_DEPTH);
+
+	dm_write_reg(opp80->base.ctx, addr, value);
+
+}
+
+/**
+ *	set_spatial_dither
+ *	1) set spatial dithering mode: pattern of seed
+ *	2) set spatical dithering depth: 0 for 18bpp or 1 for 24bpp
+ *	3) set random seed
+ *	4) set random mode
+ *		lfsr is reset every frame or not reset
+ *		RGB dithering method
+ *		0: RGB data are all dithered with x^28+x^3+1
+ *		1: R data is dithered with x^28+x^3+1
+ *		G data is dithered with x^28+X^9+1
+ *		B data is dithered with x^28+x^13+1
+ *		enable high pass filter or not
+ *	5) enable spatical dithering
+ */
+static void set_spatial_dither(
+	struct dce80_opp *opp80,
+	const struct bit_depth_reduction_params *params)
+{
+	uint32_t addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+	uint32_t depth_cntl_value = 0;
+	uint32_t dither_r_value = 0;
+	uint32_t dither_g_value = 0;
+	uint32_t dither_b_value = 0;
+
+	/*Disable spatial (random) dithering*/
+	depth_cntl_value = dm_read_reg(opp80->base.ctx, addr);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_SPATIAL_DITHER_EN);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_SPATIAL_DITHER_MODE);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_SPATIAL_DITHER_DEPTH);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_TEMPORAL_DITHER_EN);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_HIGHPASS_RANDOM_ENABLE);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_FRAME_RANDOM_ENABLE);
+	set_reg_field_value(depth_cntl_value, 0,
+		FMT_BIT_DEPTH_CONTROL, FMT_RGB_RANDOM_ENABLE);
+
+	dm_write_reg(opp80->base.ctx, addr, depth_cntl_value);
+
+	/* no 10bpc on DCE8*/
+	if (params->flags.SPATIAL_DITHER_ENABLED == 0 ||
+		params->flags.SPATIAL_DITHER_DEPTH == 2)
+		return;
+
+	/*Set seed for random values for
+	 * spatial dithering for R,G,B channels*/
+	addr = FMT_REG(mmFMT_DITHER_RAND_R_SEED);
+	set_reg_field_value(dither_r_value, params->r_seed_value,
+		FMT_DITHER_RAND_R_SEED,
+		FMT_RAND_R_SEED);
+	dm_write_reg(opp80->base.ctx, addr, dither_r_value);
+
+	addr = FMT_REG(mmFMT_DITHER_RAND_G_SEED);
+	set_reg_field_value(dither_g_value,
+		params->g_seed_value,
+		FMT_DITHER_RAND_G_SEED,
+		FMT_RAND_G_SEED);
+	dm_write_reg(opp80->base.ctx, addr, dither_g_value);
+
+	addr = FMT_REG(mmFMT_DITHER_RAND_B_SEED);
+	set_reg_field_value(dither_b_value, params->b_seed_value,
+		FMT_DITHER_RAND_B_SEED,
+		FMT_RAND_B_SEED);
+	dm_write_reg(opp80->base.ctx, addr, dither_b_value);
+
+	/* FMT_OFFSET_R_Cr  31:16 0x0 Setting the zero
+	 * offset for the R/Cr channel, lower 4LSB
+	 * is forced to zeros. Typically set to 0
+	 * RGB and 0x80000 YCbCr.
+	 */
+	/* FMT_OFFSET_G_Y   31:16 0x0 Setting the zero
+	 * offset for the G/Y  channel, lower 4LSB is
+	 * forced to zeros. Typically set to 0 RGB
+	 * and 0x80000 YCbCr.
+	 */
+	/* FMT_OFFSET_B_Cb  31:16 0x0 Setting the zero
+	 * offset for the B/Cb channel, lower 4LSB is
+	 * forced to zeros. Typically set to 0 RGB and
+	 * 0x80000 YCbCr.
+	 */
+
+	/*Set spatial dithering bit depth*/
+	set_reg_field_value(depth_cntl_value,
+		params->flags.SPATIAL_DITHER_DEPTH,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_SPATIAL_DITHER_DEPTH);
+
+	/* Set spatial dithering mode
+	 * (default is Seed patterrn AAAA...)
+	 */
+	set_reg_field_value(depth_cntl_value,
+		params->flags.SPATIAL_DITHER_MODE,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_SPATIAL_DITHER_MODE);
+
+	/*Reset only at startup*/
+	set_reg_field_value(depth_cntl_value,
+		params->flags.FRAME_RANDOM,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_FRAME_RANDOM_ENABLE);
+
+	/*Set RGB data dithered with x^28+x^3+1*/
+	set_reg_field_value(depth_cntl_value,
+		params->flags.RGB_RANDOM,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_RGB_RANDOM_ENABLE);
+
+	/*Disable High pass filter*/
+	set_reg_field_value(depth_cntl_value,
+		params->flags.HIGHPASS_RANDOM,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_HIGHPASS_RANDOM_ENABLE);
+
+	/*Enable spatial dithering*/
+	set_reg_field_value(depth_cntl_value,
+		1,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_SPATIAL_DITHER_EN);
+
+	addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+	dm_write_reg(opp80->base.ctx, addr, depth_cntl_value);
+
+}
+
+/**
+ *	SetTemporalDither (Frame Modulation)
+ *	1) set temporal dither depth
+ *	2) select pattern: from hard-coded pattern or programmable pattern
+ *	3) select optimized strips for BGR or RGB LCD sub-pixel
+ *	4) set s matrix
+ *	5) set t matrix
+ *	6) set grey level for 0.25, 0.5, 0.75
+ *	7) enable temporal dithering
+ */
+static void set_temporal_dither(
+	struct dce80_opp *opp80,
+	const struct bit_depth_reduction_params *params)
+{
+	uint32_t addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+	uint32_t value;
+
+	/*Disable temporal (frame modulation) dithering first*/
+	value = dm_read_reg(opp80->base.ctx, addr);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_EN);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_RESET);
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_OFFSET);
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_DEPTH);
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_LEVEL);
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_25FRC_SEL);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_50FRC_SEL);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_75FRC_SEL);
+
+	dm_write_reg(opp80->base.ctx, addr, value);
+
+	/* no 10bpc dither on DCE8*/
+	if (params->flags.FRAME_MODULATION_ENABLED == 0 ||
+		params->flags.FRAME_MODULATION_DEPTH == 2)
+		return;
+
+	/* Set temporal dithering depth*/
+	set_reg_field_value(value,
+		params->flags.FRAME_MODULATION_DEPTH,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_DEPTH);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_RESET);
+
+	set_reg_field_value(value,
+		0,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_OFFSET);
+
+	/*Select legacy pattern based on FRC and Temporal level*/
+	addr = FMT_REG(mmFMT_TEMPORAL_DITHER_PATTERN_CONTROL);
+	dm_write_reg(opp80->base.ctx, addr, 0);
+	/*Set s matrix*/
+	addr = FMT_REG(
+		mmFMT_TEMPORAL_DITHER_PROGRAMMABLE_PATTERN_S_MATRIX);
+	dm_write_reg(opp80->base.ctx, addr, 0);
+	/*Set t matrix*/
+	addr = FMT_REG(
+		mmFMT_TEMPORAL_DITHER_PROGRAMMABLE_PATTERN_T_MATRIX);
+	dm_write_reg(opp80->base.ctx, addr, 0);
+
+	/*Select patterns for 0.25, 0.5 and 0.75 grey level*/
+	set_reg_field_value(value,
+		params->flags.TEMPORAL_LEVEL,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_LEVEL);
+
+	set_reg_field_value(value,
+		params->flags.FRC25,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_25FRC_SEL);
+
+	set_reg_field_value(value,
+		params->flags.FRC50,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_50FRC_SEL);
+
+	set_reg_field_value(value,
+		params->flags.FRC75,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_75FRC_SEL);
+
+	/*Enable bit reduction by temporal (frame modulation) dithering*/
+	set_reg_field_value(value,
+		1,
+		FMT_BIT_DEPTH_CONTROL,
+		FMT_TEMPORAL_DITHER_EN);
+
+	addr = FMT_REG(mmFMT_BIT_DEPTH_CONTROL);
+	dm_write_reg(opp80->base.ctx, addr, value);
+
+}
+
+/**
+ *	Set Clamping
+ *	1) Set clamping format based on bpc - 0 for 6bpc (No clamping)
+ *		1 for 8 bpc
+ *		2 for 10 bpc
+ *		3 for 12 bpc
+ *		7 for programable
+ *	2) Enable clamp if Limited range requested
+ */
+static void set_clamping(
+	struct dce80_opp *opp80,
+	const struct clamping_and_pixel_encoding_params *params)
+{
+	uint32_t clamp_cntl_value = 0;
+	uint32_t red_clamp_value = 0;
+	uint32_t green_clamp_value = 0;
+	uint32_t blue_clamp_value = 0;
+	uint32_t addr = FMT_REG(mmFMT_CLAMP_CNTL);
+
+	clamp_cntl_value = dm_read_reg(opp80->base.ctx, addr);
+
+	set_reg_field_value(clamp_cntl_value,
+		0,
+		FMT_CLAMP_CNTL,
+		FMT_CLAMP_DATA_EN);
+
+	set_reg_field_value(clamp_cntl_value,
+		0,
+		FMT_CLAMP_CNTL,
+		FMT_CLAMP_COLOR_FORMAT);
+
+	switch (params->clamping_level) {
+	case CLAMPING_FULL_RANGE:
+		break;
+
+	case CLAMPING_LIMITED_RANGE_8BPC:
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_DATA_EN);
+
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_COLOR_FORMAT);
+
+		break;
+
+	case CLAMPING_LIMITED_RANGE_10BPC:
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_DATA_EN);
+
+		set_reg_field_value(clamp_cntl_value,
+			2,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_COLOR_FORMAT);
+
+		break;
+	case CLAMPING_LIMITED_RANGE_12BPC:
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_DATA_EN);
+
+		set_reg_field_value(clamp_cntl_value,
+			3,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_COLOR_FORMAT);
+
+		break;
+	case CLAMPING_LIMITED_RANGE_PROGRAMMABLE:
+		set_reg_field_value(clamp_cntl_value,
+			1,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_DATA_EN);
+
+		set_reg_field_value(clamp_cntl_value,
+			7,
+			FMT_CLAMP_CNTL,
+			FMT_CLAMP_COLOR_FORMAT);
+
+		/*set the defaults*/
+		set_reg_field_value(red_clamp_value,
+			0x10,
+			FMT_CLAMP_COMPONENT_R,
+			FMT_CLAMP_LOWER_R);
+
+		set_reg_field_value(red_clamp_value,
+			0xFEF,
+			FMT_CLAMP_COMPONENT_R,
+			FMT_CLAMP_UPPER_R);
+
+		addr = FMT_REG(mmFMT_CLAMP_COMPONENT_R);
+		dm_write_reg(opp80->base.ctx, addr, red_clamp_value);
+
+		set_reg_field_value(green_clamp_value,
+			0x10,
+			FMT_CLAMP_COMPONENT_G,
+			FMT_CLAMP_LOWER_G);
+
+		set_reg_field_value(green_clamp_value,
+			0xFEF,
+			FMT_CLAMP_COMPONENT_G,
+			FMT_CLAMP_UPPER_G);
+
+		addr = FMT_REG(mmFMT_CLAMP_COMPONENT_G);
+		dm_write_reg(opp80->base.ctx, addr, green_clamp_value);
+
+		set_reg_field_value(blue_clamp_value,
+			0x10,
+			FMT_CLAMP_COMPONENT_B,
+			FMT_CLAMP_LOWER_B);
+
+		set_reg_field_value(blue_clamp_value,
+			0xFEF,
+			FMT_CLAMP_COMPONENT_B,
+			FMT_CLAMP_UPPER_B);
+
+		addr = FMT_REG(mmFMT_CLAMP_COMPONENT_B);
+		dm_write_reg(opp80->base.ctx, addr, blue_clamp_value);
+
+		break;
+
+	default:
+		break;
+	}
+
+	addr = FMT_REG(mmFMT_CLAMP_CNTL);
+	/*Set clamp control*/
+	dm_write_reg(opp80->base.ctx, addr, clamp_cntl_value);
+
+}
+
+/**
+ *	set_pixel_encoding
+ *
+ *	Set Pixel Encoding
+ *		0: RGB 4:4:4 or YCbCr 4:4:4 or YOnly
+ *		1: YCbCr 4:2:2
+ */
+static void set_pixel_encoding(
+	struct dce80_opp *opp80,
+	const struct clamping_and_pixel_encoding_params *params)
+{
+	uint32_t fmt_cntl_value;
+	uint32_t addr = FMT_REG(mmFMT_CONTROL);
+
+	/*RGB 4:4:4 or YCbCr 4:4:4 - 0; YCbCr 4:2:2 -1.*/
+	fmt_cntl_value = dm_read_reg(opp80->base.ctx, addr);
+
+	set_reg_field_value(fmt_cntl_value,
+		0,
+		FMT_CONTROL,
+		FMT_PIXEL_ENCODING);
+
+	if (params->pixel_encoding == PIXEL_ENCODING_YCBCR422) {
+		set_reg_field_value(fmt_cntl_value,
+			1,
+			FMT_CONTROL,
+			FMT_PIXEL_ENCODING);
+
+		/*00 - Pixels drop mode ,01 - Pixels average mode*/
+		set_reg_field_value(fmt_cntl_value,
+			0,
+			FMT_CONTROL,
+			FMT_SUBSAMPLING_MODE);
+
+		/*00 - Cb before Cr ,01 - Cr before Cb*/
+		set_reg_field_value(fmt_cntl_value,
+			0,
+			FMT_CONTROL,
+			FMT_SUBSAMPLING_ORDER);
+	}
+	dm_write_reg(opp80->base.ctx, addr, fmt_cntl_value);
+
+}
+
+void dce80_opp_program_bit_depth_reduction(
+	struct output_pixel_processor *opp,
+	const struct bit_depth_reduction_params *params)
+{
+	struct dce80_opp *opp80 = TO_DCE80_OPP(opp);
+
+	set_truncation(opp80, params);
+	set_spatial_dither(opp80, params);
+	set_temporal_dither(opp80, params);
+}
+
+void dce80_opp_program_clamping_and_pixel_encoding(
+	struct output_pixel_processor *opp,
+	const struct clamping_and_pixel_encoding_params *params)
+{
+	struct dce80_opp *opp80 = TO_DCE80_OPP(opp);
+
+	set_clamping(opp80, params);
+	set_pixel_encoding(opp80, params);
+}
+
+void dce80_opp_set_dyn_expansion(
+	struct output_pixel_processor *opp,
+	enum dc_color_space color_sp,
+	enum dc_color_depth color_dpth,
+	enum signal_type signal)
+{
+	struct dce80_opp *opp80 = TO_DCE80_OPP(opp);
+	uint32_t value;
+	bool enable_dyn_exp = false;
+	uint32_t addr = FMT_REG(mmFMT_DYNAMIC_EXP_CNTL);
+
+	value = dm_read_reg(opp->ctx, addr);
+
+	set_reg_field_value(value, 0,
+		FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_EN);
+	set_reg_field_value(value, 0,
+		FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_MODE);
+
+	/* From HW programming guide:
+		FMT_DYNAMIC_EXP_EN = 0 for limited RGB or YCbCr output
+		FMT_DYNAMIC_EXP_EN = 1 for RGB full range only*/
+	if (color_sp == COLOR_SPACE_SRGB)
+		enable_dyn_exp = true;
+
+	/*00 - 10-bit -> 12-bit dynamic expansion*/
+	/*01 - 8-bit  -> 12-bit dynamic expansion*/
+	if (signal == SIGNAL_TYPE_HDMI_TYPE_A) {
+		switch (color_dpth) {
+		case COLOR_DEPTH_888:
+			set_reg_field_value(value, enable_dyn_exp ? 1:0,
+				FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_EN);
+			set_reg_field_value(value, 1,
+				FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_MODE);
+			break;
+		case COLOR_DEPTH_101010:
+			set_reg_field_value(value, enable_dyn_exp ? 1:0,
+				FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_EN);
+			set_reg_field_value(value, 0,
+				FMT_DYNAMIC_EXP_CNTL, FMT_DYNAMIC_EXP_MODE);
+			break;
+		case COLOR_DEPTH_121212:
+			break;
+		default:
+			break;
+		}
+	}
+
+	dm_write_reg(opp->ctx, addr, value);
+}
diff --git a/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_regamma.c b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_regamma.c
new file mode 100644
index 0000000..648e3ef
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/dce80/dce80_opp_regamma.c
@@ -0,0 +1,543 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+/* include DCE8 register header files */
+#include "dce/dce_8_0_d.h"
+#include "dce/dce_8_0_sh_mask.h"
+
+#include "dce80_opp.h"
+#include "gamma_types.h"
+
+#define DCP_REG(reg)\
+	(reg + opp80->offsets.dcp_offset)
+
+#define DCFE_REG(reg)\
+	(reg + opp80->offsets.crtc_offset)
+
+enum {
+	MAX_PWL_ENTRY = 128,
+	MAX_REGIONS_NUMBER = 16
+
+};
+
+struct curve_config {
+	uint32_t offset;
+	int8_t segments[MAX_REGIONS_NUMBER];
+	int8_t begin;
+};
+
+/*
+ *****************************************************************************
+ *  Function: regamma_config_regions_and_segments
+ *
+ *     build regamma curve by using predefined hw points
+ *     uses interface parameters ,like EDID coeff.
+ *
+ * @param   : parameters   interface parameters
+ *  @return void
+ *
+ *  @note
+ *
+ *  @see
+ *
+ *****************************************************************************
+ */
+static void regamma_config_regions_and_segments(
+	struct dce80_opp *opp80, const struct pwl_params *params)
+{
+	const struct gamma_curve *curve;
+	uint32_t value = 0;
+
+	{
+		set_reg_field_value(
+			value,
+			params->arr_points[0].custom_float_x,
+			REGAMMA_CNTLA_START_CNTL,
+			REGAMMA_CNTLA_EXP_REGION_START);
+
+		set_reg_field_value(
+			value,
+			0,
+			REGAMMA_CNTLA_START_CNTL,
+			REGAMMA_CNTLA_EXP_REGION_START_SEGMENT);
+
+		dm_write_reg(opp80->base.ctx,
+				DCP_REG(mmREGAMMA_CNTLA_START_CNTL),
+				value);
+	}
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			params->arr_points[0].custom_float_slope,
+			REGAMMA_CNTLA_SLOPE_CNTL,
+			REGAMMA_CNTLA_EXP_REGION_LINEAR_SLOPE);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_SLOPE_CNTL), value);
+	}
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			params->arr_points[1].custom_float_x,
+			REGAMMA_CNTLA_END_CNTL1,
+			REGAMMA_CNTLA_EXP_REGION_END);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_END_CNTL1), value);
+	}
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			params->arr_points[2].custom_float_slope,
+			REGAMMA_CNTLA_END_CNTL2,
+			REGAMMA_CNTLA_EXP_REGION_END_BASE);
+
+		set_reg_field_value(
+			value,
+			params->arr_points[1].custom_float_y,
+			REGAMMA_CNTLA_END_CNTL2,
+			REGAMMA_CNTLA_EXP_REGION_END_SLOPE);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_END_CNTL2), value);
+	}
+
+	curve = params->arr_curve_points;
+
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_0_1,
+			REGAMMA_CNTLA_EXP_REGION0_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_0_1,
+			REGAMMA_CNTLA_EXP_REGION0_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_0_1,
+			REGAMMA_CNTLA_EXP_REGION1_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_0_1,
+			REGAMMA_CNTLA_EXP_REGION1_NUM_SEGMENTS);
+
+		dm_write_reg(
+			opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_0_1),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_2_3,
+			REGAMMA_CNTLA_EXP_REGION2_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_2_3,
+			REGAMMA_CNTLA_EXP_REGION2_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_2_3,
+			REGAMMA_CNTLA_EXP_REGION3_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_2_3,
+			REGAMMA_CNTLA_EXP_REGION3_NUM_SEGMENTS);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_2_3),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_4_5,
+			REGAMMA_CNTLA_EXP_REGION4_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_4_5,
+			REGAMMA_CNTLA_EXP_REGION4_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_4_5,
+			REGAMMA_CNTLA_EXP_REGION5_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_4_5,
+			REGAMMA_CNTLA_EXP_REGION5_NUM_SEGMENTS);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_4_5),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_6_7,
+			REGAMMA_CNTLA_EXP_REGION6_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_6_7,
+			REGAMMA_CNTLA_EXP_REGION6_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_6_7,
+			REGAMMA_CNTLA_EXP_REGION7_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_6_7,
+			REGAMMA_CNTLA_EXP_REGION7_NUM_SEGMENTS);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_6_7),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_8_9,
+			REGAMMA_CNTLA_EXP_REGION8_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_8_9,
+			REGAMMA_CNTLA_EXP_REGION8_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_8_9,
+			REGAMMA_CNTLA_EXP_REGION9_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_8_9,
+			REGAMMA_CNTLA_EXP_REGION9_NUM_SEGMENTS);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_8_9),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_10_11,
+			REGAMMA_CNTLA_EXP_REGION10_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_10_11,
+			REGAMMA_CNTLA_EXP_REGION10_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_10_11,
+			REGAMMA_CNTLA_EXP_REGION11_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_10_11,
+			REGAMMA_CNTLA_EXP_REGION11_NUM_SEGMENTS);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_10_11),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_12_13,
+			REGAMMA_CNTLA_EXP_REGION12_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_12_13,
+			REGAMMA_CNTLA_EXP_REGION12_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_12_13,
+			REGAMMA_CNTLA_EXP_REGION13_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_12_13,
+			REGAMMA_CNTLA_EXP_REGION13_NUM_SEGMENTS);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_12_13),
+			value);
+	}
+
+	curve += 2;
+	{
+		value = 0;
+		set_reg_field_value(
+			value,
+			curve[0].offset,
+			REGAMMA_CNTLA_REGION_14_15,
+			REGAMMA_CNTLA_EXP_REGION14_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[0].segments_num,
+			REGAMMA_CNTLA_REGION_14_15,
+			REGAMMA_CNTLA_EXP_REGION14_NUM_SEGMENTS);
+
+		set_reg_field_value(
+			value,
+			curve[1].offset,
+			REGAMMA_CNTLA_REGION_14_15,
+			REGAMMA_CNTLA_EXP_REGION15_LUT_OFFSET);
+
+		set_reg_field_value(
+			value,
+			curve[1].segments_num,
+			REGAMMA_CNTLA_REGION_14_15,
+			REGAMMA_CNTLA_EXP_REGION15_NUM_SEGMENTS);
+
+		dm_write_reg(opp80->base.ctx,
+			DCP_REG(mmREGAMMA_CNTLA_REGION_14_15),
+			value);
+	}
+}
+
+static void program_pwl(
+	struct dce80_opp *opp80,
+	const struct pwl_params *params)
+{
+	uint32_t value;
+
+	{
+		uint8_t max_tries = 10;
+		uint8_t counter = 0;
+
+		/* Power on LUT memory */
+		value = dm_read_reg(opp80->base.ctx,
+				DCFE_REG(mmDCFE_MEM_LIGHT_SLEEP_CNTL));
+
+		set_reg_field_value(
+			value,
+			1,
+			DCFE_MEM_LIGHT_SLEEP_CNTL,
+			REGAMMA_LUT_LIGHT_SLEEP_DIS);
+
+		dm_write_reg(opp80->base.ctx,
+				DCFE_REG(mmDCFE_MEM_LIGHT_SLEEP_CNTL), value);
+
+		while (counter < max_tries) {
+			value =
+				dm_read_reg(
+					opp80->base.ctx,
+					DCFE_REG(mmDCFE_MEM_LIGHT_SLEEP_CNTL));
+
+			if (get_reg_field_value(
+				value,
+				DCFE_MEM_LIGHT_SLEEP_CNTL,
+				REGAMMA_LUT_MEM_PWR_STATE) == 0)
+				break;
+
+			++counter;
+		}
+
+		if (counter == max_tries) {
+			dm_logger_write(opp80->base.ctx->logger, LOG_WARNING,
+				"%s: regamma lut was not powered on "
+				"in a timely manner,"
+				" programming still proceeds\n",
+				__func__);
+		}
+	}
+
+	value = 0;
+
+	set_reg_field_value(
+		value,
+		7,
+		REGAMMA_LUT_WRITE_EN_MASK,
+		REGAMMA_LUT_WRITE_EN_MASK);
+
+	dm_write_reg(opp80->base.ctx,
+		DCP_REG(mmREGAMMA_LUT_WRITE_EN_MASK), value);
+	dm_write_reg(opp80->base.ctx,
+		DCP_REG(mmREGAMMA_LUT_INDEX), 0);
+
+	/* Program REGAMMA_LUT_DATA */
+	{
+		const uint32_t addr = DCP_REG(mmREGAMMA_LUT_DATA);
+
+		uint32_t i = 0;
+
+		const struct pwl_result_data *rgb =
+				params->rgb_resulted;
+
+		while (i != params->hw_points_num) {
+			dm_write_reg(opp80->base.ctx, addr, rgb->red_reg);
+			dm_write_reg(opp80->base.ctx, addr, rgb->green_reg);
+			dm_write_reg(opp80->base.ctx, addr, rgb->blue_reg);
+
+			dm_write_reg(opp80->base.ctx, addr,
+				rgb->delta_red_reg);
+			dm_write_reg(opp80->base.ctx, addr,
+				rgb->delta_green_reg);
+			dm_write_reg(opp80->base.ctx, addr,
+				rgb->delta_blue_reg);
+
+			++rgb;
+			++i;
+		}
+	}
+
+	/*  we are done with DCP LUT memory; re-enable low power mode */
+	value = dm_read_reg(opp80->base.ctx,
+			DCFE_REG(mmDCFE_MEM_LIGHT_SLEEP_CNTL));
+
+	set_reg_field_value(
+		value,
+		0,
+		DCFE_MEM_LIGHT_SLEEP_CNTL,
+		REGAMMA_LUT_LIGHT_SLEEP_DIS);
+
+	dm_write_reg(opp80->base.ctx, DCFE_REG(mmDCFE_MEM_LIGHT_SLEEP_CNTL),
+			value);
+}
+
+void dce80_opp_power_on_regamma_lut(
+	struct output_pixel_processor *opp,
+	bool power_on)
+{
+	struct dce80_opp *opp80 = TO_DCE80_OPP(opp);
+
+	uint32_t value =
+		dm_read_reg(opp->ctx, DCFE_REG(mmDCFE_MEM_LIGHT_SLEEP_CNTL));
+
+	set_reg_field_value(
+		value,
+		power_on,
+		DCFE_MEM_LIGHT_SLEEP_CNTL,
+		REGAMMA_LUT_LIGHT_SLEEP_DIS);
+
+	set_reg_field_value(
+		value,
+		power_on,
+		DCFE_MEM_LIGHT_SLEEP_CNTL,
+		DCP_LUT_LIGHT_SLEEP_DIS);
+
+	dm_write_reg(opp->ctx, DCFE_REG(mmDCFE_MEM_LIGHT_SLEEP_CNTL), value);
+}
+
+bool dce80_opp_program_regamma_pwl(
+	struct output_pixel_processor *opp,
+	const struct pwl_params *params)
+{
+
+	struct dce80_opp *opp80 = TO_DCE80_OPP(opp);
+
+	regamma_config_regions_and_segments(opp80, params);
+
+	program_pwl(opp80, params);
+
+	return true;
+}
+
+void dce80_opp_set_regamma_mode(struct output_pixel_processor *opp,
+		enum opp_regamma mode)
+{
+	struct dce80_opp *opp80 = TO_DCE80_OPP(opp);
+	uint32_t value = dm_read_reg(opp80->base.ctx,
+				DCP_REG(mmREGAMMA_CONTROL));
+
+	set_reg_field_value(
+		value,
+		mode,
+		REGAMMA_CONTROL,
+		GRPH_REGAMMA_MODE);
+
+	dm_write_reg(opp80->base.ctx, DCP_REG(mmREGAMMA_CONTROL), value);
+}
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/Makefile b/drivers/gpu/drm/amd/display/dc/gpu/Makefile
new file mode 100644
index 0000000..fd17af1
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/Makefile
@@ -0,0 +1,36 @@
+#
+# Makefile for the 'gpu' sub-component of DAL.
+# It provides the control and status of HW adapter resources,
+# that are global for the ASIC and sharable between pipes.
+
+GPU = display_clock.o divider_range.o
+
+AMD_DAL_GPU = $(addprefix $(AMDDALPATH)/dc/gpu/,$(GPU))
+
+AMD_DISPLAY_FILES += $(AMD_DAL_GPU)
+
+###############################################################################
+# DCE 80 family
+###############################################################################
+GPU_DCE80 = display_clock_dce80.o
+
+AMD_DAL_GPU_DCE80 = $(addprefix $(AMDDALPATH)/dc/gpu/dce80/,$(GPU_DCE80))
+
+AMD_DISPLAY_FILES += $(AMD_DAL_GPU_DCE80)
+
+
+###############################################################################
+# DCE 110 family
+###############################################################################
+GPU_DCE110 = display_clock_dce110.o
+
+AMD_DAL_GPU_DCE110 = $(addprefix $(AMDDALPATH)/dc/gpu/dce110/,$(GPU_DCE110))
+
+AMD_DISPLAY_FILES += $(AMD_DAL_GPU_DCE110)
+
+GPU_DCE112 = display_clock_dce112.o
+
+AMD_DAL_GPU_DCE112 = $(addprefix $(AMDDALPATH)/dc/gpu/dce112/,$(GPU_DCE112))
+
+AMD_DISPLAY_FILES += $(AMD_DAL_GPU_DCE110) $(AMD_DAL_GPU_DCE112)
+
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/dce110/display_clock_dce110.c b/drivers/gpu/drm/amd/display/dc/gpu/dce110/display_clock_dce110.c
new file mode 100644
index 0000000..1bc39f1
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/dce110/display_clock_dce110.c
@@ -0,0 +1,1035 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+#include "dce/dce_11_0_d.h"
+#include "dce/dce_11_0_sh_mask.h"
+
+#include "include/bios_parser_interface.h"
+#include "include/fixed32_32.h"
+#include "include/logger_interface.h"
+
+#include "../divider_range.h"
+
+#include "display_clock_dce110.h"
+#include "dc.h"
+
+#define FROM_DISPLAY_CLOCK(base) \
+	container_of(base, struct display_clock_dce110, disp_clk_base)
+
+#define PSR_SET_WAITLOOP 0x31
+
+static struct state_dependent_clocks max_clks_by_state[] = {
+/*ClocksStateInvalid - should not be used*/
+{ .display_clk_khz = 0, .pixel_clk_khz = 0 },
+/*ClocksStateUltraLow - currently by HW design team not supposed to be used*/
+{ .display_clk_khz = 352000, .pixel_clk_khz = 330000 },
+/*ClocksStateLow*/
+{ .display_clk_khz = 352000, .pixel_clk_khz = 330000 },
+/*ClocksStateNominal*/
+{ .display_clk_khz = 467000, .pixel_clk_khz = 400000 },
+/*ClocksStatePerformance*/
+{ .display_clk_khz = 643000, .pixel_clk_khz = 400000 } };
+
+/* Starting point for each divider range.*/
+enum divider_range_start {
+	DIVIDER_RANGE_01_START = 200, /* 2.00*/
+	DIVIDER_RANGE_02_START = 1600, /* 16.00*/
+	DIVIDER_RANGE_03_START = 3200, /* 32.00*/
+	DIVIDER_RANGE_SCALE_FACTOR = 100 /* Results are scaled up by 100.*/
+};
+
+/* Array identifiers and count for the divider ranges.*/
+enum divider_range_count {
+	DIVIDER_RANGE_01 = 0,
+	DIVIDER_RANGE_02,
+	DIVIDER_RANGE_03,
+	DIVIDER_RANGE_MAX /* == 3*/
+};
+
+/* Ranges for divider identifiers (Divider ID or DID)
+ mmDENTIST_DISPCLK_CNTL.DENTIST_DISPCLK_WDIVIDER*/
+enum divider_id_register_setting {
+	DIVIDER_RANGE_01_BASE_DIVIDER_ID = 0X08,
+	DIVIDER_RANGE_02_BASE_DIVIDER_ID = 0X40,
+	DIVIDER_RANGE_03_BASE_DIVIDER_ID = 0X60,
+	DIVIDER_RANGE_MAX_DIVIDER_ID = 0X80
+};
+
+/* Step size between each divider within a range.
+ Incrementing the DENTIST_DISPCLK_WDIVIDER by one
+ will increment the divider by this much.*/
+enum divider_range_step_size {
+	DIVIDER_RANGE_01_STEP_SIZE = 25, /* 0.25*/
+	DIVIDER_RANGE_02_STEP_SIZE = 50, /* 0.50*/
+	DIVIDER_RANGE_03_STEP_SIZE = 100 /* 1.00 */
+};
+
+union dce110_dmcu_psr_config_data_wait_loop_reg1 {
+	struct {
+		unsigned int waitLoop:16; /* [15:0] */
+		unsigned int reserved:16; /* [31:16] */
+	} bits;
+	unsigned int u32All;
+};
+
+static struct divider_range divider_ranges[DIVIDER_RANGE_MAX];
+
+#define DCE110_DFS_BYPASS_THRESHOLD_KHZ 400000
+/*****************************************************************************
+ * static functions
+ *****************************************************************************/
+
+/*
+ * store_max_clocks_state
+ *
+ * @brief
+ * Cache the clock state
+ *
+ * @param
+ * struct display_clock *base - [out] cach the state in this structure
+ * enum clocks_state max_clocks_state - [in] state to be stored
+ */
+static void store_max_clocks_state(
+	struct display_clock *base,
+	enum clocks_state max_clocks_state)
+{
+	struct display_clock_dce110 *dc = DCLCK110_FROM_BASE(base);
+
+	switch (max_clocks_state) {
+	case CLOCKS_STATE_LOW:
+	case CLOCKS_STATE_NOMINAL:
+	case CLOCKS_STATE_PERFORMANCE:
+	case CLOCKS_STATE_ULTRA_LOW:
+		dc->max_clks_state = max_clocks_state;
+		break;
+
+	case CLOCKS_STATE_INVALID:
+	default:
+		/*Invalid Clocks State!*/
+		ASSERT_CRITICAL(false);
+		break;
+	}
+}
+
+static enum clocks_state get_min_clocks_state(struct display_clock *base)
+{
+	return base->cur_min_clks_state;
+}
+
+static bool set_min_clocks_state(
+	struct display_clock *base,
+	enum clocks_state clocks_state)
+{
+	struct display_clock_dce110 *dc = DCLCK110_FROM_BASE(base);
+	struct dm_pp_power_level_change_request level_change_req = {
+			DM_PP_POWER_LEVEL_INVALID};
+
+	if (clocks_state > dc->max_clks_state) {
+		/*Requested state exceeds max supported state.*/
+		dm_logger_write(base->ctx->logger, LOG_WARNING,
+				"Requested state exceeds max supported state");
+		return false;
+	} else if (clocks_state == base->cur_min_clks_state) {
+		/*if we're trying to set the same state, we can just return
+		 * since nothing needs to be done*/
+		return true;
+	}
+
+	switch (clocks_state) {
+	case CLOCKS_STATE_ULTRA_LOW:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_ULTRA_LOW;
+		break;
+	case CLOCKS_STATE_LOW:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_LOW;
+		break;
+	case CLOCKS_STATE_NOMINAL:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_NOMINAL;
+		break;
+	case CLOCKS_STATE_PERFORMANCE:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_PERFORMANCE;
+		break;
+	case CLOCKS_STATE_INVALID:
+	default:
+		dm_logger_write(base->ctx->logger, LOG_WARNING,
+				"Requested state invalid state");
+		return false;
+	}
+
+	/* get max clock state from PPLIB */
+	if (dm_pp_apply_power_level_change_request(
+			base->ctx, &level_change_req))
+		base->cur_min_clks_state = clocks_state;
+
+	return true;
+}
+
+static uint32_t get_dp_ref_clk_frequency(struct display_clock *dc)
+{
+	uint32_t dispclk_cntl_value;
+	uint32_t dp_ref_clk_cntl_value;
+	uint32_t dp_ref_clk_cntl_src_sel_value;
+	uint32_t dp_ref_clk_khz = 600000;
+	uint32_t target_div = INVALID_DIVIDER;
+	struct display_clock_dce110 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	/* ASSERT DP Reference Clock source is from DFS*/
+	dp_ref_clk_cntl_value = dm_read_reg(dc->ctx,
+			mmDPREFCLK_CNTL);
+
+	dp_ref_clk_cntl_src_sel_value =
+			get_reg_field_value(
+				dp_ref_clk_cntl_value,
+				DPREFCLK_CNTL, DPREFCLK_SRC_SEL);
+
+	ASSERT(dp_ref_clk_cntl_src_sel_value == 0);
+
+	/* Read the mmDENTIST_DISPCLK_CNTL to get the currently
+	 * programmed DID DENTIST_DPREFCLK_WDIVIDER*/
+	dispclk_cntl_value = dm_read_reg(dc->ctx,
+			mmDENTIST_DISPCLK_CNTL);
+
+	/* Convert DENTIST_DPREFCLK_WDIVIDERto actual divider*/
+	target_div = dal_divider_range_get_divider(
+		divider_ranges,
+		DIVIDER_RANGE_MAX,
+		get_reg_field_value(dispclk_cntl_value,
+			DENTIST_DISPCLK_CNTL,
+			DENTIST_DPREFCLK_WDIVIDER));
+
+	if (target_div != INVALID_DIVIDER) {
+		/* Calculate the current DFS clock, in kHz.*/
+		dp_ref_clk_khz = (DIVIDER_RANGE_SCALE_FACTOR
+			* disp_clk->dentist_vco_freq_khz) / target_div;
+	}
+
+	/* SW will adjust DP REF Clock average value for all purposes
+	 * (DP DTO / DP Audio DTO and DP GTC)
+	 if clock is spread for all cases:
+	 -if SS enabled on DP Ref clock and HW de-spreading enabled with SW
+	 calculations for DS_INCR/DS_MODULO (this is planned to be default case)
+	 -if SS enabled on DP Ref clock and HW de-spreading enabled with HW
+	 calculations (not planned to be used, but average clock should still
+	 be valid)
+	 -if SS enabled on DP Ref clock and HW de-spreading disabled
+	 (should not be case with CIK) then SW should program all rates
+	 generated according to average value (case as with previous ASICs)
+	  */
+	if ((disp_clk->ss_on_gpu_pll) && (disp_clk->gpu_pll_ss_divider != 0)) {
+		struct fixed32_32 ss_percentage = dal_fixed32_32_div_int(
+				dal_fixed32_32_from_fraction(
+					disp_clk->gpu_pll_ss_percentage,
+					disp_clk->gpu_pll_ss_divider), 200);
+		struct fixed32_32 adj_dp_ref_clk_khz;
+
+		ss_percentage = dal_fixed32_32_sub(dal_fixed32_32_one,
+								ss_percentage);
+		adj_dp_ref_clk_khz =
+			dal_fixed32_32_mul_int(
+				ss_percentage,
+				dp_ref_clk_khz);
+		dp_ref_clk_khz = dal_fixed32_32_floor(adj_dp_ref_clk_khz);
+	}
+
+	return dp_ref_clk_khz;
+}
+
+static void destroy(struct display_clock **base)
+{
+	struct display_clock_dce110 *dc110;
+
+	dc110 = DCLCK110_FROM_BASE(*base);
+
+	dm_free(dc110);
+
+	*base = NULL;
+}
+
+static uint32_t get_validation_clock(struct display_clock *dc)
+{
+	uint32_t clk = 0;
+	struct display_clock_dce110 *disp_clk = DCLCK110_FROM_BASE(dc);
+
+	switch (disp_clk->max_clks_state) {
+	case CLOCKS_STATE_ULTRA_LOW:
+		/*Currently not supported, it has 0 in table entry*/
+	case CLOCKS_STATE_LOW:
+		clk = max_clks_by_state[CLOCKS_STATE_LOW].
+						display_clk_khz;
+		break;
+
+	case CLOCKS_STATE_NOMINAL:
+		clk = max_clks_by_state[CLOCKS_STATE_NOMINAL].
+						display_clk_khz;
+		break;
+
+	case CLOCKS_STATE_PERFORMANCE:
+		clk = max_clks_by_state[CLOCKS_STATE_PERFORMANCE].
+						display_clk_khz;
+		break;
+
+	case CLOCKS_STATE_INVALID:
+	default:
+		/*Invalid Clocks State*/
+		dm_logger_write(dc->ctx->logger, LOG_WARNING,
+				"Invalid clock state");
+		/* just return the display engine clock for
+		 * lowest supported state*/
+		clk = max_clks_by_state[CLOCKS_STATE_LOW].
+						display_clk_khz;
+		break;
+	}
+	return clk;
+}
+
+static struct fixed32_32 get_deep_color_factor(struct min_clock_params *params)
+{
+	/* DeepColorFactor = IF (HDMI = True, bpp / 24, 1)*/
+	struct fixed32_32 deep_color_factor = dal_fixed32_32_from_int(1);
+
+	if (params->signal_type != SIGNAL_TYPE_HDMI_TYPE_A)
+		return deep_color_factor;
+
+	switch (params->deep_color_depth) {
+	case COLOR_DEPTH_101010:
+		/*deep color ratio for 30bpp is 30/24 = 1.25*/
+		deep_color_factor = dal_fixed32_32_from_fraction(30, 24);
+		break;
+
+	case COLOR_DEPTH_121212:
+		/* deep color ratio for 36bpp is 36/24 = 1.5*/
+		deep_color_factor = dal_fixed32_32_from_fraction(36, 24);
+		break;
+
+	case COLOR_DEPTH_161616:
+		/* deep color ratio for 48bpp is 48/24 = 2.0 */
+		deep_color_factor = dal_fixed32_32_from_fraction(48, 24);
+		break;
+	default:
+		break;
+	}
+	return deep_color_factor;
+}
+
+static struct fixed32_32 get_scaler_efficiency(
+	struct dc_context *ctx,
+	struct min_clock_params *params)
+{
+	struct fixed32_32 scaler_efficiency = dal_fixed32_32_from_int(3);
+
+	if (params->scaler_efficiency == V_SCALER_EFFICIENCY_LB18BPP) {
+		scaler_efficiency =
+			dal_fixed32_32_add(
+				dal_fixed32_32_from_fraction(35555, 10000),
+				dal_fixed32_32_from_fraction(
+					55556,
+					100000 * 10000));
+	} else if (params->scaler_efficiency == V_SCALER_EFFICIENCY_LB24BPP) {
+		scaler_efficiency =
+			dal_fixed32_32_add(
+				dal_fixed32_32_from_fraction(34285, 10000),
+				dal_fixed32_32_from_fraction(
+					71429,
+					100000 * 10000));
+	} else if (params->scaler_efficiency == V_SCALER_EFFICIENCY_LB30BPP)
+		scaler_efficiency = dal_fixed32_32_from_fraction(32, 10);
+
+	return scaler_efficiency;
+}
+
+static struct fixed32_32 get_lb_lines_in_per_line_out(
+		struct min_clock_params *params,
+		struct fixed32_32 v_scale_ratio)
+{
+	struct fixed32_32 two = dal_fixed32_32_from_int(2);
+	struct fixed32_32 four = dal_fixed32_32_from_int(4);
+	struct fixed32_32 f4_to_3 = dal_fixed32_32_from_fraction(4, 3);
+	struct fixed32_32 f6_to_4 = dal_fixed32_32_from_fraction(6, 4);
+
+	if (params->line_buffer_prefetch_enabled)
+		return dal_fixed32_32_max(v_scale_ratio, dal_fixed32_32_one);
+	else if (dal_fixed32_32_le(v_scale_ratio, dal_fixed32_32_one))
+		return dal_fixed32_32_one;
+	else if (dal_fixed32_32_le(v_scale_ratio, f4_to_3))
+		return f4_to_3;
+	else if (dal_fixed32_32_le(v_scale_ratio, f6_to_4))
+		return f6_to_4;
+	else if (dal_fixed32_32_le(v_scale_ratio, two))
+		return two;
+	else if (dal_fixed32_32_le(v_scale_ratio, dal_fixed32_32_from_int(3)))
+		return four;
+	else
+		return dal_fixed32_32_zero;
+}
+
+static uint32_t get_actual_required_display_clk(
+	struct display_clock_dce110 *disp_clk,
+	uint32_t target_clk_khz)
+{
+	uint32_t disp_clk_khz = target_clk_khz;
+	uint32_t div = INVALID_DIVIDER;
+	uint32_t did = INVALID_DID;
+	uint32_t scaled_vco =
+		disp_clk->dentist_vco_freq_khz * DIVIDER_RANGE_SCALE_FACTOR;
+
+	ASSERT_CRITICAL(!!disp_clk_khz);
+
+	if (disp_clk_khz)
+		div = scaled_vco / disp_clk_khz;
+
+	did = dal_divider_range_get_did(divider_ranges, DIVIDER_RANGE_MAX, div);
+
+	if (did != INVALID_DID) {
+		div = dal_divider_range_get_divider(
+			divider_ranges, DIVIDER_RANGE_MAX, did);
+
+		if ((div != INVALID_DIVIDER) &&
+			(did > DIVIDER_RANGE_01_BASE_DIVIDER_ID))
+			if (disp_clk_khz > (scaled_vco / div))
+				div = dal_divider_range_get_divider(
+					divider_ranges, DIVIDER_RANGE_MAX,
+					did - 1);
+
+		if (div != INVALID_DIVIDER)
+			disp_clk_khz = scaled_vco / div;
+
+	}
+	/* We need to add 10KHz to this value because the accuracy in VBIOS is
+	 in 10KHz units. So we need to always round the last digit up in order
+	 to reach the next div level.*/
+	return disp_clk_khz + 10;
+}
+
+static uint32_t calc_single_display_min_clks(
+	struct display_clock *base,
+	struct min_clock_params *params,
+	bool set_clk)
+{
+	struct fixed32_32 h_scale_ratio = dal_fixed32_32_one;
+	struct fixed32_32 v_scale_ratio = dal_fixed32_32_one;
+	uint32_t pix_clk_khz = 0;
+	uint32_t lb_source_width = 0;
+	struct fixed32_32 deep_color_factor;
+	struct fixed32_32 scaler_efficiency;
+	struct fixed32_32 v_filter_init;
+	uint32_t v_filter_init_trunc;
+	uint32_t num_lines_at_frame_start = 3;
+	struct fixed32_32 v_filter_init_ceil;
+	struct fixed32_32 lines_per_lines_out_at_frame_start;
+	struct fixed32_32 lb_lines_in_per_line_out; /* in middle of the frame*/
+	uint32_t src_wdth_rnd_to_chunks;
+	struct fixed32_32 scaling_coeff;
+	struct fixed32_32 h_blank_granularity_factor =
+			dal_fixed32_32_one;
+	struct fixed32_32 fx_disp_clk_mhz;
+	struct fixed32_32 line_time;
+	struct fixed32_32 disp_pipe_pix_throughput;
+	struct fixed32_32 fx_alt_disp_clk_mhz;
+	uint32_t disp_clk_khz;
+	uint32_t alt_disp_clk_khz;
+	struct display_clock_dce110 *disp_clk_110 = DCLCK110_FROM_BASE(base);
+	uint32_t max_clk_khz = get_validation_clock(base);
+	bool panning_allowed = false; /* TODO: receive this value from AS */
+
+	if (params == NULL) {
+		dm_logger_write(base->ctx->logger, LOG_WARNING,
+				"Invalid input parameter in %s",
+				__func__);
+		return 0;
+	}
+
+	deep_color_factor = get_deep_color_factor(params);
+	scaler_efficiency = get_scaler_efficiency(base->ctx, params);
+	pix_clk_khz = params->requested_pixel_clock;
+	lb_source_width = params->source_view.width;
+
+	if (0 != params->dest_view.height && 0 != params->dest_view.width) {
+
+		h_scale_ratio = dal_fixed32_32_from_fraction(
+			params->source_view.width,
+			params->dest_view.width);
+		v_scale_ratio = dal_fixed32_32_from_fraction(
+			params->source_view.height,
+			params->dest_view.height);
+	} else {
+		dm_logger_write(base->ctx->logger, LOG_WARNING,
+				"Destination height or width is 0!\n");
+	}
+
+	v_filter_init =
+		dal_fixed32_32_add(
+			v_scale_ratio,
+			dal_fixed32_32_add_int(
+				dal_fixed32_32_div_int(
+					dal_fixed32_32_mul_int(
+						v_scale_ratio,
+						params->timing_info.INTERLACED),
+					2),
+				params->scaling_info.v_taps + 1));
+	v_filter_init = dal_fixed32_32_div_int(v_filter_init, 2);
+
+	v_filter_init_trunc = dal_fixed32_32_floor(v_filter_init);
+
+	v_filter_init_ceil = dal_fixed32_32_from_fraction(
+						v_filter_init_trunc, 2);
+	v_filter_init_ceil = dal_fixed32_32_from_int(
+		dal_fixed32_32_ceil(v_filter_init_ceil));
+	v_filter_init_ceil = dal_fixed32_32_mul_int(v_filter_init_ceil, 2);
+
+	lines_per_lines_out_at_frame_start =
+			dal_fixed32_32_div_int(v_filter_init_ceil,
+					num_lines_at_frame_start);
+	lb_lines_in_per_line_out =
+			get_lb_lines_in_per_line_out(params, v_scale_ratio);
+
+	if (panning_allowed)
+		src_wdth_rnd_to_chunks =
+			((lb_source_width - 1) / 128) * 128 + 256;
+	else
+		src_wdth_rnd_to_chunks =
+			((lb_source_width + 127) / 128) * 128;
+
+	scaling_coeff =
+		dal_fixed32_32_div(
+			dal_fixed32_32_from_int(params->scaling_info.v_taps),
+			scaler_efficiency);
+
+	if (dal_fixed32_32_le(h_scale_ratio, dal_fixed32_32_one))
+		scaling_coeff = dal_fixed32_32_max(
+			dal_fixed32_32_from_int(
+				dal_fixed32_32_ceil(
+					dal_fixed32_32_from_fraction(
+						params->scaling_info.h_taps,
+						4))),
+			dal_fixed32_32_max(
+				dal_fixed32_32_mul(
+					scaling_coeff,
+					h_scale_ratio),
+				dal_fixed32_32_one));
+
+	if (!params->line_buffer_prefetch_enabled &&
+		dal_fixed32_32_floor(lb_lines_in_per_line_out) != 2 &&
+		dal_fixed32_32_floor(lb_lines_in_per_line_out) != 4) {
+		uint32_t line_total_pixel =
+			params->timing_info.h_total + lb_source_width - 256;
+		h_blank_granularity_factor = dal_fixed32_32_div(
+			dal_fixed32_32_from_int(params->timing_info.h_total),
+			dal_fixed32_32_div(
+			dal_fixed32_32_from_fraction(
+				line_total_pixel, 2),
+				h_scale_ratio));
+	}
+
+	/* Calculate display clock with ramping. Ramping factor is 1.1*/
+	fx_disp_clk_mhz =
+		dal_fixed32_32_div_int(
+			dal_fixed32_32_mul_int(scaling_coeff, 11),
+			10);
+	line_time = dal_fixed32_32_from_fraction(
+			params->timing_info.h_total * 1000, pix_clk_khz);
+
+	disp_pipe_pix_throughput = dal_fixed32_32_mul(
+			lb_lines_in_per_line_out, h_blank_granularity_factor);
+	disp_pipe_pix_throughput = dal_fixed32_32_max(
+			disp_pipe_pix_throughput,
+			lines_per_lines_out_at_frame_start);
+	disp_pipe_pix_throughput = dal_fixed32_32_div(dal_fixed32_32_mul_int(
+			disp_pipe_pix_throughput, src_wdth_rnd_to_chunks),
+			line_time);
+
+	if (0 != params->timing_info.h_total) {
+		fx_disp_clk_mhz =
+			dal_fixed32_32_max(
+				dal_fixed32_32_div_int(
+					dal_fixed32_32_mul_int(
+						scaling_coeff, pix_clk_khz),
+						1000),
+				disp_pipe_pix_throughput);
+		fx_disp_clk_mhz =
+			dal_fixed32_32_mul(
+				fx_disp_clk_mhz,
+				dal_fixed32_32_from_fraction(11, 10));
+	}
+
+	fx_disp_clk_mhz = dal_fixed32_32_max(fx_disp_clk_mhz,
+		dal_fixed32_32_mul(deep_color_factor,
+		dal_fixed32_32_from_fraction(11, 10)));
+
+	/* Calculate display clock without ramping */
+	fx_alt_disp_clk_mhz = scaling_coeff;
+
+	if (0 != params->timing_info.h_total) {
+		fx_alt_disp_clk_mhz = dal_fixed32_32_max(
+				dal_fixed32_32_div_int(dal_fixed32_32_mul_int(
+						scaling_coeff, pix_clk_khz),
+						1000),
+				dal_fixed32_32_div_int(dal_fixed32_32_mul_int(
+						disp_pipe_pix_throughput, 105),
+						100));
+	}
+
+	if (set_clk && disp_clk_110->ss_on_gpu_pll &&
+			disp_clk_110->gpu_pll_ss_divider)
+		fx_alt_disp_clk_mhz = dal_fixed32_32_mul(fx_alt_disp_clk_mhz,
+				dal_fixed32_32_add_int(
+				dal_fixed32_32_div_int(
+				dal_fixed32_32_div_int(
+				dal_fixed32_32_from_fraction(
+				disp_clk_110->gpu_pll_ss_percentage,
+				disp_clk_110->gpu_pll_ss_divider), 100),
+				2),
+				1));
+
+	/* convert to integer */
+	disp_clk_khz = dal_fixed32_32_round(
+			dal_fixed32_32_mul_int(fx_disp_clk_mhz, 1000));
+	alt_disp_clk_khz = dal_fixed32_32_round(
+			dal_fixed32_32_mul_int(fx_alt_disp_clk_mhz, 1000));
+
+	if ((disp_clk_khz > max_clk_khz) && (alt_disp_clk_khz <= max_clk_khz))
+		disp_clk_khz = alt_disp_clk_khz;
+
+	if (set_clk) { /* only compensate clock if we are going to set it.*/
+		disp_clk_khz = get_actual_required_display_clk(
+			disp_clk_110, disp_clk_khz);
+	}
+
+	disp_clk_khz = disp_clk_khz > max_clk_khz ? max_clk_khz : disp_clk_khz;
+
+	return disp_clk_khz;
+}
+
+static uint32_t calculate_min_clock(
+	struct display_clock *base,
+	uint32_t path_num,
+	struct min_clock_params *params)
+{
+	uint32_t i;
+	uint32_t validation_clk_khz =
+			get_validation_clock(base);
+	uint32_t min_clk_khz = validation_clk_khz;
+	uint32_t max_clk_khz = 0;
+	struct display_clock_dce110 *dc = DCLCK110_FROM_BASE(base);
+
+	if (dc->use_max_disp_clk)
+		return min_clk_khz;
+
+	if (params != NULL) {
+		uint32_t disp_clk_khz = 0;
+
+		for (i = 0; i < path_num; ++i) {
+
+			disp_clk_khz = calc_single_display_min_clks(
+							base, params, true);
+
+			/* update the max required clock found*/
+			if (disp_clk_khz > max_clk_khz)
+				max_clk_khz = disp_clk_khz;
+
+			params++;
+		}
+	}
+
+	min_clk_khz = max_clk_khz;
+
+	if (min_clk_khz > validation_clk_khz)
+		min_clk_khz = validation_clk_khz;
+	else if (min_clk_khz < base->min_display_clk_threshold_khz)
+		min_clk_khz = base->min_display_clk_threshold_khz;
+
+	if (dc->use_max_disp_clk)
+		min_clk_khz = get_validation_clock(base);
+
+	return min_clk_khz;
+}
+
+static bool display_clock_integrated_info_construct(
+	struct display_clock_dce110 *disp_clk)
+{
+	struct dc_debug *debug = &disp_clk->disp_clk_base.ctx->dc->debug;
+	struct dc_bios *bp = disp_clk->disp_clk_base.ctx->dc_bios;
+	struct integrated_info info;
+	struct firmware_info fw_info;
+	uint32_t i;
+	struct display_clock *base = &disp_clk->disp_clk_base;
+
+	memset(&info, 0, sizeof(struct integrated_info));
+	memset(&fw_info, 0, sizeof(struct firmware_info));
+
+	if (bp->integrated_info)
+		info = *bp->integrated_info;
+
+	disp_clk->dentist_vco_freq_khz = info.dentist_vco_freq;
+	if (disp_clk->dentist_vco_freq_khz == 0) {
+		bp->funcs->get_firmware_info(bp, &fw_info);
+		disp_clk->dentist_vco_freq_khz =
+			fw_info.smu_gpu_pll_output_freq;
+		if (disp_clk->dentist_vco_freq_khz == 0)
+			disp_clk->dentist_vco_freq_khz = 3600000;
+	}
+
+	base->min_display_clk_threshold_khz =
+		disp_clk->dentist_vco_freq_khz / 64;
+
+	if (bp->integrated_info == NULL)
+		return false;
+
+	/*update the maximum display clock for each power state*/
+	for (i = 0; i < NUMBER_OF_DISP_CLK_VOLTAGE; ++i) {
+		enum clocks_state clk_state = CLOCKS_STATE_INVALID;
+
+		switch (i) {
+		case 0:
+			clk_state = CLOCKS_STATE_ULTRA_LOW;
+			break;
+
+		case 1:
+			clk_state = CLOCKS_STATE_LOW;
+			break;
+
+		case 2:
+			clk_state = CLOCKS_STATE_NOMINAL;
+			break;
+
+		case 3:
+			clk_state = CLOCKS_STATE_PERFORMANCE;
+			break;
+
+		default:
+			clk_state = CLOCKS_STATE_INVALID;
+			break;
+		}
+
+		/*Do not allow bad VBIOS/SBIOS to override with invalid values,
+		 * check for > 100MHz*/
+		if (info.disp_clk_voltage[i].max_supported_clk >= 100000) {
+			max_clks_by_state[clk_state].display_clk_khz =
+				info.disp_clk_voltage[i].max_supported_clk;
+		}
+	}
+
+	disp_clk->dfs_bypass_enabled = false;
+	if (!debug->disable_dfs_bypass)
+		if (bp->integrated_info->gpu_cap_info & DFS_BYPASS_ENABLE)
+			disp_clk->dfs_bypass_enabled = true;
+
+	disp_clk->use_max_disp_clk = debug->max_disp_clk;
+
+	return true;
+}
+
+static uint32_t get_clock(struct display_clock *dc)
+{
+	uint32_t disp_clock = get_validation_clock(dc);
+	uint32_t target_div = INVALID_DIVIDER;
+	uint32_t addr = mmDENTIST_DISPCLK_CNTL;
+	uint32_t value = 0;
+	uint32_t field = 0;
+	struct display_clock_dce110 *disp_clk = DCLCK110_FROM_BASE(dc);
+
+	if (disp_clk->dfs_bypass_enabled && disp_clk->dfs_bypass_disp_clk)
+		return disp_clk->dfs_bypass_disp_clk;
+
+	/* Read the mmDENTIST_DISPCLK_CNTL to get the currently programmed
+	 DID DENTIST_DISPCLK_WDIVIDER.*/
+	value = dm_read_reg(dc->ctx, addr);
+	field = get_reg_field_value(
+			value, DENTIST_DISPCLK_CNTL, DENTIST_DISPCLK_WDIVIDER);
+
+	/* Convert DENTIST_DISPCLK_WDIVIDER to actual divider*/
+	target_div = dal_divider_range_get_divider(
+		divider_ranges,
+		DIVIDER_RANGE_MAX,
+		field);
+
+	if (target_div != INVALID_DIVIDER)
+		/* Calculate the current DFS clock in KHz.
+		 Should be okay up to 42.9 THz before overflowing.*/
+		disp_clock = (DIVIDER_RANGE_SCALE_FACTOR
+			* disp_clk->dentist_vco_freq_khz) / target_div;
+	return disp_clock;
+}
+
+static enum clocks_state get_required_clocks_state(
+		struct display_clock *dc,
+		struct state_dependent_clocks *req_clocks)
+{
+	int32_t i;
+	struct display_clock_dce110 *disp_clk = DCLCK110_FROM_BASE(dc);
+	enum clocks_state low_req_clk = disp_clk->max_clks_state;
+
+	if (!req_clocks) {
+		/* NULL pointer*/
+		dm_logger_write(dc->ctx->logger, LOG_WARNING,
+				"%s: Invalid parameter",
+				__func__);
+		return CLOCKS_STATE_INVALID;
+	}
+
+	/* Iterate from highest supported to lowest valid state, and update
+	 * lowest RequiredState with the lowest state that satisfies
+	 * all required clocks
+	 */
+	for (i = disp_clk->max_clks_state; i >= CLOCKS_STATE_ULTRA_LOW; --i) {
+		if ((req_clocks->display_clk_khz <=
+			max_clks_by_state[i].display_clk_khz) &&
+			(req_clocks->pixel_clk_khz <=
+				max_clks_by_state[i].pixel_clk_khz))
+			low_req_clk = i;
+	}
+	return low_req_clk;
+}
+
+static void psr_wait_loop(struct dc_context *ctx, unsigned int display_clk_khz)
+{
+	unsigned int dmcu_max_retry_on_wait_reg_ready = 801;
+	unsigned int dmcu_wait_reg_ready_interval = 100;
+	unsigned int regValue;
+	uint32_t masterCmd;
+	uint32_t masterComCntl;
+	union dce110_dmcu_psr_config_data_wait_loop_reg1 masterCmdData1;
+
+	/* waitDMCUReadyForCmd */
+	do {
+		dm_delay_in_microseconds(ctx, dmcu_wait_reg_ready_interval);
+		regValue = dm_read_reg(ctx, mmMASTER_COMM_CNTL_REG);
+		dmcu_max_retry_on_wait_reg_ready--;
+	} while
+	/* expected value is 0, loop while not 0*/
+	((MASTER_COMM_CNTL_REG__MASTER_COMM_INTERRUPT_MASK & regValue) &&
+		dmcu_max_retry_on_wait_reg_ready > 0);
+
+	masterCmdData1.u32All = 0;
+	masterCmdData1.bits.waitLoop = display_clk_khz / 1000 / 7;
+	dm_write_reg(ctx, mmMASTER_COMM_DATA_REG1, masterCmdData1.u32All);
+
+	/* setDMCUParam_Cmd */
+	masterCmd = dm_read_reg(ctx, mmMASTER_COMM_CMD_REG);
+	set_reg_field_value(
+		masterCmd,
+		PSR_SET_WAITLOOP,
+		MASTER_COMM_CMD_REG,
+		MASTER_COMM_CMD_REG_BYTE0);
+
+	dm_write_reg(ctx, mmMASTER_COMM_CMD_REG, masterCmd);
+
+	/* notifyDMCUMsg */
+	masterComCntl = dm_read_reg(ctx, mmMASTER_COMM_CNTL_REG);
+	set_reg_field_value(
+		masterComCntl,
+		1,
+		MASTER_COMM_CNTL_REG,
+		MASTER_COMM_INTERRUPT);
+	dm_write_reg(ctx, mmMASTER_COMM_CNTL_REG, masterComCntl);
+}
+
+static void set_clock(
+	struct display_clock *base,
+	uint32_t requested_clk_khz)
+{
+	struct bp_pixel_clock_parameters pxl_clk_params;
+	struct display_clock_dce110 *dc = DCLCK110_FROM_BASE(base);
+	struct dc_bios *bp = base->ctx->dc_bios;
+
+	/* Prepare to program display clock*/
+	memset(&pxl_clk_params, 0, sizeof(pxl_clk_params));
+
+	/* Make sure requested clock isn't lower than minimum threshold*/
+	if (requested_clk_khz > 0)
+		requested_clk_khz = dm_max(requested_clk_khz,
+				base->min_display_clk_threshold_khz);
+
+	pxl_clk_params.target_pixel_clock = requested_clk_khz;
+	pxl_clk_params.pll_id = base->id;
+
+	bp->funcs->program_display_engine_pll(bp, &pxl_clk_params);
+
+	if (dc->dfs_bypass_enabled) {
+
+		/* Cache the fixed display clock*/
+		dc->dfs_bypass_disp_clk =
+			pxl_clk_params.dfs_bypass_display_clock;
+	}
+
+	/* from power down, we need mark the clock state as ClocksStateNominal
+	 * from HWReset, so when resume we will call pplib voltage regulator.*/
+	if (requested_clk_khz == 0)
+		base->cur_min_clks_state = CLOCKS_STATE_NOMINAL;
+
+	psr_wait_loop(base->ctx, requested_clk_khz);
+}
+
+static void set_clock_state(
+	struct display_clock *dc,
+	struct display_clock_state clk_state)
+{
+	struct display_clock_dce110 *disp_clk = DCLCK110_FROM_BASE(dc);
+
+	disp_clk->clock_state = clk_state;
+}
+
+static struct display_clock_state get_clock_state(
+	struct display_clock *dc)
+{
+	struct display_clock_dce110 *disp_clk = DCLCK110_FROM_BASE(dc);
+
+	return disp_clk->clock_state;
+}
+
+static uint32_t get_dfs_bypass_threshold(struct display_clock *dc)
+{
+	return DCE110_DFS_BYPASS_THRESHOLD_KHZ;
+}
+
+static const struct display_clock_funcs funcs = {
+	.destroy = destroy,
+	.calculate_min_clock = calculate_min_clock,
+	.get_clock = get_clock,
+	.get_clock_state = get_clock_state,
+	.get_dfs_bypass_threshold = get_dfs_bypass_threshold,
+	.get_dp_ref_clk_frequency = get_dp_ref_clk_frequency,
+	.get_min_clocks_state = get_min_clocks_state,
+	.get_required_clocks_state = get_required_clocks_state,
+	.get_validation_clock = get_validation_clock,
+	.set_clock = set_clock,
+	.set_clock_state = set_clock_state,
+	.set_dp_ref_clock_source = NULL,
+	.set_min_clocks_state = set_min_clocks_state,
+	.store_max_clocks_state = store_max_clocks_state,
+	.validate = NULL,
+};
+
+static bool dal_display_clock_dce110_construct(
+	struct display_clock_dce110 *dc110,
+	struct dc_context *ctx)
+{
+	struct display_clock *dc_base = &dc110->disp_clk_base;
+	struct dc_bios *bp = ctx->dc_bios;
+
+	if (!dal_display_clock_construct_base(dc_base, ctx))
+		return false;
+
+	dc_base->funcs = &funcs;
+
+	dc110->dfs_bypass_disp_clk = 0;
+
+	if (!display_clock_integrated_info_construct(dc110))
+		dm_logger_write(dc_base->ctx->logger, LOG_WARNING,
+			"Cannot obtain VBIOS integrated info\n");
+
+	dc110->gpu_pll_ss_percentage = 0;
+	dc110->gpu_pll_ss_divider = 1000;
+	dc110->ss_on_gpu_pll = false;
+
+	dc_base->id = CLOCK_SOURCE_ID_DFS;
+/* Initially set max clocks state to nominal.  This should be updated by
+ * via a pplib call to DAL IRI eventually calling a
+ * DisplayEngineClock_Dce110::StoreMaxClocksState().  This call will come in
+ * on PPLIB init. This is from DCE5x. in case HW wants to use mixed method.*/
+	dc110->max_clks_state = CLOCKS_STATE_NOMINAL;
+
+	dal_divider_range_construct(
+		&divider_ranges[DIVIDER_RANGE_01],
+		DIVIDER_RANGE_01_START,
+		DIVIDER_RANGE_01_STEP_SIZE,
+		DIVIDER_RANGE_01_BASE_DIVIDER_ID,
+		DIVIDER_RANGE_02_BASE_DIVIDER_ID);
+	dal_divider_range_construct(
+		&divider_ranges[DIVIDER_RANGE_02],
+		DIVIDER_RANGE_02_START,
+		DIVIDER_RANGE_02_STEP_SIZE,
+		DIVIDER_RANGE_02_BASE_DIVIDER_ID,
+		DIVIDER_RANGE_03_BASE_DIVIDER_ID);
+	dal_divider_range_construct(
+		&divider_ranges[DIVIDER_RANGE_03],
+		DIVIDER_RANGE_03_START,
+		DIVIDER_RANGE_03_STEP_SIZE,
+		DIVIDER_RANGE_03_BASE_DIVIDER_ID,
+		DIVIDER_RANGE_MAX_DIVIDER_ID);
+
+	{
+		uint32_t ss_info_num =
+				bp->funcs->get_ss_entry_number(bp,
+				AS_SIGNAL_TYPE_GPU_PLL);
+
+		if (ss_info_num) {
+			struct spread_spectrum_info info;
+			enum bp_result result;
+
+			memset(&info, 0, sizeof(info));
+
+			result = bp->funcs->get_spread_spectrum_info(bp,
+					AS_SIGNAL_TYPE_GPU_PLL,
+					0,
+					&info);
+
+			/* Based on VBIOS, VBIOS will keep entry for GPU PLL SS
+			 * even if SS not enabled and in that case
+			 * SSInfo.spreadSpectrumPercentage !=0 would be sign
+			 * that SS is enabled
+			 */
+			if (result == BP_RESULT_OK &&
+					info.spread_spectrum_percentage != 0) {
+				dc110->ss_on_gpu_pll = true;
+				dc110->gpu_pll_ss_divider =
+					info.spread_percentage_divider;
+
+				if (info.type.CENTER_MODE == 0) {
+					/* Currently for DP Reference clock we
+					 * need only SS percentage for
+					 * downspread */
+					dc110->gpu_pll_ss_percentage =
+						info.spread_spectrum_percentage;
+				}
+			}
+
+		}
+	}
+
+	return true;
+}
+
+/*****************************************************************************
+ * public functions
+ *****************************************************************************/
+
+struct display_clock *dal_display_clock_dce110_create(
+	struct dc_context *ctx)
+{
+	struct display_clock_dce110 *dc110;
+
+	dc110 = dm_alloc(sizeof(struct display_clock_dce110));
+
+	if (dc110 == NULL)
+		return NULL;
+
+	if (dal_display_clock_dce110_construct(dc110, ctx))
+		return &dc110->disp_clk_base;
+
+	dm_free(dc110);
+
+	return NULL;
+}
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/dce110/display_clock_dce110.h b/drivers/gpu/drm/amd/display/dc/gpu/dce110/display_clock_dce110.h
new file mode 100644
index 0000000..0cdc7b5
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/dce110/display_clock_dce110.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+#ifndef __DAL_DISPLAY_CLOCK_DCE110_H__
+#define __DAL_DISPLAY_CLOCK_DCE110_H__
+
+#include "gpu/display_clock.h"
+
+struct display_clock_dce110 {
+	struct display_clock disp_clk_base;
+	/* Max display block clocks state*/
+	enum clocks_state max_clks_state;
+	bool use_max_disp_clk;
+	uint32_t dentist_vco_freq_khz;
+	/* Cache the status of DFS-bypass feature*/
+	bool dfs_bypass_enabled;
+	/* GPU PLL SS percentage (if down-spread enabled) */
+	uint32_t gpu_pll_ss_percentage;
+	/* GPU PLL SS percentage Divider (100 or 1000) */
+	uint32_t gpu_pll_ss_divider;
+	/* Flag for Enabled SS on GPU PLL */
+	bool ss_on_gpu_pll;
+	/* Cache the display clock returned by VBIOS if DFS-bypass is enabled.
+	 * This is basically "Crystal Frequency In KHz" (XTALIN) frequency */
+	uint32_t dfs_bypass_disp_clk;
+	struct display_clock_state clock_state;
+};
+
+#define DCLCK110_FROM_BASE(dc_base) \
+	container_of(dc_base, struct display_clock_dce110, disp_clk_base)
+
+#endif /* __DAL_DISPLAY_CLOCK_DCE110_H__ */
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/dce112/display_clock_dce112.c b/drivers/gpu/drm/amd/display/dc/gpu/dce112/display_clock_dce112.c
new file mode 100644
index 0000000..9b7c975
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/dce112/display_clock_dce112.c
@@ -0,0 +1,964 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+#include "dce/dce_11_2_d.h"
+#include "dce/dce_11_2_sh_mask.h"
+
+#include "include/bios_parser_interface.h"
+#include "include/fixed32_32.h"
+#include "include/logger_interface.h"
+
+#include "../divider_range.h"
+
+#include "display_clock_dce112.h"
+
+#define FROM_DISPLAY_CLOCK(base) \
+	container_of(base, struct display_clock_dce112, disp_clk_base)
+
+static struct state_dependent_clocks max_clks_by_state[] = {
+/*ClocksStateInvalid - should not be used*/
+{ .display_clk_khz = 0, .pixel_clk_khz = 0 },
+/*ClocksStateUltraLow - currently by HW design team not supposed to be used*/
+{ .display_clk_khz = 389189, .pixel_clk_khz = 346672 },
+/*ClocksStateLow*/
+{ .display_clk_khz = 459000, .pixel_clk_khz = 400000 },
+/*ClocksStateNominal*/
+{ .display_clk_khz = 667000, .pixel_clk_khz = 600000 },
+/*ClocksStatePerformance*/
+{ .display_clk_khz = 1132000, .pixel_clk_khz = 600000 } };
+
+/* Ranges for divider identifiers (Divider ID or DID)
+ mmDENTIST_DISPCLK_CNTL.DENTIST_DISPCLK_WDIVIDER*/
+enum divider_id_register_setting {
+	DIVIDER_RANGE_01_BASE_DIVIDER_ID = 0X08,
+	DIVIDER_RANGE_02_BASE_DIVIDER_ID = 0X40,
+	DIVIDER_RANGE_03_BASE_DIVIDER_ID = 0X60,
+	DIVIDER_RANGE_MAX_DIVIDER_ID = 0X80
+};
+
+/* Step size between each divider within a range.
+ Incrementing the DENTIST_DISPCLK_WDIVIDER by one
+ will increment the divider by this much.*/
+enum divider_range_step_size {
+	DIVIDER_RANGE_01_STEP_SIZE = 25, /* 0.25*/
+	DIVIDER_RANGE_02_STEP_SIZE = 50, /* 0.50*/
+	DIVIDER_RANGE_03_STEP_SIZE = 100 /* 1.00 */
+};
+
+static struct divider_range divider_ranges[DIVIDER_RANGE_MAX];
+
+#define dce112_DFS_BYPASS_THRESHOLD_KHZ 400000
+/*****************************************************************************
+ * static functions
+ *****************************************************************************/
+
+/*
+ * store_max_clocks_state
+ *
+ * @brief
+ * Cache the clock state
+ *
+ * @param
+ * struct display_clock *base - [out] cach the state in this structure
+ * enum clocks_state max_clocks_state - [in] state to be stored
+ */
+void dispclk_dce112_store_max_clocks_state(
+	struct display_clock *base,
+	enum clocks_state max_clocks_state)
+{
+	struct display_clock_dce112 *dc = DCLCK112_FROM_BASE(base);
+
+	switch (max_clocks_state) {
+	case CLOCKS_STATE_LOW:
+	case CLOCKS_STATE_NOMINAL:
+	case CLOCKS_STATE_PERFORMANCE:
+	case CLOCKS_STATE_ULTRA_LOW:
+		dc->max_clks_state = max_clocks_state;
+		break;
+
+	case CLOCKS_STATE_INVALID:
+	default:
+		/*Invalid Clocks State!*/
+		ASSERT_CRITICAL(false);
+		break;
+	}
+}
+
+enum clocks_state dispclk_dce112_get_min_clocks_state(
+	struct display_clock *base)
+{
+	return base->cur_min_clks_state;
+}
+
+bool dispclk_dce112_set_min_clocks_state(
+	struct display_clock *base,
+	enum clocks_state clocks_state)
+{
+	struct display_clock_dce112 *dc = DCLCK112_FROM_BASE(base);
+	struct dm_pp_power_level_change_request level_change_req = {
+			DM_PP_POWER_LEVEL_INVALID};
+
+	if (clocks_state > dc->max_clks_state) {
+		/*Requested state exceeds max supported state.*/
+		dm_logger_write(base->ctx->logger, LOG_WARNING,
+				"Requested state exceeds max supported state");
+		return false;
+	} else if (clocks_state == base->cur_min_clks_state) {
+		/*if we're trying to set the same state, we can just return
+		 * since nothing needs to be done*/
+		return true;
+	}
+
+	switch (clocks_state) {
+	case CLOCKS_STATE_ULTRA_LOW:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_ULTRA_LOW;
+		break;
+	case CLOCKS_STATE_LOW:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_LOW;
+		break;
+	case CLOCKS_STATE_NOMINAL:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_NOMINAL;
+		break;
+	case CLOCKS_STATE_PERFORMANCE:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_PERFORMANCE;
+		break;
+	case CLOCKS_STATE_INVALID:
+	default:
+		dm_logger_write(base->ctx->logger, LOG_WARNING,
+				"Requested state invalid state");
+		return false;
+	}
+
+	/* get max clock state from PPLIB */
+	if (dm_pp_apply_power_level_change_request(
+			base->ctx, &level_change_req))
+		base->cur_min_clks_state = clocks_state;
+
+	return true;
+}
+
+static uint32_t get_dp_ref_clk_frequency(struct display_clock *dc)
+{
+	uint32_t dispclk_cntl_value;
+	uint32_t dp_ref_clk_cntl_value;
+	uint32_t dp_ref_clk_cntl_src_sel_value;
+	uint32_t dp_ref_clk_khz = 600000;
+	uint32_t target_div = INVALID_DIVIDER;
+	struct display_clock_dce112 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	/* ASSERT DP Reference Clock source is from DFS*/
+	dp_ref_clk_cntl_value = dm_read_reg(dc->ctx,
+			mmDPREFCLK_CNTL);
+
+	dp_ref_clk_cntl_src_sel_value =
+			get_reg_field_value(
+				dp_ref_clk_cntl_value,
+				DPREFCLK_CNTL, DPREFCLK_SRC_SEL);
+
+	ASSERT(dp_ref_clk_cntl_src_sel_value == 0);
+
+	/* Read the mmDENTIST_DISPCLK_CNTL to get the currently
+	 * programmed DID DENTIST_DPREFCLK_WDIVIDER*/
+	dispclk_cntl_value = dm_read_reg(dc->ctx,
+			mmDENTIST_DISPCLK_CNTL);
+
+	/* Convert DENTIST_DPREFCLK_WDIVIDERto actual divider*/
+	target_div = dal_divider_range_get_divider(
+		divider_ranges,
+		DIVIDER_RANGE_MAX,
+		get_reg_field_value(dispclk_cntl_value,
+			DENTIST_DISPCLK_CNTL,
+			DENTIST_DPREFCLK_WDIVIDER));
+
+	if (target_div != INVALID_DIVIDER) {
+		/* Calculate the current DFS clock, in kHz.*/
+		dp_ref_clk_khz = (DIVIDER_RANGE_SCALE_FACTOR
+			* disp_clk->dentist_vco_freq_khz) / target_div;
+	}
+
+	/* SW will adjust DP REF Clock average value for all purposes
+	 * (DP DTO / DP Audio DTO and DP GTC)
+	 if clock is spread for all cases:
+	 -if SS enabled on DP Ref clock and HW de-spreading enabled with SW
+	 calculations for DS_INCR/DS_MODULO (this is planned to be default case)
+	 -if SS enabled on DP Ref clock and HW de-spreading enabled with HW
+	 calculations (not planned to be used, but average clock should still
+	 be valid)
+	 -if SS enabled on DP Ref clock and HW de-spreading disabled
+	 (should not be case with CIK) then SW should program all rates
+	 generated according to average value (case as with previous ASICs)
+	  */
+	if ((disp_clk->ss_on_gpu_pll) && (disp_clk->gpu_pll_ss_divider != 0)) {
+		struct fixed32_32 ss_percentage = dal_fixed32_32_div_int(
+				dal_fixed32_32_from_fraction(
+					disp_clk->gpu_pll_ss_percentage,
+					disp_clk->gpu_pll_ss_divider), 200);
+		struct fixed32_32 adj_dp_ref_clk_khz;
+
+		ss_percentage = dal_fixed32_32_sub(dal_fixed32_32_one,
+								ss_percentage);
+		adj_dp_ref_clk_khz =
+			dal_fixed32_32_mul_int(
+				ss_percentage,
+				dp_ref_clk_khz);
+		dp_ref_clk_khz = dal_fixed32_32_floor(adj_dp_ref_clk_khz);
+	}
+
+	return dp_ref_clk_khz;
+}
+
+void dispclk_dce112_destroy(struct display_clock **base)
+{
+	struct display_clock_dce112 *dc112;
+
+	dc112 = DCLCK112_FROM_BASE(*base);
+
+	dm_free(dc112);
+
+	*base = NULL;
+}
+
+uint32_t dispclk_dce112_get_validation_clock(struct display_clock *dc)
+{
+	uint32_t clk = 0;
+	struct display_clock_dce112 *disp_clk = DCLCK112_FROM_BASE(dc);
+
+	switch (disp_clk->max_clks_state) {
+	case CLOCKS_STATE_ULTRA_LOW:
+		clk = (disp_clk->max_clks_by_state + CLOCKS_STATE_ULTRA_LOW)->
+			display_clk_khz;
+
+	case CLOCKS_STATE_LOW:
+		clk = (disp_clk->max_clks_by_state + CLOCKS_STATE_LOW)->
+			display_clk_khz;
+		break;
+
+	case CLOCKS_STATE_NOMINAL:
+		clk = (disp_clk->max_clks_by_state + CLOCKS_STATE_NOMINAL)->
+			display_clk_khz;
+		break;
+
+	case CLOCKS_STATE_PERFORMANCE:
+		clk = (disp_clk->max_clks_by_state + CLOCKS_STATE_PERFORMANCE)->
+			display_clk_khz;
+		break;
+
+	case CLOCKS_STATE_INVALID:
+	default:
+		/*Invalid Clocks State*/
+		dm_logger_write(dc->ctx->logger, LOG_WARNING,
+				"Invalid clock state");
+		/* just return the display engine clock for
+		 * lowest supported state*/
+		clk = (disp_clk->max_clks_by_state + CLOCKS_STATE_LOW)->
+				display_clk_khz;
+		break;
+	}
+	return clk;
+}
+
+static struct fixed32_32 get_deep_color_factor(struct min_clock_params *params)
+{
+	/* DeepColorFactor = IF (HDMI = True, bpp / 24, 1)*/
+	struct fixed32_32 deep_color_factor = dal_fixed32_32_from_int(1);
+
+	if (params->signal_type != SIGNAL_TYPE_HDMI_TYPE_A)
+		return deep_color_factor;
+
+	switch (params->deep_color_depth) {
+	case COLOR_DEPTH_101010:
+		/*deep color ratio for 30bpp is 30/24 = 1.25*/
+		deep_color_factor = dal_fixed32_32_from_fraction(30, 24);
+		break;
+
+	case COLOR_DEPTH_121212:
+		/* deep color ratio for 36bpp is 36/24 = 1.5*/
+		deep_color_factor = dal_fixed32_32_from_fraction(36, 24);
+		break;
+
+	case COLOR_DEPTH_161616:
+		/* deep color ratio for 48bpp is 48/24 = 2.0 */
+		deep_color_factor = dal_fixed32_32_from_fraction(48, 24);
+		break;
+	default:
+		break;
+	}
+	return deep_color_factor;
+}
+
+static struct fixed32_32 get_scaler_efficiency(
+	struct dc_context *ctx,
+	struct min_clock_params *params)
+{
+	struct fixed32_32 scaler_efficiency = dal_fixed32_32_from_int(3);
+
+	if (params->scaler_efficiency == V_SCALER_EFFICIENCY_LB18BPP) {
+		scaler_efficiency =
+			dal_fixed32_32_add(
+				dal_fixed32_32_from_fraction(35555, 10000),
+				dal_fixed32_32_from_fraction(
+					55556,
+					100000 * 10000));
+	} else if (params->scaler_efficiency == V_SCALER_EFFICIENCY_LB24BPP) {
+		scaler_efficiency =
+			dal_fixed32_32_add(
+				dal_fixed32_32_from_fraction(34285, 10000),
+				dal_fixed32_32_from_fraction(
+					71429,
+					100000 * 10000));
+	} else if (params->scaler_efficiency == V_SCALER_EFFICIENCY_LB30BPP)
+		scaler_efficiency = dal_fixed32_32_from_fraction(32, 10);
+
+	return scaler_efficiency;
+}
+
+static struct fixed32_32 get_lb_lines_in_per_line_out(
+		struct min_clock_params *params,
+		struct fixed32_32 v_scale_ratio)
+{
+	struct fixed32_32 two = dal_fixed32_32_from_int(2);
+	struct fixed32_32 four = dal_fixed32_32_from_int(4);
+	struct fixed32_32 f4_to_3 = dal_fixed32_32_from_fraction(4, 3);
+	struct fixed32_32 f6_to_4 = dal_fixed32_32_from_fraction(6, 4);
+
+	if (params->line_buffer_prefetch_enabled)
+		return dal_fixed32_32_max(v_scale_ratio, dal_fixed32_32_one);
+	else if (dal_fixed32_32_le(v_scale_ratio, dal_fixed32_32_one))
+		return dal_fixed32_32_one;
+	else if (dal_fixed32_32_le(v_scale_ratio, f4_to_3))
+		return f4_to_3;
+	else if (dal_fixed32_32_le(v_scale_ratio, f6_to_4))
+		return f6_to_4;
+	else if (dal_fixed32_32_le(v_scale_ratio, two))
+		return two;
+	else if (dal_fixed32_32_le(v_scale_ratio, dal_fixed32_32_from_int(3)))
+		return four;
+	else
+		return dal_fixed32_32_zero;
+}
+
+static uint32_t get_actual_required_display_clk(
+	struct display_clock_dce112 *disp_clk,
+	uint32_t target_clk_khz)
+{
+	uint32_t disp_clk_khz = target_clk_khz;
+	uint32_t div = INVALID_DIVIDER;
+	uint32_t did = INVALID_DID;
+	uint32_t scaled_vco =
+		disp_clk->dentist_vco_freq_khz * DIVIDER_RANGE_SCALE_FACTOR;
+
+	ASSERT_CRITICAL(!!disp_clk_khz);
+
+	if (disp_clk_khz)
+		div = scaled_vco / disp_clk_khz;
+
+	did = dal_divider_range_get_did(divider_ranges, DIVIDER_RANGE_MAX, div);
+
+	if (did != INVALID_DID) {
+		div = dal_divider_range_get_divider(
+			divider_ranges, DIVIDER_RANGE_MAX, did);
+
+		if ((div != INVALID_DIVIDER) &&
+			(did > DIVIDER_RANGE_01_BASE_DIVIDER_ID))
+			if (disp_clk_khz > (scaled_vco / div))
+				div = dal_divider_range_get_divider(
+					divider_ranges, DIVIDER_RANGE_MAX,
+					did - 1);
+
+		if (div != INVALID_DIVIDER)
+			disp_clk_khz = scaled_vco / div;
+
+	}
+	/* We need to add 10KHz to this value because the accuracy in VBIOS is
+	 in 10KHz units. So we need to always round the last digit up in order
+	 to reach the next div level.*/
+	return disp_clk_khz + 10;
+}
+
+static uint32_t calc_single_display_min_clks(
+	struct display_clock *base,
+	struct min_clock_params *params,
+	bool set_clk)
+{
+	struct fixed32_32 h_scale_ratio = dal_fixed32_32_one;
+	struct fixed32_32 v_scale_ratio = dal_fixed32_32_one;
+	uint32_t pix_clk_khz = 0;
+	uint32_t lb_source_width = 0;
+	struct fixed32_32 deep_color_factor;
+	struct fixed32_32 scaler_efficiency;
+	struct fixed32_32 v_filter_init;
+	uint32_t v_filter_init_trunc;
+	uint32_t num_lines_at_frame_start = 3;
+	struct fixed32_32 v_filter_init_ceil;
+	struct fixed32_32 lines_per_lines_out_at_frame_start;
+	struct fixed32_32 lb_lines_in_per_line_out; /* in middle of the frame*/
+	uint32_t src_wdth_rnd_to_chunks;
+	struct fixed32_32 scaling_coeff;
+	struct fixed32_32 h_blank_granularity_factor =
+			dal_fixed32_32_one;
+	struct fixed32_32 fx_disp_clk_mhz;
+	struct fixed32_32 line_time;
+	struct fixed32_32 disp_pipe_pix_throughput;
+	struct fixed32_32 fx_alt_disp_clk_mhz;
+	uint32_t disp_clk_khz;
+	uint32_t alt_disp_clk_khz;
+	struct display_clock_dce112 *disp_clk_110 = DCLCK112_FROM_BASE(base);
+	uint32_t max_clk_khz = dispclk_dce112_get_validation_clock(base);
+	bool panning_allowed = false; /* TODO: receive this value from AS */
+
+	if (params == NULL) {
+		dm_logger_write(base->ctx->logger, LOG_WARNING,
+				"Invalid input parameter in %s",
+				__func__);
+		return 0;
+	}
+
+	deep_color_factor = get_deep_color_factor(params);
+	scaler_efficiency = get_scaler_efficiency(base->ctx, params);
+	pix_clk_khz = params->requested_pixel_clock;
+	lb_source_width = params->source_view.width;
+
+	if (0 != params->dest_view.height && 0 != params->dest_view.width) {
+
+		h_scale_ratio = dal_fixed32_32_from_fraction(
+			params->source_view.width,
+			params->dest_view.width);
+		v_scale_ratio = dal_fixed32_32_from_fraction(
+			params->source_view.height,
+			params->dest_view.height);
+	} else {
+		dm_logger_write(base->ctx->logger, LOG_WARNING,
+				"Destination height or width is 0!\n");
+	}
+
+	v_filter_init =
+		dal_fixed32_32_add(
+			v_scale_ratio,
+			dal_fixed32_32_add_int(
+				dal_fixed32_32_div_int(
+					dal_fixed32_32_mul_int(
+						v_scale_ratio,
+						params->timing_info.INTERLACED),
+					2),
+				params->scaling_info.v_taps + 1));
+	v_filter_init = dal_fixed32_32_div_int(v_filter_init, 2);
+
+	v_filter_init_trunc = dal_fixed32_32_floor(v_filter_init);
+
+	v_filter_init_ceil = dal_fixed32_32_from_fraction(
+						v_filter_init_trunc, 2);
+	v_filter_init_ceil = dal_fixed32_32_from_int(
+		dal_fixed32_32_ceil(v_filter_init_ceil));
+	v_filter_init_ceil = dal_fixed32_32_mul_int(v_filter_init_ceil, 2);
+
+	lines_per_lines_out_at_frame_start =
+			dal_fixed32_32_div_int(v_filter_init_ceil,
+					num_lines_at_frame_start);
+	lb_lines_in_per_line_out =
+			get_lb_lines_in_per_line_out(params, v_scale_ratio);
+
+	if (panning_allowed)
+		src_wdth_rnd_to_chunks =
+			((lb_source_width - 1) / 128) * 128 + 256;
+	else
+		src_wdth_rnd_to_chunks =
+			((lb_source_width + 127) / 128) * 128;
+
+	scaling_coeff =
+		dal_fixed32_32_div(
+			dal_fixed32_32_from_int(params->scaling_info.v_taps),
+			scaler_efficiency);
+
+	if (dal_fixed32_32_le(h_scale_ratio, dal_fixed32_32_one))
+		scaling_coeff = dal_fixed32_32_max(
+			dal_fixed32_32_from_int(
+				dal_fixed32_32_ceil(
+					dal_fixed32_32_from_fraction(
+						params->scaling_info.h_taps,
+						4))),
+			dal_fixed32_32_max(
+				dal_fixed32_32_mul(
+					scaling_coeff,
+					h_scale_ratio),
+				dal_fixed32_32_one));
+
+	if (!params->line_buffer_prefetch_enabled &&
+		dal_fixed32_32_floor(lb_lines_in_per_line_out) != 2 &&
+		dal_fixed32_32_floor(lb_lines_in_per_line_out) != 4) {
+		uint32_t line_total_pixel =
+			params->timing_info.h_total + lb_source_width - 256;
+		h_blank_granularity_factor = dal_fixed32_32_div(
+			dal_fixed32_32_from_int(params->timing_info.h_total),
+			dal_fixed32_32_div(
+			dal_fixed32_32_from_fraction(
+				line_total_pixel, 2),
+				h_scale_ratio));
+	}
+
+	/* Calculate display clock with ramping. Ramping factor is 1.1*/
+	fx_disp_clk_mhz =
+		dal_fixed32_32_div_int(
+			dal_fixed32_32_mul_int(scaling_coeff, 11),
+			10);
+	line_time = dal_fixed32_32_from_fraction(
+			params->timing_info.h_total * 1000, pix_clk_khz);
+
+	disp_pipe_pix_throughput = dal_fixed32_32_mul(
+			lb_lines_in_per_line_out, h_blank_granularity_factor);
+	disp_pipe_pix_throughput = dal_fixed32_32_max(
+			disp_pipe_pix_throughput,
+			lines_per_lines_out_at_frame_start);
+	disp_pipe_pix_throughput = dal_fixed32_32_div(dal_fixed32_32_mul_int(
+			disp_pipe_pix_throughput, src_wdth_rnd_to_chunks),
+			line_time);
+
+	if (0 != params->timing_info.h_total) {
+		fx_disp_clk_mhz =
+			dal_fixed32_32_max(
+				dal_fixed32_32_div_int(
+					dal_fixed32_32_mul_int(
+						scaling_coeff, pix_clk_khz),
+						1000),
+				disp_pipe_pix_throughput);
+		fx_disp_clk_mhz =
+			dal_fixed32_32_mul(
+				fx_disp_clk_mhz,
+				dal_fixed32_32_from_fraction(11, 10));
+	}
+
+	fx_disp_clk_mhz = dal_fixed32_32_max(fx_disp_clk_mhz,
+		dal_fixed32_32_mul(deep_color_factor,
+		dal_fixed32_32_from_fraction(11, 10)));
+
+	/* Calculate display clock without ramping */
+	fx_alt_disp_clk_mhz = scaling_coeff;
+
+	if (0 != params->timing_info.h_total) {
+		fx_alt_disp_clk_mhz = dal_fixed32_32_max(
+				dal_fixed32_32_div_int(dal_fixed32_32_mul_int(
+						scaling_coeff, pix_clk_khz),
+						1000),
+				dal_fixed32_32_div_int(dal_fixed32_32_mul_int(
+						disp_pipe_pix_throughput, 105),
+						100));
+	}
+
+	if (set_clk && disp_clk_110->ss_on_gpu_pll &&
+			disp_clk_110->gpu_pll_ss_divider)
+		fx_alt_disp_clk_mhz = dal_fixed32_32_mul(fx_alt_disp_clk_mhz,
+				dal_fixed32_32_add_int(
+				dal_fixed32_32_div_int(
+				dal_fixed32_32_div_int(
+				dal_fixed32_32_from_fraction(
+				disp_clk_110->gpu_pll_ss_percentage,
+				disp_clk_110->gpu_pll_ss_divider), 100),
+				2),
+				1));
+
+	/* convert to integer */
+	disp_clk_khz = dal_fixed32_32_round(
+			dal_fixed32_32_mul_int(fx_disp_clk_mhz, 1000));
+	alt_disp_clk_khz = dal_fixed32_32_round(
+			dal_fixed32_32_mul_int(fx_alt_disp_clk_mhz, 1000));
+
+	if ((disp_clk_khz > max_clk_khz) && (alt_disp_clk_khz <= max_clk_khz))
+		disp_clk_khz = alt_disp_clk_khz;
+
+	if (set_clk) { /* only compensate clock if we are going to set it.*/
+		disp_clk_khz = get_actual_required_display_clk(
+			disp_clk_110, disp_clk_khz);
+	}
+
+	disp_clk_khz = disp_clk_khz > max_clk_khz ? max_clk_khz : disp_clk_khz;
+
+	return disp_clk_khz;
+}
+
+uint32_t dispclk_dce112_calculate_min_clock(
+	struct display_clock *base,
+	uint32_t path_num,
+	struct min_clock_params *params)
+{
+	uint32_t i;
+	uint32_t validation_clk_khz =
+			dispclk_dce112_get_validation_clock(base);
+	uint32_t min_clk_khz = validation_clk_khz;
+	uint32_t max_clk_khz = 0;
+	struct display_clock_dce112 *dc = DCLCK112_FROM_BASE(base);
+
+	if (dc->use_max_disp_clk)
+		return min_clk_khz;
+
+	if (params != NULL) {
+		uint32_t disp_clk_khz = 0;
+
+		for (i = 0; i < path_num; ++i) {
+
+			disp_clk_khz = calc_single_display_min_clks(
+							base, params, true);
+
+			/* update the max required clock found*/
+			if (disp_clk_khz > max_clk_khz)
+				max_clk_khz = disp_clk_khz;
+
+			params++;
+		}
+	}
+
+	min_clk_khz = max_clk_khz;
+
+	if (min_clk_khz > validation_clk_khz)
+		min_clk_khz = validation_clk_khz;
+	else if (min_clk_khz < base->min_display_clk_threshold_khz)
+		min_clk_khz = base->min_display_clk_threshold_khz;
+
+	if (dc->use_max_disp_clk)
+		min_clk_khz = dispclk_dce112_get_validation_clock(base);
+
+	return min_clk_khz;
+}
+
+static bool display_clock_integrated_info_construct(
+	struct display_clock_dce112 *disp_clk)
+{
+	struct integrated_info info;
+	uint32_t i;
+	struct display_clock *base = &disp_clk->disp_clk_base;
+
+	memset(&info, 0, sizeof(struct integrated_info));
+
+	disp_clk->dentist_vco_freq_khz = info.dentist_vco_freq;
+	if (disp_clk->dentist_vco_freq_khz == 0)
+		disp_clk->dentist_vco_freq_khz = 3600000;
+
+	disp_clk->crystal_freq_khz = 100000;
+
+	base->min_display_clk_threshold_khz =
+		disp_clk->dentist_vco_freq_khz / 64;
+
+	/*update the maximum display clock for each power state*/
+	for (i = 0; i < NUMBER_OF_DISP_CLK_VOLTAGE; ++i) {
+		enum clocks_state clk_state = CLOCKS_STATE_INVALID;
+
+		switch (i) {
+		case 0:
+			clk_state = CLOCKS_STATE_ULTRA_LOW;
+			break;
+
+		case 1:
+			clk_state = CLOCKS_STATE_LOW;
+			break;
+
+		case 2:
+			clk_state = CLOCKS_STATE_NOMINAL;
+			break;
+
+		case 3:
+			clk_state = CLOCKS_STATE_PERFORMANCE;
+			break;
+
+		default:
+			clk_state = CLOCKS_STATE_INVALID;
+			break;
+		}
+
+		/*Do not allow bad VBIOS/SBIOS to override with invalid values,
+		 * check for > 100MHz*/
+		if (info.disp_clk_voltage[i].max_supported_clk >= 100000) {
+			(disp_clk->max_clks_by_state + clk_state)->
+					display_clk_khz =
+				info.disp_clk_voltage[i].max_supported_clk;
+		}
+	}
+
+	return true;
+}
+
+static uint32_t get_clock(struct display_clock *dc)
+{
+	uint32_t disp_clock = dispclk_dce112_get_validation_clock(dc);
+	uint32_t target_div = INVALID_DIVIDER;
+	uint32_t addr = mmDENTIST_DISPCLK_CNTL;
+	uint32_t value = 0;
+	uint32_t field = 0;
+	struct display_clock_dce112 *disp_clk = DCLCK112_FROM_BASE(dc);
+
+	/* Read the mmDENTIST_DISPCLK_CNTL to get the currently programmed
+	 DID DENTIST_DISPCLK_WDIVIDER.*/
+	value = dm_read_reg(dc->ctx, addr);
+	field = get_reg_field_value(
+			value, DENTIST_DISPCLK_CNTL, DENTIST_DISPCLK_WDIVIDER);
+
+	/* Convert DENTIST_DISPCLK_WDIVIDER to actual divider*/
+	target_div = dal_divider_range_get_divider(
+		divider_ranges,
+		DIVIDER_RANGE_MAX,
+		field);
+
+	if (target_div != INVALID_DIVIDER)
+		/* Calculate the current DFS clock in KHz.
+		 Should be okay up to 42.9 THz before overflowing.*/
+		disp_clock = (DIVIDER_RANGE_SCALE_FACTOR
+			* disp_clk->dentist_vco_freq_khz) / target_div;
+	return disp_clock;
+}
+
+enum clocks_state dispclk_dce112_get_required_clocks_state(
+	struct display_clock *dc,
+	struct state_dependent_clocks *req_clocks)
+{
+	int32_t i;
+	struct display_clock_dce112 *disp_clk = DCLCK112_FROM_BASE(dc);
+	enum clocks_state low_req_clk = disp_clk->max_clks_state;
+
+	if (!req_clocks) {
+		/* NULL pointer*/
+		dm_logger_write(dc->ctx->logger, LOG_WARNING,
+				"%s: Invalid parameter",
+				__func__);
+		return CLOCKS_STATE_INVALID;
+	}
+
+	/* Iterate from highest supported to lowest valid state, and update
+	 * lowest RequiredState with the lowest state that satisfies
+	 * all required clocks
+	 */
+	for (i = disp_clk->max_clks_state; i >= CLOCKS_STATE_ULTRA_LOW; --i) {
+		if ((req_clocks->display_clk_khz <=
+				(disp_clk->max_clks_by_state + i)->
+					display_clk_khz) &&
+			(req_clocks->pixel_clk_khz <=
+					(disp_clk->max_clks_by_state + i)->
+					pixel_clk_khz))
+			low_req_clk = i;
+	}
+	return low_req_clk;
+}
+
+void dispclk_dce112_set_clock(
+	struct display_clock *base,
+	uint32_t requested_clk_khz)
+{
+	struct bp_set_dce_clock_parameters dce_clk_params;
+	struct display_clock_dce112 *dc = DCLCK112_FROM_BASE(base);
+	struct dc_bios *bp = base->ctx->dc_bios;
+
+	/* Prepare to program display clock*/
+	memset(&dce_clk_params, 0, sizeof(dce_clk_params));
+
+	/* Make sure requested clock isn't lower than minimum threshold*/
+	if (requested_clk_khz > 0)
+		requested_clk_khz = dm_max(requested_clk_khz,
+				base->min_display_clk_threshold_khz);
+
+	dce_clk_params.target_clock_frequency = requested_clk_khz;
+	dce_clk_params.pll_id = dc->disp_clk_base.id;
+	dce_clk_params.clock_type = DCECLOCK_TYPE_DISPLAY_CLOCK;
+
+	bp->funcs->set_dce_clock(bp, &dce_clk_params);
+
+	/* from power down, we need mark the clock state as ClocksStateNominal
+	 * from HWReset, so when resume we will call pplib voltage regulator.*/
+	if (requested_clk_khz == 0)
+		base->cur_min_clks_state = CLOCKS_STATE_NOMINAL;
+
+	/*Program DP ref Clock*/
+	/*VBIOS will determine DPREFCLK frequency, so we don't set it*/
+	dce_clk_params.target_clock_frequency = 0;
+	dce_clk_params.clock_type = DCECLOCK_TYPE_DPREFCLK;
+	dce_clk_params.flags.USE_GENLOCK_AS_SOURCE_FOR_DPREFCLK =
+			(dce_clk_params.pll_id ==
+					CLOCK_SOURCE_COMBO_DISPLAY_PLL0);
+
+	bp->funcs->set_dce_clock(bp, &dce_clk_params);
+}
+
+void dispclk_dce112_set_clock_state(
+	struct display_clock *dc,
+	struct display_clock_state clk_state)
+{
+	struct display_clock_dce112 *disp_clk = DCLCK112_FROM_BASE(dc);
+
+	disp_clk->clock_state = clk_state;
+}
+
+struct display_clock_state dispclk_dce112_get_clock_state(
+	struct display_clock *dc)
+{
+	struct display_clock_dce112 *disp_clk = DCLCK112_FROM_BASE(dc);
+
+	return disp_clk->clock_state;
+}
+
+uint32_t dispclk_dce112_get_dfs_bypass_threshold(
+	struct display_clock *dc)
+{
+	return dce112_DFS_BYPASS_THRESHOLD_KHZ;
+}
+
+static const struct display_clock_funcs funcs = {
+	.destroy = dispclk_dce112_destroy,
+	.calculate_min_clock = dispclk_dce112_calculate_min_clock,
+	.get_clock = get_clock,
+	.get_clock_state = dispclk_dce112_get_clock_state,
+	.get_dfs_bypass_threshold = dispclk_dce112_get_dfs_bypass_threshold,
+	.get_dp_ref_clk_frequency = get_dp_ref_clk_frequency,
+	.get_min_clocks_state = dispclk_dce112_get_min_clocks_state,
+	.get_required_clocks_state = dispclk_dce112_get_required_clocks_state,
+	.get_validation_clock = dispclk_dce112_get_validation_clock,
+	.set_clock = dispclk_dce112_set_clock,
+	.set_clock_state = dispclk_dce112_set_clock_state,
+	.set_dp_ref_clock_source = NULL,
+	.set_min_clocks_state = dispclk_dce112_set_min_clocks_state,
+	.store_max_clocks_state = dispclk_dce112_store_max_clocks_state,
+	.validate = NULL,
+};
+
+bool dal_display_clock_dce112_construct(
+	struct display_clock_dce112 *dc112,
+	struct dc_context *ctx)
+{
+	struct display_clock *dc_base = &dc112->disp_clk_base;
+
+	/*if (NULL == as)
+		return false;*/
+
+	if (!dal_display_clock_construct_base(dc_base, ctx))
+		return false;
+
+	dc_base->funcs = &funcs;
+
+	dc112->dfs_bypass_disp_clk = 0;
+
+	if (!display_clock_integrated_info_construct(dc112))
+		dm_logger_write(dc_base->ctx->logger, LOG_WARNING,
+			"Cannot obtain VBIOS integrated info\n");
+
+	dc112->gpu_pll_ss_percentage = 0;
+	dc112->gpu_pll_ss_divider = 1000;
+	dc112->ss_on_gpu_pll = false;
+
+	dc_base->id = CLOCK_SOURCE_ID_DFS;
+/* Initially set max clocks state to nominal.  This should be updated by
+ * via a pplib call to DAL IRI eventually calling a
+ * DisplayEngineClock_dce112::StoreMaxClocksState().  This call will come in
+ * on PPLIB init. This is from DCE5x. in case HW wants to use mixed method.*/
+	dc112->max_clks_state = CLOCKS_STATE_NOMINAL;
+
+	dc112->disp_clk_base.min_display_clk_threshold_khz =
+			dc112->crystal_freq_khz;
+
+	if (dc112->disp_clk_base.min_display_clk_threshold_khz <
+			(dc112->dentist_vco_freq_khz / 62))
+		dc112->disp_clk_base.min_display_clk_threshold_khz =
+				(dc112->dentist_vco_freq_khz / 62);
+
+	dal_divider_range_construct(
+		&divider_ranges[DIVIDER_RANGE_01],
+		DIVIDER_RANGE_01_START,
+		DIVIDER_RANGE_01_STEP_SIZE,
+		DIVIDER_RANGE_01_BASE_DIVIDER_ID,
+		DIVIDER_RANGE_02_BASE_DIVIDER_ID);
+	dal_divider_range_construct(
+		&divider_ranges[DIVIDER_RANGE_02],
+		DIVIDER_RANGE_02_START,
+		DIVIDER_RANGE_02_STEP_SIZE,
+		DIVIDER_RANGE_02_BASE_DIVIDER_ID,
+		DIVIDER_RANGE_03_BASE_DIVIDER_ID);
+	dal_divider_range_construct(
+		&divider_ranges[DIVIDER_RANGE_03],
+		DIVIDER_RANGE_03_START,
+		DIVIDER_RANGE_03_STEP_SIZE,
+		DIVIDER_RANGE_03_BASE_DIVIDER_ID,
+		DIVIDER_RANGE_MAX_DIVIDER_ID);
+
+	{
+		uint32_t ss_info_num =
+			ctx->dc_bios->funcs->
+			get_ss_entry_number(ctx->dc_bios, AS_SIGNAL_TYPE_GPU_PLL);
+
+		if (ss_info_num) {
+			struct spread_spectrum_info info;
+			bool result;
+
+			memset(&info, 0, sizeof(info));
+
+			result =
+					(BP_RESULT_OK == ctx->dc_bios->funcs->
+					get_spread_spectrum_info(ctx->dc_bios,
+					AS_SIGNAL_TYPE_GPU_PLL, 0, &info)) ? true : false;
+
+
+			/* Based on VBIOS, VBIOS will keep entry for GPU PLL SS
+			 * even if SS not enabled and in that case
+			 * SSInfo.spreadSpectrumPercentage !=0 would be sign
+			 * that SS is enabled
+			 */
+			if (result && info.spread_spectrum_percentage != 0) {
+				dc112->ss_on_gpu_pll = true;
+				dc112->gpu_pll_ss_divider =
+					info.spread_percentage_divider;
+
+				if (info.type.CENTER_MODE == 0) {
+					/* Currently for DP Reference clock we
+					 * need only SS percentage for
+					 * downspread */
+					dc112->gpu_pll_ss_percentage =
+						info.spread_spectrum_percentage;
+				}
+			}
+
+		}
+	}
+
+	dc112->use_max_disp_clk = true;
+	dc112->max_clks_by_state = max_clks_by_state;
+
+	return true;
+}
+
+/*****************************************************************************
+ * public functions
+ *****************************************************************************/
+
+struct display_clock *dal_display_clock_dce112_create(
+	struct dc_context *ctx)
+{
+	struct display_clock_dce112 *dc112;
+
+	dc112 = dm_alloc(sizeof(struct display_clock_dce112));
+
+	if (dc112 == NULL)
+		return NULL;
+
+	if (dal_display_clock_dce112_construct(dc112, ctx))
+		return &dc112->disp_clk_base;
+
+	dm_free(dc112);
+
+	return NULL;
+}
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/dce112/display_clock_dce112.h b/drivers/gpu/drm/amd/display/dc/gpu/dce112/display_clock_dce112.h
new file mode 100644
index 0000000..937e179
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/dce112/display_clock_dce112.h
@@ -0,0 +1,114 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+#ifndef __DAL_DISPLAY_CLOCK_DCE112_H__
+#define __DAL_DISPLAY_CLOCK_DCE112_H__
+
+#include "gpu/display_clock.h"
+
+struct display_clock_dce112 {
+	struct display_clock disp_clk_base;
+	/* Max display block clocks state*/
+	enum clocks_state max_clks_state;
+	bool use_max_disp_clk;
+	uint32_t crystal_freq_khz;
+	uint32_t dentist_vco_freq_khz;
+	/* Cache the status of DFS-bypass feature*/
+	bool dfs_bypass_enabled;
+	/* GPU PLL SS percentage (if down-spread enabled) */
+	uint32_t gpu_pll_ss_percentage;
+	/* GPU PLL SS percentage Divider (100 or 1000) */
+	uint32_t gpu_pll_ss_divider;
+	/* Flag for Enabled SS on GPU PLL */
+	bool ss_on_gpu_pll;
+	/* Cache the display clock returned by VBIOS if DFS-bypass is enabled.
+	 * This is basically "Crystal Frequency In KHz" (XTALIN) frequency */
+	uint32_t dfs_bypass_disp_clk;
+	struct display_clock_state clock_state;
+	struct state_dependent_clocks *max_clks_by_state;
+
+};
+
+#define DCLCK112_FROM_BASE(dc_base) \
+	container_of(dc_base, struct display_clock_dce112, disp_clk_base)
+
+/* Array identifiers and count for the divider ranges.*/
+enum divider_range_count {
+	DIVIDER_RANGE_01 = 0,
+	DIVIDER_RANGE_02,
+	DIVIDER_RANGE_03,
+	DIVIDER_RANGE_MAX /* == 3*/
+};
+
+/* Starting point for each divider range.*/
+enum divider_range_start {
+	DIVIDER_RANGE_01_START = 200, /* 2.00*/
+	DIVIDER_RANGE_02_START = 1600, /* 16.00*/
+	DIVIDER_RANGE_03_START = 3200, /* 32.00*/
+	DIVIDER_RANGE_SCALE_FACTOR = 100 /* Results are scaled up by 100.*/
+};
+
+bool dal_display_clock_dce112_construct(
+	struct display_clock_dce112 *dc112,
+	struct dc_context *ctx);
+
+void dispclk_dce112_destroy(struct display_clock **base);
+
+uint32_t dispclk_dce112_calculate_min_clock(
+	struct display_clock *base,
+	uint32_t path_num,
+	struct min_clock_params *params);
+
+struct display_clock_state dispclk_dce112_get_clock_state(
+	struct display_clock *dc);
+
+uint32_t dispclk_dce112_get_dfs_bypass_threshold(
+	struct display_clock *dc);
+
+enum clocks_state dispclk_dce112_get_min_clocks_state(
+	struct display_clock *base);
+
+enum clocks_state dispclk_dce112_get_required_clocks_state(
+	struct display_clock *dc,
+	struct state_dependent_clocks *req_clocks);
+
+uint32_t dispclk_dce112_get_validation_clock(struct display_clock *dc);
+
+void dispclk_dce112_set_clock(
+	struct display_clock *base,
+	uint32_t requested_clk_khz);
+
+void dispclk_dce112_set_clock_state(
+	struct display_clock *dc,
+	struct display_clock_state clk_state);
+
+bool dispclk_dce112_set_min_clocks_state(
+	struct display_clock *base,
+	enum clocks_state clocks_state);
+
+void dispclk_dce112_store_max_clocks_state(
+	struct display_clock *base,
+	enum clocks_state max_clocks_state);
+
+#endif /* __DAL_DISPLAY_CLOCK_DCE112_H__ */
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/dce80/display_clock_dce80.c b/drivers/gpu/drm/amd/display/dc/gpu/dce80/display_clock_dce80.c
new file mode 100644
index 0000000..eedcfd6
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/dce80/display_clock_dce80.c
@@ -0,0 +1,934 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+
+#include "dce/dce_8_0_d.h"
+#include "dce/dce_8_0_sh_mask.h"
+
+#include "include/bios_parser_interface.h"
+#include "include/fixed32_32.h"
+#include "include/logger_interface.h"
+
+#include "../divider_range.h"
+#include "display_clock_dce80.h"
+#include "dc.h"
+
+#define DCE80_DFS_BYPASS_THRESHOLD_KHZ 100000
+
+/* Max clock values for each state indexed by "enum clocks_state": */
+static struct state_dependent_clocks max_clks_by_state[] = {
+/* ClocksStateInvalid - should not be used */
+{ .display_clk_khz = 0, .pixel_clk_khz = 0 },
+/* ClocksStateUltraLow - not expected to be used for DCE 8.0 */
+{ .display_clk_khz = 0, .pixel_clk_khz = 0 },
+/* ClocksStateLow */
+{ .display_clk_khz = 352000, .pixel_clk_khz = 330000},
+/* ClocksStateNominal */
+{ .display_clk_khz = 600000, .pixel_clk_khz = 400000 },
+/* ClocksStatePerformance */
+{ .display_clk_khz = 600000, .pixel_clk_khz = 400000 } };
+
+/* Starting point for each divider range.*/
+enum divider_range_start {
+	DIVIDER_RANGE_01_START = 200, /* 2.00*/
+	DIVIDER_RANGE_02_START = 1600, /* 16.00*/
+	DIVIDER_RANGE_03_START = 3200, /* 32.00*/
+	DIVIDER_RANGE_SCALE_FACTOR = 100 /* Results are scaled up by 100.*/
+};
+
+/* Ranges for divider identifiers (Divider ID or DID)
+ mmDENTIST_DISPCLK_CNTL.DENTIST_DISPCLK_WDIVIDER*/
+enum divider_id_register_setting {
+	DIVIDER_RANGE_01_BASE_DIVIDER_ID = 0X08,
+	DIVIDER_RANGE_02_BASE_DIVIDER_ID = 0X40,
+	DIVIDER_RANGE_03_BASE_DIVIDER_ID = 0X60,
+	DIVIDER_RANGE_MAX_DIVIDER_ID = 0X80
+};
+
+/* Step size between each divider within a range.
+ Incrementing the DENTIST_DISPCLK_WDIVIDER by one
+ will increment the divider by this much.*/
+enum divider_range_step_size {
+	DIVIDER_RANGE_01_STEP_SIZE = 25, /* 0.25*/
+	DIVIDER_RANGE_02_STEP_SIZE = 50, /* 0.50*/
+	DIVIDER_RANGE_03_STEP_SIZE = 100 /* 1.00 */
+};
+
+/* Array identifiers and count for the divider ranges.*/
+enum divider_range_count {
+	DIVIDER_RANGE_01 = 0,
+	DIVIDER_RANGE_02,
+	DIVIDER_RANGE_03,
+	DIVIDER_RANGE_MAX /* == 3*/
+};
+
+static struct divider_range divider_ranges[DIVIDER_RANGE_MAX];
+
+#define FROM_DISPLAY_CLOCK(base) \
+	container_of(base, struct display_clock_dce80, disp_clk)
+
+static struct fixed32_32 get_deep_color_factor(struct min_clock_params *params)
+{
+	/* DeepColorFactor = IF (HDMI = True, bpp / 24, 1)*/
+	struct fixed32_32 deep_color_factor = dal_fixed32_32_from_int(1);
+
+	if (params->signal_type != SIGNAL_TYPE_HDMI_TYPE_A)
+		return deep_color_factor;
+
+	switch (params->deep_color_depth) {
+	case COLOR_DEPTH_101010:
+		/*deep color ratio for 30bpp is 30/24 = 1.25*/
+		deep_color_factor = dal_fixed32_32_from_fraction(30, 24);
+		break;
+
+	case COLOR_DEPTH_121212:
+		/* deep color ratio for 36bpp is 36/24 = 1.5*/
+		deep_color_factor = dal_fixed32_32_from_fraction(36, 24);
+		break;
+
+	case COLOR_DEPTH_161616:
+		/* deep color ratio for 48bpp is 48/24 = 2.0 */
+		deep_color_factor = dal_fixed32_32_from_fraction(48, 24);
+		break;
+	default:
+		break;
+	}
+	return deep_color_factor;
+}
+
+static uint32_t get_scaler_efficiency(struct min_clock_params *params)
+{
+	uint32_t scaler_efficiency = 3;
+
+	switch (params->scaler_efficiency) {
+	case V_SCALER_EFFICIENCY_LB18BPP:
+	case V_SCALER_EFFICIENCY_LB24BPP:
+		scaler_efficiency = 4;
+		break;
+
+	case V_SCALER_EFFICIENCY_LB30BPP:
+	case V_SCALER_EFFICIENCY_LB36BPP:
+		scaler_efficiency = 3;
+		break;
+
+	default:
+		break;
+	}
+
+	return scaler_efficiency;
+}
+
+static uint32_t get_actual_required_display_clk(
+	struct display_clock_dce80 *disp_clk,
+	uint32_t  target_clk_khz)
+{
+	uint32_t disp_clk_khz = target_clk_khz;
+	uint32_t div = INVALID_DIVIDER;
+	uint32_t did = INVALID_DID;
+	uint32_t scaled_vco =
+		disp_clk->dentist_vco_freq_khz * DIVIDER_RANGE_SCALE_FACTOR;
+
+	ASSERT(disp_clk_khz);
+
+	if (disp_clk_khz)
+		div = scaled_vco / disp_clk_khz;
+
+	did = dal_divider_range_get_did(divider_ranges, DIVIDER_RANGE_MAX, div);
+
+	if (did != INVALID_DID) {
+		div = dal_divider_range_get_divider(
+			divider_ranges, DIVIDER_RANGE_MAX, did);
+
+		if ((div != INVALID_DIVIDER) &&
+			(did > DIVIDER_RANGE_01_BASE_DIVIDER_ID))
+			if (disp_clk_khz > (scaled_vco / div))
+				div = dal_divider_range_get_divider(
+					divider_ranges, DIVIDER_RANGE_MAX,
+					did - 1);
+
+		if (div != INVALID_DIVIDER)
+			disp_clk_khz = scaled_vco / div;
+
+	}
+	/* We need to add 10KHz to this value because the accuracy in VBIOS is
+	 in 10KHz units. So we need to always round the last digit up in order
+	 to reach the next div level.*/
+	return disp_clk_khz + 10;
+}
+
+static uint32_t get_validation_clock(struct display_clock *dc)
+{
+	uint32_t clk = 0;
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	switch (disp_clk->max_clks_state) {
+	case CLOCKS_STATE_ULTRA_LOW:
+		/*Currently not supported, it has 0 in table entry*/
+	case CLOCKS_STATE_LOW:
+		clk = max_clks_by_state[CLOCKS_STATE_LOW].
+						display_clk_khz;
+		break;
+
+	case CLOCKS_STATE_NOMINAL:
+		clk = max_clks_by_state[CLOCKS_STATE_NOMINAL].
+						display_clk_khz;
+		break;
+
+	case CLOCKS_STATE_PERFORMANCE:
+		clk = max_clks_by_state[CLOCKS_STATE_PERFORMANCE].
+						display_clk_khz;
+		break;
+
+	case CLOCKS_STATE_INVALID:
+	default:
+		/*Invalid Clocks State*/
+		BREAK_TO_DEBUGGER();
+		/* just return the display engine clock for
+		 * lowest supported state*/
+		clk = max_clks_by_state[CLOCKS_STATE_LOW].
+						display_clk_khz;
+		break;
+	}
+	return clk;
+}
+
+static uint32_t calc_single_display_min_clks(
+	struct display_clock *base,
+	struct min_clock_params *params,
+	bool set_clk)
+{
+	struct fixed32_32 h_scale = dal_fixed32_32_from_int(1);
+	struct fixed32_32 v_scale = dal_fixed32_32_from_int(1);
+	uint32_t pix_clk_khz = params->requested_pixel_clock;
+	uint32_t line_total = params->timing_info.h_total;
+	uint32_t max_clk_khz = get_validation_clock(base);
+	struct fixed32_32 deep_color_factor = get_deep_color_factor(params);
+	uint32_t scaler_efficiency = get_scaler_efficiency(params);
+	struct fixed32_32 v_filter_init;
+	uint32_t v_filter_init_trunc;
+	struct fixed32_32 v_filter_init_ceil;
+	struct fixed32_32 src_lines_per_dst_line;
+	uint32_t src_wdth_rnd_to_chunks;
+	struct fixed32_32 scaling_coeff;
+	struct fixed32_32 fx_disp_clk_khz;
+	struct fixed32_32 fx_alt_disp_clk_khz;
+	uint32_t disp_clk_khz;
+	uint32_t alt_disp_clk_khz;
+	struct display_clock_dce80 *dc = FROM_DISPLAY_CLOCK(base);
+
+	if (0 != params->dest_view.height && 0 != params->dest_view.width) {
+
+		h_scale = dal_fixed32_32_from_fraction(
+			params->source_view.width,
+			params->dest_view.width);
+		v_scale = dal_fixed32_32_from_fraction(
+			params->source_view.height,
+			params->dest_view.height);
+	}
+
+	v_filter_init = dal_fixed32_32_from_fraction(
+		params->scaling_info.v_taps, 2u);
+	v_filter_init = dal_fixed32_32_add(v_filter_init,
+		dal_fixed32_32_div_int(v_scale, 2));
+	v_filter_init = dal_fixed32_32_add(v_filter_init,
+		dal_fixed32_32_from_fraction(15, 10));
+
+	v_filter_init_trunc = dal_fixed32_32_floor(v_filter_init);
+
+	v_filter_init_ceil = dal_fixed32_32_from_fraction(
+						v_filter_init_trunc, 2);
+	v_filter_init_ceil = dal_fixed32_32_from_int(
+		dal_fixed32_32_ceil(v_filter_init_ceil));
+	v_filter_init_ceil = dal_fixed32_32_mul_int(v_filter_init_ceil, 2);
+	v_filter_init_ceil = dal_fixed32_32_div_int(v_filter_init_ceil, 3);
+	v_filter_init_ceil = dal_fixed32_32_from_int(
+		dal_fixed32_32_ceil(v_filter_init_ceil));
+
+	src_lines_per_dst_line = dal_fixed32_32_max(
+		dal_fixed32_32_from_int(dal_fixed32_32_ceil(v_scale)),
+		v_filter_init_ceil);
+
+	src_wdth_rnd_to_chunks =
+		((params->source_view.width - 1) / 128) * 128 + 256;
+
+	scaling_coeff = dal_fixed32_32_max(
+		dal_fixed32_32_from_fraction(params->scaling_info.h_taps, 4),
+		dal_fixed32_32_mul(
+			dal_fixed32_32_from_fraction(
+				params->scaling_info.v_taps,
+				scaler_efficiency),
+					h_scale));
+
+	scaling_coeff = dal_fixed32_32_max(scaling_coeff, h_scale);
+
+	fx_disp_clk_khz = dal_fixed32_32_mul(
+		scaling_coeff, dal_fixed32_32_from_fraction(11, 10));
+	if (0 != line_total) {
+		struct fixed32_32 d_clk = dal_fixed32_32_mul_int(
+			src_lines_per_dst_line, src_wdth_rnd_to_chunks);
+		d_clk = dal_fixed32_32_div_int(d_clk, line_total);
+		d_clk = dal_fixed32_32_mul(d_clk,
+				dal_fixed32_32_from_fraction(11, 10));
+		fx_disp_clk_khz = dal_fixed32_32_max(fx_disp_clk_khz, d_clk);
+	}
+
+	fx_disp_clk_khz = dal_fixed32_32_max(fx_disp_clk_khz,
+		dal_fixed32_32_mul(deep_color_factor,
+		dal_fixed32_32_from_fraction(11, 10)));
+
+	fx_disp_clk_khz = dal_fixed32_32_mul_int(fx_disp_clk_khz, pix_clk_khz);
+	fx_disp_clk_khz = dal_fixed32_32_mul(fx_disp_clk_khz,
+		dal_fixed32_32_from_fraction(1005, 1000));
+
+	fx_alt_disp_clk_khz = scaling_coeff;
+
+	if (0 != line_total) {
+		struct fixed32_32 d_clk = dal_fixed32_32_mul_int(
+			src_lines_per_dst_line, src_wdth_rnd_to_chunks);
+		d_clk = dal_fixed32_32_div_int(d_clk, line_total);
+		d_clk = dal_fixed32_32_mul(d_clk,
+			dal_fixed32_32_from_fraction(105, 100));
+		fx_alt_disp_clk_khz = dal_fixed32_32_max(
+			fx_alt_disp_clk_khz, d_clk);
+	}
+	fx_alt_disp_clk_khz = dal_fixed32_32_max(
+		fx_alt_disp_clk_khz, fx_alt_disp_clk_khz);
+
+	fx_alt_disp_clk_khz = dal_fixed32_32_mul_int(
+		fx_alt_disp_clk_khz, pix_clk_khz);
+
+	/* convert to integer*/
+	disp_clk_khz = dal_fixed32_32_floor(fx_disp_clk_khz);
+	alt_disp_clk_khz = dal_fixed32_32_floor(fx_alt_disp_clk_khz);
+
+	if (set_clk) { /* only compensate clock if we are going to set it.*/
+		disp_clk_khz = get_actual_required_display_clk(
+			dc, disp_clk_khz);
+		alt_disp_clk_khz = get_actual_required_display_clk(
+			dc, alt_disp_clk_khz);
+	}
+
+	if ((disp_clk_khz > max_clk_khz) && (alt_disp_clk_khz <= max_clk_khz))
+		disp_clk_khz = alt_disp_clk_khz;
+
+	return disp_clk_khz;
+
+}
+
+static uint32_t calc_cursor_bw_for_min_clks(struct min_clock_params *params)
+{
+
+	struct fixed32_32 v_scale = dal_fixed32_32_from_int(1);
+	struct fixed32_32 v_filter_ceiling;
+	struct fixed32_32 src_lines_per_dst_line;
+	struct fixed32_32 cursor_bw;
+
+	/*  DCE8 Mode Support and Mode Set Architecture Specification Rev 1.3
+	 6.3.3	Cursor data Throughput requirement on DISPCLK
+	 The MCIF to DCP cursor data return throughput is one pixel per DISPCLK
+	  shared among the display heads.
+	 If (Total Cursor Bandwidth in pixels for All heads> DISPCLK)
+	 The mode is not supported
+	 Cursor Bandwidth in Pixels = Cursor Width *
+	 (SourceLinesPerDestinationLine / Line Time)
+	 Assuming that Cursor Width = 128
+	 */
+	/*In the hardware doc they mention an Interlace Factor
+	  It is not used here because we have already used it when
+	  calculating destination view*/
+	if (0 != params->dest_view.height)
+		v_scale = dal_fixed32_32_from_fraction(
+			params->source_view.height,
+			params->dest_view.height);
+
+	{
+	/*Do: Vertical Filter Init = 0.5 + VTAPS/2 + VSR/2 * Interlace Factor*/
+	/*Interlace Factor is included in verticalScaleRatio*/
+	struct fixed32_32 v_filter = dal_fixed32_32_add(
+		dal_fixed32_32_from_fraction(params->scaling_info.v_taps, 2),
+		dal_fixed32_32_div_int(v_scale, 2));
+	/*Do : Ceiling (Vertical Filter Init, 2)/3 )*/
+	v_filter_ceiling = dal_fixed32_32_div_int(v_filter, 2);
+	v_filter_ceiling = dal_fixed32_32_mul_int(
+		dal_fixed32_32_from_int(dal_fixed32_32_ceil(v_filter_ceiling)),
+							2);
+	v_filter_ceiling = dal_fixed32_32_div_int(v_filter_ceiling, 3);
+	}
+	/*Do : MAX( CeilCeiling (VSR), Ceiling (Vertical Filter Init, 2)/3 )*/
+	/*Do : SourceLinesPerDestinationLine =
+	 * MAX( Ceiling (VSR), Ceiling (Vertical Filter Init, 2)/3 )*/
+	src_lines_per_dst_line = dal_fixed32_32_max(v_scale, v_filter_ceiling);
+
+	if ((params->requested_pixel_clock != 0) &&
+		(params->timing_info.h_total != 0)) {
+		/* pixelClock is in units of KHz.  Calc lineTime in us*/
+		struct fixed32_32 inv_line_time = dal_fixed32_32_from_fraction(
+			params->requested_pixel_clock,
+			params->timing_info.h_total);
+		cursor_bw = dal_fixed32_32_mul(
+			dal_fixed32_32_mul_int(inv_line_time, 128),
+						src_lines_per_dst_line);
+	}
+
+	/* convert to integer*/
+	return dal_fixed32_32_floor(cursor_bw);
+}
+
+static bool validate(
+	struct display_clock *dc,
+	struct min_clock_params *params)
+{
+	uint32_t max_clk_khz = get_validation_clock(dc);
+	uint32_t req_clk_khz;
+
+	if (params == NULL)
+		return false;
+
+	req_clk_khz = calc_single_display_min_clks(dc, params, false);
+
+	return (req_clk_khz <= max_clk_khz);
+}
+
+static uint32_t calculate_min_clock(
+	struct display_clock *dc,
+	uint32_t path_num,
+	struct min_clock_params *params)
+{
+	uint32_t i;
+	uint32_t validation_clk_khz = get_validation_clock(dc);
+	uint32_t min_clk_khz = validation_clk_khz;
+	uint32_t max_clk_khz = 0;
+	uint32_t total_cursor_bw = 0;
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	if (disp_clk->use_max_disp_clk)
+		return min_clk_khz;
+
+	if (params != NULL) {
+		uint32_t disp_clk_khz = 0;
+
+		for (i = 0; i < path_num; ++i) {
+			disp_clk_khz = calc_single_display_min_clks(
+				dc, params, true);
+
+			/* update the max required clock found*/
+			if (disp_clk_khz > max_clk_khz)
+				max_clk_khz = disp_clk_khz;
+
+			disp_clk_khz = calc_cursor_bw_for_min_clks(params);
+
+			total_cursor_bw += disp_clk_khz;
+
+			params++;
+
+		}
+	}
+
+	max_clk_khz = (total_cursor_bw > max_clk_khz) ? total_cursor_bw :
+								max_clk_khz;
+
+	min_clk_khz = max_clk_khz;
+
+	/*"Cursor data Throughput requirement on DISPCLK is now a factor,
+	 *  need to change the code */
+	ASSERT(total_cursor_bw < validation_clk_khz);
+
+	if (min_clk_khz > validation_clk_khz)
+		min_clk_khz = validation_clk_khz;
+	else if (min_clk_khz < dc->min_display_clk_threshold_khz)
+		min_clk_khz = dc->min_display_clk_threshold_khz;
+
+	return min_clk_khz;
+}
+
+static void set_clock(
+	struct display_clock *dc,
+	uint32_t requested_clk_khz)
+{
+	struct bp_pixel_clock_parameters pxl_clk_params;
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+	struct dc_bios *bp = dc->ctx->dc_bios;
+
+	/* Prepare to program display clock*/
+	memset(&pxl_clk_params, 0, sizeof(pxl_clk_params));
+
+	pxl_clk_params.target_pixel_clock = requested_clk_khz;
+	pxl_clk_params.pll_id = dc->id;
+
+	bp->funcs->program_display_engine_pll(bp, &pxl_clk_params);
+
+	if (disp_clk->dfs_bypass_enabled) {
+
+		/* Cache the fixed display clock*/
+		disp_clk->dfs_bypass_disp_clk =
+			pxl_clk_params.dfs_bypass_display_clock;
+	}
+
+	/* from power down, we need mark the clock state as ClocksStateNominal
+	 * from HWReset, so when resume we will call pplib voltage regulator.*/
+	if (requested_clk_khz == 0)
+		disp_clk->cur_min_clks_state = CLOCKS_STATE_NOMINAL;
+}
+
+static uint32_t get_clock(struct display_clock *dc)
+{
+	uint32_t disp_clock = get_validation_clock(dc);
+	uint32_t target_div = INVALID_DIVIDER;
+	uint32_t addr = mmDENTIST_DISPCLK_CNTL;
+	uint32_t value = 0;
+	uint32_t field = 0;
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	if (disp_clk->dfs_bypass_enabled && disp_clk->dfs_bypass_disp_clk)
+		return disp_clk->dfs_bypass_disp_clk;
+
+	/* Read the mmDENTIST_DISPCLK_CNTL to get the currently programmed
+	 DID DENTIST_DISPCLK_WDIVIDER.*/
+	value = dm_read_reg(dc->ctx, addr);
+	field = get_reg_field_value(
+			value, DENTIST_DISPCLK_CNTL, DENTIST_DISPCLK_WDIVIDER);
+
+	/* Convert DENTIST_DISPCLK_WDIVIDER to actual divider*/
+	target_div = dal_divider_range_get_divider(
+		divider_ranges,
+		DIVIDER_RANGE_MAX,
+		field);
+
+	if (target_div != INVALID_DIVIDER)
+		/* Calculate the current DFS clock in KHz.
+		 Should be okay up to 42.9 THz before overflowing.*/
+		disp_clock = (DIVIDER_RANGE_SCALE_FACTOR
+			* disp_clk->dentist_vco_freq_khz) / target_div;
+	return disp_clock;
+}
+
+static void set_clock_state(
+	struct display_clock *dc,
+	struct display_clock_state clk_state)
+{
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	disp_clk->clock_state = clk_state;
+}
+static struct display_clock_state get_clock_state(
+	struct display_clock *dc)
+{
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	return disp_clk->clock_state;
+}
+
+static enum clocks_state get_min_clocks_state(struct display_clock *dc)
+{
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	return disp_clk->cur_min_clks_state;
+}
+
+static enum clocks_state get_required_clocks_state
+	(struct display_clock *dc,
+	struct state_dependent_clocks *req_clocks)
+{
+	int32_t i;
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+	enum clocks_state low_req_clk = disp_clk->max_clks_state;
+
+	if (!req_clocks) {
+		/* NULL pointer*/
+		BREAK_TO_DEBUGGER();
+		return CLOCKS_STATE_INVALID;
+	}
+
+	/* Iterate from highest supported to lowest valid state, and update
+	 * lowest RequiredState with the lowest state that satisfies
+	 * all required clocks
+	 */
+	for (i = disp_clk->max_clks_state; i >= CLOCKS_STATE_ULTRA_LOW; --i) {
+		if ((req_clocks->display_clk_khz <=
+			max_clks_by_state[i].display_clk_khz) &&
+			(req_clocks->pixel_clk_khz <=
+				max_clks_by_state[i].pixel_clk_khz))
+			low_req_clk = i;
+	}
+	return low_req_clk;
+}
+
+static bool set_min_clocks_state(
+	struct display_clock *dc,
+	enum clocks_state clocks_state)
+{
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	struct dm_pp_power_level_change_request level_change_req = {
+			DM_PP_POWER_LEVEL_INVALID};
+
+	if (clocks_state > disp_clk->max_clks_state) {
+		/*Requested state exceeds max supported state.*/
+		dm_logger_write(dc->ctx->logger, LOG_WARNING,
+				"Requested state exceeds max supported state");
+		return false;
+	} else if (clocks_state == dc->cur_min_clks_state) {
+		/*if we're trying to set the same state, we can just return
+		 * since nothing needs to be done*/
+		return true;
+	}
+
+	switch (clocks_state) {
+	case CLOCKS_STATE_ULTRA_LOW:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_ULTRA_LOW;
+		break;
+	case CLOCKS_STATE_LOW:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_LOW;
+		break;
+	case CLOCKS_STATE_NOMINAL:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_NOMINAL;
+		break;
+	case CLOCKS_STATE_PERFORMANCE:
+		level_change_req.power_level = DM_PP_POWER_LEVEL_PERFORMANCE;
+		break;
+	case CLOCKS_STATE_INVALID:
+	default:
+		dm_logger_write(dc->ctx->logger, LOG_WARNING,
+				"Requested state invalid state");
+		return false;
+	}
+
+	/* get max clock state from PPLIB */
+	if (dm_pp_apply_power_level_change_request(dc->ctx, &level_change_req))
+		dc->cur_min_clks_state = clocks_state;
+
+	return true;
+}
+
+static uint32_t get_dp_ref_clk_frequency(struct display_clock *dc)
+{
+	uint32_t dispclk_cntl_value;
+	uint32_t dp_ref_clk_cntl_value;
+	uint32_t dp_ref_clk_cntl_src_sel_value;
+	uint32_t dp_ref_clk_khz = 600000;
+	uint32_t target_div = INVALID_DIVIDER;
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	/* ASSERT DP Reference Clock source is from DFS*/
+	dp_ref_clk_cntl_value = dm_read_reg(dc->ctx,
+			mmDPREFCLK_CNTL);
+
+	dp_ref_clk_cntl_src_sel_value =
+			get_reg_field_value(
+				dp_ref_clk_cntl_value,
+				DPREFCLK_CNTL, DPREFCLK_SRC_SEL);
+
+	ASSERT(dp_ref_clk_cntl_src_sel_value == 0);
+
+	/* Read the mmDENTIST_DISPCLK_CNTL to get the currently
+	 * programmed DID DENTIST_DPREFCLK_WDIVIDER*/
+	dispclk_cntl_value = dm_read_reg(dc->ctx,
+			mmDENTIST_DISPCLK_CNTL);
+
+	/* Convert DENTIST_DPREFCLK_WDIVIDERto actual divider*/
+	target_div = dal_divider_range_get_divider(
+		divider_ranges,
+		DIVIDER_RANGE_MAX,
+		get_reg_field_value(dispclk_cntl_value,
+			DENTIST_DISPCLK_CNTL,
+			DENTIST_DPREFCLK_WDIVIDER));
+
+	if (target_div != INVALID_DIVIDER) {
+		/* Calculate the current DFS clock, in kHz.*/
+		dp_ref_clk_khz = (DIVIDER_RANGE_SCALE_FACTOR
+			* disp_clk->dentist_vco_freq_khz) / target_div;
+	}
+
+	/* SW will adjust DP REF Clock average value for all purposes
+	 * (DP DTO / DP Audio DTO and DP GTC)
+	 if clock is spread for all cases:
+	 -if SS enabled on DP Ref clock and HW de-spreading enabled with SW
+	 calculations for DS_INCR/DS_MODULO (this is planned to be default case)
+	 -if SS enabled on DP Ref clock and HW de-spreading enabled with HW
+	 calculations (not planned to be used, but average clock should still
+	 be valid)
+	 -if SS enabled on DP Ref clock and HW de-spreading disabled
+	 (should not be case with CIK) then SW should program all rates
+	 generated according to average value (case as with previous ASICs)
+	  */
+	if ((disp_clk->ss_on_gpu_pll) && (disp_clk->gpu_pll_ss_divider != 0)) {
+		struct fixed32_32 ss_percentage = dal_fixed32_32_div_int(
+				dal_fixed32_32_from_fraction(
+					disp_clk->gpu_pll_ss_percentage,
+					disp_clk->gpu_pll_ss_divider), 200);
+		struct fixed32_32 adj_dp_ref_clk_khz;
+
+		ss_percentage = dal_fixed32_32_sub(dal_fixed32_32_one,
+								ss_percentage);
+		adj_dp_ref_clk_khz =
+			dal_fixed32_32_mul_int(
+				ss_percentage,
+				dp_ref_clk_khz);
+		dp_ref_clk_khz = dal_fixed32_32_floor(adj_dp_ref_clk_khz);
+	}
+
+	return dp_ref_clk_khz;
+}
+
+static void store_max_clocks_state(
+	struct display_clock *dc,
+	enum clocks_state max_clocks_state)
+{
+	struct display_clock_dce80 *disp_clk = FROM_DISPLAY_CLOCK(dc);
+
+	switch (max_clocks_state) {
+	case CLOCKS_STATE_LOW:
+	case CLOCKS_STATE_NOMINAL:
+	case CLOCKS_STATE_PERFORMANCE:
+	case CLOCKS_STATE_ULTRA_LOW:
+		disp_clk->max_clks_state = max_clocks_state;
+		break;
+
+	case CLOCKS_STATE_INVALID:
+	default:
+		/*Invalid Clocks State!*/
+		BREAK_TO_DEBUGGER();
+		break;
+	}
+}
+
+static void display_clock_ss_construct(
+	struct display_clock_dce80 *disp_clk)
+{
+	struct dc_bios *bp = disp_clk->disp_clk.ctx->dc_bios;
+	uint32_t ss_entry_num = bp->funcs->get_ss_entry_number(bp,
+		AS_SIGNAL_TYPE_GPU_PLL);
+
+	/*Read SS Info from VBIOS SS Info table for DP Reference Clock spread.*/
+	if (ss_entry_num > 0) {/* Should be only one entry */
+		struct spread_spectrum_info ss_info;
+		enum bp_result res;
+
+		memset(&ss_info, 0, sizeof(struct spread_spectrum_info));
+
+		res = bp->funcs->get_spread_spectrum_info(bp,
+			AS_SIGNAL_TYPE_GPU_PLL, 0, &ss_info);
+
+		/* Based on VBIOS, VBIOS will keep entry for GPU PLL SS even if
+		 * SS not enabled and in that case
+		 * SSInfo.spreadSpectrumPercentage !=0 would be
+		 * sign that SS is enabled*/
+		if (res == BP_RESULT_OK && ss_info.spread_spectrum_percentage != 0) {
+			disp_clk->ss_on_gpu_pll = true;
+			disp_clk->gpu_pll_ss_divider =
+				ss_info.spread_percentage_divider;
+			if (ss_info.type.CENTER_MODE == 0)
+				/* Currently we need only SS
+				 * percentage for down-spread*/
+				disp_clk->gpu_pll_ss_percentage =
+					ss_info.spread_spectrum_percentage;
+		}
+	}
+}
+
+static bool display_clock_integrated_info_construct(
+	struct display_clock_dce80 *disp_clk)
+{
+	struct dc_debug *debug = &disp_clk->disp_clk.ctx->dc->debug;
+	struct dc_bios *bp = disp_clk->disp_clk.ctx->dc_bios;
+	struct integrated_info info = { { { 0 } } };
+	struct firmware_info fw_info = { { 0 } };
+	uint32_t i;
+
+	if (bp->integrated_info)
+		info = *bp->integrated_info;
+
+	disp_clk->dentist_vco_freq_khz = info.dentist_vco_freq;
+	if (disp_clk->dentist_vco_freq_khz == 0) {
+		bp->funcs->get_firmware_info(bp, &fw_info);
+		disp_clk->dentist_vco_freq_khz =
+			fw_info.smu_gpu_pll_output_freq;
+		if (disp_clk->dentist_vco_freq_khz == 0)
+			disp_clk->dentist_vco_freq_khz = 3600000;
+		}
+	disp_clk->disp_clk.min_display_clk_threshold_khz =
+		disp_clk->dentist_vco_freq_khz / 64;
+
+	/* TODO: initialise disp_clk->dfs_bypass_disp_clk */
+
+	/*update the maximum display clock for each power state*/
+	for (i = 0; i < NUMBER_OF_DISP_CLK_VOLTAGE; ++i) {
+		enum clocks_state clk_state = CLOCKS_STATE_INVALID;
+
+		switch (i) {
+		case 0:
+			clk_state = CLOCKS_STATE_ULTRA_LOW;
+			break;
+
+		case 1:
+			clk_state = CLOCKS_STATE_LOW;
+			break;
+
+		case 2:
+			clk_state = CLOCKS_STATE_NOMINAL;
+			break;
+
+		case 3:
+			clk_state = CLOCKS_STATE_PERFORMANCE;
+			break;
+
+		default:
+			clk_state = CLOCKS_STATE_INVALID;
+			break;
+		}
+
+		/*Do not allow bad VBIOS/SBIOS to override with invalid values,
+		 * check for > 100MHz*/
+		if (info.disp_clk_voltage[i].max_supported_clk >= 100000) {
+			max_clks_by_state[clk_state].display_clk_khz =
+				info.disp_clk_voltage[i].max_supported_clk;
+		}
+	}
+
+	disp_clk->dfs_bypass_enabled = false;
+	if (!debug->disable_dfs_bypass)
+		if (bp->integrated_info->gpu_cap_info & DFS_BYPASS_ENABLE)
+			disp_clk->dfs_bypass_enabled = true;
+
+	disp_clk->use_max_disp_clk = debug->max_disp_clk;
+
+	return true;
+}
+
+static uint32_t get_dfs_bypass_threshold(struct display_clock *dc)
+{
+	return DCE80_DFS_BYPASS_THRESHOLD_KHZ;
+}
+
+static void destroy(struct display_clock **dc)
+{
+	struct display_clock_dce80 *disp_clk;
+
+	disp_clk = FROM_DISPLAY_CLOCK(*dc);
+	dm_free(disp_clk);
+	*dc = NULL;
+}
+
+static const struct display_clock_funcs funcs = {
+	.calculate_min_clock = calculate_min_clock,
+	.destroy = destroy,
+	.get_clock = get_clock,
+	.get_clock_state = get_clock_state,
+	.get_dfs_bypass_threshold = get_dfs_bypass_threshold,
+	.get_dp_ref_clk_frequency = get_dp_ref_clk_frequency,
+	.get_min_clocks_state = get_min_clocks_state,
+	.get_required_clocks_state = get_required_clocks_state,
+	.get_validation_clock = get_validation_clock,
+	.set_clock = set_clock,
+	.set_clock_state = set_clock_state,
+	.set_dp_ref_clock_source =
+		dal_display_clock_base_set_dp_ref_clock_source,
+	.set_min_clocks_state = set_min_clocks_state,
+	.store_max_clocks_state = store_max_clocks_state,
+	.validate = validate,
+};
+
+static bool display_clock_construct(
+	struct dc_context *ctx,
+	struct display_clock_dce80 *disp_clk)
+{
+	struct display_clock *dc_base = &disp_clk->disp_clk;
+
+	if (!dal_display_clock_construct_base(dc_base, ctx))
+		return false;
+
+	dc_base->funcs = &funcs;
+	/*
+	 * set_dp_ref_clock_source
+	 * set_clock_state
+	 * get_clock_state
+	 * get_dfs_bypass_threshold
+	 */
+
+	disp_clk->gpu_pll_ss_percentage = 0;
+	disp_clk->gpu_pll_ss_divider = 1000;
+	disp_clk->ss_on_gpu_pll = false;
+	disp_clk->dfs_bypass_enabled = false;
+	disp_clk->dfs_bypass_disp_clk = 0;
+	disp_clk->use_max_disp_clk = true;/* false will hang the system! */
+
+	disp_clk->disp_clk.id = CLOCK_SOURCE_ID_DFS;
+/* Initially set max clocks state to nominal.  This should be updated by
+ * via a pplib call to DAL IRI eventually calling a
+ * DisplayEngineClock_Dce50::StoreMaxClocksState().  This call will come in
+ * on PPLIB init. This is from DCE5x. in case HW wants to use mixed method.*/
+	disp_clk->max_clks_state = CLOCKS_STATE_NOMINAL;
+/* Initially set current min clocks state to invalid since we
+ * cannot make any assumption about PPLIB's initial state. This will be updated
+ * by HWSS via SetMinClocksState() on first mode set prior to programming
+ * state dependent clocks.*/
+	disp_clk->cur_min_clks_state = CLOCKS_STATE_INVALID;
+
+	display_clock_ss_construct(disp_clk);
+
+	if (!display_clock_integrated_info_construct(disp_clk)) {
+		dm_logger_write(dc_base->ctx->logger, LOG_WARNING,
+			"Cannot obtain VBIOS integrated info");
+	}
+
+	dal_divider_range_construct(
+		&divider_ranges[DIVIDER_RANGE_01],
+		DIVIDER_RANGE_01_START,
+		DIVIDER_RANGE_01_STEP_SIZE,
+		DIVIDER_RANGE_01_BASE_DIVIDER_ID,
+		DIVIDER_RANGE_02_BASE_DIVIDER_ID);
+	dal_divider_range_construct(
+		&divider_ranges[DIVIDER_RANGE_02],
+		DIVIDER_RANGE_02_START,
+		DIVIDER_RANGE_02_STEP_SIZE,
+		DIVIDER_RANGE_02_BASE_DIVIDER_ID,
+		DIVIDER_RANGE_03_BASE_DIVIDER_ID);
+	dal_divider_range_construct(
+		&divider_ranges[DIVIDER_RANGE_03],
+		DIVIDER_RANGE_03_START,
+		DIVIDER_RANGE_03_STEP_SIZE,
+		DIVIDER_RANGE_03_BASE_DIVIDER_ID,
+		DIVIDER_RANGE_MAX_DIVIDER_ID);
+	return true;
+}
+
+struct display_clock *dal_display_clock_dce80_create(
+	struct dc_context *ctx)
+{
+	struct display_clock_dce80 *disp_clk;
+
+	disp_clk = dm_alloc(sizeof(struct display_clock_dce80));
+
+	if (disp_clk == NULL)
+		return NULL;
+
+	if (display_clock_construct(ctx, disp_clk))
+		return &disp_clk->disp_clk;
+
+	dm_free(disp_clk);
+	return NULL;
+}
+
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/dce80/display_clock_dce80.h b/drivers/gpu/drm/amd/display/dc/gpu/dce80/display_clock_dce80.h
new file mode 100644
index 0000000..944dd03
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/dce80/display_clock_dce80.h
@@ -0,0 +1,57 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+#ifndef __DAL_DISPLAY_CLOCK_DCE80_H__
+#define __DAL_DISPLAY_CLOCK_DCE80_H__
+
+#include "gpu/display_clock.h"
+
+struct display_clock_dce80 {
+	struct display_clock disp_clk;
+	/* DFS input - GPUPLL VCO frequency - from VBIOS Firmware info. */
+	uint32_t dentist_vco_freq_khz;
+	/* GPU PLL SS percentage (if down-spread enabled)*/
+	uint32_t gpu_pll_ss_percentage;
+	/* GPU PLL SS percentage Divider (100 or 1000)*/
+	uint32_t gpu_pll_ss_divider;
+	/* Flag for Enabled SS on GPU PLL*/
+	bool ss_on_gpu_pll;
+	/* Max display block clocks state*/
+	enum clocks_state max_clks_state;
+	/* Current minimum display block clocks state*/
+	enum clocks_state cur_min_clks_state;
+	/* DFS-bypass feature variable
+	 Cache the status of DFS-bypass feature*/
+	bool dfs_bypass_enabled;
+	/* Cache the display clock returned by VBIOS if DFS-bypass is enabled.
+	 * This is basically "Crystal Frequency In KHz" (XTALIN) frequency */
+	uint32_t dfs_bypass_disp_clk;
+	bool use_max_disp_clk;
+	struct display_clock_state clock_state;
+};
+
+struct display_clock *dal_display_clock_dce80_create(
+	struct dc_context *ctx);
+
+#endif /* __DAL_DISPLAY_CLOCK_DCE80_H__ */
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/display_clock.c b/drivers/gpu/drm/amd/display/dc/gpu/display_clock.c
new file mode 100644
index 0000000..bcc0a51
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/display_clock.c
@@ -0,0 +1,217 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "display_clock.h"
+
+void dal_display_clock_base_set_dp_ref_clock_source(
+	struct display_clock *disp_clk,
+	enum clock_source_id clk_src)
+{/*must be implemented in derived*/
+
+}
+
+void dal_display_clock_base_set_clock_state(struct display_clock *disp_clk,
+	struct display_clock_state clk_state)
+{
+	/*Implemented only in DCE81*/
+}
+struct display_clock_state dal_display_clock_base_get_clock_state(
+	struct display_clock *disp_clk)
+{
+	/*Implemented only in DCE81*/
+	struct display_clock_state state = {0};
+	return state;
+}
+uint32_t dal_display_clock_base_get_dfs_bypass_threshold(
+	struct display_clock *disp_clk)
+{
+	/*Implemented only in DCE81*/
+	return 0;
+}
+
+bool dal_display_clock_construct_base(
+	struct display_clock *base,
+	struct dc_context *ctx)
+{
+	base->ctx = ctx;
+	base->id = CLOCK_SOURCE_ID_DCPLL;
+	base->min_display_clk_threshold_khz = 0;
+
+/* Initially set current min clocks state to invalid since we
+ * cannot make any assumption about PPLIB's initial state. This will be updated
+ * by HWSS via SetMinClocksState() on first mode set prior to programming
+ * state dependent clocks.*/
+	base->cur_min_clks_state = CLOCKS_STATE_INVALID;
+
+	return true;
+}
+
+void dal_display_clock_destroy(struct display_clock **disp_clk)
+{
+	if (!disp_clk || !*disp_clk) {
+		BREAK_TO_DEBUGGER();
+		return;
+	}
+
+	(*disp_clk)->funcs->destroy(disp_clk);
+
+	*disp_clk = NULL;
+}
+
+bool dal_display_clock_validate(
+	struct display_clock *disp_clk,
+	struct min_clock_params *params)
+{
+	return disp_clk->funcs->validate(disp_clk, params);
+}
+
+uint32_t dal_display_clock_calculate_min_clock(
+	struct display_clock *disp_clk,
+	uint32_t path_num,
+	struct min_clock_params *params)
+{
+	return disp_clk->funcs->calculate_min_clock(disp_clk, path_num, params);
+}
+
+uint32_t dal_display_clock_get_validation_clock(struct display_clock *disp_clk)
+{
+	return disp_clk->funcs->get_validation_clock(disp_clk);
+}
+
+void dal_display_clock_set_clock(
+	struct display_clock *disp_clk,
+	uint32_t requested_clock_khz)
+{
+	disp_clk->funcs->set_clock(disp_clk, requested_clock_khz);
+}
+
+uint32_t dal_display_clock_get_clock(struct display_clock *disp_clk)
+{
+	return disp_clk->funcs->get_clock(disp_clk);
+}
+
+bool dal_display_clock_get_min_clocks_state(
+	struct display_clock *disp_clk,
+	enum clocks_state *clocks_state)
+{
+	if (!disp_clk->funcs->get_min_clocks_state)
+		return false;
+
+	*clocks_state = disp_clk->funcs->get_min_clocks_state(disp_clk);
+	return true;
+}
+
+bool dal_display_clock_get_required_clocks_state(
+	struct display_clock *disp_clk,
+	struct state_dependent_clocks *req_clocks,
+	enum clocks_state *clocks_state)
+{
+	if (!disp_clk->funcs->get_required_clocks_state)
+		return false;
+
+	*clocks_state = disp_clk->funcs->get_required_clocks_state(
+			disp_clk, req_clocks);
+	return true;
+}
+
+bool dal_display_clock_set_min_clocks_state(
+	struct display_clock *disp_clk,
+	enum clocks_state clocks_state)
+{
+	if (!disp_clk->funcs->set_min_clocks_state)
+		return false;
+
+	disp_clk->funcs->set_min_clocks_state(disp_clk, clocks_state);
+	return true;
+}
+
+uint32_t dal_display_clock_get_dp_ref_clk_frequency(
+	struct display_clock *disp_clk)
+{
+	return disp_clk->funcs->get_dp_ref_clk_frequency(disp_clk);
+}
+
+/*the second parameter of "switchreferenceclock" is
+ * a dummy argument for all pre dce 6.0 versions*/
+
+void dal_display_clock_switch_reference_clock(
+	struct display_clock *disp_clk,
+	bool use_external_ref_clk,
+	uint32_t requested_clk_khz)
+{
+	/* TODO: requires Asic Control*/
+	/*
+	struct ac_pixel_clk_params params;
+	struct asic_control *ac =
+		dal_adapter_service_get_asic_control(disp_clk->as);
+	dc_service_memset(&params, 0, sizeof(struct ac_pixel_clk_params));
+
+	params.tgt_pixel_clk_khz = requested_clk_khz;
+	params.flags.SET_EXTERNAL_REF_DIV_SRC = use_external_ref_clk;
+	params.pll_id = disp_clk->id;
+	dal_asic_control_program_display_engine_pll(ac, &params);
+	*/
+}
+
+void dal_display_clock_set_dp_ref_clock_source(
+	struct display_clock *disp_clk,
+	enum clock_source_id clk_src)
+{
+	disp_clk->funcs->set_dp_ref_clock_source(disp_clk, clk_src);
+}
+
+void dal_display_clock_store_max_clocks_state(
+	struct display_clock *disp_clk,
+	enum clocks_state max_clocks_state)
+{
+	disp_clk->funcs->store_max_clocks_state(disp_clk, max_clocks_state);
+}
+
+void dal_display_clock_set_clock_state(
+	struct display_clock *disp_clk,
+	struct display_clock_state clk_state)
+{
+	disp_clk->funcs->set_clock_state(disp_clk, clk_state);
+}
+
+struct display_clock_state dal_display_clock_get_clock_state(
+	struct display_clock *disp_clk)
+{
+	return disp_clk->funcs->get_clock_state(disp_clk);
+}
+
+uint32_t dal_display_clock_get_dfs_bypass_threshold(
+	struct display_clock *disp_clk)
+{
+	return disp_clk->funcs->get_dfs_bypass_threshold(disp_clk);
+}
+
+void dal_display_clock_invalid_clock_state(
+	struct display_clock *disp_clk)
+{
+	disp_clk->cur_min_clks_state = CLOCKS_STATE_INVALID;
+}
+
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/display_clock.h b/drivers/gpu/drm/amd/display/dc/gpu/display_clock.h
new file mode 100644
index 0000000..663580d
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/display_clock.h
@@ -0,0 +1,89 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DAL_DISPLAY_CLOCK_H__
+#define __DAL_DISPLAY_CLOCK_H__
+
+#include "include/display_clock_interface.h"
+
+struct display_clock_funcs {
+	void (*destroy)(struct display_clock **to_destroy);
+	bool (*validate)(struct display_clock *disp_clk,
+		struct min_clock_params *params);
+	uint32_t (*calculate_min_clock)(struct display_clock *disp_clk,
+		uint32_t path_num, struct min_clock_params *params);
+	uint32_t (*get_validation_clock)(struct display_clock *disp_clk);
+	void (*set_clock)(struct display_clock *disp_clk,
+		uint32_t requested_clock_khz);
+	uint32_t (*get_clock)(struct display_clock *disp_clk);
+	enum clocks_state (*get_min_clocks_state)(
+		struct display_clock *disp_clk);
+	enum clocks_state (*get_required_clocks_state)(
+		struct display_clock *disp_clk,
+		struct state_dependent_clocks *req_clocks);
+	bool (*set_min_clocks_state)(struct display_clock *disp_clk,
+		enum clocks_state clocks_state);
+	uint32_t (*get_dp_ref_clk_frequency)(struct display_clock *disp_clk);
+	void (*set_dp_ref_clock_source)(struct display_clock *disp_clk,
+		enum clock_source_id clk_src);
+	void (*store_max_clocks_state)(struct display_clock *disp_clk,
+		enum clocks_state max_clocks_state);
+	void (*set_clock_state)(struct display_clock *disp_clk,
+		struct display_clock_state clk_state);
+	struct display_clock_state (*get_clock_state)(
+		struct display_clock *disp_clk);
+	uint32_t (*get_dfs_bypass_threshold)(struct display_clock *disp_clk);
+
+};
+
+struct display_clock {
+	struct dc_context *ctx;
+	const struct display_clock_funcs *funcs;
+	uint32_t min_display_clk_threshold_khz;
+	enum clock_source_id id;
+
+	enum clocks_state cur_min_clks_state;
+};
+void dal_display_clock_base_set_dp_ref_clock_source(
+	struct display_clock *disp_clk,
+	enum clock_source_id clk_src);
+struct display_clock_state dal_display_clock_base_get_clock_state(
+	struct display_clock *disp_clk);
+uint32_t dal_display_clock_base_get_dfs_bypass_threshold(
+	struct display_clock *disp_clk);
+void dal_display_clock_base_set_clock_state(struct display_clock *disp_clk,
+	struct display_clock_state clk_state);
+bool dal_display_clock_construct_base(
+	struct display_clock *base,
+	struct dc_context *ctx);
+
+uint32_t dal_display_clock_get_validation_clock(struct display_clock *disp_clk);
+
+void dal_display_clock_store_max_clocks_state(
+	struct display_clock *disp_clk,
+	enum clocks_state max_clocks_state);
+
+
+#endif /* __DAL_DISPLAY_CLOCK_H__*/
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/divider_range.c b/drivers/gpu/drm/amd/display/dc/gpu/divider_range.c
new file mode 100644
index 0000000..59d4400
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/divider_range.c
@@ -0,0 +1,127 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+#include "dm_services.h"
+#include "divider_range.h"
+
+bool dal_divider_range_construct(
+	struct divider_range *div_range,
+	uint32_t range_start,
+	uint32_t range_step,
+	uint32_t did_min,
+	uint32_t did_max)
+{
+	div_range->div_range_start = range_start;
+	div_range->div_range_step = range_step;
+	div_range->did_min = did_min;
+	div_range->did_max = did_max;
+
+	if (div_range->div_range_step == 0) {
+		div_range->div_range_step = 1;
+		/*div_range_step cannot be zero*/
+		BREAK_TO_DEBUGGER();
+	}
+	/* Calculate this based on the other inputs.*/
+	/* See DividerRange.h for explanation of */
+	/* the relationship between divider id (DID) and a divider.*/
+	/* Number of Divider IDs = (Maximum Divider ID - Minimum Divider ID)*/
+	/* Maximum divider identified in this range =
+	 * (Number of Divider IDs)*Step size between dividers
+	 *  + The start of this range.*/
+	div_range->div_range_end = (did_max - did_min) * range_step
+		+ range_start;
+	return true;
+}
+
+static uint32_t dal_divider_range_calc_divider(
+	struct divider_range *div_range,
+	uint32_t did)
+{
+	/* Is this DID within our range?*/
+	if ((did < div_range->did_min) || (did >= div_range->did_max))
+		return INVALID_DIVIDER;
+
+	return ((did - div_range->did_min) * div_range->div_range_step)
+			+ div_range->div_range_start;
+
+}
+
+static uint32_t dal_divider_range_calc_did(
+	struct divider_range *div_range,
+	uint32_t div)
+{
+	uint32_t did;
+	/* Check before dividing.*/
+	if (div_range->div_range_step == 0) {
+		div_range->div_range_step = 1;
+		/*div_range_step cannot be zero*/
+		BREAK_TO_DEBUGGER();
+	}
+	/* Is this divider within our range?*/
+	if ((div < div_range->div_range_start)
+		|| (div >= div_range->div_range_end))
+		return INVALID_DID;
+/* did = (divider - range_start + (range_step-1)) / range_step) + did_min*/
+	did = div - div_range->div_range_start;
+	did += div_range->div_range_step - 1;
+	did /= div_range->div_range_step;
+	did += div_range->did_min;
+	return did;
+}
+
+uint32_t dal_divider_range_get_divider(
+	struct divider_range *div_range,
+	uint32_t ranges_num,
+	uint32_t did)
+{
+	uint32_t div = INVALID_DIVIDER;
+	uint32_t i;
+
+	for (i = 0; i < ranges_num; i++) {
+		/* Calculate divider with given divider ID*/
+		div = dal_divider_range_calc_divider(&div_range[i], did);
+		/* Found a valid return divider*/
+		if (div != INVALID_DIVIDER)
+			break;
+	}
+	return div;
+}
+uint32_t dal_divider_range_get_did(
+	struct divider_range *div_range,
+	uint32_t ranges_num,
+	uint32_t divider)
+{
+	uint32_t did = INVALID_DID;
+	uint32_t i;
+
+	for (i = 0; i < ranges_num; i++) {
+		/*  CalcDid returns InvalidDid if a divider ID isn't found*/
+		did = dal_divider_range_calc_did(&div_range[i], divider);
+		/* Found a valid return did*/
+		if (did != INVALID_DID)
+			break;
+	}
+	return did;
+}
+
diff --git a/drivers/gpu/drm/amd/display/dc/gpu/divider_range.h b/drivers/gpu/drm/amd/display/dc/gpu/divider_range.h
new file mode 100644
index 0000000..e53522f
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/gpu/divider_range.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DAL_DIVIDER_RANGE_H__
+#define __DAL_DIVIDER_RANGE_H__
+
+enum divider_error_types {
+	INVALID_DID = 0,
+	INVALID_DIVIDER = 1
+};
+
+struct divider_range {
+	uint32_t div_range_start;
+	/* The end of this range of dividers.*/
+	uint32_t div_range_end;
+	/* The distance between each divider in this range.*/
+	uint32_t div_range_step;
+	/* The divider id for the lowest divider.*/
+	uint32_t did_min;
+	/* The divider id for the highest divider.*/
+	uint32_t did_max;
+};
+
+bool dal_divider_range_construct(
+	struct divider_range *div_range,
+	uint32_t range_start,
+	uint32_t range_step,
+	uint32_t did_min,
+	uint32_t did_max);
+
+uint32_t dal_divider_range_get_divider(
+	struct divider_range *div_range,
+	uint32_t ranges_num,
+	uint32_t did);
+uint32_t dal_divider_range_get_did(
+	struct divider_range *div_range,
+	uint32_t ranges_num,
+	uint32_t divider);
+
+#endif /* __DAL_DIVIDER_RANGE_H__ */
diff --git a/drivers/gpu/drm/amd/display/dc/inc/bandwidth_calcs.h b/drivers/gpu/drm/amd/display/dc/inc/bandwidth_calcs.h
new file mode 100644
index 0000000..f9b871b
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/inc/bandwidth_calcs.h
@@ -0,0 +1,503 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+/**
+ * Bandwidth and Watermark calculations interface.
+ * (Refer to "DCEx_mode_support.xlsm" from Perforce.)
+ */
+#ifndef __BANDWIDTH_CALCS_H__
+#define __BANDWIDTH_CALCS_H__
+
+#include "bw_fixed.h"
+
+struct pipe_ctx;
+
+enum bw_calcs_version {
+	BW_CALCS_VERSION_INVALID,
+	BW_CALCS_VERSION_CARRIZO,
+	BW_CALCS_VERSION_POLARIS10,
+	BW_CALCS_VERSION_POLARIS11,
+	BW_CALCS_VERSION_STONEY,
+};
+
+/*******************************************************************************
+ * There are three types of input into Calculations:
+ * 1. per-DCE static values - these are "hardcoded" properties of the DCEIP
+ * 2. board-level values - these are generally coming from VBIOS parser
+ * 3. mode/configuration values - depending Mode, Scaling number of Displays etc.
+ ******************************************************************************/
+
+enum bw_defines {
+	//Common
+	bw_def_no = 0,
+	bw_def_none = 0,
+	bw_def_yes = 1,
+	bw_def_ok = 1,
+	bw_def_high = 2,
+	bw_def_mid = 1,
+	bw_def_low = 0,
+
+	//Internal
+	bw_defs_start = 255,
+	bw_def_underlay422,
+	bw_def_underlay420_luma,
+	bw_def_underlay420_chroma,
+	bw_def_underlay444,
+	bw_def_graphics,
+	bw_def_display_write_back420_luma,
+	bw_def_display_write_back420_chroma,
+	bw_def_portrait,
+	bw_def_hsr_mtn_4,
+	bw_def_hsr_mtn_h_taps,
+	bw_def_ceiling__h_taps_div_4___meq_hsr,
+	bw_def_invalid_linear_or_stereo_mode,
+	bw_def_invalid_rotation_or_bpp_or_stereo,
+	bw_def_vsr_mtn_v_taps,
+	bw_def_vsr_mtn_4,
+	bw_def_auto,
+	bw_def_manual,
+	bw_def_exceeded_allowed_maximum_sclk,
+	bw_def_exceeded_allowed_page_close_open,
+	bw_def_exceeded_allowed_outstanding_pte_req_queue_size,
+	bw_def_exceeded_allowed_maximum_bw,
+	bw_def_landscape,
+
+	//Panning and bezel
+	bw_def_any_lines,
+
+	//Underlay mode
+	bw_def_underlay_only,
+	bw_def_blended,
+	bw_def_blend,
+
+	//Stereo mode
+	bw_def_mono,
+	bw_def_side_by_side,
+	bw_def_top_bottom,
+
+	//Underlay surface type
+	bw_def_420,
+	bw_def_422,
+	bw_def_444,
+
+	//Tiling mode
+	bw_def_linear,
+	bw_def_tiled,
+	bw_def_array_linear_general,
+	bw_def_array_linear_aligned,
+	bw_def_rotated_micro_tiling,
+	bw_def_display_micro_tiling,
+
+	//Memory type
+	bw_def_gddr5,
+	bw_def_hbm,
+
+	//Voltage
+	bw_def_high_no_nbp_state_change,
+	bw_def_0_72,
+	bw_def_0_8,
+	bw_def_0_9,
+
+	bw_def_notok = -1,
+	bw_def_na = -1
+};
+
+struct bw_calcs_dceip {
+	enum bw_calcs_version version;
+	bool large_cursor;
+	uint32_t cursor_max_outstanding_group_num;
+	bool dmif_pipe_en_fbc_chunk_tracker;
+	struct bw_fixed dmif_request_buffer_size;
+	uint32_t lines_interleaved_into_lb;
+	uint32_t low_power_tiling_mode;
+	uint32_t chunk_width;
+	uint32_t number_of_graphics_pipes;
+	uint32_t number_of_underlay_pipes;
+	bool display_write_back_supported;
+	bool argb_compression_support;
+	struct bw_fixed underlay_vscaler_efficiency6_bit_per_component;
+	struct bw_fixed underlay_vscaler_efficiency8_bit_per_component;
+	struct bw_fixed underlay_vscaler_efficiency10_bit_per_component;
+	struct bw_fixed underlay_vscaler_efficiency12_bit_per_component;
+	struct bw_fixed graphics_vscaler_efficiency6_bit_per_component;
+	struct bw_fixed graphics_vscaler_efficiency8_bit_per_component;
+	struct bw_fixed graphics_vscaler_efficiency10_bit_per_component;
+	struct bw_fixed graphics_vscaler_efficiency12_bit_per_component;
+	struct bw_fixed alpha_vscaler_efficiency;
+	uint32_t max_dmif_buffer_allocated;
+	uint32_t graphics_dmif_size;
+	uint32_t underlay_luma_dmif_size;
+	uint32_t underlay_chroma_dmif_size;
+	bool pre_downscaler_enabled;
+	bool underlay_downscale_prefetch_enabled;
+	struct bw_fixed lb_write_pixels_per_dispclk;
+	struct bw_fixed lb_size_per_component444;
+	bool graphics_lb_nodownscaling_multi_line_prefetching;
+	struct bw_fixed stutter_and_dram_clock_state_change_gated_before_cursor;
+	struct bw_fixed underlay420_luma_lb_size_per_component;
+	struct bw_fixed underlay420_chroma_lb_size_per_component;
+	struct bw_fixed underlay422_lb_size_per_component;
+	struct bw_fixed cursor_chunk_width;
+	struct bw_fixed cursor_dcp_buffer_lines;
+	struct bw_fixed underlay_maximum_width_efficient_for_tiling;
+	struct bw_fixed underlay_maximum_height_efficient_for_tiling;
+	struct bw_fixed peak_pte_request_to_eviction_ratio_limiting_multiple_displays_or_single_rotated_display;
+	struct bw_fixed peak_pte_request_to_eviction_ratio_limiting_single_display_no_rotation;
+	struct bw_fixed minimum_outstanding_pte_request_limit;
+	struct bw_fixed maximum_total_outstanding_pte_requests_allowed_by_saw;
+	bool limit_excessive_outstanding_dmif_requests;
+	struct bw_fixed linear_mode_line_request_alternation_slice;
+	uint32_t scatter_gather_lines_of_pte_prefetching_in_linear_mode;
+	uint32_t display_write_back420_luma_mcifwr_buffer_size;
+	uint32_t display_write_back420_chroma_mcifwr_buffer_size;
+	struct bw_fixed request_efficiency;
+	struct bw_fixed dispclk_per_request;
+	struct bw_fixed dispclk_ramping_factor;
+	struct bw_fixed display_pipe_throughput_factor;
+	uint32_t scatter_gather_pte_request_rows_in_tiling_mode;
+	struct bw_fixed mcifwr_all_surfaces_burst_time;
+};
+
+struct bw_calcs_vbios {
+	enum bw_defines memory_type;
+	uint32_t dram_channel_width_in_bits;
+	uint32_t number_of_dram_channels;
+	uint32_t number_of_dram_banks;
+	struct bw_fixed low_yclk; /*m_hz*/
+	struct bw_fixed mid_yclk; /*m_hz*/
+	struct bw_fixed high_yclk; /*m_hz*/
+	struct bw_fixed low_sclk; /*m_hz*/
+	struct bw_fixed mid1_sclk; /*m_hz*/
+	struct bw_fixed mid2_sclk; /*m_hz*/
+	struct bw_fixed mid3_sclk; /*m_hz*/
+	struct bw_fixed mid4_sclk; /*m_hz*/
+	struct bw_fixed mid5_sclk; /*m_hz*/
+	struct bw_fixed mid6_sclk; /*m_hz*/
+	struct bw_fixed high_sclk; /*m_hz*/
+	struct bw_fixed low_voltage_max_dispclk; /*m_hz*/
+	struct bw_fixed mid_voltage_max_dispclk; /*m_hz*/
+	struct bw_fixed high_voltage_max_dispclk; /*m_hz*/
+	struct bw_fixed low_voltage_max_phyclk;
+	struct bw_fixed mid_voltage_max_phyclk;
+	struct bw_fixed high_voltage_max_phyclk;
+	struct bw_fixed data_return_bus_width;
+	struct bw_fixed trc;
+	struct bw_fixed dmifmc_urgent_latency;
+	struct bw_fixed stutter_self_refresh_exit_latency;
+	struct bw_fixed stutter_self_refresh_entry_latency;
+	struct bw_fixed nbp_state_change_latency;
+	struct bw_fixed mcifwrmc_urgent_latency;
+	bool scatter_gather_enable;
+	struct bw_fixed down_spread_percentage;
+	uint32_t cursor_width;
+	uint32_t average_compression_rate;
+	uint32_t number_of_request_slots_gmc_reserves_for_dmif_per_channel;
+	struct bw_fixed blackout_duration;
+	struct bw_fixed maximum_blackout_recovery_time;
+};
+
+/*******************************************************************************
+ * Temporary data structure(s).
+ ******************************************************************************/
+#define maximum_number_of_surfaces 12
+/*Units : MHz, us */
+
+struct bw_calcs_data {
+	/* data for all displays */
+	uint32_t number_of_displays;
+	enum bw_defines underlay_surface_type;
+	enum bw_defines panning_and_bezel_adjustment;
+	enum bw_defines graphics_tiling_mode;
+	uint32_t graphics_lb_bpc;
+	uint32_t underlay_lb_bpc;
+	enum bw_defines underlay_tiling_mode;
+	enum bw_defines d0_underlay_mode;
+	bool d1_display_write_back_dwb_enable;
+	enum bw_defines d1_underlay_mode;
+
+	bool cpup_state_change_enable;
+	bool cpuc_state_change_enable;
+	bool nbp_state_change_enable;
+	bool stutter_mode_enable;
+	uint32_t y_clk_level;
+	uint32_t sclk_level;
+	uint32_t number_of_underlay_surfaces;
+	uint32_t number_of_dram_wrchannels;
+	uint32_t chunk_request_delay;
+	uint32_t number_of_dram_channels;
+	enum bw_defines underlay_micro_tile_mode;
+	enum bw_defines graphics_micro_tile_mode;
+	struct bw_fixed max_phyclk;
+	struct bw_fixed dram_efficiency;
+	struct bw_fixed src_width_after_surface_type;
+	struct bw_fixed src_height_after_surface_type;
+	struct bw_fixed hsr_after_surface_type;
+	struct bw_fixed vsr_after_surface_type;
+	struct bw_fixed src_width_after_rotation;
+	struct bw_fixed src_height_after_rotation;
+	struct bw_fixed hsr_after_rotation;
+	struct bw_fixed vsr_after_rotation;
+	struct bw_fixed source_height_pixels;
+	struct bw_fixed hsr_after_stereo;
+	struct bw_fixed vsr_after_stereo;
+	struct bw_fixed source_width_in_lb;
+	struct bw_fixed lb_line_pitch;
+	struct bw_fixed underlay_maximum_source_efficient_for_tiling;
+	struct bw_fixed num_lines_at_frame_start;
+	struct bw_fixed min_dmif_size_in_time;
+	struct bw_fixed min_mcifwr_size_in_time;
+	struct bw_fixed total_requests_for_dmif_size;
+	struct bw_fixed peak_pte_request_to_eviction_ratio_limiting;
+	struct bw_fixed useful_pte_per_pte_request;
+	struct bw_fixed scatter_gather_pte_request_rows;
+	struct bw_fixed scatter_gather_row_height;
+	struct bw_fixed scatter_gather_pte_requests_in_vblank;
+	struct bw_fixed inefficient_linear_pitch_in_bytes;
+	struct bw_fixed cursor_total_data;
+	struct bw_fixed cursor_total_request_groups;
+	struct bw_fixed scatter_gather_total_pte_requests;
+	struct bw_fixed scatter_gather_total_pte_request_groups;
+	struct bw_fixed tile_width_in_pixels;
+	struct bw_fixed dmif_total_number_of_data_request_page_close_open;
+	struct bw_fixed mcifwr_total_number_of_data_request_page_close_open;
+	struct bw_fixed bytes_per_page_close_open;
+	struct bw_fixed mcifwr_total_page_close_open_time;
+	struct bw_fixed total_requests_for_adjusted_dmif_size;
+	struct bw_fixed total_dmifmc_urgent_trips;
+	struct bw_fixed total_dmifmc_urgent_latency;
+	struct bw_fixed total_display_reads_required_data;
+	struct bw_fixed total_display_reads_required_dram_access_data;
+	struct bw_fixed total_display_writes_required_data;
+	struct bw_fixed total_display_writes_required_dram_access_data;
+	struct bw_fixed display_reads_required_data;
+	struct bw_fixed display_reads_required_dram_access_data;
+	struct bw_fixed dmif_total_page_close_open_time;
+	struct bw_fixed min_cursor_memory_interface_buffer_size_in_time;
+	struct bw_fixed min_read_buffer_size_in_time;
+	struct bw_fixed display_reads_time_for_data_transfer;
+	struct bw_fixed display_writes_time_for_data_transfer;
+	struct bw_fixed dmif_required_dram_bandwidth;
+	struct bw_fixed mcifwr_required_dram_bandwidth;
+	struct bw_fixed required_dmifmc_urgent_latency_for_page_close_open;
+	struct bw_fixed required_mcifmcwr_urgent_latency;
+	struct bw_fixed required_dram_bandwidth_gbyte_per_second;
+	struct bw_fixed dram_bandwidth;
+	struct bw_fixed dmif_required_sclk;
+	struct bw_fixed mcifwr_required_sclk;
+	struct bw_fixed required_sclk;
+	struct bw_fixed downspread_factor;
+	struct bw_fixed v_scaler_efficiency;
+	struct bw_fixed scaler_limits_factor;
+	struct bw_fixed display_pipe_pixel_throughput;
+	struct bw_fixed total_dispclk_required_with_ramping;
+	struct bw_fixed total_dispclk_required_without_ramping;
+	struct bw_fixed total_read_request_bandwidth;
+	struct bw_fixed total_write_request_bandwidth;
+	struct bw_fixed dispclk_required_for_total_read_request_bandwidth;
+	struct bw_fixed total_dispclk_required_with_ramping_with_request_bandwidth;
+	struct bw_fixed total_dispclk_required_without_ramping_with_request_bandwidth;
+	struct bw_fixed dispclk;
+	struct bw_fixed blackout_recovery_time;
+	struct bw_fixed min_pixels_per_data_fifo_entry;
+	struct bw_fixed sclk_deep_sleep;
+	struct bw_fixed chunk_request_time;
+	struct bw_fixed cursor_request_time;
+	struct bw_fixed line_source_pixels_transfer_time;
+	struct bw_fixed dmifdram_access_efficiency;
+	struct bw_fixed mcifwrdram_access_efficiency;
+	struct bw_fixed total_average_bandwidth_no_compression;
+	struct bw_fixed total_average_bandwidth;
+	struct bw_fixed total_stutter_cycle_duration;
+	struct bw_fixed stutter_burst_time;
+	struct bw_fixed time_in_self_refresh;
+	struct bw_fixed stutter_efficiency;
+	struct bw_fixed worst_number_of_trips_to_memory;
+	struct bw_fixed immediate_flip_time;
+	struct bw_fixed latency_for_non_dmif_clients;
+	struct bw_fixed latency_for_non_mcifwr_clients;
+	struct bw_fixed dmifmc_urgent_latency_supported_in_high_sclk_and_yclk;
+	struct bw_fixed nbp_state_dram_speed_change_margin;
+	struct bw_fixed display_reads_time_for_data_transfer_and_urgent_latency;
+	struct bw_fixed dram_speed_change_margin;
+	struct bw_fixed min_vblank_dram_speed_change_margin;
+	struct bw_fixed min_stutter_refresh_duration;
+	uint32_t total_stutter_dmif_buffer_size;
+	uint32_t total_bytes_requested;
+	uint32_t min_stutter_dmif_buffer_size;
+	uint32_t num_stutter_bursts;
+	struct bw_fixed v_blank_nbp_state_dram_speed_change_latency_supported;
+	struct bw_fixed nbp_state_dram_speed_change_latency_supported;
+	bool fbc_en[maximum_number_of_surfaces];
+	bool lpt_en[maximum_number_of_surfaces];
+	bool displays_match_flag[maximum_number_of_surfaces];
+	bool use_alpha[maximum_number_of_surfaces];
+	bool orthogonal_rotation[maximum_number_of_surfaces];
+	bool enable[maximum_number_of_surfaces];
+	bool access_one_channel_only[maximum_number_of_surfaces];
+	bool scatter_gather_enable_for_pipe[maximum_number_of_surfaces];
+	bool interlace_mode[maximum_number_of_surfaces];
+	bool display_pstate_change_enable[maximum_number_of_surfaces];
+	bool line_buffer_prefetch[maximum_number_of_surfaces];
+	uint32_t bytes_per_pixel[maximum_number_of_surfaces];
+	uint32_t max_chunks_non_fbc_mode[maximum_number_of_surfaces];
+	uint32_t lb_bpc[maximum_number_of_surfaces];
+	uint32_t output_bpphdmi[maximum_number_of_surfaces];
+	uint32_t output_bppdp4_lane_hbr[maximum_number_of_surfaces];
+	uint32_t output_bppdp4_lane_hbr2[maximum_number_of_surfaces];
+	uint32_t output_bppdp4_lane_hbr3[maximum_number_of_surfaces];
+	enum bw_defines stereo_mode[maximum_number_of_surfaces];
+	struct bw_fixed dmif_buffer_transfer_time[maximum_number_of_surfaces];
+	struct bw_fixed displays_with_same_mode[maximum_number_of_surfaces];
+	struct bw_fixed stutter_dmif_buffer_size[maximum_number_of_surfaces];
+	struct bw_fixed stutter_refresh_duration[maximum_number_of_surfaces];
+	struct bw_fixed stutter_exit_watermark[maximum_number_of_surfaces];
+	struct bw_fixed stutter_entry_watermark[maximum_number_of_surfaces];
+	struct bw_fixed h_total[maximum_number_of_surfaces];
+	struct bw_fixed v_total[maximum_number_of_surfaces];
+	struct bw_fixed pixel_rate[maximum_number_of_surfaces];
+	struct bw_fixed src_width[maximum_number_of_surfaces];
+	struct bw_fixed pitch_in_pixels[maximum_number_of_surfaces];
+	struct bw_fixed pitch_in_pixels_after_surface_type[maximum_number_of_surfaces];
+	struct bw_fixed src_height[maximum_number_of_surfaces];
+	struct bw_fixed scale_ratio[maximum_number_of_surfaces];
+	struct bw_fixed h_taps[maximum_number_of_surfaces];
+	struct bw_fixed v_taps[maximum_number_of_surfaces];
+	struct bw_fixed h_scale_ratio[maximum_number_of_surfaces];
+	struct bw_fixed v_scale_ratio[maximum_number_of_surfaces];
+	struct bw_fixed rotation_angle[maximum_number_of_surfaces];
+	struct bw_fixed compression_rate[maximum_number_of_surfaces];
+	struct bw_fixed hsr[maximum_number_of_surfaces];
+	struct bw_fixed vsr[maximum_number_of_surfaces];
+	struct bw_fixed source_width_rounded_up_to_chunks[maximum_number_of_surfaces];
+	struct bw_fixed source_width_pixels[maximum_number_of_surfaces];
+	struct bw_fixed source_height_rounded_up_to_chunks[maximum_number_of_surfaces];
+	struct bw_fixed display_bandwidth[maximum_number_of_surfaces];
+	struct bw_fixed request_bandwidth[maximum_number_of_surfaces];
+	struct bw_fixed bytes_per_request[maximum_number_of_surfaces];
+	struct bw_fixed useful_bytes_per_request[maximum_number_of_surfaces];
+	struct bw_fixed lines_interleaved_in_mem_access[maximum_number_of_surfaces];
+	struct bw_fixed latency_hiding_lines[maximum_number_of_surfaces];
+	struct bw_fixed lb_partitions[maximum_number_of_surfaces];
+	struct bw_fixed lb_partitions_max[maximum_number_of_surfaces];
+	struct bw_fixed dispclk_required_with_ramping[maximum_number_of_surfaces];
+	struct bw_fixed dispclk_required_without_ramping[maximum_number_of_surfaces];
+	struct bw_fixed data_buffer_size[maximum_number_of_surfaces];
+	struct bw_fixed outstanding_chunk_request_limit[maximum_number_of_surfaces];
+	struct bw_fixed urgent_watermark[maximum_number_of_surfaces];
+	struct bw_fixed nbp_state_change_watermark[maximum_number_of_surfaces];
+	struct bw_fixed v_filter_init[maximum_number_of_surfaces];
+	struct bw_fixed stutter_cycle_duration[maximum_number_of_surfaces];
+	struct bw_fixed average_bandwidth[maximum_number_of_surfaces];
+	struct bw_fixed average_bandwidth_no_compression[maximum_number_of_surfaces];
+	struct bw_fixed scatter_gather_pte_request_limit[maximum_number_of_surfaces];
+	struct bw_fixed lb_size_per_component[maximum_number_of_surfaces];
+	struct bw_fixed memory_chunk_size_in_bytes[maximum_number_of_surfaces];
+	struct bw_fixed pipe_chunk_size_in_bytes[maximum_number_of_surfaces];
+	struct bw_fixed number_of_trips_to_memory_for_getting_apte_row[maximum_number_of_surfaces];
+	struct bw_fixed adjusted_data_buffer_size[maximum_number_of_surfaces];
+	struct bw_fixed adjusted_data_buffer_size_in_memory[maximum_number_of_surfaces];
+	struct bw_fixed pixels_per_data_fifo_entry[maximum_number_of_surfaces];
+	struct bw_fixed scatter_gather_pte_requests_in_row[maximum_number_of_surfaces];
+	struct bw_fixed pte_request_per_chunk[maximum_number_of_surfaces];
+	struct bw_fixed scatter_gather_page_width[maximum_number_of_surfaces];
+	struct bw_fixed scatter_gather_page_height[maximum_number_of_surfaces];
+	struct bw_fixed lb_lines_in_per_line_out_in_beginning_of_frame[maximum_number_of_surfaces];
+	struct bw_fixed lb_lines_in_per_line_out_in_middle_of_frame[maximum_number_of_surfaces];
+	struct bw_fixed cursor_width_pixels[maximum_number_of_surfaces];
+	struct bw_fixed minimum_latency_hiding[maximum_number_of_surfaces];
+	struct bw_fixed maximum_latency_hiding[maximum_number_of_surfaces];
+	struct bw_fixed minimum_latency_hiding_with_cursor[maximum_number_of_surfaces];
+	struct bw_fixed maximum_latency_hiding_with_cursor[maximum_number_of_surfaces];
+	struct bw_fixed src_pixels_for_first_output_pixel[maximum_number_of_surfaces];
+	struct bw_fixed src_pixels_for_last_output_pixel[maximum_number_of_surfaces];
+	struct bw_fixed src_data_for_first_output_pixel[maximum_number_of_surfaces];
+	struct bw_fixed src_data_for_last_output_pixel[maximum_number_of_surfaces];
+	struct bw_fixed active_time[maximum_number_of_surfaces];
+	struct bw_fixed horizontal_blank_and_chunk_granularity_factor[maximum_number_of_surfaces];
+	struct bw_fixed cursor_latency_hiding[maximum_number_of_surfaces];
+	struct bw_fixed v_blank_dram_speed_change_margin[maximum_number_of_surfaces];
+	uint32_t num_displays_with_margin[3][8];
+	struct bw_fixed dmif_burst_time[3][8];
+	struct bw_fixed mcifwr_burst_time[3][8];
+	struct bw_fixed line_source_transfer_time[maximum_number_of_surfaces][3][8];
+	struct bw_fixed dram_speed_change_line_source_transfer_time[maximum_number_of_surfaces][3][8];
+	struct bw_fixed min_dram_speed_change_margin[3][8];
+	struct bw_fixed dispclk_required_for_dram_speed_change[3][8];
+	struct bw_fixed blackout_duration_margin[3][8];
+	struct bw_fixed dispclk_required_for_blackout_duration[3][8];
+	struct bw_fixed dispclk_required_for_blackout_recovery[3][8];
+	struct bw_fixed dmif_required_sclk_for_urgent_latency[6];
+};
+
+/*******************************************************************************
+ * Output data structures.
+ ******************************************************************************/
+struct bw_watermarks {
+	uint32_t a_mark;
+	uint32_t b_mark;
+	uint32_t c_mark;
+	uint32_t d_mark;
+};
+
+struct bw_calcs_output {
+	bool cpuc_state_change_enable;
+	bool cpup_state_change_enable;
+	bool stutter_mode_enable;
+	bool nbp_state_change_enable;
+	bool all_displays_in_sync;
+	struct bw_watermarks urgent_wm_ns[6];
+	struct bw_watermarks stutter_exit_wm_ns[6];
+	struct bw_watermarks nbp_state_change_wm_ns[6];
+	uint32_t required_sclk;
+	uint32_t required_sclk_deep_sleep;
+	uint32_t required_yclk;
+	uint32_t dispclk_khz;
+	int blackout_recovery_time_us;
+};
+
+/**
+ * Initialize structures with data which will NOT change at runtime.
+ */
+void bw_calcs_init(
+	struct bw_calcs_dceip *bw_dceip,
+	struct bw_calcs_vbios *bw_vbios,
+	enum bw_calcs_version version);
+
+/**
+ * Return:
+ *	true -	Display(s) configuration supported.
+ *		In this case 'calcs_output' contains data for HW programming
+ *	false - Display(s) configuration not supported (not enough bandwidth).
+ */
+bool bw_calcs(
+	struct dc_context *ctx,
+	const struct bw_calcs_dceip *dceip,
+	const struct bw_calcs_vbios *vbios,
+	const struct pipe_ctx *pipe,
+	int pipe_count,
+	struct bw_calcs_output *calcs_output);
+
+#endif /* __BANDWIDTH_CALCS_H__ */
+
diff --git a/drivers/gpu/drm/amd/display/dc/inc/gamma_calcs.h b/drivers/gpu/drm/amd/display/dc/inc/gamma_calcs.h
new file mode 100644
index 0000000..e2c63fd
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/inc/gamma_calcs.h
@@ -0,0 +1,19 @@
+/*
+ * gamma_calcs.h
+ *
+ *  Created on: Feb 9, 2016
+ *      Author: yonsun
+ */
+
+#ifndef DRIVERS_GPU_DRM_AMD_DC_DEV_DC_INC_GAMMA_CALCS_H_
+#define DRIVERS_GPU_DRM_AMD_DC_DEV_DC_INC_GAMMA_CALCS_H_
+
+#include "opp.h"
+#include "core_types.h"
+#include "dc.h"
+
+bool calculate_regamma_params(struct pwl_params *params,
+		const struct core_gamma *ramp,
+		const struct core_surface *surface);
+
+#endif /* DRIVERS_GPU_DRM_AMD_DC_DEV_DC_INC_GAMMA_CALCS_H_ */
diff --git a/drivers/gpu/drm/amd/display/dc/inc/gamma_types.h b/drivers/gpu/drm/amd/display/dc/inc/gamma_types.h
new file mode 100644
index 0000000..7948d2c
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/inc/gamma_types.h
@@ -0,0 +1,38 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+#ifndef GAMMA_TYPES_H_
+
+#define GAMMA_TYPES_H_
+
+#include "dc_types.h"
+
+/* TODO: Used in IPP and OPP */
+
+struct dev_c_lut16 {
+	uint16_t red;
+	uint16_t green;
+	uint16_t blue;
+};
+#endif
diff --git a/drivers/gpu/drm/amd/display/include/asic_capability_interface.h b/drivers/gpu/drm/amd/display/include/asic_capability_interface.h
new file mode 100644
index 0000000..57cc72f
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/include/asic_capability_interface.h
@@ -0,0 +1,55 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of enc software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and enc permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DAL_ASIC_CAPABILITY_INTERFACE_H__
+#define __DAL_ASIC_CAPABILITY_INTERFACE_H__
+
+/* Include */
+#include "include/asic_capability_types.h"
+
+/* Forward declaration */
+struct hw_asic_id;
+
+/* ASIC capability */
+struct asic_capability {
+	struct dc_context *ctx;
+	struct asic_caps caps;
+	struct asic_stereo_3d_caps stereo_3d_caps;
+	struct asic_bugs bugs;
+	uint32_t data[ASIC_DATA_MAX_NUMBER];
+};
+
+/**
+ * Interfaces
+ */
+
+/* Create and initialize ASIC capability */
+struct asic_capability *dal_asic_capability_create(struct hw_asic_id *init,
+		struct dc_context *ctx);
+
+/* Destroy ASIC capability and free memory space */
+void dal_asic_capability_destroy(struct asic_capability **cap);
+
+#endif /* __DAL_ASIC_CAPABILITY_INTERFACE_H__ */
diff --git a/drivers/gpu/drm/amd/display/include/asic_capability_types.h b/drivers/gpu/drm/amd/display/include/asic_capability_types.h
new file mode 100644
index 0000000..c44dae0
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/include/asic_capability_types.h
@@ -0,0 +1,116 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+#ifndef __DAL_ASIC_CAPABILITY_TYPES_H__
+#define __DAL_ASIC_CAPABILITY_TYPES_H__
+
+/*
+ * ASIC Capabilities
+ */
+struct asic_caps {
+	bool CONSUMER_SINGLE_SELECTED_TIMING:1;
+	bool UNDERSCAN_ADJUST:1;
+	bool DELTA_SIGMA_SUPPORT:1;
+	bool PANEL_SELF_REFRESH_SUPPORTED:1;
+	bool IS_FUSION:1;
+	bool DP_MST_SUPPORTED:1;
+	bool UNDERSCAN_FOR_HDMI_ONLY:1;
+	bool DVI_CLOCK_SHARE_CAPABILITY:1;
+	bool SUPPORT_CEA861E_FINAL:1;
+	bool MIRABILIS_SUPPORTED:1;
+	bool MIRABILIS_ENABLED_BY_DEFAULT:1;
+	bool DEVICE_TAG_REMAP_SUPPORTED:1;
+	bool HEADLESS_NO_OPM_SUPPORTED:1;
+	bool WIRELESS_LIMIT_TO_720P:1;
+	bool WIRELESS_FULL_TIMING_ADJUSTMENT:1;
+	bool WIRELESS_TIMING_ADJUSTMENT:1;
+	bool WIRELESS_COMPRESSED_AUDIO:1;
+	bool VCE_SUPPORTED:1;
+	bool HPD_CHECK_FOR_EDID:1;
+	bool NEED_MC_TUNING:1;
+	bool SKIP_PSR_WAIT_FOR_PLL_LOCK_BIT:1;
+	bool DFSBYPASS_DYNAMIC_SUPPORT:1;
+	bool SUPPORT_8BPP:1;
+};
+
+/*
+ * ASIC Stereo 3D Caps
+ */
+struct asic_stereo_3d_caps {
+	bool SUPPORTED:1;
+	bool DISPLAY_BASED_ON_WS:1;
+	bool HDMI_FRAME_PACK:1;
+	bool INTERLACE_FRAME_PACK:1;
+	bool DISPLAYPORT_FRAME_PACK:1;
+	bool DISPLAYPORT_FRAME_ALT:1;
+	bool INTERLEAVE:1;
+};
+
+/*
+ * ASIC Bugs
+ */
+struct asic_bugs {
+	bool MST_SYMBOL_MISALIGNMENT:1;
+	bool PSR_2X_LANE_GANGING:1;
+	bool LB_WA_IS_SUPPORTED:1;
+	bool ROM_REGISTER_ACCESS:1;
+	bool PSR_WA_OVERSCAN_CRC_ERROR:1;
+};
+
+/*
+ * ASIC Data
+ */
+enum asic_data {
+	ASIC_DATA_FIRST = 0,
+	ASIC_DATA_DCE_VERSION = ASIC_DATA_FIRST,
+	ASIC_DATA_DCE_VERSION_MINOR,
+	ASIC_DATA_LINEBUFFER_SIZE,
+	ASIC_DATA_DRAM_BANDWIDTH_EFFICIENCY,
+	ASIC_DATA_MC_LATENCY,
+	ASIC_DATA_MC_LATENCY_SLOW,
+	ASIC_DATA_MEMORYTYPE_MULTIPLIER,
+	ASIC_DATA_PATH_NUM_PER_DPMST_CONNECTOR,
+	ASIC_DATA_MAX_UNDERSCAN_PERCENTAGE,
+	ASIC_DATA_VIEWPORT_PIXEL_GRANULARITY,
+	ASIC_DATA_MIN_DISPCLK_FOR_UNDERSCAN,
+	ASIC_DATA_DOWNSCALE_LIMIT,
+	ASIC_DATA_MAX_NUMBER /* end of enum */
+};
+
+/*
+ * ASIC Feature Flags
+ */
+struct asic_feature_flags {
+	union {
+		uint32_t raw;
+		struct {
+			uint32_t LEGACY_CLIENT:1;
+			uint32_t PACKED_PIXEL_FORMAT:1;
+			uint32_t WORKSTATION_STEREO:1;
+			uint32_t WORKSTATION:1;
+		} bits;
+	};
+};
+
+#endif /* __DAL_ASIC_CAPABILITY_TYPES_H__ */
diff --git a/drivers/gpu/drm/amd/display/include/dal_register_logger.h b/drivers/gpu/drm/amd/display/include/dal_register_logger.h
new file mode 100644
index 0000000..00dfcd7
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/include/dal_register_logger.h
@@ -0,0 +1,42 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DAL_REGISTER_LOGGER__
+#define __DAL_REGISTER_LOGGER__
+
+/****************
+ * API functions
+ ***************/
+
+/* dal_reg_logger_push - begin Register Logging */
+void dal_reg_logger_push(const char *caller_func);
+/* dal_reg_logger_pop - stop Register Logging */
+void dal_reg_logger_pop(void);
+
+/* for internal use of the Logger only */
+void dal_reg_logger_rw_count_increment(void);
+bool dal_reg_logger_should_dump_register(void);
+
+#endif /* __DAL_REGISTER_LOGGER__ */
diff --git a/drivers/gpu/drm/amd/display/include/display_clock_interface.h b/drivers/gpu/drm/amd/display/include/display_clock_interface.h
new file mode 100644
index 0000000..2006fa2
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/include/display_clock_interface.h
@@ -0,0 +1,175 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DISPLAY_CLOCK_INTERFACE_H__
+#define __DISPLAY_CLOCK_INTERFACE_H__
+
+#include "hw_sequencer_types.h"
+#include "grph_object_defs.h"
+#include "signal_types.h"
+
+/* Timing related information*/
+struct dc_timing_params {
+	uint32_t INTERLACED:1;
+	uint32_t HCOUNT_BY_TWO:1;
+	uint32_t PIXEL_REPETITION:4; /*< values 1 to 10 supported*/
+	uint32_t PREFETCH:1;
+
+	uint32_t h_total;
+	uint32_t h_addressable;
+	uint32_t h_sync_width;
+};
+
+/* Scaling related information*/
+struct dc_scaling_params {
+	uint32_t h_overscan_right;
+	uint32_t h_overscan_left;
+	uint32_t h_taps;
+	uint32_t v_taps;
+};
+
+/* VScalerEfficiency */
+enum v_scaler_efficiency {
+	V_SCALER_EFFICIENCY_LB36BPP = 0,
+	V_SCALER_EFFICIENCY_LB30BPP = 1,
+	V_SCALER_EFFICIENCY_LB24BPP = 2,
+	V_SCALER_EFFICIENCY_LB18BPP = 3
+};
+
+/* Parameters required for minimum Engine
+ * and minimum Display clock calculations*/
+struct min_clock_params {
+	uint32_t id;
+	uint32_t requested_pixel_clock; /* in KHz */
+	uint32_t actual_pixel_clock; /* in KHz */
+	struct view source_view;
+	struct view dest_view;
+	struct dc_timing_params timing_info;
+	struct dc_scaling_params scaling_info;
+	enum signal_type signal_type;
+	enum dc_color_depth deep_color_depth;
+	enum v_scaler_efficiency scaler_efficiency;
+	bool line_buffer_prefetch_enabled;
+};
+
+/* Result of Minimum System and Display clock calculations.
+ * Minimum System clock and Display clock, source and path to be used
+ * for Display clock*/
+struct minimum_clocks_calculation_result {
+	uint32_t min_sclk_khz;
+	uint32_t min_dclk_khz;
+	uint32_t min_mclk_khz;
+	uint32_t min_deep_sleep_sclk;
+};
+
+/* Enumeration of all clocks states */
+enum clocks_state {
+	CLOCKS_STATE_INVALID = 0,
+	CLOCKS_STATE_ULTRA_LOW,
+	CLOCKS_STATE_LOW,
+	CLOCKS_STATE_NOMINAL,
+	CLOCKS_STATE_PERFORMANCE,
+	/* Starting from DCE11, Max 8 level DPM state supported */
+	CLOCKS_DPM_STATE_LEVEL_INVALID = CLOCKS_STATE_INVALID,
+	CLOCKS_DPM_STATE_LEVEL_0 = CLOCKS_STATE_ULTRA_LOW,
+	CLOCKS_DPM_STATE_LEVEL_1 = CLOCKS_STATE_LOW,
+	CLOCKS_DPM_STATE_LEVEL_2 = CLOCKS_STATE_NOMINAL,
+	CLOCKS_DPM_STATE_LEVEL_3 = CLOCKS_STATE_PERFORMANCE,
+	CLOCKS_DPM_STATE_LEVEL_4 = CLOCKS_DPM_STATE_LEVEL_3 + 1,
+	CLOCKS_DPM_STATE_LEVEL_5 = CLOCKS_DPM_STATE_LEVEL_4 + 1,
+	CLOCKS_DPM_STATE_LEVEL_6 = CLOCKS_DPM_STATE_LEVEL_5 + 1,
+	CLOCKS_DPM_STATE_LEVEL_7 = CLOCKS_DPM_STATE_LEVEL_6 + 1,
+};
+
+/* Structure containing all state-dependent clocks
+ * (dependent on "enum clocks_state") */
+struct state_dependent_clocks {
+	uint32_t display_clk_khz;
+	uint32_t pixel_clk_khz;
+};
+
+struct display_clock_state {
+	uint32_t DFS_BYPASS_ACTIVE:1;
+};
+
+struct display_clock;
+
+struct display_clock *dal_display_clock_dce112_create(
+	struct dc_context *ctx);
+
+struct display_clock *dal_display_clock_dce110_create(
+	struct dc_context *ctx);
+
+struct display_clock *dal_display_clock_dce80_create(
+	struct dc_context *ctx);
+
+void dal_display_clock_destroy(struct display_clock **to_destroy);
+bool dal_display_clock_validate(
+	struct display_clock *disp_clk,
+	struct min_clock_params *params);
+uint32_t dal_display_clock_calculate_min_clock(
+	struct display_clock *disp_clk,
+	uint32_t path_num,
+	struct min_clock_params *params);
+uint32_t dal_display_clock_get_validation_clock(struct display_clock *disp_clk);
+void dal_display_clock_set_clock(
+	struct display_clock *disp_clk,
+	uint32_t requested_clock_khz);
+uint32_t dal_display_clock_get_clock(struct display_clock *disp_clk);
+bool dal_display_clock_get_min_clocks_state(
+	struct display_clock *disp_clk,
+	enum clocks_state *clocks_state);
+bool dal_display_clock_get_required_clocks_state(
+	struct display_clock *disp_clk,
+	struct state_dependent_clocks *req_clocks,
+	enum clocks_state *clocks_state);
+bool dal_display_clock_set_min_clocks_state(
+	struct display_clock *disp_clk,
+	enum clocks_state clocks_state);
+uint32_t dal_display_clock_get_dp_ref_clk_frequency(
+	struct display_clock *disp_clk);
+/*the second parameter of "switchreferenceclock" is
+ * a dummy argument for all pre dce 6.0 versions*/
+void dal_display_clock_switch_reference_clock(
+	struct display_clock *disp_clk,
+	bool use_external_ref_clk,
+	uint32_t requested_clock_khz);
+void dal_display_clock_set_dp_ref_clock_source(
+	struct display_clock *disp_clk,
+	enum clock_source_id clk_src);
+void dal_display_clock_store_max_clocks_state(
+	struct display_clock *disp_clk,
+	enum clocks_state max_clocks_state);
+void dal_display_clock_set_clock_state(
+	struct display_clock *disp_clk,
+	struct display_clock_state clk_state);
+struct display_clock_state dal_display_clock_get_clock_state(
+	struct display_clock *disp_clk);
+uint32_t dal_display_clock_get_dfs_bypass_threshold(
+	struct display_clock *disp_clk);
+void dal_display_clock_invalid_clock_state(
+	struct display_clock *disp_clk);
+
+#endif /* __DISPLAY_CLOCK_INTERFACE_H__ */
diff --git a/drivers/gpu/drm/amd/display/include/irq_interface.h b/drivers/gpu/drm/amd/display/include/irq_interface.h
new file mode 100644
index 0000000..077ded3
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/include/irq_interface.h
@@ -0,0 +1,31 @@
+/*
+ * Copyright 2012-15 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef __DAL_IRQ_INTERFACE_H__
+#define __DAL_IRQ_INTERFACE_H__
+
+#include "gpio_types.h"
+
+#endif
diff --git a/drivers/gpu/drm/amd/display/modules/color/color.c b/drivers/gpu/drm/amd/display/modules/color/color.c
new file mode 100644
index 0000000..cf030b1
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/modules/color/color.c
@@ -0,0 +1,2094 @@
+/*
+ * Copyright 2016 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "dc.h"
+#include "mod_color.h"
+#include "core_types.h"
+#include "fixed31_32.h"
+#include "core_dc.h"
+
+#define MOD_COLOR_MAX_CONCURRENT_SINKS 32
+#define DIVIDER 10000
+/* S2D13 value in [-3.00...0.9999] */
+#define S2D13_MIN (-3 * DIVIDER)
+#define S2D13_MAX (3 * DIVIDER)
+#define S0D13_MIN (-1 * DIVIDER)
+#define S0D13_MAX (1 * DIVIDER)
+
+struct sink_caps {
+	const struct dc_sink *sink;
+};
+
+struct gamut_calculation_matrix {
+	struct fixed31_32 MTransposed[9];
+	struct fixed31_32 XYZtoRGB_Custom[9];
+	struct fixed31_32 XYZtoRGB_Ref[9];
+	struct fixed31_32 RGBtoXYZ_Final[9];
+
+	struct fixed31_32 MResult[9];
+	struct fixed31_32 fXYZofWhiteRef[9];
+	struct fixed31_32 fXYZofRGBRef[9];
+};
+
+struct gamut_src_dst_matrix {
+	struct fixed31_32 rgbCoeffDst[9];
+	struct fixed31_32 whiteCoeffDst[3];
+	struct fixed31_32 rgbCoeffSrc[9];
+	struct fixed31_32 whiteCoeffSrc[3];
+};
+
+struct color_state {
+	bool user_enable_color_temperature;
+	int custom_color_temperature;
+	struct color_space_coordinates source_gamut;
+	struct color_space_coordinates destination_gamut;
+	struct color_range contrast;
+	struct color_range saturation;
+	struct color_range brightness;
+	struct color_range hue;
+	enum dc_quantization_range preferred_quantization_range;
+};
+
+struct core_color {
+	struct mod_color public;
+	struct dc *dc;
+	int num_sinks;
+	struct sink_caps *caps;
+	struct color_state *state;
+};
+
+#define MOD_COLOR_TO_CORE(mod_color)\
+		container_of(mod_color, struct core_color, public)
+
+#define COLOR_REGISTRY_NAME "color_v1"
+
+/*Matrix Calculation Functions*/
+/**
+ *****************************************************************************
+ *  Function: transposeMatrix
+ *
+ *  @brief
+ *    rotate the matrix 90 degrees clockwise
+ *    rows become a columns and columns to rows
+ *  @param [ in ] M            - source matrix
+ *  @param [ in ] Rows         - num of Rows of the original matrix
+ *  @param [ in ] Cols         - num of Cols of the original matrix
+ *  @param [ out] MTransposed  - result matrix
+ *  @return  void
+ *
+ *****************************************************************************
+ */
+static void transpose_matrix(const struct fixed31_32 *M, unsigned int Rows,
+		unsigned int Cols,  struct fixed31_32 *MTransposed)
+{
+	unsigned int i, j;
+
+	for (i = 0; i < Rows; i++) {
+		for (j = 0; j < Cols; j++)
+			MTransposed[(j*Rows)+i] = M[(i*Cols)+j];
+	}
+}
+
+/**
+ *****************************************************************************
+ *  Function: multiplyMatrices
+ *
+ *  @brief
+ *    multiplies produce of two matrices: M =  M1[ulRows1 x ulCols1] *
+ *    M2[ulCols1 x ulCols2].
+ *
+ *  @param [ in ] M1      - first Matrix.
+ *  @param [ in ] M2      - second Matrix.
+ *  @param [ in ] Rows1   - num of Rows of the first Matrix
+ *  @param [ in ] Cols1   - num of Cols of the first Matrix/Num of Rows
+ *  of the second Matrix
+ *  @param [ in ] Cols2   - num of Cols of the second Matrix
+ *  @param [out ] mResult - resulting matrix.
+ *  @return  void
+ *
+ *****************************************************************************
+ */
+static void multiply_matrices(struct fixed31_32 *mResult,
+		const struct fixed31_32 *M1,
+		const struct fixed31_32 *M2, unsigned int Rows1,
+		unsigned int Cols1, unsigned int Cols2)
+{
+	unsigned int i, j, k;
+
+	for (i = 0; i < Rows1; i++) {
+		for (j = 0; j < Cols2; j++) {
+			mResult[(i * Cols2) + j] = dal_fixed31_32_zero;
+			for (k = 0; k < Cols1; k++)
+				mResult[(i * Cols2) + j] =
+					dal_fixed31_32_add
+					(mResult[(i * Cols2) + j],
+					dal_fixed31_32_mul(M1[(i * Cols1) + k],
+					M2[(k * Cols2) + j]));
+		}
+	}
+}
+
+/**
+ *****************************************************************************
+ *  Function: cFind3X3Det
+ *
+ *  @brief
+ *    finds determinant of given 3x3 matrix
+ *
+ *  @param [ in  ] m     - matrix
+ *  @return determinate whioch could not be zero
+ *
+ *****************************************************************************
+ */
+static struct fixed31_32 find_3X3_det(const struct fixed31_32 *m)
+{
+	struct fixed31_32 det, A1, A2, A3;
+
+	A1 = dal_fixed31_32_mul(m[0],
+			dal_fixed31_32_sub(dal_fixed31_32_mul(m[4], m[8]),
+					dal_fixed31_32_mul(m[5], m[7])));
+	A2 = dal_fixed31_32_mul(m[1],
+			dal_fixed31_32_sub(dal_fixed31_32_mul(m[3], m[8]),
+					dal_fixed31_32_mul(m[5], m[6])));
+	A3 = dal_fixed31_32_mul(m[2],
+			dal_fixed31_32_sub(dal_fixed31_32_mul(m[3], m[7]),
+					dal_fixed31_32_mul(m[4], m[6])));
+	det = dal_fixed31_32_add(dal_fixed31_32_sub(A1, A2), A3);
+	return det;
+}
+
+
+/**
+ *****************************************************************************
+ *  Function: computeInverseMatrix_3x3
+ *
+ *  @brief
+ *    builds inverse matrix
+ *
+ *  @param [ in   ] m     - matrix
+ *  @param [ out  ] im    - result matrix
+ *  @return true if success
+ *
+ *****************************************************************************
+ */
+static bool compute_inverse_matrix_3x3(const struct fixed31_32 *m,
+		struct fixed31_32 *im)
+{
+	struct fixed31_32 determinant = find_3X3_det(m);
+
+	if (dal_fixed31_32_eq(determinant, dal_fixed31_32_zero) == false) {
+		im[0] = dal_fixed31_32_div(dal_fixed31_32_sub
+				(dal_fixed31_32_mul(m[4], m[8]),
+				dal_fixed31_32_mul(m[5], m[7])), determinant);
+		im[1] = dal_fixed31_32_neg(dal_fixed31_32_div(dal_fixed31_32_sub
+				(dal_fixed31_32_mul(m[1], m[8]),
+				dal_fixed31_32_mul(m[2], m[7])), determinant));
+		im[2] = dal_fixed31_32_div(dal_fixed31_32_sub
+				(dal_fixed31_32_mul(m[1], m[5]),
+				dal_fixed31_32_mul(m[2], m[4])), determinant);
+		im[3] = dal_fixed31_32_neg(dal_fixed31_32_div(dal_fixed31_32_sub
+				(dal_fixed31_32_mul(m[3], m[8]),
+				dal_fixed31_32_mul(m[5], m[6])), determinant));
+		im[4] = dal_fixed31_32_div(dal_fixed31_32_sub
+				(dal_fixed31_32_mul(m[0], m[8]),
+				dal_fixed31_32_mul(m[2], m[6])), determinant);
+		im[5] = dal_fixed31_32_neg(dal_fixed31_32_div(dal_fixed31_32_sub
+				(dal_fixed31_32_mul(m[0], m[5]),
+				dal_fixed31_32_mul(m[2], m[3])), determinant));
+		im[6] = dal_fixed31_32_div(dal_fixed31_32_sub
+				(dal_fixed31_32_mul(m[3], m[7]),
+				dal_fixed31_32_mul(m[4], m[6])), determinant);
+		im[7] = dal_fixed31_32_neg(dal_fixed31_32_div(dal_fixed31_32_sub
+				(dal_fixed31_32_mul(m[0], m[7]),
+				dal_fixed31_32_mul(m[1], m[6])), determinant));
+		im[8] = dal_fixed31_32_div(dal_fixed31_32_sub
+				(dal_fixed31_32_mul(m[0], m[4]),
+				dal_fixed31_32_mul(m[1], m[3])), determinant);
+		return true;
+	}
+	return false;
+}
+
+/**
+ *****************************************************************************
+ *  Function: calculateXYZtoRGB_M3x3
+ *
+ *  @brief
+ *    Calculates transformation matrix from XYZ coordinates to RBG
+ *
+ *  @param [ in  ] XYZofRGB     - primaries XYZ
+ *  @param [ in  ] XYZofWhite   - white point.
+ *  @param [ out ] XYZtoRGB     - RGB primires
+ *  @return  true if success
+ *
+ *****************************************************************************
+ */
+static bool calculate_XYZ_to_RGB_3x3(const struct fixed31_32 *XYZofRGB,
+		const struct fixed31_32 *XYZofWhite,
+		struct fixed31_32 *XYZtoRGB)
+{
+
+	struct fixed31_32 MInversed[9];
+	struct fixed31_32 SVector[3];
+
+	/*1. Find Inverse matrix 3x3 of MTransposed*/
+	if (!compute_inverse_matrix_3x3(XYZofRGB, MInversed))
+	return false;
+
+	/*2. Calculate vector: |Sr Sg Sb| = [MInversed] * |Wx Wy Wz|*/
+	multiply_matrices(SVector, MInversed, XYZofWhite, 3, 3, 1);
+
+	/*3. Calculate matrix XYZtoRGB 3x3*/
+	XYZtoRGB[0] = dal_fixed31_32_mul(XYZofRGB[0], SVector[0]);
+	XYZtoRGB[1] = dal_fixed31_32_mul(XYZofRGB[1], SVector[1]);
+	XYZtoRGB[2] = dal_fixed31_32_mul(XYZofRGB[2], SVector[2]);
+
+	XYZtoRGB[3] = dal_fixed31_32_mul(XYZofRGB[3], SVector[0]);
+	XYZtoRGB[4] = dal_fixed31_32_mul(XYZofRGB[4], SVector[1]);
+	XYZtoRGB[5] = dal_fixed31_32_mul(XYZofRGB[5], SVector[2]);
+
+	XYZtoRGB[6] = dal_fixed31_32_mul(XYZofRGB[6], SVector[0]);
+	XYZtoRGB[7] = dal_fixed31_32_mul(XYZofRGB[7], SVector[1]);
+	XYZtoRGB[8] = dal_fixed31_32_mul(XYZofRGB[8], SVector[2]);
+
+	return true;
+}
+
+static bool gamut_to_color_matrix(
+	const struct fixed31_32 *pXYZofRGB,/*destination gamut*/
+	const struct fixed31_32 *pXYZofWhite,/*destination of white point*/
+	const struct fixed31_32 *pRefXYZofRGB,/*source gamut*/
+	const struct fixed31_32 *pRefXYZofWhite,/*source of white point*/
+	bool invert,
+	struct fixed31_32 *tempMatrix3X3)
+{
+	int i = 0;
+	struct gamut_calculation_matrix *matrix =
+			dm_alloc(sizeof(struct gamut_calculation_matrix));
+
+	struct fixed31_32 *pXYZtoRGB_Temp;
+	struct fixed31_32 *pXYZtoRGB_Final;
+
+	matrix->fXYZofWhiteRef[0] = pRefXYZofWhite[0];
+	matrix->fXYZofWhiteRef[1] = pRefXYZofWhite[1];
+	matrix->fXYZofWhiteRef[2] = pRefXYZofWhite[2];
+
+
+	matrix->fXYZofRGBRef[0] = pRefXYZofRGB[0];
+	matrix->fXYZofRGBRef[1] = pRefXYZofRGB[1];
+	matrix->fXYZofRGBRef[2] = pRefXYZofRGB[2];
+
+	matrix->fXYZofRGBRef[3] = pRefXYZofRGB[3];
+	matrix->fXYZofRGBRef[4] = pRefXYZofRGB[4];
+	matrix->fXYZofRGBRef[5] = pRefXYZofRGB[5];
+
+	matrix->fXYZofRGBRef[6] = pRefXYZofRGB[6];
+	matrix->fXYZofRGBRef[7] = pRefXYZofRGB[7];
+	matrix->fXYZofRGBRef[8] = pRefXYZofRGB[8];
+
+	/*default values -  unity matrix*/
+	while (i < 9) {
+		if (i == 0 || i == 4 || i == 8)
+			tempMatrix3X3[i] = dal_fixed31_32_one;
+		else
+			tempMatrix3X3[i] = dal_fixed31_32_zero;
+		i++;
+	}
+
+	/*1. Decide about the order of calculation.
+	 * bInvert == FALSE --> RGBtoXYZ_Ref * XYZtoRGB_Custom
+	 * bInvert == TRUE  --> RGBtoXYZ_Custom * XYZtoRGB_Ref */
+	if (invert) {
+		pXYZtoRGB_Temp = matrix->XYZtoRGB_Custom;
+		pXYZtoRGB_Final = matrix->XYZtoRGB_Ref;
+	} else {
+		pXYZtoRGB_Temp = matrix->XYZtoRGB_Ref;
+		pXYZtoRGB_Final = matrix->XYZtoRGB_Custom;
+	}
+
+	/*2. Calculate XYZtoRGB_Ref*/
+	transpose_matrix(matrix->fXYZofRGBRef, 3, 3, matrix->MTransposed);
+
+	if (!calculate_XYZ_to_RGB_3x3(
+		matrix->MTransposed,
+		matrix->fXYZofWhiteRef,
+		matrix->XYZtoRGB_Ref))
+		goto function_fail;
+
+	/*3. Calculate XYZtoRGB_Custom*/
+	transpose_matrix(pXYZofRGB, 3, 3, matrix->MTransposed);
+
+	if (!calculate_XYZ_to_RGB_3x3(
+		matrix->MTransposed,
+		pXYZofWhite,
+		matrix->XYZtoRGB_Custom))
+		goto function_fail;
+
+	/*4. Calculate RGBtoXYZ -
+	 * inverse matrix 3x3 of XYZtoRGB_Ref or XYZtoRGB_Custom*/
+	if (!compute_inverse_matrix_3x3(pXYZtoRGB_Temp, matrix->RGBtoXYZ_Final))
+		goto function_fail;
+
+	/*5. Calculate M(3x3) = RGBtoXYZ * XYZtoRGB*/
+	multiply_matrices(matrix->MResult, matrix->RGBtoXYZ_Final,
+			pXYZtoRGB_Final, 3, 3, 3);
+
+	for (i = 0; i < 9; i++)
+		tempMatrix3X3[i] = matrix->MResult[i];
+
+	dm_free(matrix);
+
+	return true;
+
+function_fail:
+	dm_free(matrix);
+	return false;
+}
+
+static bool build_gamut_remap_matrix
+		(struct color_space_coordinates gamut_description,
+		struct fixed31_32 *rgb_matrix,
+		struct fixed31_32 *white_point_matrix)
+{
+	struct fixed31_32 fixed_blueX = dal_fixed31_32_from_fraction
+			(gamut_description.blueX, DIVIDER);
+	struct fixed31_32 fixed_blueY = dal_fixed31_32_from_fraction
+			(gamut_description.blueY, DIVIDER);
+	struct fixed31_32 fixed_greenX = dal_fixed31_32_from_fraction
+			(gamut_description.greenX, DIVIDER);
+	struct fixed31_32 fixed_greenY = dal_fixed31_32_from_fraction
+			(gamut_description.greenY, DIVIDER);
+	struct fixed31_32 fixed_redX = dal_fixed31_32_from_fraction
+			(gamut_description.redX, DIVIDER);
+	struct fixed31_32 fixed_redY = dal_fixed31_32_from_fraction
+			(gamut_description.redY, DIVIDER);
+	struct fixed31_32 fixed_whiteX = dal_fixed31_32_from_fraction
+			(gamut_description.whiteX, DIVIDER);
+	struct fixed31_32 fixed_whiteY = dal_fixed31_32_from_fraction
+			(gamut_description.whiteY, DIVIDER);
+
+	rgb_matrix[0] = dal_fixed31_32_div(fixed_redX, fixed_redY);
+	rgb_matrix[1] = dal_fixed31_32_one;
+	rgb_matrix[2] = dal_fixed31_32_div(dal_fixed31_32_sub
+			(dal_fixed31_32_sub(dal_fixed31_32_one, fixed_redX),
+					fixed_redY), fixed_redY);
+
+	rgb_matrix[3] = dal_fixed31_32_div(fixed_greenX, fixed_greenY);
+	rgb_matrix[4] = dal_fixed31_32_one;
+	rgb_matrix[5] = dal_fixed31_32_div(dal_fixed31_32_sub
+			(dal_fixed31_32_sub(dal_fixed31_32_one, fixed_greenX),
+					fixed_greenY), fixed_greenY);
+
+	rgb_matrix[6] = dal_fixed31_32_div(fixed_blueX, fixed_blueY);
+	rgb_matrix[7] = dal_fixed31_32_one;
+	rgb_matrix[8] = dal_fixed31_32_div(dal_fixed31_32_sub
+			(dal_fixed31_32_sub(dal_fixed31_32_one, fixed_blueX),
+					fixed_blueY), fixed_blueY);
+
+	white_point_matrix[0] = dal_fixed31_32_div(fixed_whiteX, fixed_whiteY);
+	white_point_matrix[1] = dal_fixed31_32_one;
+	white_point_matrix[2] = dal_fixed31_32_div(dal_fixed31_32_sub
+			(dal_fixed31_32_sub(dal_fixed31_32_one, fixed_whiteX),
+					fixed_whiteY), fixed_whiteY);
+
+	return true;
+}
+
+static bool check_dc_support(const struct dc *dc)
+{
+	if (dc->stream_funcs.set_gamut_remap == NULL)
+		return false;
+
+	return true;
+}
+
+static uint16_t fixed_point_to_int_frac(
+	struct fixed31_32 arg,
+	uint8_t integer_bits,
+	uint8_t fractional_bits)
+{
+	int32_t numerator;
+	int32_t divisor = 1 << fractional_bits;
+
+	uint16_t result;
+
+	uint16_t d = (uint16_t)dal_fixed31_32_floor(
+		dal_fixed31_32_abs(
+			arg));
+
+	if (d <= (uint16_t)(1 << integer_bits) - (1 / (uint16_t)divisor))
+		numerator = (uint16_t)dal_fixed31_32_floor(
+			dal_fixed31_32_mul_int(
+				arg,
+				divisor));
+	else {
+		numerator = dal_fixed31_32_floor(
+			dal_fixed31_32_sub(
+				dal_fixed31_32_from_int(
+					1LL << integer_bits),
+				dal_fixed31_32_recip(
+					dal_fixed31_32_from_int(
+						divisor))));
+	}
+
+	if (numerator >= 0)
+		result = (uint16_t)numerator;
+	else
+		result = (uint16_t)(
+		(1 << (integer_bits + fractional_bits + 1)) + numerator);
+
+	if ((result != 0) && dal_fixed31_32_lt(
+		arg, dal_fixed31_32_zero))
+		result |= 1 << (integer_bits + fractional_bits);
+
+	return result;
+}
+
+/**
+* convert_float_matrix
+* This converts a double into HW register spec defined format S2D13.
+* @param :
+* @return None
+*/
+
+static void convert_float_matrix_legacy(
+	uint16_t *matrix,
+	struct fixed31_32 *flt,
+	uint32_t buffer_size)
+{
+	const struct fixed31_32 min_2_13 =
+		dal_fixed31_32_from_fraction(S2D13_MIN, DIVIDER);
+	const struct fixed31_32 max_2_13 =
+		dal_fixed31_32_from_fraction(S2D13_MAX, DIVIDER);
+	uint32_t i;
+
+	for (i = 0; i < buffer_size; ++i) {
+		uint32_t reg_value =
+				fixed_point_to_int_frac(
+					dal_fixed31_32_clamp(
+						flt[i],
+						min_2_13,
+						max_2_13),
+						2,
+						13);
+
+		matrix[i] = (uint16_t)reg_value;
+	}
+}
+
+static void convert_float_matrix(
+	uint16_t *matrix,
+	struct fixed31_32 *flt,
+	uint32_t buffer_size)
+{
+	const struct fixed31_32 min_0_13 =
+		dal_fixed31_32_from_fraction(S0D13_MIN, DIVIDER);
+	const struct fixed31_32 max_0_13 =
+		dal_fixed31_32_from_fraction(S0D13_MAX, DIVIDER);
+	const struct fixed31_32 min_2_13 =
+		dal_fixed31_32_from_fraction(S2D13_MIN, DIVIDER);
+	const struct fixed31_32 max_2_13 =
+		dal_fixed31_32_from_fraction(S2D13_MAX, DIVIDER);
+	uint32_t i;
+	uint16_t temp_matrix[12];
+
+	for (i = 0; i < buffer_size; ++i) {
+		if (i == 3 || i == 7 || i == 11) {
+			uint32_t reg_value =
+					fixed_point_to_int_frac(
+						dal_fixed31_32_clamp(
+							flt[i],
+							min_0_13,
+							max_0_13),
+							2,
+							13);
+
+			temp_matrix[i] = (uint16_t)reg_value;
+		} else {
+			uint32_t reg_value =
+					fixed_point_to_int_frac(
+						dal_fixed31_32_clamp(
+							flt[i],
+							min_2_13,
+							max_2_13),
+							2,
+							13);
+
+			temp_matrix[i] = (uint16_t)reg_value;
+		}
+	}
+
+	matrix[4] = temp_matrix[0];
+	matrix[5] = temp_matrix[1];
+	matrix[6] = temp_matrix[2];
+	matrix[7] = temp_matrix[3];
+
+	matrix[8] = temp_matrix[4];
+	matrix[9] = temp_matrix[5];
+	matrix[10] = temp_matrix[6];
+	matrix[11] = temp_matrix[7];
+
+	matrix[0] = temp_matrix[8];
+	matrix[1] = temp_matrix[9];
+	matrix[2] = temp_matrix[10];
+	matrix[3] = temp_matrix[11];
+}
+
+static int get_hw_value_from_sw_value(int swVal, int swMin,
+		int swMax, int hwMin, int hwMax)
+{
+	int dSW = swMax - swMin; /*software adjustment range size*/
+	int dHW = hwMax - hwMin; /*hardware adjustment range size*/
+	int hwVal; /*HW adjustment value*/
+
+	/* error case, I preserve the behavior from the predecessor
+	 *getHwStepFromSwHwMinMaxValue (removed in Feb 2013)
+	 *which was the FP version that only computed SCLF (i.e. dHW/dSW).
+	 *it would return 0 in this case so
+	 *hwVal = hwMin from the formula given in @brief
+	*/
+	if (dSW == 0)
+		return hwMin;
+
+	/*it's quite often that ranges match,
+	 *e.g. for overlay colors currently (Feb 2013)
+	 *only brightness has a different
+	 *HW range, and in this case no multiplication or division is needed,
+	 *and if minimums match, no calculation at all
+	*/
+	if (dSW != dHW) {
+		hwVal = (swVal - swMin)*dHW/dSW + hwMin;
+	} else {
+		hwVal = swVal;
+		if (swMin != hwMin)
+			hwVal += (hwMin - swMin);
+	}
+
+	return hwVal;
+}
+
+static void initialize_fix_point_color_values(
+	struct core_color *core_color,
+	unsigned int sink_index,
+	struct fixed31_32 *grph_cont,
+	struct fixed31_32 *grph_sat,
+	struct fixed31_32 *grph_bright,
+	struct fixed31_32 *sin_grph_hue,
+	struct fixed31_32 *cos_grph_hue)
+{
+	/* Hue adjustment could be negative. -45 ~ +45 */
+	struct fixed31_32 hue =
+		dal_fixed31_32_mul(
+			dal_fixed31_32_from_fraction
+			(get_hw_value_from_sw_value
+				(core_color->state[sink_index].hue.current,
+				core_color->state[sink_index].hue.min,
+				core_color->state[sink_index].hue.max,
+				-30, 30), 180),
+			dal_fixed31_32_pi);
+
+	*sin_grph_hue = dal_fixed31_32_sin(hue);
+	*cos_grph_hue = dal_fixed31_32_cos(hue);
+
+	*grph_cont =
+		dal_fixed31_32_from_fraction(get_hw_value_from_sw_value
+			(core_color->state[sink_index].contrast.current,
+			core_color->state[sink_index].contrast.min,
+			core_color->state[sink_index].contrast.max,
+			50, 150), 100);
+	*grph_sat =
+		dal_fixed31_32_from_fraction(get_hw_value_from_sw_value
+			(core_color->state[sink_index].saturation.current,
+			core_color->state[sink_index].saturation.min,
+			core_color->state[sink_index].saturation.max,
+			0, 200), 100);
+	*grph_bright =
+		dal_fixed31_32_from_fraction(get_hw_value_from_sw_value
+			(core_color->state[sink_index].brightness.current,
+			core_color->state[sink_index].brightness.min,
+			core_color->state[sink_index].brightness.max,
+			-25, 25), 100);
+}
+
+
+/* Given a specific dc_sink* this function finds its equivalent
+ * on the dc_sink array and returns the corresponding index
+ */
+static unsigned int sink_index_from_sink(struct core_color *core_color,
+		const struct dc_sink *sink)
+{
+	unsigned int index = 0;
+
+	for (index = 0; index < core_color->num_sinks; index++)
+		if (core_color->caps[index].sink == sink)
+			return index;
+
+	/* Could not find sink requested */
+	ASSERT(false);
+	return index;
+}
+
+static void calculate_rgb_matrix_legacy(struct core_color *core_color,
+		unsigned int sink_index,
+		struct fixed31_32 *rgb_matrix)
+{
+	const struct fixed31_32 k1 =
+		dal_fixed31_32_from_fraction(701000, 1000000);
+	const struct fixed31_32 k2 =
+		dal_fixed31_32_from_fraction(236568, 1000000);
+	const struct fixed31_32 k3 =
+		dal_fixed31_32_from_fraction(-587000, 1000000);
+	const struct fixed31_32 k4 =
+		dal_fixed31_32_from_fraction(464432, 1000000);
+	const struct fixed31_32 k5 =
+		dal_fixed31_32_from_fraction(-114000, 1000000);
+	const struct fixed31_32 k6 =
+		dal_fixed31_32_from_fraction(-701000, 1000000);
+	const struct fixed31_32 k7 =
+		dal_fixed31_32_from_fraction(-299000, 1000000);
+	const struct fixed31_32 k8 =
+		dal_fixed31_32_from_fraction(-292569, 1000000);
+	const struct fixed31_32 k9 =
+		dal_fixed31_32_from_fraction(413000, 1000000);
+	const struct fixed31_32 k10 =
+		dal_fixed31_32_from_fraction(-92482, 1000000);
+	const struct fixed31_32 k11 =
+		dal_fixed31_32_from_fraction(-114000, 1000000);
+	const struct fixed31_32 k12 =
+		dal_fixed31_32_from_fraction(385051, 1000000);
+	const struct fixed31_32 k13 =
+		dal_fixed31_32_from_fraction(-299000, 1000000);
+	const struct fixed31_32 k14 =
+		dal_fixed31_32_from_fraction(886000, 1000000);
+	const struct fixed31_32 k15 =
+		dal_fixed31_32_from_fraction(-587000, 1000000);
+	const struct fixed31_32 k16 =
+		dal_fixed31_32_from_fraction(-741914, 1000000);
+	const struct fixed31_32 k17 =
+		dal_fixed31_32_from_fraction(886000, 1000000);
+	const struct fixed31_32 k18 =
+		dal_fixed31_32_from_fraction(-144086, 1000000);
+
+	const struct fixed31_32 luma_r =
+		dal_fixed31_32_from_fraction(299, 1000);
+	const struct fixed31_32 luma_g =
+		dal_fixed31_32_from_fraction(587, 1000);
+	const struct fixed31_32 luma_b =
+		dal_fixed31_32_from_fraction(114, 1000);
+
+	struct fixed31_32 grph_cont;
+	struct fixed31_32 grph_sat;
+	struct fixed31_32 grph_bright;
+	struct fixed31_32 sin_grph_hue;
+	struct fixed31_32 cos_grph_hue;
+
+	initialize_fix_point_color_values(
+		core_color, sink_index, &grph_cont, &grph_sat,
+		&grph_bright, &sin_grph_hue, &cos_grph_hue);
+
+	/* COEF_1_1 = GrphCont * (LumaR + GrphSat * (Cos(GrphHue) * K1 +*/
+	/* Sin(GrphHue) * K2))*/
+	/* (Cos(GrphHue) * K1 + Sin(GrphHue) * K2)*/
+	rgb_matrix[0] =
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(cos_grph_hue, k1),
+			dal_fixed31_32_mul(sin_grph_hue, k2));
+	/* GrphSat * (Cos(GrphHue) * K1 + Sin(GrphHue) * K2 */
+	rgb_matrix[0] = dal_fixed31_32_mul(grph_sat, rgb_matrix[0]);
+	/* (LumaR + GrphSat * (Cos(GrphHue) * K1 + Sin(GrphHue) * K2))*/
+	rgb_matrix[0] = dal_fixed31_32_add(luma_r, rgb_matrix[0]);
+	/* GrphCont * (LumaR + GrphSat * (Cos(GrphHue) * K1 + Sin(GrphHue)**/
+	/* K2))*/
+	rgb_matrix[0] = dal_fixed31_32_mul(grph_cont, rgb_matrix[0]);
+
+	/* COEF_1_2 = GrphCont * (LumaG + GrphSat * (Cos(GrphHue) * K3 +*/
+	/* Sin(GrphHue) * K4))*/
+	/* (Cos(GrphHue) * K3 + Sin(GrphHue) * K4)*/
+	rgb_matrix[1] =
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(cos_grph_hue, k3),
+			dal_fixed31_32_mul(sin_grph_hue, k4));
+	/* GrphSat * (Cos(GrphHue) * K3 + Sin(GrphHue) * K4)*/
+	rgb_matrix[1] = dal_fixed31_32_mul(grph_sat, rgb_matrix[1]);
+	/* (LumaG + GrphSat * (Cos(GrphHue) * K3 + Sin(GrphHue) * K4))*/
+	rgb_matrix[1] = dal_fixed31_32_add(luma_g, rgb_matrix[1]);
+	/* GrphCont * (LumaG + GrphSat * (Cos(GrphHue) * K3 + Sin(GrphHue)**/
+	/* K4))*/
+	rgb_matrix[1] = dal_fixed31_32_mul(grph_cont, rgb_matrix[1]);
+
+	/* COEF_1_3 = GrphCont * (LumaB + GrphSat * (Cos(GrphHue) * K5 +*/
+	/* Sin(GrphHue) * K6))*/
+	/* (Cos(GrphHue) * K5 + Sin(GrphHue) * K6)*/
+	rgb_matrix[2] =
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(cos_grph_hue, k5),
+			dal_fixed31_32_mul(sin_grph_hue, k6));
+	/* GrphSat * (Cos(GrphHue) * K5 + Sin(GrphHue) * K6)*/
+	rgb_matrix[2] = dal_fixed31_32_mul(grph_sat, rgb_matrix[2]);
+	/* LumaB + GrphSat * (Cos(GrphHue) * K5 + Sin(GrphHue) * K6)*/
+	rgb_matrix[2] = dal_fixed31_32_add(luma_b, rgb_matrix[2]);
+	/* GrphCont  * (LumaB + GrphSat * (Cos(GrphHue) * K5 + Sin(GrphHue)**/
+	/* K6))*/
+	rgb_matrix[2] = dal_fixed31_32_mul(grph_cont, rgb_matrix[2]);
+
+	/* COEF_1_4 = GrphBright*/
+	rgb_matrix[3] = grph_bright;
+
+	/* COEF_2_1 = GrphCont * (LumaR + GrphSat * (Cos(GrphHue) * K7 +*/
+	/* Sin(GrphHue) * K8))*/
+	/* (Cos(GrphHue) * K7 + Sin(GrphHue) * K8)*/
+	rgb_matrix[4] =
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(cos_grph_hue, k7),
+			dal_fixed31_32_mul(sin_grph_hue, k8));
+	/* GrphSat * (Cos(GrphHue) * K7 + Sin(GrphHue) * K8)*/
+	rgb_matrix[4] = dal_fixed31_32_mul(grph_sat, rgb_matrix[4]);
+	/* (LumaR + GrphSat * (Cos(GrphHue) * K7 + Sin(GrphHue) * K8))*/
+	rgb_matrix[4] = dal_fixed31_32_add(luma_r, rgb_matrix[4]);
+	/* GrphCont * (LumaR + GrphSat * (Cos(GrphHue) * K7 + Sin(GrphHue)**/
+	/* K8))*/
+	rgb_matrix[4] = dal_fixed31_32_mul(grph_cont, rgb_matrix[4]);
+
+	/* COEF_2_2 = GrphCont * (LumaG + GrphSat * (Cos(GrphHue) * K9 +*/
+	/* Sin(GrphHue) * K10))*/
+	/* (Cos(GrphHue) * K9 + Sin(GrphHue) * K10))*/
+	rgb_matrix[5] =
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(cos_grph_hue, k9),
+			dal_fixed31_32_mul(sin_grph_hue, k10));
+	/* GrphSat * (Cos(GrphHue) * K9 + Sin(GrphHue) * K10))*/
+	rgb_matrix[5] = dal_fixed31_32_mul(grph_sat, rgb_matrix[5]);
+	/* (LumaG + GrphSat * (Cos(GrphHue) * K9 + Sin(GrphHue) * K10))*/
+	rgb_matrix[5] = dal_fixed31_32_add(luma_g, rgb_matrix[5]);
+	/* GrphCont * (LumaG + GrphSat * (Cos(GrphHue) * K9 + Sin(GrphHue)**/
+	/* K10))*/
+	rgb_matrix[5] = dal_fixed31_32_mul(grph_cont, rgb_matrix[5]);
+
+	/* COEF_2_3 = GrphCont * (LumaB + GrphSat * (Cos(GrphHue) * K11 +*/
+	/* Sin(GrphHue) * K12))*/
+	/* (Cos(GrphHue) * K11 + Sin(GrphHue) * K12))*/
+	rgb_matrix[6] =
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(cos_grph_hue, k11),
+			dal_fixed31_32_mul(sin_grph_hue, k12));
+	/* GrphSat * (Cos(GrphHue) * K11 + Sin(GrphHue) * K12))*/
+	rgb_matrix[6] = dal_fixed31_32_mul(grph_sat, rgb_matrix[6]);
+	/* (LumaB + GrphSat * (Cos(GrphHue) * K11 + Sin(GrphHue) * K12))*/
+	rgb_matrix[6] = dal_fixed31_32_add(luma_b, rgb_matrix[6]);
+	/* GrphCont * (LumaB + GrphSat * (Cos(GrphHue) * K11 + Sin(GrphHue)**/
+	/* K12))*/
+	rgb_matrix[6] = dal_fixed31_32_mul(grph_cont, rgb_matrix[6]);
+
+	/* COEF_2_4 = GrphBright*/
+	rgb_matrix[7] = grph_bright;
+
+	/* COEF_3_1 = GrphCont  * (LumaR + GrphSat * (Cos(GrphHue) * K13 +*/
+	/* Sin(GrphHue) * K14))*/
+	/* (Cos(GrphHue) * K13 + Sin(GrphHue) * K14)) */
+	rgb_matrix[8] =
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(cos_grph_hue, k13),
+			dal_fixed31_32_mul(sin_grph_hue, k14));
+	/* GrphSat * (Cos(GrphHue) * K13 + Sin(GrphHue) * K14)) */
+	rgb_matrix[8] = dal_fixed31_32_mul(grph_sat, rgb_matrix[8]);
+	/* (LumaR + GrphSat * (Cos(GrphHue) * K13 + Sin(GrphHue) * K14)) */
+	rgb_matrix[8] = dal_fixed31_32_add(luma_r, rgb_matrix[8]);
+	/* GrphCont  * (LumaR + GrphSat * (Cos(GrphHue) * K13 + Sin(GrphHue)**/
+	/* K14)) */
+	rgb_matrix[8] = dal_fixed31_32_mul(grph_cont, rgb_matrix[8]);
+
+	/* COEF_3_2    = GrphCont * (LumaG + GrphSat * (Cos(GrphHue) * K15 +*/
+	/* Sin(GrphHue) * K16)) */
+	/* GrphSat * (Cos(GrphHue) * K15 + Sin(GrphHue) * K16) */
+	rgb_matrix[9] =
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(cos_grph_hue, k15),
+			dal_fixed31_32_mul(sin_grph_hue, k16));
+	/* (LumaG + GrphSat * (Cos(GrphHue) * K15 + Sin(GrphHue) * K16)) */
+	rgb_matrix[9] = dal_fixed31_32_mul(grph_sat, rgb_matrix[9]);
+	/* (LumaG + GrphSat * (Cos(GrphHue) * K15 + Sin(GrphHue) * K16)) */
+	rgb_matrix[9] = dal_fixed31_32_add(luma_g, rgb_matrix[9]);
+	/* GrphCont * (LumaG + GrphSat * (Cos(GrphHue) * K15 + Sin(GrphHue)**/
+	/* K16)) */
+	rgb_matrix[9] = dal_fixed31_32_mul(grph_cont, rgb_matrix[9]);
+
+	/*  COEF_3_3 = GrphCont * (LumaB + GrphSat * (Cos(GrphHue) * K17 +*/
+	/* Sin(GrphHue) * K18)) */
+	/* (Cos(GrphHue) * K17 + Sin(GrphHue) * K18)) */
+	rgb_matrix[10] =
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(cos_grph_hue, k17),
+			dal_fixed31_32_mul(sin_grph_hue, k18));
+	/*  GrphSat * (Cos(GrphHue) * K17 + Sin(GrphHue) * K18)) */
+	rgb_matrix[10] = dal_fixed31_32_mul(grph_sat, rgb_matrix[10]);
+	/* (LumaB + GrphSat * (Cos(GrphHue) * K17 + Sin(GrphHue) * K18)) */
+	rgb_matrix[10] = dal_fixed31_32_add(luma_b, rgb_matrix[10]);
+	/* GrphCont * (LumaB + GrphSat * (Cos(GrphHue) * K17 + Sin(GrphHue)**/
+	/* K18)) */
+	rgb_matrix[10] = dal_fixed31_32_mul(grph_cont, rgb_matrix[10]);
+
+	/* COEF_3_4 = GrphBright */
+	rgb_matrix[11] = grph_bright;
+}
+
+static void calculate_rgb_limited_range_matrix(struct core_color *core_color,
+		unsigned int sink_index, struct fixed31_32 *rgb_matrix)
+{
+	struct fixed31_32 ideal[12];
+
+	static const int32_t matrix_[] = {
+			85546875, 0, 0, 6250000,
+			0, 85546875, 0, 6250000,
+			0, 0, 85546875, 6250000
+		};
+
+	uint32_t i = 0;
+
+	do {
+		ideal[i] = dal_fixed31_32_from_fraction(
+			matrix_[i],
+			100000000);
+		++i;
+	} while (i != ARRAY_SIZE(matrix_));
+
+
+	struct fixed31_32 grph_cont;
+	struct fixed31_32 grph_sat;
+	struct fixed31_32 grph_bright;
+	struct fixed31_32 sin_grph_hue;
+	struct fixed31_32 cos_grph_hue;
+
+	initialize_fix_point_color_values(
+		core_color, sink_index, &grph_cont, &grph_sat,
+		&grph_bright, &sin_grph_hue, &cos_grph_hue);
+
+	const struct fixed31_32 multiplier =
+		dal_fixed31_32_mul(grph_cont, grph_sat);
+
+	rgb_matrix[8] = dal_fixed31_32_mul(ideal[0], grph_cont);
+
+	rgb_matrix[9] = dal_fixed31_32_mul(ideal[1], grph_cont);
+
+	rgb_matrix[10] = dal_fixed31_32_mul(ideal[2], grph_cont);
+
+	rgb_matrix[11] = dal_fixed31_32_add(
+			ideal[3],
+			dal_fixed31_32_mul(
+				grph_bright,
+				dal_fixed31_32_from_fraction(86, 100)));
+
+	rgb_matrix[0] = dal_fixed31_32_mul(
+		multiplier,
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(
+				ideal[8],
+				sin_grph_hue),
+			dal_fixed31_32_mul(
+				ideal[4],
+				cos_grph_hue)));
+
+	rgb_matrix[1] = dal_fixed31_32_mul(
+		multiplier,
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(
+				ideal[9],
+				sin_grph_hue),
+			dal_fixed31_32_mul(
+				ideal[5],
+				cos_grph_hue)));
+
+	rgb_matrix[2] = dal_fixed31_32_mul(
+		multiplier,
+		dal_fixed31_32_add(
+			dal_fixed31_32_mul(
+				ideal[10],
+				sin_grph_hue),
+			dal_fixed31_32_mul(
+				ideal[6],
+				cos_grph_hue)));
+
+	rgb_matrix[3] = ideal[7];
+
+	rgb_matrix[4] = dal_fixed31_32_mul(
+		multiplier,
+		dal_fixed31_32_sub(
+			dal_fixed31_32_mul(
+				ideal[8],
+				cos_grph_hue),
+			dal_fixed31_32_mul(
+				ideal[4],
+				sin_grph_hue)));
+
+	rgb_matrix[5] = dal_fixed31_32_mul(
+		multiplier,
+		dal_fixed31_32_sub(
+			dal_fixed31_32_mul(
+				ideal[9],
+				cos_grph_hue),
+			dal_fixed31_32_mul(
+				ideal[5],
+				sin_grph_hue)));
+
+	rgb_matrix[6] = dal_fixed31_32_mul(
+		multiplier,
+		dal_fixed31_32_sub(
+			dal_fixed31_32_mul(
+				ideal[10],
+				cos_grph_hue),
+			dal_fixed31_32_mul(
+				ideal[6],
+				sin_grph_hue)));
+
+	rgb_matrix[7] = ideal[11];
+}
+
+static void calculate_yuv_matrix(struct core_color *core_color,
+		unsigned int sink_index,
+		enum dc_color_space color_space,
+		struct fixed31_32 *yuv_matrix)
+{
+	struct fixed31_32 ideal[12];
+	uint32_t i = 0;
+
+	if ((color_space == COLOR_SPACE_YPBPR601) ||
+			(color_space == COLOR_SPACE_YCBCR601) ||
+			(color_space == COLOR_SPACE_YCBCR601_LIMITED)) {
+		static const int32_t matrix_[] = {
+				25578516, 50216016, 9752344, 6250000,
+				-14764391, -28985609, 43750000, 50000000,
+				43750000, -36635164, -7114836, 50000000
+			};
+		do {
+			ideal[i] = dal_fixed31_32_from_fraction(
+					matrix_[i],
+				100000000);
+			++i;
+		} while (i != ARRAY_SIZE(matrix_));
+	} else {
+		static const int32_t matrix_[] = {
+				18187266, 61183125, 6176484, 6250000,
+				-10025059, -33724941, 43750000, 50000000,
+				43750000, -39738379, -4011621, 50000000
+			};
+		do {
+			ideal[i] = dal_fixed31_32_from_fraction(
+					matrix_[i],
+				100000000);
+			++i;
+		} while (i != ARRAY_SIZE(matrix_));
+	}
+
+	struct fixed31_32 grph_cont;
+	struct fixed31_32 grph_sat;
+	struct fixed31_32 grph_bright;
+	struct fixed31_32 sin_grph_hue;
+	struct fixed31_32 cos_grph_hue;
+
+	initialize_fix_point_color_values(
+		core_color, sink_index, &grph_cont, &grph_sat,
+		&grph_bright, &sin_grph_hue, &cos_grph_hue);
+
+	const struct fixed31_32 multiplier =
+			dal_fixed31_32_mul(grph_cont, grph_sat);
+
+	yuv_matrix[0] = dal_fixed31_32_mul(ideal[0], grph_cont);
+
+	yuv_matrix[1] = dal_fixed31_32_mul(ideal[1], grph_cont);
+
+	yuv_matrix[2] = dal_fixed31_32_mul(ideal[2], grph_cont);
+
+	yuv_matrix[4] = dal_fixed31_32_mul(
+			multiplier,
+			dal_fixed31_32_add(
+				dal_fixed31_32_mul(
+					ideal[4],
+					cos_grph_hue),
+				dal_fixed31_32_mul(
+					ideal[8],
+					sin_grph_hue)));
+
+	yuv_matrix[5] = dal_fixed31_32_mul(
+			multiplier,
+			dal_fixed31_32_add(
+				dal_fixed31_32_mul(
+					ideal[5],
+					cos_grph_hue),
+				dal_fixed31_32_mul(
+					ideal[9],
+					sin_grph_hue)));
+
+	yuv_matrix[6] = dal_fixed31_32_mul(
+			multiplier,
+			dal_fixed31_32_add(
+				dal_fixed31_32_mul(
+					ideal[6],
+					cos_grph_hue),
+				dal_fixed31_32_mul(
+					ideal[10],
+					sin_grph_hue)));
+
+	yuv_matrix[7] = ideal[7];
+
+	yuv_matrix[8] = dal_fixed31_32_mul(
+			multiplier,
+			dal_fixed31_32_sub(
+				dal_fixed31_32_mul(
+					ideal[8],
+					cos_grph_hue),
+				dal_fixed31_32_mul(
+					ideal[4],
+					sin_grph_hue)));
+
+	yuv_matrix[9] = dal_fixed31_32_mul(
+			multiplier,
+			dal_fixed31_32_sub(
+				dal_fixed31_32_mul(
+					ideal[9],
+					cos_grph_hue),
+				dal_fixed31_32_mul(
+					ideal[5],
+					sin_grph_hue)));
+
+	yuv_matrix[10] = dal_fixed31_32_mul(
+			multiplier,
+			dal_fixed31_32_sub(
+				dal_fixed31_32_mul(
+					ideal[10],
+					cos_grph_hue),
+				dal_fixed31_32_mul(
+					ideal[6],
+					sin_grph_hue)));
+
+	yuv_matrix[11] = ideal[11];
+
+	if ((color_space == COLOR_SPACE_YCBCR601_LIMITED) ||
+			(color_space == COLOR_SPACE_YCBCR709_LIMITED)) {
+		yuv_matrix[3] = dal_fixed31_32_add(ideal[3], grph_bright);
+	} else {
+		yuv_matrix[3] = dal_fixed31_32_add(
+			ideal[3],
+			dal_fixed31_32_mul(
+				grph_bright,
+				dal_fixed31_32_from_fraction(86, 100)));
+	}
+}
+
+static void calculate_csc_matrix(struct core_color *core_color,
+		unsigned int sink_index,
+		enum dc_color_space color_space,
+		uint16_t *csc_matrix)
+{
+	struct fixed31_32 fixed_csc_matrix[12];
+	switch (color_space) {
+	case COLOR_SPACE_SRGB:
+		calculate_rgb_matrix_legacy
+			(core_color, sink_index, fixed_csc_matrix);
+		convert_float_matrix_legacy
+			(csc_matrix, fixed_csc_matrix, 12);
+		break;
+	case COLOR_SPACE_SRGB_LIMITED:
+		calculate_rgb_limited_range_matrix(core_color, sink_index,
+				fixed_csc_matrix);
+		convert_float_matrix(csc_matrix, fixed_csc_matrix, 12);
+		break;
+	case COLOR_SPACE_YCBCR601:
+	case COLOR_SPACE_YCBCR709:
+	case COLOR_SPACE_YCBCR601_LIMITED:
+	case COLOR_SPACE_YCBCR709_LIMITED:
+	case COLOR_SPACE_YPBPR601:
+	case COLOR_SPACE_YPBPR709:
+		calculate_yuv_matrix(core_color, sink_index, color_space,
+				fixed_csc_matrix);
+		convert_float_matrix(csc_matrix, fixed_csc_matrix, 12);
+		break;
+	default:
+		calculate_rgb_matrix_legacy
+			(core_color, sink_index, fixed_csc_matrix);
+		convert_float_matrix_legacy
+			(csc_matrix, fixed_csc_matrix, 12);
+		break;
+	}
+}
+
+struct mod_color *mod_color_create(struct dc *dc)
+{
+	int i = 0;
+	struct core_color *core_color =
+				dm_alloc(sizeof(struct core_color));
+	struct core_dc *core_dc = DC_TO_CORE(dc);
+	struct persistent_data_flag flag;
+
+	if (core_color == NULL)
+		goto fail_alloc_context;
+
+	core_color->caps = dm_alloc(sizeof(struct sink_caps) *
+			MOD_COLOR_MAX_CONCURRENT_SINKS);
+
+	if (core_color->caps == NULL)
+		goto fail_alloc_caps;
+
+	for (i = 0; i < MOD_COLOR_MAX_CONCURRENT_SINKS; i++)
+		core_color->caps[i].sink = NULL;
+
+	core_color->state = dm_alloc(sizeof(struct color_state) *
+			MOD_COLOR_MAX_CONCURRENT_SINKS);
+
+	/*hardcoded to sRGB with 6500 color temperature*/
+	for (i = 0; i < MOD_COLOR_MAX_CONCURRENT_SINKS; i++) {
+		core_color->state[i].source_gamut.blueX = 1500;
+		core_color->state[i].source_gamut.blueY = 600;
+		core_color->state[i].source_gamut.greenX = 3000;
+		core_color->state[i].source_gamut.greenY = 6000;
+		core_color->state[i].source_gamut.redX = 6400;
+		core_color->state[i].source_gamut.redY = 3300;
+		core_color->state[i].source_gamut.whiteX = 3127;
+		core_color->state[i].source_gamut.whiteY = 3290;
+
+		core_color->state[i].destination_gamut.blueX = 1500;
+		core_color->state[i].destination_gamut.blueY = 600;
+		core_color->state[i].destination_gamut.greenX = 3000;
+		core_color->state[i].destination_gamut.greenY = 6000;
+		core_color->state[i].destination_gamut.redX = 6400;
+		core_color->state[i].destination_gamut.redY = 3300;
+		core_color->state[i].destination_gamut.whiteX = 3127;
+		core_color->state[i].destination_gamut.whiteY = 3290;
+
+		core_color->state[i].custom_color_temperature = 6500;
+
+		core_color->state[i].contrast.current = 100;
+		core_color->state[i].contrast.min = 0;
+		core_color->state[i].contrast.max = 200;
+
+		core_color->state[i].saturation.current = 100;
+		core_color->state[i].saturation.min = 0;
+		core_color->state[i].saturation.max = 200;
+
+		core_color->state[i].brightness.current = 0;
+		core_color->state[i].brightness.min = -100;
+		core_color->state[i].brightness.max = 100;
+
+		core_color->state[i].hue.current = 0;
+		core_color->state[i].hue.min = -30;
+		core_color->state[i].hue.max = 30;
+	}
+
+	if (core_color->state == NULL)
+		goto fail_alloc_state;
+
+	core_color->num_sinks = 0;
+
+	if (dc == NULL)
+		goto fail_construct;
+
+	core_color->dc = dc;
+
+	if (!check_dc_support(dc))
+		goto fail_construct;
+
+	/* Create initial module folder in registry for color adjustment */
+	flag.save_per_edid = true;
+	flag.save_per_link = false;
+
+	dm_write_persistent_data(core_dc->ctx, NULL, COLOR_REGISTRY_NAME, NULL,
+			NULL, 0, &flag);
+
+	return &core_color->public;
+
+fail_construct:
+	dm_free(core_color->state);
+
+fail_alloc_state:
+	dm_free(core_color->caps);
+
+fail_alloc_caps:
+	dm_free(core_color);
+
+fail_alloc_context:
+	return NULL;
+}
+
+void mod_color_destroy(struct mod_color *mod_color)
+{
+	if (mod_color != NULL) {
+		int i;
+		struct core_color *core_color =
+				MOD_COLOR_TO_CORE(mod_color);
+
+		dm_free(core_color->state);
+
+		for (i = 0; i < core_color->num_sinks; i++)
+			dc_sink_release(core_color->caps[i].sink);
+
+		dm_free(core_color->caps);
+
+		dm_free(core_color);
+	}
+}
+
+bool mod_color_add_sink(struct mod_color *mod_color, const struct dc_sink *sink)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+	struct core_dc *core_dc = DC_TO_CORE(core_color->dc);
+	bool persistent_color_temp_enable;
+	int persistent_custom_color_temp = 0;
+	struct color_space_coordinates persistent_source_gamut;
+	struct color_space_coordinates persistent_destination_gamut;
+	int persistent_brightness;
+	int persistent_contrast;
+	int persistent_hue;
+	int persistent_saturation;
+	enum dc_quantization_range persistent_quantization_range;
+	struct persistent_data_flag flag;
+
+	if (core_color->num_sinks < MOD_COLOR_MAX_CONCURRENT_SINKS) {
+		dc_sink_retain(sink);
+		core_color->caps[core_color->num_sinks].sink = sink;
+		core_color->state[core_color->num_sinks].
+				user_enable_color_temperature = true;
+
+		/* get persistent data from registry */
+		flag.save_per_edid = true;
+		flag.save_per_link = false;
+
+
+		if (dm_read_persistent_data(core_dc->ctx, sink,
+						COLOR_REGISTRY_NAME,
+						"enablecolortempadj",
+						&persistent_color_temp_enable,
+						sizeof(bool), &flag))
+			core_color->state[core_color->num_sinks].
+				user_enable_color_temperature =
+						persistent_color_temp_enable;
+		else
+			core_color->state[core_color->num_sinks].
+				user_enable_color_temperature = true;
+
+		if (dm_read_persistent_data(core_dc->ctx, sink,
+						COLOR_REGISTRY_NAME,
+						"customcolortemp",
+						&persistent_custom_color_temp,
+						sizeof(int), &flag))
+			core_color->state[core_color->num_sinks].
+					custom_color_temperature
+					= persistent_custom_color_temp;
+		else
+			core_color->state[core_color->num_sinks].
+					custom_color_temperature = 6500;
+
+		if (dm_read_persistent_data(core_dc->ctx, sink,
+					COLOR_REGISTRY_NAME,
+					"sourcegamut",
+					&persistent_source_gamut,
+					sizeof(struct color_space_coordinates),
+					&flag)) {
+			memcpy(&core_color->state[core_color->num_sinks].
+				source_gamut, &persistent_source_gamut,
+				sizeof(struct color_space_coordinates));
+		} else {
+			core_color->state[core_color->num_sinks].
+					source_gamut.blueX = 1500;
+			core_color->state[core_color->num_sinks].
+					source_gamut.blueY = 600;
+			core_color->state[core_color->num_sinks].
+					source_gamut.greenX = 3000;
+			core_color->state[core_color->num_sinks].
+					source_gamut.greenY = 6000;
+			core_color->state[core_color->num_sinks].
+					source_gamut.redX = 6400;
+			core_color->state[core_color->num_sinks].
+					source_gamut.redY = 3300;
+			core_color->state[core_color->num_sinks].
+					source_gamut.whiteX = 3127;
+			core_color->state[core_color->num_sinks].
+					source_gamut.whiteY = 3290;
+		}
+
+		if (dm_read_persistent_data(core_dc->ctx, sink, COLOR_REGISTRY_NAME,
+					"destgamut",
+					&persistent_destination_gamut,
+					sizeof(struct color_space_coordinates),
+					&flag)) {
+			memcpy(&core_color->state[core_color->num_sinks].
+				destination_gamut,
+				&persistent_destination_gamut,
+				sizeof(struct color_space_coordinates));
+		} else {
+			core_color->state[core_color->num_sinks].
+					destination_gamut.blueX = 1500;
+			core_color->state[core_color->num_sinks].
+					destination_gamut.blueY = 600;
+			core_color->state[core_color->num_sinks].
+					destination_gamut.greenX = 3000;
+			core_color->state[core_color->num_sinks].
+					destination_gamut.greenY = 6000;
+			core_color->state[core_color->num_sinks].
+					destination_gamut.redX = 6400;
+			core_color->state[core_color->num_sinks].
+					destination_gamut.redY = 3300;
+			core_color->state[core_color->num_sinks].
+					destination_gamut.whiteX = 3127;
+			core_color->state[core_color->num_sinks].
+					destination_gamut.whiteY = 3290;
+		}
+
+		if (dm_read_persistent_data(core_dc->ctx, sink, COLOR_REGISTRY_NAME,
+						"brightness",
+						&persistent_brightness,
+						sizeof(int), &flag))
+			core_color->state[core_color->num_sinks].
+				brightness.current = persistent_brightness;
+		else
+			core_color->state[core_color->num_sinks].
+				brightness.current = 0;
+
+		if (dm_read_persistent_data(core_dc->ctx, sink, COLOR_REGISTRY_NAME,
+						"contrast",
+						&persistent_contrast,
+						sizeof(int), &flag))
+			core_color->state[core_color->num_sinks].
+				contrast.current = persistent_contrast;
+		else
+			core_color->state[core_color->num_sinks].
+				contrast.current = 100;
+
+		if (dm_read_persistent_data(core_dc->ctx, sink, COLOR_REGISTRY_NAME,
+						"hue",
+						&persistent_hue,
+						sizeof(int), &flag))
+			core_color->state[core_color->num_sinks].
+				hue.current = persistent_hue;
+		else
+			core_color->state[core_color->num_sinks].
+				hue.current = 0;
+
+		if (dm_read_persistent_data(core_dc->ctx, sink, COLOR_REGISTRY_NAME,
+						"saturation",
+						&persistent_saturation,
+						sizeof(int), &flag))
+			core_color->state[core_color->num_sinks].
+				saturation.current = persistent_saturation;
+		else
+			core_color->state[core_color->num_sinks].
+				saturation.current = 100;
+
+		if (dm_read_persistent_data(core_dc->ctx, sink,
+						COLOR_REGISTRY_NAME,
+						"preferred_quantization_range",
+						&persistent_quantization_range,
+						sizeof(int), &flag))
+			core_color->state[core_color->num_sinks].
+			preferred_quantization_range =
+					persistent_quantization_range;
+		else
+			core_color->state[core_color->num_sinks].
+			preferred_quantization_range = QUANTIZATION_RANGE_FULL;
+
+		core_color->num_sinks++;
+		return true;
+	}
+	return false;
+}
+
+bool mod_color_remove_sink(struct mod_color *mod_color,
+		const struct dc_sink *sink)
+{
+	int i = 0, j = 0;
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+
+	for (i = 0; i < core_color->num_sinks; i++) {
+		if (core_color->caps[i].sink == sink) {
+			/* To remove this sink, shift everything after down */
+			for (j = i; j < core_color->num_sinks - 1; j++) {
+				core_color->caps[j].sink =
+					core_color->caps[j + 1].sink;
+
+				memcpy(&core_color->state[j],
+					&core_color->state[j + 1],
+					sizeof(struct color_state));
+			}
+
+			core_color->num_sinks--;
+
+			dc_sink_release(sink);
+
+			return true;
+		}
+	}
+
+	return false;
+}
+
+bool mod_color_update_gamut_to_stream(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+	struct core_dc *core_dc = DC_TO_CORE(core_color->dc);
+	struct persistent_data_flag flag;
+	struct gamut_src_dst_matrix *matrix =
+			dm_alloc(sizeof(struct gamut_src_dst_matrix));
+
+	unsigned int stream_index, sink_index, j;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+
+		/* Write persistent data in registry*/
+		flag.save_per_edid = true;
+		flag.save_per_link = false;
+
+		dm_write_persistent_data(core_dc->ctx,
+					streams[stream_index]->sink,
+					COLOR_REGISTRY_NAME,
+					"sourcegamut",
+					&core_color->state[sink_index].
+							source_gamut,
+					sizeof(struct color_space_coordinates),
+					&flag);
+
+		dm_write_persistent_data(core_dc->ctx,
+					streams[stream_index]->sink,
+					COLOR_REGISTRY_NAME,
+					"destgamut",
+					&core_color->state[sink_index].
+							destination_gamut,
+					sizeof(struct color_space_coordinates),
+					&flag);
+
+		if (!build_gamut_remap_matrix
+				(core_color->state[sink_index].source_gamut,
+						matrix->rgbCoeffSrc,
+						matrix->whiteCoeffSrc))
+			goto function_fail;
+
+		if (!build_gamut_remap_matrix
+				(core_color->state[sink_index].
+				destination_gamut,
+				matrix->rgbCoeffDst, matrix->whiteCoeffDst))
+			goto function_fail;
+
+		struct fixed31_32 gamut_result[12];
+		struct fixed31_32 temp_matrix[9];
+
+		if (!gamut_to_color_matrix(
+				matrix->rgbCoeffDst,
+				matrix->whiteCoeffDst,
+				matrix->rgbCoeffSrc,
+				matrix->whiteCoeffSrc,
+				true,
+				temp_matrix))
+			goto function_fail;
+
+		gamut_result[0] = temp_matrix[0];
+		gamut_result[1] = temp_matrix[1];
+		gamut_result[2] = temp_matrix[2];
+		gamut_result[3] = matrix->whiteCoeffSrc[0];
+		gamut_result[4] = temp_matrix[3];
+		gamut_result[5] = temp_matrix[4];
+		gamut_result[6] = temp_matrix[5];
+		gamut_result[7] = matrix->whiteCoeffSrc[1];
+		gamut_result[8] = temp_matrix[6];
+		gamut_result[9] = temp_matrix[7];
+		gamut_result[10] = temp_matrix[8];
+		gamut_result[11] = matrix->whiteCoeffSrc[2];
+
+		struct core_stream *core_stream =
+				DC_STREAM_TO_CORE
+				(streams[stream_index]);
+
+		core_stream->public.gamut_remap_matrix.enable_remap = true;
+
+		for (j = 0; j < 12; j++)
+			core_stream->public.
+			gamut_remap_matrix.matrix[j] =
+					gamut_result[j];
+	}
+
+	dm_free(matrix);
+	core_color->dc->stream_funcs.set_gamut_remap
+			(core_color->dc, streams, num_streams);
+
+	return true;
+
+function_fail:
+	dm_free(matrix);
+	return false;
+}
+
+bool mod_color_adjust_source_gamut(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		struct gamut_space_coordinates *input_gamut_coordinates,
+		struct white_point_coodinates *input_white_point_coordinates)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int stream_index, sink_index;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+
+		core_color->state[sink_index].source_gamut.blueX =
+				input_gamut_coordinates->blueX;
+		core_color->state[sink_index].source_gamut.blueY =
+				input_gamut_coordinates->blueY;
+		core_color->state[sink_index].source_gamut.greenX =
+				input_gamut_coordinates->greenX;
+		core_color->state[sink_index].source_gamut.greenY =
+				input_gamut_coordinates->greenY;
+		core_color->state[sink_index].source_gamut.redX =
+				input_gamut_coordinates->redX;
+		core_color->state[sink_index].source_gamut.redY =
+				input_gamut_coordinates->redY;
+		core_color->state[sink_index].source_gamut.whiteX =
+				input_white_point_coordinates->whiteX;
+		core_color->state[sink_index].source_gamut.whiteY =
+				input_white_point_coordinates->whiteY;
+	}
+
+	if (!mod_color_update_gamut_to_stream(mod_color, streams, num_streams))
+		return false;
+
+	return true;
+}
+
+bool mod_color_adjust_destination_gamut(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		struct gamut_space_coordinates *input_gamut_coordinates,
+		struct white_point_coodinates *input_white_point_coordinates)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int stream_index, sink_index;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+
+		core_color->state[sink_index].destination_gamut.blueX =
+				input_gamut_coordinates->blueX;
+		core_color->state[sink_index].destination_gamut.blueY =
+				input_gamut_coordinates->blueY;
+		core_color->state[sink_index].destination_gamut.greenX =
+				input_gamut_coordinates->greenX;
+		core_color->state[sink_index].destination_gamut.greenY =
+				input_gamut_coordinates->greenY;
+		core_color->state[sink_index].destination_gamut.redX =
+				input_gamut_coordinates->redX;
+		core_color->state[sink_index].destination_gamut.redY =
+				input_gamut_coordinates->redY;
+		core_color->state[sink_index].destination_gamut.whiteX =
+				input_white_point_coordinates->whiteX;
+		core_color->state[sink_index].destination_gamut.whiteY =
+				input_white_point_coordinates->whiteY;
+	}
+
+	if (!mod_color_update_gamut_to_stream(mod_color, streams, num_streams))
+		return false;
+
+	return true;
+}
+
+bool mod_color_set_white_point(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		struct white_point_coodinates *white_point)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int stream_index, sink_index;
+
+	for (stream_index = 0; stream_index < num_streams;
+			stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+		core_color->state[sink_index].source_gamut.whiteX =
+				white_point->whiteX;
+		core_color->state[sink_index].source_gamut.whiteY =
+				white_point->whiteY;
+	}
+
+	if (!mod_color_update_gamut_to_stream(mod_color, streams, num_streams))
+		return false;
+
+	return true;
+}
+
+bool mod_color_set_user_enable(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		bool user_enable)
+{
+	struct core_color *core_color =
+			MOD_COLOR_TO_CORE(mod_color);
+	struct core_dc *core_dc = DC_TO_CORE(core_color->dc);
+	struct persistent_data_flag flag;
+	unsigned int stream_index, sink_index;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+		core_color->state[sink_index].user_enable_color_temperature
+				= user_enable;
+
+		/* Write persistent data in registry*/
+		flag.save_per_edid = true;
+		flag.save_per_link = false;
+
+		dm_write_persistent_data(core_dc->ctx,
+					streams[stream_index]->sink,
+					COLOR_REGISTRY_NAME,
+					"enablecolortempadj",
+					&user_enable,
+					sizeof(bool),
+					&flag);
+	}
+	return true;
+}
+
+bool mod_color_get_user_enable(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		bool *user_enable)
+{
+	struct core_color *core_color =
+			MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int sink_index = sink_index_from_sink(core_color, sink);
+
+	*user_enable = core_color->state[sink_index].
+					user_enable_color_temperature;
+
+	return true;
+}
+
+bool mod_color_get_custom_color_temperature(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		int *color_temperature)
+{
+	struct core_color *core_color =
+			MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int sink_index = sink_index_from_sink(core_color, sink);
+
+	*color_temperature = core_color->state[sink_index].
+			custom_color_temperature;
+
+	return true;
+}
+
+bool mod_color_set_custom_color_temperature(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int color_temperature)
+{
+	struct core_color *core_color =
+			MOD_COLOR_TO_CORE(mod_color);
+	struct core_dc *core_dc = DC_TO_CORE(core_color->dc);
+	struct persistent_data_flag flag;
+	unsigned int stream_index, sink_index;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+		core_color->state[sink_index].custom_color_temperature
+				= color_temperature;
+
+		/* Write persistent data in registry*/
+		flag.save_per_edid = true;
+		flag.save_per_link = false;
+
+		dm_write_persistent_data(core_dc->ctx,
+					streams[stream_index]->sink,
+					COLOR_REGISTRY_NAME,
+					"customcolortemp",
+					&color_temperature,
+					sizeof(int),
+					&flag);
+	}
+	return true;
+}
+
+bool mod_color_get_color_saturation(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_range *color_saturation)
+{
+	struct core_color *core_color =
+			MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int sink_index = sink_index_from_sink(core_color, sink);
+
+	*color_saturation = core_color->state[sink_index].saturation;
+
+	return true;
+}
+
+bool mod_color_get_color_contrast(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_range *color_contrast)
+{
+	struct core_color *core_color =
+			MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int sink_index = sink_index_from_sink(core_color, sink);
+
+	*color_contrast = core_color->state[sink_index].contrast;
+
+	return true;
+}
+
+bool mod_color_get_color_brightness(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_range *color_brightness)
+{
+	struct core_color *core_color =
+			MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int sink_index = sink_index_from_sink(core_color, sink);
+
+	*color_brightness = core_color->state[sink_index].brightness;
+
+	return true;
+}
+
+bool mod_color_get_color_hue(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_range *color_hue)
+{
+	struct core_color *core_color =
+			MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int sink_index = sink_index_from_sink(core_color, sink);
+
+	*color_hue = core_color->state[sink_index].hue;
+
+	return true;
+}
+
+bool mod_color_get_source_gamut(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_space_coordinates *source_gamut)
+{
+	struct core_color *core_color =
+			MOD_COLOR_TO_CORE(mod_color);
+
+	unsigned int sink_index = sink_index_from_sink(core_color, sink);
+
+	*source_gamut = core_color->state[sink_index].source_gamut;
+
+	return true;
+}
+
+bool mod_color_notify_mode_change(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+
+	struct gamut_src_dst_matrix *matrix =
+			dm_alloc(sizeof(struct gamut_src_dst_matrix));
+
+	unsigned int stream_index, sink_index, j;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+
+		if (!build_gamut_remap_matrix
+				(core_color->state[sink_index].source_gamut,
+						matrix->rgbCoeffSrc,
+						matrix->whiteCoeffSrc))
+			goto function_fail;
+
+		if (!build_gamut_remap_matrix
+				(core_color->state[sink_index].
+				destination_gamut,
+				matrix->rgbCoeffDst, matrix->whiteCoeffDst))
+			goto function_fail;
+
+		struct fixed31_32 gamut_result[12];
+		struct fixed31_32 temp_matrix[9];
+
+		if (!gamut_to_color_matrix(
+				matrix->rgbCoeffDst,
+				matrix->whiteCoeffDst,
+				matrix->rgbCoeffSrc,
+				matrix->whiteCoeffSrc,
+				true,
+				temp_matrix))
+			goto function_fail;
+
+		gamut_result[0] = temp_matrix[0];
+		gamut_result[1] = temp_matrix[1];
+		gamut_result[2] = temp_matrix[2];
+		gamut_result[3] = matrix->whiteCoeffSrc[0];
+		gamut_result[4] = temp_matrix[3];
+		gamut_result[5] = temp_matrix[4];
+		gamut_result[6] = temp_matrix[5];
+		gamut_result[7] = matrix->whiteCoeffSrc[1];
+		gamut_result[8] = temp_matrix[6];
+		gamut_result[9] = temp_matrix[7];
+		gamut_result[10] = temp_matrix[8];
+		gamut_result[11] = matrix->whiteCoeffSrc[2];
+
+
+		struct core_stream *core_stream =
+				DC_STREAM_TO_CORE
+				(streams[stream_index]);
+
+		core_stream->public.gamut_remap_matrix.enable_remap = true;
+
+		for (j = 0; j < 12; j++)
+			core_stream->public.
+			gamut_remap_matrix.matrix[j] =
+					gamut_result[j];
+
+		calculate_csc_matrix(core_color, sink_index,
+				core_stream->public.output_color_space,
+				core_stream->public.csc_color_matrix.matrix);
+
+		core_stream->public.csc_color_matrix.enable_adjustment = true;
+	}
+
+	dm_free(matrix);
+
+	return true;
+
+function_fail:
+	dm_free(matrix);
+	return false;
+}
+
+bool mod_color_set_brightness(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int brightness_value)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+	struct core_dc *core_dc = DC_TO_CORE(core_color->dc);
+	struct persistent_data_flag flag;
+	unsigned int stream_index, sink_index;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+
+		struct core_stream *core_stream =
+						DC_STREAM_TO_CORE
+						(streams[stream_index]);
+
+		core_color->state[sink_index].brightness.current =
+				brightness_value;
+
+		calculate_csc_matrix(core_color, sink_index,
+				core_stream->public.output_color_space,
+				core_stream->public.csc_color_matrix.matrix);
+
+		core_stream->public.csc_color_matrix.enable_adjustment = true;
+
+		/* Write persistent data in registry*/
+		flag.save_per_edid = true;
+		flag.save_per_link = false;
+		dm_write_persistent_data(core_dc->ctx,
+					streams[stream_index]->sink,
+					COLOR_REGISTRY_NAME,
+					"brightness",
+					&brightness_value,
+					sizeof(int),
+					&flag);
+	}
+
+	core_color->dc->stream_funcs.set_gamut_remap
+			(core_color->dc, streams, num_streams);
+
+	return true;
+}
+
+bool mod_color_set_contrast(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int contrast_value)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+	struct core_dc *core_dc = DC_TO_CORE(core_color->dc);
+	struct persistent_data_flag flag;
+	unsigned int stream_index, sink_index;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+
+		struct core_stream *core_stream =
+						DC_STREAM_TO_CORE
+						(streams[stream_index]);
+
+		core_color->state[sink_index].contrast.current =
+				contrast_value;
+
+		calculate_csc_matrix(core_color, sink_index,
+				core_stream->public.output_color_space,
+				core_stream->public.csc_color_matrix.matrix);
+
+		core_stream->public.csc_color_matrix.enable_adjustment = true;
+
+		/* Write persistent data in registry*/
+		flag.save_per_edid = true;
+		flag.save_per_link = false;
+		dm_write_persistent_data(core_dc->ctx,
+					streams[stream_index]->sink,
+					COLOR_REGISTRY_NAME,
+					"contrast",
+					&contrast_value,
+					sizeof(int),
+					&flag);
+	}
+
+	core_color->dc->stream_funcs.set_gamut_remap
+			(core_color->dc, streams, num_streams);
+
+	return true;
+}
+
+bool mod_color_set_hue(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int hue_value)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+	struct core_dc *core_dc = DC_TO_CORE(core_color->dc);
+	struct persistent_data_flag flag;
+	unsigned int stream_index, sink_index;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+
+		struct core_stream *core_stream =
+						DC_STREAM_TO_CORE
+						(streams[stream_index]);
+
+		core_color->state[sink_index].hue.current = hue_value;
+
+		calculate_csc_matrix(core_color, sink_index,
+				core_stream->public.output_color_space,
+				core_stream->public.csc_color_matrix.matrix);
+
+		core_stream->public.csc_color_matrix.enable_adjustment = true;
+
+		/* Write persistent data in registry*/
+		flag.save_per_edid = true;
+		flag.save_per_link = false;
+		dm_write_persistent_data(core_dc->ctx,
+					streams[stream_index]->sink,
+					COLOR_REGISTRY_NAME,
+					"hue",
+					&hue_value,
+					sizeof(int),
+					&flag);
+	}
+
+	core_color->dc->stream_funcs.set_gamut_remap
+			(core_color->dc, streams, num_streams);
+
+	return true;
+}
+
+bool mod_color_set_saturation(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int saturation_value)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+	struct core_dc *core_dc = DC_TO_CORE(core_color->dc);
+	struct persistent_data_flag flag;
+	unsigned int stream_index, sink_index;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		sink_index = sink_index_from_sink(core_color,
+				streams[stream_index]->sink);
+
+		struct core_stream *core_stream =
+						DC_STREAM_TO_CORE
+						(streams[stream_index]);
+
+		core_color->state[sink_index].saturation.current =
+				saturation_value;
+
+		calculate_csc_matrix(core_color, sink_index,
+				core_stream->public.output_color_space,
+				core_stream->public.csc_color_matrix.matrix);
+
+		core_stream->public.csc_color_matrix.enable_adjustment = true;
+
+		/* Write persistent data in registry*/
+		flag.save_per_edid = true;
+		flag.save_per_link = false;
+		dm_write_persistent_data(core_dc->ctx,
+					streams[stream_index]->sink,
+					COLOR_REGISTRY_NAME,
+					"saturation",
+					&saturation_value,
+					sizeof(int),
+					&flag);
+	}
+
+	core_color->dc->stream_funcs.set_gamut_remap
+			(core_color->dc, streams, num_streams);
+
+	return true;
+}
+
+bool mod_color_set_preferred_quantization_range(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		enum dc_quantization_range quantization_range)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+	struct core_dc *core_dc = DC_TO_CORE(core_color->dc);
+	struct persistent_data_flag flag;
+	unsigned int sink_index;
+
+	sink_index = sink_index_from_sink(core_color, sink);
+	if (core_color->state[sink_index].
+			preferred_quantization_range != quantization_range) {
+		core_color->state[sink_index].preferred_quantization_range =
+				quantization_range;
+		flag.save_per_edid = true;
+		flag.save_per_link = false;
+		dm_write_persistent_data(core_dc->ctx,
+					sink,
+					COLOR_REGISTRY_NAME,
+					"quantization_range",
+					&quantization_range,
+					sizeof(int),
+					&flag);
+	}
+
+	return true;
+}
+
+bool mod_color_get_preferred_quantization_range(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		enum dc_quantization_range *quantization_range)
+{
+	struct core_color *core_color = MOD_COLOR_TO_CORE(mod_color);
+	unsigned int sink_index;
+
+	sink_index = sink_index_from_sink(core_color, sink);
+	*quantization_range = core_color->state[sink_index].
+			preferred_quantization_range;
+	return true;
+}
diff --git a/drivers/gpu/drm/amd/display/modules/inc/mod_color.h b/drivers/gpu/drm/amd/display/modules/inc/mod_color.h
new file mode 100644
index 0000000..e54fe2c
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/modules/inc/mod_color.h
@@ -0,0 +1,179 @@
+/*
+ * Copyright 2016 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+
+#ifndef MOD_COLOR_H_
+#define MOD_COLOR_H_
+
+#include "dm_services.h"
+
+struct mod_color {
+	int dummy;
+};
+
+struct color_space_coordinates {
+	unsigned int redX;
+	unsigned int redY;
+	unsigned int greenX;
+	unsigned int greenY;
+	unsigned int blueX;
+	unsigned int blueY;
+	unsigned int whiteX;
+	unsigned int whiteY;
+};
+
+struct gamut_space_coordinates {
+	unsigned int redX;
+	unsigned int redY;
+	unsigned int greenX;
+	unsigned int greenY;
+	unsigned int blueX;
+	unsigned int blueY;
+};
+
+struct gamut_space_entry {
+	unsigned int index;
+	unsigned int redX;
+	unsigned int redY;
+	unsigned int greenX;
+	unsigned int greenY;
+	unsigned int blueX;
+	unsigned int blueY;
+
+	int a0;
+	int a1;
+	int a2;
+	int a3;
+	int gamma;
+};
+
+struct white_point_coodinates {
+	unsigned int whiteX;
+	unsigned int whiteY;
+};
+
+struct white_point_coodinates_entry {
+	unsigned int index;
+	unsigned int whiteX;
+	unsigned int whiteY;
+};
+
+struct color_range {
+	int current;
+	int min;
+	int max;
+};
+
+struct mod_color *mod_color_create(struct dc *dc);
+
+void mod_color_destroy(struct mod_color *mod_color);
+
+bool mod_color_add_sink(struct mod_color *mod_color,
+		const struct dc_sink *sink);
+
+bool mod_color_remove_sink(struct mod_color *mod_color,
+		const struct dc_sink *sink);
+
+bool mod_color_update_gamut_to_stream(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams);
+
+bool mod_color_set_white_point(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		struct white_point_coodinates *white_point);
+
+bool mod_color_adjust_source_gamut(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		struct gamut_space_coordinates *input_gamut_coordinates,
+		struct white_point_coodinates *input_white_point_coordinates);
+
+bool mod_color_adjust_destination_gamut(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		struct gamut_space_coordinates *input_gamut_coordinates,
+		struct white_point_coodinates *input_white_point_coordinates);
+
+bool mod_color_get_user_enable(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		bool *user_enable);
+
+bool mod_color_set_user_enable(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		bool user_enable);
+
+bool mod_color_get_custom_color_temperature(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		int *color_temperature);
+
+bool mod_color_set_custom_color_temperature(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int color_temperature);
+
+bool mod_color_get_color_saturation(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_range *color_saturation);
+
+bool mod_color_get_color_contrast(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_range *color_contrast);
+
+bool mod_color_get_color_brightness(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_range *color_brightness);
+
+bool mod_color_get_color_hue(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_range *color_hue);
+
+bool mod_color_get_source_gamut(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		struct color_space_coordinates *source_gamut);
+
+bool mod_color_notify_mode_change(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams);
+
+bool mod_color_set_brightness(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int brightness_value);
+
+bool mod_color_set_contrast(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int contrast_value);
+
+bool mod_color_set_hue(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int hue_value);
+
+bool mod_color_set_saturation(struct mod_color *mod_color,
+		const struct dc_stream **streams, int num_streams,
+		int saturation_value);
+
+bool mod_color_set_preferred_quantization_range(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		enum dc_quantization_range quantization_range);
+
+bool mod_color_get_preferred_quantization_range(struct mod_color *mod_color,
+		const struct dc_sink *sink,
+		enum dc_quantization_range *quantization_range);
+
+#endif /* MOD_COLOR_H_ */
diff --git a/drivers/gpu/drm/amd/display/modules/inc/mod_power.h b/drivers/gpu/drm/amd/display/modules/inc/mod_power.h
new file mode 100644
index 0000000..a204e8d
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/modules/inc/mod_power.h
@@ -0,0 +1,112 @@
+/*
+ * Copyright 2016 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#ifndef MODULES_INC_MOD_POWER_H_
+#define MODULES_INC_MOD_POWER_H_
+
+#include "dm_services.h"
+
+struct mod_power {
+	int dummy;
+};
+
+/* VariBright related commands */
+enum varibright_command {
+	VariBright_Cmd__SetVBLevel = 0,
+	VariBright_Cmd__UserEnable,
+	VariBright_Cmd__PreDisplayConfigChange,
+	VariBright_Cmd__PostDisplayConfigChange,
+	VariBright_Cmd__SuspendABM,
+	VariBright_Cmd__ResumeABM,
+
+	VariBright_Cmd__Unknown,
+};
+
+/* VariBright settings structure */
+struct varibright_info {
+	enum varibright_command cmd;
+
+	unsigned int level;
+	bool enable;
+	bool activate;
+};
+
+enum dmcu_block_psr_reason {
+	/* This is a bitfield mask */
+	dmcu_block_psr_reason_invalid = 0x0,
+	dmcu_block_psr_reason_vsync_int = 0x1,
+	dmcu_block_psr_reason_shared_primary = 0x2,
+	dmcu_block_psr_reason_unsupported_link_rate = 0x4
+};
+
+struct mod_power *mod_power_create(struct dc *dc);
+
+void mod_power_destroy(struct mod_power *mod_power);
+
+bool mod_power_add_sink(struct mod_power *mod_power,
+		const struct dc_sink *sink);
+
+bool mod_power_remove_sink(struct mod_power *mod_power,
+		const struct dc_sink *sink);
+
+bool mod_power_set_backlight(struct mod_power *mod_power,
+		const struct dc_stream **streams, int num_streams,
+		unsigned int backlight_8bit);
+
+bool mod_power_get_backlight(struct mod_power *mod_power,
+		const struct dc_sink *sink,
+		unsigned int *backlight_8bit);
+
+void mod_power_initialize_backlight_caps
+		(struct mod_power *mod_power);
+
+unsigned int mod_power_backlight_level_percentage_to_signal
+		(struct mod_power *mod_power, unsigned int percentage);
+
+unsigned int mod_power_backlight_level_signal_to_percentage
+	(struct mod_power *mod_power, unsigned int signalLevel8bit);
+
+bool mod_power_get_panel_backlight_boundaries
+				(struct mod_power *mod_power,
+				unsigned int *min_backlight,
+				unsigned int *max_backlight,
+				unsigned int *output_ac_level_percentage,
+				unsigned int *output_dc_level_percentage);
+
+bool mod_power_set_smooth_brightness(struct mod_power *mod_power,
+		const struct dc_sink *sink, bool enable_brightness);
+
+bool mod_power_notify_mode_change(struct mod_power *mod_power,
+		const struct dc_stream *stream);
+
+bool mod_power_varibright_control(struct mod_power *mod_power,
+		struct varibright_info *input_varibright_info);
+
+bool mod_power_block_psr(bool block_enable, enum dmcu_block_psr_reason reason);
+
+bool mod_power_set_psr_enable(struct mod_power *mod_power,
+		bool psr_enable);
+
+#endif /* MODULES_INC_MOD_POWER_H_ */
diff --git a/drivers/gpu/drm/amd/display/modules/power/power.c b/drivers/gpu/drm/amd/display/modules/power/power.c
new file mode 100644
index 0000000..ea07e84
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/modules/power/power.c
@@ -0,0 +1,784 @@
+/*
+ * Copyright 2016 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "mod_power.h"
+#include "dm_services.h"
+#include "dc.h"
+#include "core_types.h"
+#include "core_dc.h"
+
+#define MOD_POWER_MAX_CONCURRENT_SINKS 32
+#define SMOOTH_BRIGHTNESS_ADJUSTMENT_TIME_IN_MS 500
+
+struct sink_caps {
+	const struct dc_sink *sink;
+};
+
+struct backlight_state {
+	unsigned int backlight;
+	unsigned int frame_ramp;
+	bool smooth_brightness_enabled;
+};
+
+struct core_power {
+	struct mod_power public;
+	struct dc *dc;
+	int num_sinks;
+	struct sink_caps *caps;
+	struct backlight_state *state;
+};
+
+union dmcu_abm_set_bl_params {
+	struct {
+		unsigned int gradual_change : 1; /* [0:0] */
+		unsigned int reserved : 15; /* [15:1] */
+		unsigned int frame_ramp : 16; /* [31:16] */
+	} bits;
+	unsigned int u32All;
+};
+
+/* Backlight cached properties */
+static unsigned int backlight_8bit_lut_array[101];
+static unsigned int ac_level_percentage;
+static unsigned int dc_level_percentage;
+static bool  backlight_caps_valid;
+/* we use lazy initialization of backlight capabilities cache */
+static bool backlight_caps_initialized;
+/* AC/DC levels initialized later in separate context */
+static bool  backlight_def_levels_valid;
+
+/* ABM cached properties */
+static unsigned int abm_level;
+static bool abm_user_enable;
+static bool abm_active;
+
+/*PSR cached properties*/
+static unsigned int block_psr;
+
+/* Defines default backlight curve F(x) = A(x*x) + Bx + C.
+ *
+ * Backlight curve should always  satisfy F(0) = min, F(100) = max,
+ * so polynom coefficients are:
+ * A is 0.0255 - B/100 - min/10000 - (255-max)/10000 = (max - min)/10000 - B/100
+ * B is adjustable factor to modify the curve.
+ * Bigger B results in less concave curve. B range is [0..(max-min)/100]
+ * C is backlight minimum
+ */
+static const unsigned int backlight_curve_coeff_a_factor = 10000;
+static const unsigned int backlight_curve_coeff_b        = 100;
+static const unsigned int backlight_curve_coeff_b_factor = 100;
+
+/* Minimum and maximum backlight input signal levels */
+static const unsigned int default_min_backlight          = 12;
+static const unsigned int default_max_backlight          = 255;
+
+/* Other backlight constants */
+static const unsigned int absolute_backlight_max         = 255;
+
+#define MOD_POWER_TO_CORE(mod_power)\
+		container_of(mod_power, struct core_power, public)
+
+static bool check_dc_support(const struct dc *dc)
+{
+	if (dc->stream_funcs.set_backlight == NULL)
+		return false;
+
+	return true;
+}
+
+/* Given a specific dc_sink* this function finds its equivalent
+ * on the dc_sink array and returns the corresponding index
+ */
+static unsigned int sink_index_from_sink(struct core_power *core_power,
+		const struct dc_sink *sink)
+{
+	unsigned int index = 0;
+
+	for (index = 0; index < core_power->num_sinks; index++)
+		if (core_power->caps[index].sink == sink)
+			return index;
+
+	/* Could not find sink requested */
+	ASSERT(false);
+	return index;
+}
+
+static unsigned int convertBL8to17(unsigned int backlight_8bit)
+{
+	unsigned int temp_ulong = backlight_8bit * 0x10101;
+	unsigned char temp_uchar =
+			(unsigned char)(((temp_ulong & 0x80) >> 7) & 1);
+
+	temp_ulong = (temp_ulong >> 8) + temp_uchar;
+
+	return temp_ulong;
+}
+
+static uint16_t convertBL8to16(unsigned int backlight_8bit)
+{
+	return (uint16_t)((backlight_8bit * 0x10101) >> 8);
+}
+
+/*This is used when OS wants to retrieve the current BL.
+ * We return the 8bit value to OS.
+ */
+static unsigned int convertBL17to8(unsigned int backlight_17bit)
+{
+	if (backlight_17bit & 0x10000)
+		return default_max_backlight;
+	else
+		return (backlight_17bit >> 8);
+}
+
+struct mod_power *mod_power_create(struct dc *dc)
+{
+	struct core_power *core_power =
+			dm_alloc(sizeof(struct core_power));
+
+	struct core_dc *core_dc = DC_TO_CORE(dc);
+
+	int i = 0;
+
+	if (core_power == NULL)
+		goto fail_alloc_context;
+
+	core_power->caps = dm_alloc(sizeof(struct sink_caps) *
+			MOD_POWER_MAX_CONCURRENT_SINKS);
+
+	if (core_power->caps == NULL)
+		goto fail_alloc_caps;
+
+	for (i = 0; i < MOD_POWER_MAX_CONCURRENT_SINKS; i++)
+		core_power->caps[i].sink = NULL;
+
+	core_power->state = dm_alloc(sizeof(struct backlight_state) *
+				MOD_POWER_MAX_CONCURRENT_SINKS);
+
+	if (core_power->state == NULL)
+		goto fail_alloc_state;
+
+	core_power->num_sinks = 0;
+	backlight_caps_valid = false;
+
+	if (dc == NULL)
+		goto fail_construct;
+
+	core_power->dc = dc;
+
+	if (!check_dc_support(dc))
+		goto fail_construct;
+
+	abm_user_enable = false;
+	abm_active = false;
+
+	return &core_power->public;
+
+fail_construct:
+	dm_free(core_power->state);
+
+fail_alloc_state:
+	dm_free(core_power->caps);
+
+fail_alloc_caps:
+	dm_free(core_power);
+
+fail_alloc_context:
+	return NULL;
+}
+
+
+void mod_power_destroy(struct mod_power *mod_power)
+{
+	if (mod_power != NULL) {
+		int i;
+		struct core_power *core_power =
+				MOD_POWER_TO_CORE(mod_power);
+
+		dm_free(core_power->state);
+
+		for (i = 0; i < core_power->num_sinks; i++)
+			dc_sink_release(core_power->caps[i].sink);
+
+		dm_free(core_power->caps);
+
+		dm_free(core_power);
+	}
+}
+
+bool mod_power_add_sink(struct mod_power *mod_power,
+						const struct dc_sink *sink)
+{
+	if (sink->sink_signal == SIGNAL_TYPE_VIRTUAL)
+		return false;
+
+	struct core_power *core_power =
+				MOD_POWER_TO_CORE(mod_power);
+	struct core_dc *core_dc = DC_TO_CORE(core_power->dc);
+
+	if (core_power->num_sinks < MOD_POWER_MAX_CONCURRENT_SINKS) {
+		dc_sink_retain(sink);
+		core_power->caps[core_power->num_sinks].sink = sink;
+		core_power->state[core_power->num_sinks].
+					smooth_brightness_enabled = false;
+		core_power->state[core_power->num_sinks].
+					backlight = 100;
+		core_power->num_sinks++;
+		return true;
+	}
+
+	return false;
+}
+
+bool mod_power_remove_sink(struct mod_power *mod_power,
+		const struct dc_sink *sink)
+{
+	int i = 0, j = 0;
+	struct core_power *core_power =
+			MOD_POWER_TO_CORE(mod_power);
+
+	for (i = 0; i < core_power->num_sinks; i++) {
+		if (core_power->caps[i].sink == sink) {
+			/* To remove this sink, shift everything after down */
+			for (j = i; j < core_power->num_sinks - 1; j++) {
+				core_power->caps[j].sink =
+					core_power->caps[j + 1].sink;
+
+				memcpy(&core_power->state[j],
+					&core_power->state[j + 1],
+					sizeof(struct backlight_state));
+			}
+			core_power->num_sinks--;
+			dc_sink_release(sink);
+			return true;
+		}
+	}
+	return false;
+}
+
+bool mod_power_set_backlight(struct mod_power *mod_power,
+		const struct dc_stream **streams, int num_streams,
+		unsigned int backlight_8bit)
+{
+	struct core_power *core_power =
+			MOD_POWER_TO_CORE(mod_power);
+
+	unsigned int frame_ramp = 0;
+
+	unsigned int stream_index, sink_index, vsync_rate_hz;
+
+	union dmcu_abm_set_bl_params params;
+
+	for (stream_index = 0; stream_index < num_streams; stream_index++) {
+		if (streams[stream_index]->sink->sink_signal == SIGNAL_TYPE_VIRTUAL) {
+			core_power->state[sink_index].backlight = 0;
+			core_power->state[sink_index].frame_ramp = 0;
+			core_power->state[sink_index].smooth_brightness_enabled = false;
+			continue;
+		}
+
+		sink_index = sink_index_from_sink(core_power,
+				streams[stream_index]->sink);
+
+		vsync_rate_hz = div64_u64(div64_u64((streams[stream_index]->
+				timing.pix_clk_khz * 1000),
+				streams[stream_index]->timing.v_total),
+				streams[stream_index]->timing.h_total);
+
+		core_power->state[sink_index].backlight = backlight_8bit;
+
+		if (core_power->state[sink_index].smooth_brightness_enabled)
+			frame_ramp = ((vsync_rate_hz *
+				SMOOTH_BRIGHTNESS_ADJUSTMENT_TIME_IN_MS) + 500)
+				/ 1000;
+		else
+			frame_ramp = 0;
+
+		core_power->state[sink_index].frame_ramp = frame_ramp;
+	}
+
+	params.u32All = 0;
+	params.bits.gradual_change = (frame_ramp > 0);
+	params.bits.frame_ramp = frame_ramp;
+
+	core_power->dc->stream_funcs.set_backlight
+		(core_power->dc, backlight_8bit, params.u32All, streams[0]);
+
+	return true;
+}
+
+bool mod_power_get_backlight(struct mod_power *mod_power,
+		const struct dc_sink *sink,
+		unsigned int *backlight_8bit)
+{
+	if (sink->sink_signal == SIGNAL_TYPE_VIRTUAL)
+		return false;
+
+	struct core_power *core_power =
+				MOD_POWER_TO_CORE(mod_power);
+
+	unsigned int sink_index = sink_index_from_sink(core_power, sink);
+
+	*backlight_8bit = core_power->state[sink_index].backlight;
+
+	return true;
+}
+
+/* hard coded to default backlight curve. */
+void mod_power_initialize_backlight_caps(struct mod_power
+							*mod_power)
+{
+	struct core_power *core_power =
+			MOD_POWER_TO_CORE(mod_power);
+	struct core_dc *core_dc = DC_TO_CORE(core_power->dc);
+	unsigned int i;
+
+	backlight_caps_initialized = true;
+
+	struct dm_acpi_atif_backlight_caps *pExtCaps = NULL;
+	bool customCurvePresent = false;
+	bool customMinMaxPresent = false;
+	bool customDefLevelsPresent = false;
+
+	/* Allocate memory for ATIF output
+	 * (do not want to use 256 bytes on the stack)
+	 */
+	pExtCaps = (struct dm_acpi_atif_backlight_caps *)
+			(dm_alloc(sizeof(struct dm_acpi_atif_backlight_caps)));
+	if (pExtCaps == NULL)
+		return;
+
+	/* Retrieve ACPI extended brightness caps */
+	if (dm_query_extended_brightness_caps
+			(core_dc->ctx, AcpiDisplayType_LCD1, pExtCaps)) {
+		ac_level_percentage    = pExtCaps->acLevelPercentage;
+		dc_level_percentage    = pExtCaps->dcLevelPercentage;
+		customMinMaxPresent    = true;
+		customDefLevelsPresent = true;
+		customCurvePresent     = (pExtCaps->numOfDataPoints > 0);
+
+		ASSERT(pExtCaps->numOfDataPoints <= 99);
+	} else {
+		dm_free(pExtCaps);
+		return;
+	}
+
+	if (customMinMaxPresent)
+		backlight_8bit_lut_array[0] = pExtCaps->minInputSignal;
+	else
+		backlight_8bit_lut_array[0] = default_min_backlight;
+
+	if (customMinMaxPresent)
+		backlight_8bit_lut_array[100] = pExtCaps->maxInputSignal;
+	else
+		backlight_8bit_lut_array[100] = default_max_backlight;
+
+	ASSERT(backlight_8bit_lut_array[100] <= absolute_backlight_max);
+	ASSERT(backlight_8bit_lut_array[0] <=
+					backlight_8bit_lut_array[100]);
+
+	/* Just to make sure we use valid values */
+	if (backlight_8bit_lut_array[100] > absolute_backlight_max)
+		backlight_8bit_lut_array[100] = absolute_backlight_max;
+	if (backlight_8bit_lut_array[0] > backlight_8bit_lut_array[100]) {
+		unsigned int swap;
+
+		swap = backlight_8bit_lut_array[0];
+		backlight_8bit_lut_array[0] = backlight_8bit_lut_array[100];
+		backlight_8bit_lut_array[100] = swap;
+	}
+
+	/* Build backlight translation table for custom curve */
+	if (customCurvePresent) {
+		unsigned int index = 1;
+		unsigned int numOfDataPoints =
+				(pExtCaps->numOfDataPoints <= 99 ?
+						pExtCaps->numOfDataPoints : 99);
+
+		/* Filling translation table from data points -
+		 * between every two provided data points we
+		 * lineary interpolate missing values
+		 */
+		for (i = 0; i < numOfDataPoints; i++) {
+			/* Clamp signal level between min and max
+			 * (since min and max might come other
+			 * soruce like registry)
+			 */
+			unsigned int luminance =
+					pExtCaps->dataPoints[i].luminance;
+			unsigned int signalLevel =
+					pExtCaps->dataPoints[i].signalLevel;
+
+			if (signalLevel < backlight_8bit_lut_array[0])
+				signalLevel = backlight_8bit_lut_array[0];
+			if (signalLevel > backlight_8bit_lut_array[100])
+				signalLevel = backlight_8bit_lut_array[100];
+
+			/* Lineary interpolate missing values */
+			if (index < luminance) {
+				unsigned int baseValue =
+					backlight_8bit_lut_array[index-1];
+				unsigned int deltaSignal =
+					signalLevel - baseValue;
+				unsigned int deltaLuma =
+					luminance - index + 1;
+				unsigned int step  = deltaSignal;
+
+				for (; index < luminance; index++) {
+					backlight_8bit_lut_array[index] =
+						baseValue + (step / deltaLuma);
+					step += deltaSignal;
+				}
+			}
+
+			/* Now [index == luminance],
+			 * so we can add data point to the translation table
+			 */
+			backlight_8bit_lut_array[index++] = signalLevel;
+		}
+
+		/* Complete the final segment of interpolation -
+		 * between last datapoint and maximum value
+		 */
+		if (index < 100) {
+			unsigned int baseValue =
+					backlight_8bit_lut_array[index-1];
+			unsigned int deltaSignal =
+					backlight_8bit_lut_array[100] -
+								baseValue;
+			unsigned int deltaLuma = 100 - index + 1;
+			unsigned int step = deltaSignal;
+
+			for (; index < 100; index++) {
+				backlight_8bit_lut_array[index] =
+						baseValue + (step / deltaLuma);
+				step += deltaSignal;
+			}
+		}
+	/* Build backlight translation table based on default curve */
+	} else {
+		unsigned int delta =
+				backlight_8bit_lut_array[100] -
+					backlight_8bit_lut_array[0];
+		unsigned int coeffC = backlight_8bit_lut_array[0];
+		unsigned int coeffB =
+				(backlight_curve_coeff_b < delta ?
+					backlight_curve_coeff_b : delta);
+		unsigned int coeffA = delta - coeffB; /* coeffB is B*100 */
+
+		for (i = 1; i < 100; i++) {
+			backlight_8bit_lut_array[i] =
+				(coeffA * i * i) /
+				backlight_curve_coeff_a_factor +
+				(coeffB * i) /
+				backlight_curve_coeff_b_factor +
+				coeffC;
+		}
+	}
+
+	if (pExtCaps != NULL)
+		dm_free(pExtCaps);
+
+	/* Successfully initialized */
+	backlight_caps_valid = true;
+	backlight_def_levels_valid = customDefLevelsPresent;
+}
+
+unsigned int mod_power_backlight_level_percentage_to_signal(
+		struct mod_power *mod_power, unsigned int percentage)
+{
+	/* Do lazy initialization of backlight capabilities*/
+	if (!backlight_caps_initialized)
+		mod_power_initialize_backlight_caps(mod_power);
+
+	/* Since the translation table is indexed by percentage,
+	* we simply return backlight value at given percent
+	*/
+	if (backlight_caps_valid && percentage <= 100)
+		return backlight_8bit_lut_array[percentage];
+
+	return -1;
+}
+
+unsigned int mod_power_backlight_level_signal_to_percentage(
+		struct mod_power *mod_power,
+		unsigned int signalLevel8bit)
+{
+	unsigned int invalid_backlight = (unsigned int)(-1);
+	/* Do lazy initialization of backlight capabilities */
+	if (!backlight_caps_initialized)
+		mod_power_initialize_backlight_caps(mod_power);
+
+	/* If customer curve cannot convert to differentiated value near min
+	* it is important to report 0 for min signal to pass setting "Dimmed"
+	* setting in HCK brightness2 tests.
+	*/
+	if (signalLevel8bit <= backlight_8bit_lut_array[0])
+		return 0;
+
+	/* Since the translation table is indexed by percentage
+	 * we need to do a binary search over the array
+	 * Another option would be to guess entry based on linear distribution
+	 * and then do linear search in correct direction
+	 */
+	if (backlight_caps_valid && signalLevel8bit <=
+					absolute_backlight_max) {
+		unsigned int min = 0;
+		unsigned int max = 100;
+		unsigned int mid = invalid_backlight;
+
+		while (max >= min) {
+			mid = (min + max) / 2; /* floor of half range */
+
+			if (backlight_8bit_lut_array[mid] < signalLevel8bit)
+				min = mid + 1;
+			else if (backlight_8bit_lut_array[mid] >
+							signalLevel8bit)
+				max = mid - 1;
+			else
+				break;
+
+			if (max == 0 || max == 1)
+				return invalid_backlight;
+		}
+		return mid;
+	}
+
+	return invalid_backlight;
+}
+
+
+bool mod_power_get_panel_backlight_boundaries(
+				struct mod_power *mod_power,
+				unsigned int *min_backlight,
+				unsigned int *max_backlight,
+				unsigned int *output_ac_level_percentage,
+				unsigned int *output_dc_level_percentage)
+{
+	/* Do lazy initialization of backlight capabilities */
+	if (!backlight_caps_initialized)
+		mod_power_initialize_backlight_caps(mod_power);
+
+	/* If cache was successfully updated,
+	 * copy the values to output structure and return success
+	 */
+	if (backlight_caps_valid) {
+		*min_backlight = backlight_8bit_lut_array[0];
+		*max_backlight = backlight_8bit_lut_array[100];
+
+		*output_ac_level_percentage = ac_level_percentage;
+		*output_dc_level_percentage = dc_level_percentage;
+
+		return true;
+	}
+
+	return false;
+}
+
+bool mod_power_set_smooth_brightness(struct mod_power *mod_power,
+		const struct dc_sink *sink, bool enable_brightness)
+{
+	if (sink->sink_signal == SIGNAL_TYPE_VIRTUAL)
+		return false;
+
+	struct core_power *core_power =
+			MOD_POWER_TO_CORE(mod_power);
+	unsigned int sink_index = sink_index_from_sink(core_power, sink);
+
+	core_power->state[sink_index].smooth_brightness_enabled
+					= enable_brightness;
+	return true;
+}
+
+bool mod_power_notify_mode_change(struct mod_power *mod_power,
+		const struct dc_stream *stream)
+{
+	if (stream->sink->sink_signal == SIGNAL_TYPE_VIRTUAL)
+		return false;
+
+	struct core_power *core_power =
+			MOD_POWER_TO_CORE(mod_power);
+
+	unsigned int sink_index = sink_index_from_sink(core_power,
+					stream->sink);
+	unsigned int frame_ramp = core_power->state[sink_index].frame_ramp;
+	union dmcu_abm_set_bl_params params;
+
+	params.u32All = 0;
+	params.bits.gradual_change = (frame_ramp > 0);
+	params.bits.frame_ramp = frame_ramp;
+
+	core_power->dc->stream_funcs.set_backlight
+			(core_power->dc,
+			core_power->state[sink_index].backlight,
+			params.u32All, stream);
+
+	core_power->dc->stream_funcs.setup_psr
+			(core_power->dc, stream);
+
+	return true;
+}
+
+
+static bool mod_power_abm_feature_enable(struct mod_power
+		*mod_power, bool enable)
+{
+	struct core_power *core_power =
+					MOD_POWER_TO_CORE(mod_power);
+	if (abm_user_enable == enable)
+		return true;
+
+	abm_user_enable = enable;
+
+	if (enable) {
+		if (abm_level != 0 && abm_active)
+			core_power->dc->stream_funcs.set_abm_level
+					(core_power->dc, abm_level);
+	} else {
+		if (abm_level != 0 && abm_active) {
+			abm_level = 0;
+			core_power->dc->stream_funcs.set_abm_level
+					(core_power->dc, abm_level);
+		}
+	}
+
+	return true;
+}
+
+static bool mod_power_abm_activate(struct mod_power
+		*mod_power, bool activate)
+{
+	struct core_power *core_power =
+					MOD_POWER_TO_CORE(mod_power);
+	if (abm_active == activate)
+		return true;
+
+	abm_active = activate;
+
+	if (activate) {
+		if (abm_level != 0 && abm_user_enable)
+			core_power->dc->stream_funcs.set_abm_level
+					(core_power->dc, abm_level);
+	} else {
+		if (abm_level != 0 && abm_user_enable) {
+			abm_level = 0;
+			core_power->dc->stream_funcs.set_abm_level
+					(core_power->dc, abm_level);
+		}
+	}
+
+	return true;
+}
+
+static bool mod_power_abm_set_level(struct mod_power *mod_power,
+		unsigned int level)
+{
+	struct core_power *core_power =
+					MOD_POWER_TO_CORE(mod_power);
+	if (abm_level == level)
+		return true;
+
+	if (abm_active && abm_user_enable && level == 0)
+		core_power->dc->stream_funcs.set_abm_level
+			(core_power->dc, 0);
+	else if (abm_active && abm_user_enable && level != 0)
+		core_power->dc->stream_funcs.set_abm_level
+				(core_power->dc, level);
+
+	abm_level = level;
+
+	return true;
+}
+
+bool mod_power_varibright_control(struct mod_power *mod_power,
+		struct varibright_info *input_varibright_info)
+{
+	switch (input_varibright_info->cmd) {
+	case VariBright_Cmd__SetVBLevel:
+	{
+		/* Set VariBright user level. */
+		mod_power_abm_set_level(mod_power,
+				input_varibright_info->level);
+	}
+	break;
+
+	case VariBright_Cmd__UserEnable:
+	{
+		/* Set VariBright user enable state. */
+		mod_power_abm_feature_enable(mod_power,
+				input_varibright_info->enable);
+	}
+	break;
+
+	case VariBright_Cmd__PostDisplayConfigChange:
+	{
+		/* Set VariBright user level. */
+		mod_power_abm_set_level(mod_power,
+						input_varibright_info->level);
+
+		/* Set VariBright user enable state. */
+		mod_power_abm_feature_enable(mod_power,
+				input_varibright_info->enable);
+
+		/* Set VariBright activate based on power state. */
+		mod_power_abm_activate(mod_power,
+				input_varibright_info->activate);
+	}
+	break;
+
+	default:
+	{
+		return false;
+	}
+	break;
+	}
+
+	return true;
+}
+
+bool mod_power_block_psr(bool block_enable, enum dmcu_block_psr_reason reason)
+{
+	if (block_enable)
+		block_psr |= reason;
+	else
+		block_psr &= ~reason;
+
+	return true;
+}
+
+
+bool mod_power_set_psr_enable(struct mod_power *mod_power,
+		bool psr_enable)
+{
+	struct core_power *core_power =
+				MOD_POWER_TO_CORE(mod_power);
+
+	if (block_psr == 0)
+		return core_power->dc->stream_funcs.set_psr_enable
+				(core_power->dc, psr_enable);
+
+	return false;
+}
+
+
diff --git a/include/uapi/drm/amdgpu_drm.h b/include/uapi/drm/amdgpu_drm.h
index 2849cb3..c0f6384 100644
--- a/include/uapi/drm/amdgpu_drm.h
+++ b/include/uapi/drm/amdgpu_drm.h
@@ -52,6 +52,7 @@ extern "C" {
 #define DRM_AMDGPU_GEM_USERPTR		0x11
 #define DRM_AMDGPU_WAIT_FENCES		0x12
 #define DRM_AMDGPU_VM			0x13
+#define DRM_AMDGPU_FREESYNC	        0x14
 
 /* hybrid specific ioctls */
 #define DRM_AMDGPU_SEM			0x5b
@@ -73,6 +74,7 @@ extern "C" {
 #define DRM_IOCTL_AMDGPU_GEM_USERPTR	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_GEM_USERPTR, struct drm_amdgpu_gem_userptr)
 #define DRM_IOCTL_AMDGPU_WAIT_FENCES	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_WAIT_FENCES, union drm_amdgpu_wait_fences)
 #define DRM_IOCTL_AMDGPU_VM		DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_VM, union drm_amdgpu_vm)
+#define DRM_IOCTL_AMDGPU_FREESYNC	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_FREESYNC, struct drm_amdgpu_freesync)
 
 /* hybrid specific ioctls */
 #define DRM_IOCTL_AMDGPU_GEM_DGMA      DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_GEM_DGMA, struct drm_amdgpu_gem_dgma)
@@ -994,6 +996,19 @@ struct drm_amdgpu_freesync {
 	__u32 spare[7];
 };
 
+/*
+ * Definition of free sync enter and exit signals
+ * We may have more options in the future
+ */
+#define AMDGPU_FREESYNC_FULLSCREEN_ENTER		1
+#define AMDGPU_FREESYNC_FULLSCREEN_EXIT 		2
+
+struct drm_amdgpu_freesync {
+	__u32 op;			/* AMDGPU_FREESYNC_FULLSCREEN_ENTER or */
+				        /* AMDGPU_FREESYNC_FULLSCREEN_ENTER */
+	__u32 spare[7];
+};
+
 #if defined(__cplusplus)
 }
 #endif
-- 
2.7.4

