From f77fba0c1934be9f570e6f31fe4923d790c0d79d Mon Sep 17 00:00:00 2001
From: Kent Russell <kent.russell@amd.com>
Date: Mon, 12 Dec 2016 08:59:35 -0500
Subject: [PATCH 5602/5855] drm/amdkfd: Consolidate and clean up log commands

Consolidate log commands so that dev_info(NULL, "Error...") uses the more
accurate pr_err, remove the module name from the log (can be seen via
dynamic debugging with +m), and the function name (can be seen via
dynamic debugging with +f). We also don't need debug messages saying
what function we're in. Those can be added by devs when needed

Lastly, remove parentheses around %d, %i and 0x%llX.
According to kernel.org:
"Printing numbers in parentheses (%d) adds no value and should be
avoided."

Change-Id: Id74c1d43d413d5c18735e59e55b7a56c90701f03
Signed-off-by: Kent Russell <kent.russell@amd.com>

 Conflicts:
	drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
	drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
---
 drivers/gpu/drm/amd/amdkfd/kfd_chardev.c           | 138 +++++++++------------
 drivers/gpu/drm/amd/amdkfd/kfd_crat.c              |  10 +-
 drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c            |  42 +++----
 drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c            |  86 ++++---------
 drivers/gpu/drm/amd/amdkfd/kfd_device.c            |  46 +++----
 .../gpu/drm/amd/amdkfd/kfd_device_queue_manager.c  |  77 +++++-------
 .../drm/amd/amdkfd/kfd_device_queue_manager_cik.c  |   4 +-
 .../drm/amd/amdkfd/kfd_device_queue_manager_v9.c   |   2 +-
 .../drm/amd/amdkfd/kfd_device_queue_manager_vi.c   |   4 +-
 drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c          |  20 +--
 drivers/gpu/drm/amd/amdkfd/kfd_events.c            |  28 ++---
 drivers/gpu/drm/amd/amdkfd/kfd_ipc.c               |   6 +-
 drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c      |  20 +--
 drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_cik.c  |   2 -
 drivers/gpu/drm/amd/amdkfd/kfd_module.c            |   4 +-
 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c   |  12 +-
 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c    |  10 +-
 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c    |  10 +-
 drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c    |  27 ++--
 drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c        |  61 +++++----
 drivers/gpu/drm/amd/amdkfd/kfd_process.c           |   6 +-
 .../gpu/drm/amd/amdkfd/kfd_process_queue_manager.c |  32 ++---
 drivers/gpu/drm/amd/amdkfd/kfd_rdma.c              |  11 +-
 drivers/gpu/drm/amd/amdkfd/kfd_topology.c          |   4 +-
 24 files changed, 273 insertions(+), 389 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
index 6ec0790..20c31e4 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
@@ -161,12 +161,12 @@ static int set_queue_properties_from_user(struct queue_properties *q_properties,
 				struct kfd_ioctl_create_queue_args *args)
 {
 	if (args->queue_percentage > KFD_MAX_QUEUE_PERCENTAGE) {
-		pr_err("kfd: queue percentage must be between 0 to KFD_MAX_QUEUE_PERCENTAGE\n");
+		pr_err("Queue percentage must be between 0 to KFD_MAX_QUEUE_PERCENTAGE\n");
 		return -EINVAL;
 	}
 
 	if (args->queue_priority > KFD_MAX_QUEUE_PRIORITY) {
-		pr_err("kfd: queue priority must be between 0 to KFD_MAX_QUEUE_PRIORITY\n");
+		pr_err("Queue priority must be between 0 to KFD_MAX_QUEUE_PRIORITY\n");
 		return -EINVAL;
 	}
 
@@ -174,26 +174,26 @@ static int set_queue_properties_from_user(struct queue_properties *q_properties,
 		(!access_ok(VERIFY_WRITE,
 			(const void __user *) args->ring_base_address,
 			sizeof(uint64_t)))) {
-		pr_err("kfd: can't access ring base address\n");
+		pr_err("Can't access ring base address\n");
 		return -EFAULT;
 	}
 
 	if (!is_power_of_2(args->ring_size) && (args->ring_size != 0)) {
-		pr_err("kfd: ring size must be a power of 2 or 0\n");
+		pr_err("Ring size must be a power of 2 or 0\n");
 		return -EINVAL;
 	}
 
 	if (!access_ok(VERIFY_WRITE,
 			(const void __user *) args->read_pointer_address,
 			sizeof(uint32_t))) {
-		pr_err("kfd: can't access read pointer\n");
+		pr_err("Can't access read pointer\n");
 		return -EFAULT;
 	}
 
 	if (!access_ok(VERIFY_WRITE,
 			(const void __user *) args->write_pointer_address,
 			sizeof(uint32_t))) {
-		pr_err("kfd: can't access write pointer\n");
+		pr_err("Can't access write pointer\n");
 		return -EFAULT;
 	}
 
@@ -201,7 +201,7 @@ static int set_queue_properties_from_user(struct queue_properties *q_properties,
 		!access_ok(VERIFY_WRITE,
 			(const void __user *) args->eop_buffer_address,
 			sizeof(uint32_t))) {
-		pr_debug("kfd: can't access eop buffer");
+		pr_debug("Can't access eop buffer");
 		return -EFAULT;
 	}
 
@@ -209,7 +209,7 @@ static int set_queue_properties_from_user(struct queue_properties *q_properties,
 		!access_ok(VERIFY_WRITE,
 			(const void __user *) args->ctx_save_restore_address,
 			sizeof(uint32_t))) {
-		pr_debug("kfd: can't access ctx save restore buffer");
+		pr_debug("Can't access ctx save restore buffer");
 		return -EFAULT;
 	}
 
@@ -239,27 +239,27 @@ static int set_queue_properties_from_user(struct queue_properties *q_properties,
 	else
 		q_properties->format = KFD_QUEUE_FORMAT_PM4;
 
-	pr_debug("Queue Percentage (%d, %d)\n",
+	pr_debug("Queue Percentage: %d, %d\n",
 			q_properties->queue_percent, args->queue_percentage);
 
-	pr_debug("Queue Priority (%d, %d)\n",
+	pr_debug("Queue Priority: %d, %d\n",
 			q_properties->priority, args->queue_priority);
 
-	pr_debug("Queue Address (0x%llX, 0x%llX)\n",
+	pr_debug("Queue Address: 0x%llX, 0x%llX\n",
 			q_properties->queue_address, args->ring_base_address);
 
-	pr_debug("Queue Size (0x%llX, %u)\n",
+	pr_debug("Queue Size: 0x%llX, %u\n",
 			q_properties->queue_size, args->ring_size);
 
-	pr_debug("Queue r/w Pointers (0x%llX, 0x%llX)\n",
-			(uint64_t) q_properties->read_ptr,
-			(uint64_t) q_properties->write_ptr);
+	pr_debug("Queue r/w Pointers: %p, %p\n",
+			q_properties->read_ptr,
+			q_properties->write_ptr);
 
-	pr_debug("Queue Format (%d)\n", q_properties->format);
+	pr_debug("Queue Format: %d\n", q_properties->format);
 
-	pr_debug("Queue EOP (0x%llX)\n", q_properties->eop_ring_buffer_address);
+	pr_debug("Queue EOP: 0x%llX\n", q_properties->eop_ring_buffer_address);
 
-	pr_debug("Queue CTX save arex (0x%llX)\n",
+	pr_debug("Queue CTX save area: 0x%llX\n",
 			q_properties->ctx_save_restore_area_address);
 
 	return 0;
@@ -277,16 +277,16 @@ static int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p,
 
 	memset(&q_properties, 0, sizeof(struct queue_properties));
 
-	pr_debug("kfd: creating queue ioctl\n");
+	pr_debug("Creating queue ioctl\n");
 
 	err = set_queue_properties_from_user(&q_properties, args);
 	if (err)
 		return err;
 
-	pr_debug("kfd: looking for gpu id 0x%x\n", args->gpu_id);
+	pr_debug("Looking for gpu id 0x%x\n", args->gpu_id);
 	dev = kfd_device_by_id(args->gpu_id);
 	if (dev == NULL) {
-		pr_debug("kfd: gpu id 0x%x was not found\n", args->gpu_id);
+		pr_debug("Could not find gpu id 0x%x\n", args->gpu_id);
 		return -EINVAL;
 	}
 
@@ -298,7 +298,7 @@ static int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p,
 		goto err_bind_process;
 	}
 
-	pr_debug("kfd: creating queue for PASID %d on GPU 0x%x\n",
+	pr_debug("Creating queue for PASID %d on gpu 0x%x\n",
 			p->pasid,
 			dev->id);
 
@@ -323,15 +323,15 @@ static int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p,
 
 	up_write(&p->lock);
 
-	pr_debug("kfd: queue id %d was created successfully\n", args->queue_id);
+	pr_debug("Queue id %d was created successfully\n", args->queue_id);
 
-	pr_debug("ring buffer address == 0x%016llX\n",
+	pr_debug("Ring buffer address == 0x%016llX\n",
 			args->ring_base_address);
 
-	pr_debug("read ptr address    == 0x%016llX\n",
+	pr_debug("Read ptr address    == 0x%016llX\n",
 			args->read_pointer_address);
 
-	pr_debug("write ptr address   == 0x%016llX\n",
+	pr_debug("Write ptr address   == 0x%016llX\n",
 			args->write_pointer_address);
 
 	return 0;
@@ -348,7 +348,7 @@ static int kfd_ioctl_destroy_queue(struct file *filp, struct kfd_process *p,
 	int retval;
 	struct kfd_ioctl_destroy_queue_args *args = data;
 
-	pr_debug("kfd: destroying queue id %d for PASID %d\n",
+	pr_debug("Destroying queue id %d for pasid %d\n",
 				args->queue_id,
 				p->pasid);
 
@@ -368,12 +368,12 @@ static int kfd_ioctl_update_queue(struct file *filp, struct kfd_process *p,
 	struct queue_properties properties;
 
 	if (args->queue_percentage > KFD_MAX_QUEUE_PERCENTAGE) {
-		pr_err("kfd: queue percentage must be between 0 to KFD_MAX_QUEUE_PERCENTAGE\n");
+		pr_err("Queue percentage must be between 0 to KFD_MAX_QUEUE_PERCENTAGE\n");
 		return -EINVAL;
 	}
 
 	if (args->queue_priority > KFD_MAX_QUEUE_PRIORITY) {
-		pr_err("kfd: queue priority must be between 0 to KFD_MAX_QUEUE_PRIORITY\n");
+		pr_err("Queue priority must be between 0 to KFD_MAX_QUEUE_PRIORITY\n");
 		return -EINVAL;
 	}
 
@@ -381,12 +381,12 @@ static int kfd_ioctl_update_queue(struct file *filp, struct kfd_process *p,
 		(!access_ok(VERIFY_WRITE,
 			(const void __user *) args->ring_base_address,
 			sizeof(uint64_t)))) {
-		pr_err("kfd: can't access ring base address\n");
+		pr_err("Can't access ring base address\n");
 		return -EFAULT;
 	}
 
 	if (!is_power_of_2(args->ring_size) && (args->ring_size != 0)) {
-		pr_err("kfd: ring size must be a power of 2 or 0\n");
+		pr_err("Ring size must be a power of 2 or 0\n");
 		return -EINVAL;
 	}
 
@@ -395,7 +395,7 @@ static int kfd_ioctl_update_queue(struct file *filp, struct kfd_process *p,
 	properties.queue_percent = args->queue_percentage;
 	properties.priority = args->queue_priority;
 
-	pr_debug("kfd: updating queue id %d for PASID %d\n",
+	pr_debug("Updating queue id %d for pasid %d\n",
 			args->queue_id, p->pasid);
 
 	down_write(&p->lock);
@@ -418,14 +418,14 @@ static int kfd_ioctl_set_cu_mask(struct file *filp, struct kfd_process *p,
 	size_t cu_mask_size = sizeof(uint32_t) * (args->num_cu_mask / 32);
 
 	if ((args->num_cu_mask % 32) != 0) {
-		pr_debug("kfd: num_cu_mask (0x%x) must be a multiple of 32",
+		pr_debug("num_cu_mask 0x%x must be a multiple of 32",
 				args->num_cu_mask);
 		return -EINVAL;
 	}
 
 	properties.cu_mask_count = args->num_cu_mask;
 	if (properties.cu_mask_count == 0) {
-		pr_debug("kfd: CU Mask cannot be 0");
+		pr_debug("CU mask cannot be 0");
 		return -EINVAL;
 	}
 
@@ -434,7 +434,7 @@ static int kfd_ioctl_set_cu_mask(struct file *filp, struct kfd_process *p,
 	 * past max_num_cus bits and just use the first max_num_cus bits.
 	 */
 	if (properties.cu_mask_count > max_num_cus) {
-		pr_debug("kfd: CU mask cannot be greater than 1024 bits");
+		pr_debug("CU mask cannot be greater than 1024 bits");
 		properties.cu_mask_count = max_num_cus;
 		cu_mask_size = sizeof(uint32_t) * (max_num_cus/32);
 	}
@@ -445,7 +445,7 @@ static int kfd_ioctl_set_cu_mask(struct file *filp, struct kfd_process *p,
 
 	retval = copy_from_user(properties.cu_mask, cu_mask_ptr, cu_mask_size);
 	if (retval) {
-		pr_debug("kfd: Could not copy cu mask from userspace");
+		pr_debug("Could not copy CU mask from userspace");
 		kfree(properties.cu_mask);
 		return -EFAULT;
 	}
@@ -553,12 +553,9 @@ kfd_ioctl_dbg_register(struct file *filep, struct kfd_process *p, void *data)
 	struct kfd_process_device *pdd;
 	bool create_ok = false;
 
-	pr_debug("kfd:dbg: %s\n", __func__);
-
 	dev = kfd_device_by_id(args->gpu_id);
 	if (!dev) {
-		dev_info(NULL, "Error! kfd: In func %s >> getting device by id failed\n",
-				__func__);
+		pr_err("Getting device by id failed in %s\n", __func__);
 		return status;
 	}
 
@@ -603,8 +600,7 @@ static int kfd_ioctl_dbg_unregister(struct file *filep,
 
 	dev = kfd_device_by_id(args->gpu_id);
 	if (!dev) {
-		dev_info(NULL, "Error! kfd: In func %s >> getting device by id failed\n",
-				__func__);
+		pr_err("Getting device by id failed in %s\n", __func__);
 		return status;
 	}
 
@@ -649,9 +645,7 @@ kfd_ioctl_dbg_address_watch(struct file *filep,
 
 	dev = kfd_device_by_id(args->gpu_id);
 	if (!dev) {
-		dev_info(NULL,
-		"Error! kfd: In func %s >> get device by id failed\n",
-		__func__);
+		pr_err("Getting device by id failed in %s\n", __func__);
 		return -EFAULT;
 	}
 
@@ -767,22 +761,17 @@ kfd_ioctl_dbg_wave_control(struct file *filep, struct kfd_process *p,
 				sizeof(wac_info.trapId);
 
 
-	dev_info(NULL, "kfd: In func %s - start\n", __func__);
-
 	dev = kfd_device_by_id(args->gpu_id);
 	if (!dev) {
-		dev_info(NULL, "Error! kfd: In func %s >> getting device by id failed\n",
-				__func__);
+		pr_err("Getting device by id failed in %s\n", __func__);
 		return -EFAULT;
 	}
 
 	/* input size must match the computed "compact" size */
 
 	if (args->buf_size_in_bytes != computed_buff_size) {
-		dev_info(NULL,
-			 "Error! kfd: In func %s >> size mismatch, computed : actual %u : %u\n",
-			__func__, args->buf_size_in_bytes,
-			computed_buff_size);
+		pr_err("Size mismatch, computed %u : actual %u\n",
+				args->buf_size_in_bytes, computed_buff_size);
 		return -EINVAL;
 	}
 
@@ -798,9 +787,8 @@ kfd_ioctl_dbg_wave_control(struct file *filep, struct kfd_process *p,
 	if (copy_from_user(args_buff,
 			(void __user *) args->content_ptr,
 			args->buf_size_in_bytes - sizeof(*args))) {
-		dev_info(NULL,
-		"Error! kfd: In func %s >> copy_from_user failed\n",
-		 __func__);
+		pr_err("copy_from_user failed\n");
+		status = -EFAULT;
 		goto out;
 	}
 
@@ -825,16 +813,14 @@ kfd_ioctl_dbg_wave_control(struct file *filep, struct kfd_process *p,
 
 	mutex_lock(get_dbgmgr_mutex());
 
-	dev_info(NULL,
-		"kfd: In func %s >> calling dbg manager process %p, operand %u, mode %u, trapId %u, message %u\n",
-		__func__, wac_info.process, wac_info.operand,
-		wac_info.mode, wac_info.trapId,
-		wac_info.dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value);
+	pr_info("Calling dbg manager process %p, operand %u, mode %u, trapId %u, message %u\n",
+			wac_info.process, wac_info.operand, wac_info.mode,
+			wac_info.trapId,
+			wac_info.dbgWave_msg.DbgWaveMsg.WaveMsgInfoGen2.Value);
 
 	status = kfd_dbgmgr_wave_control(dev->dbgmgr, &wac_info);
 
-	dev_info(NULL, "kfd: In func %s >> returned status of dbg manager is %ld\n",
-			__func__, status);
+	pr_info("Returned status of dbg manager is %ld\n", status);
 
 	mutex_unlock(get_dbgmgr_mutex());
 
@@ -1044,12 +1030,12 @@ kfd_ioctl_create_event(struct file *filp, struct kfd_process *p, void *data)
 	int err = -EINVAL;
 	void *mem, *kern_addr = NULL;
 
-	pr_debug("amdkfd: Event page offset 0x%llx\n", args->event_page_offset);
+	pr_debug("Event page offset 0x%llx\n", args->event_page_offset);
 
 	if (args->event_page_offset) {
 		kfd = kfd_device_by_id(GET_GPU_ID(args->event_page_offset));
 		if (!kfd) {
-			pr_err("amdkfd: can't find kfd device\n");
+			pr_err("Getting device by id failed in %s\n", __func__);
 			return -EFAULT;
 		}
 		if (KFD_IS_DGPU(kfd->device_info->asic_family)) {
@@ -1063,7 +1049,7 @@ kfd_ioctl_create_event(struct file *filp, struct kfd_process *p, void *data)
 			mem = kfd_process_device_translate_handle(pdd,
 				GET_IDR_HANDLE(args->event_page_offset));
 			if (!mem) {
-				pr_err("amdkfd: can't find BO offset is 0x%llx\n",
+				pr_err("Can't find BO, offset is 0x%llx\n",
 						args->event_page_offset);
 				up_write(&p->lock);
 				return -EFAULT;
@@ -1178,7 +1164,7 @@ bool kfd_is_large_bar(struct kfd_dev *dev)
 	struct kfd_local_mem_info mem_info;
 
 	if (debug_largebar) {
-		pr_debug("amdkfd: simulate large-bar allocation on non large-bar machine\n");
+		pr_debug("Simulate large-bar allocation on non large-bar machine\n");
 		return true;
 	}
 
@@ -1342,7 +1328,7 @@ static int kfd_ioctl_map_memory_to_gpu(struct file *filep,
 
 	if (args->device_ids_array_size > 0 &&
 			(args->device_ids_array_size < sizeof(uint32_t))) {
-		pr_err("amdkfd: err node IDs array size %u\n",
+		pr_err("Node IDs array size %u\n",
 				args->device_ids_array_size);
 		return -EFAULT;
 	}
@@ -1383,7 +1369,7 @@ static int kfd_ioctl_map_memory_to_gpu(struct file *filep,
 		for (i = 0 ; i < num_dev; i++) {
 			peer = kfd_device_by_id(devices_arr[i]);
 			if (!peer) {
-				pr_err("amdkfd: didn't found kfd-dev for 0x%x\n",
+				pr_err("Getting device by id failed for 0x%x\n",
 						devices_arr[i]);
 				err = -EFAULT;
 				goto get_mem_obj_from_handle_failed;
@@ -1397,12 +1383,12 @@ static int kfd_ioctl_map_memory_to_gpu(struct file *filep,
 			}
 			err = kfd_map_memory_to_gpu(mem, peer_pdd);
 			if (err != 0)
-				pr_err("amdkfd: failed to map\n");
+				pr_err("Failed to map\n");
 		}
 	} else {
 		err = kfd_map_memory_to_gpu(mem, pdd);
 		if (err != 0)
-			pr_err("amdkfd: failed to map\n");
+			pr_err("Failed to map\n");
 	}
 
 	if (args->device_ids_array_size > 0 && devices_arr)
@@ -1450,7 +1436,7 @@ static int kfd_ioctl_unmap_memory_from_gpu(struct file *filep,
 
 	if (args->device_ids_array_size > 0 &&
 			(args->device_ids_array_size < sizeof(uint32_t))) {
-		pr_err("amdkfd: err node IDs array size %u\n",
+		pr_err("Node IDs array size %u\n",
 				args->device_ids_array_size);
 		return -EFAULT;
 	}
@@ -1672,7 +1658,7 @@ static int kfd_ioctl_import_dmabuf(struct file *filep,
 	r = kfd_ipc_import_dmabuf(dev, p, args->gpu_id, args->dmabuf_fd,
 				  args->va_addr, &args->handle, NULL);
 	if (r)
-		dev_err(kfd_device, "Failed to import dmabuf\n");
+		pr_err("Failed to import dmabuf\n");
 
 	return r;
 }
@@ -1691,7 +1677,7 @@ static int kfd_ioctl_ipc_export_handle(struct file *filep,
 
 	r = kfd_ipc_export_as_handle(dev, p, args->handle, args->share_handle);
 	if (r)
-		dev_err(kfd_device, "Failed to export IPC handle\n");
+		pr_err("Failed to export IPC handle\n");
 
 	return r;
 }
@@ -1712,7 +1698,7 @@ static int kfd_ioctl_ipc_import_handle(struct file *filep,
 				  args->va_addr, &args->handle,
 				  &args->mmap_offset);
 	if (r)
-		dev_err(kfd_device, "Failed to import IPC handle\n");
+		pr_err("Failed to import IPC handle\n");
 
 	return r;
 }
@@ -1833,7 +1819,7 @@ static int kfd_ioctl_cross_memory_copy(struct file *filep,
 			pr_err("Cross mem copy failed. Permission error\n");
 			err = -EPERM;
 		} else
-			pr_err("Cross mem copy failed. Invalid task (%d)\n",
+			pr_err("Cross mem copy failed. Invalid task %d\n",
 			       err);
 		goto mm_access_fail;
 	}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_crat.c b/drivers/gpu/drm/amd/amdkfd/kfd_crat.c
index 55c5e4e..26f6e72 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_crat.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_crat.c
@@ -411,7 +411,7 @@ static int kfd_parse_subtype(struct crat_subtype_generic *sub_type_hdr,
 		ret = kfd_parse_subtype_iolink(iolink, device_list);
 		break;
 	default:
-		pr_warn("Unknown subtype (%d) in CRAT\n",
+		pr_warn("Unknown subtype %d in CRAT\n",
 				sub_type_hdr->type);
 	}
 
@@ -471,7 +471,7 @@ int kfd_parse_crat_table(void *crat_image,
 	sub_type_hdr = (struct crat_subtype_generic *)(crat_table+1);
 	while ((char *)sub_type_hdr + sizeof(struct crat_subtype_generic) <
 			((char *)crat_image) + image_len) {
-		pr_debug("kfd parsing crat sub type header %p enabled: %s type: 0x%x length %d\n",
+		pr_debug("Parsing CRAT subtype header %p enabled: %s type: 0x%x length %d\n",
 				sub_type_hdr,
 				(sub_type_hdr->flags &
 					CRAT_SUBTYPE_FLAGS_ENABLED)
@@ -480,9 +480,9 @@ int kfd_parse_crat_table(void *crat_image,
 				sub_type_hdr->length);
 
 		if (sub_type_hdr->length == 0) {
-			pr_err("amdkfd: Parsing wrong CRAT's sub header last header type: %d last header len %d\n",
+			pr_err("Parsing wrong CRAT's subtype header last header type: %d last header len %d\n",
 				last_header_type, last_header_type);
-			pr_err("amdkfd: Current header type %d length %d\n",
+			pr_err("Current header type %d length %d\n",
 				sub_type_hdr->type, sub_type_hdr->length);
 			break;
 		}
@@ -1008,7 +1008,7 @@ static int kfd_fill_gpu_memory_affinity(int *avail_size,
 
 	sub_type_hdr->proximity_domain = proximity_domain;
 
-	pr_debug("amdkfd: fill gpu memory affinity - type 0x%x size 0x%llx\n",
+	pr_debug("Fill gpu memory affinity - type 0x%x size 0x%llx\n",
 			type, size);
 
 	sub_type_hdr->length_low = lower_32_bits(size);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
index 0fdc147..07c42d2 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
@@ -66,8 +66,7 @@ static int dbgdev_diq_submit_ib(struct kfd_dbgdev *dbgdev,
 
 	if ((kq == NULL) || (packet_buff == NULL) ||
 			(size_in_bytes == 0)) {
-		pr_debug("Error! kfd: In func %s >> Illegal packet parameters\n",
-				__func__);
+		pr_err("Illegal packet parameters\n");
 		return -EINVAL;
 	}
 	/* todo - enter proper locking to be multithreaded safe */
@@ -83,8 +82,7 @@ static int dbgdev_diq_submit_ib(struct kfd_dbgdev *dbgdev,
 			pq_packets_size_in_bytes / sizeof(uint32_t),
 			&ib_packet_buff);
 	if (status != 0) {
-		pr_debug("Error! kfd: In func %s >> acquire_packet_buffer failed\n",
-				__func__);
+		pr_err("acquire_packet_buffer failed\n");
 		return status;
 	}
 
@@ -128,8 +126,7 @@ static int dbgdev_diq_submit_ib(struct kfd_dbgdev *dbgdev,
 					&mem_obj);
 
 	if (status != 0) {
-		pr_debug("Error! kfd: In func %s >> failed to allocate GART memory\n",
-					__func__);
+		pr_err("Failed to allocate GART memory\n");
 		return status;
 	}
 
@@ -197,8 +194,7 @@ static int dbgdev_register_diq(struct kfd_dbgdev *dbgdev)
 	struct process_queue_manager *pqm = dbgdev->pqm;
 
 	if (!pqm) {
-		pr_debug("Error! kfd: In func %s >> No PQM\n",
-				__func__);
+		pr_err("No PQM\n");
 		return -EFAULT;
 	}
 
@@ -208,18 +204,16 @@ static int dbgdev_register_diq(struct kfd_dbgdev *dbgdev)
 			&properties, &qid);
 
 	if (status != 0) {
-		pr_debug("Error! kfd: In func %s >> Create Queue failed\n",
-				__func__);
+		pr_err("Create Queue failed\n");
 		return status;
 	}
 
-	pr_debug("kfd: DIQ Created with queue id: %d\n", qid);
+	pr_debug("DIQ Created with queue id: %d\n", qid);
 
 	kq = pqm_get_kernel_queue(dbgdev->pqm, qid);
 
 	if (kq == NULL) {
-		pr_debug("Error! kfd: In func %s >> Error getting Kernel Queue\n",
-				__func__);
+		pr_err("Error getting Kernel Queue\n");
 		return -ENOMEM;
 	}
 	dbgdev->kq = kq;
@@ -242,7 +236,7 @@ static int dbgdev_unregister_diq(struct kfd_dbgdev *dbgdev)
 
 	if ((dbgdev == NULL) || (dbgdev->pqm == NULL) ||
 			(dbgdev->kq == NULL)) {
-		pr_debug("kfd Err:In func %s >> can't destroy diq\n", __func__);
+		pr_err("Can't destroy diq\n");
 		status = -EFAULT;
 	} else {
 		pqm_destroy_queue(dbgdev->pqm,
@@ -295,7 +289,7 @@ static void dbgdev_address_watch_set_registers(
 }
 
 static int dbgdev_address_watch_nodiq(struct kfd_dbgdev *dbgdev,
-					struct dbg_address_watch_info *adw_info)
+				      struct dbg_address_watch_info *adw_info)
 {
 
 	union TCP_WATCH_ADDR_H_BITS addrHi;
@@ -313,8 +307,7 @@ static int dbgdev_address_watch_nodiq(struct kfd_dbgdev *dbgdev,
 	pdd = kfd_get_process_device_data(dbgdev->dev,
 					adw_info->process);
 	if (!pdd) {
-		pr_debug("Error! kfd: In func %s >> no PDD available\n",
-				__func__);
+		pr_err("No PDD available\n");
 		return -EFAULT;
 	}
 
@@ -371,7 +364,7 @@ static int dbgdev_address_watch_nodiq(struct kfd_dbgdev *dbgdev,
 }
 
 static int dbgdev_address_watch_diq(struct kfd_dbgdev *dbgdev,
-					struct dbg_address_watch_info *adw_info)
+				    struct dbg_address_watch_info *adw_info)
 {
 
 	int status = 0;
@@ -519,8 +512,7 @@ static int dbgdev_address_watch_diq(struct kfd_dbgdev *dbgdev,
 					ib_size, true);
 
 		if (status != 0) {
-			pr_debug("Error! kfd: In func %s >> failed to submit DIQ packet\n",
-					__func__);
+			pr_err("Failed to submit DIQ packet\n");
 			return status;
 		}
 
@@ -593,22 +585,22 @@ static int dbgdev_wave_control_set_registers(
 	case HSA_DBG_WAVEOP_HALT:
 		if (asic_family == CHIP_KAVERI) {
 			reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_HALT;
-			pr_debug("kfd:dbgdev: halting KV\n");
+			pr_debug("Halting KV\n");
 		} else {
 			reg_sq_cmd.bits_sethalt.cmd  = SQ_IND_CMD_NEW_SETHALT;
 			reg_sq_cmd.bits_sethalt.data = SQ_IND_CMD_DATA_HALT;
-			pr_debug("kfd:dbgdev: halting CZ\n");
+			pr_debug("Halting CZ\n");
 		}
 		break;
 
 	case HSA_DBG_WAVEOP_RESUME:
 		if (asic_family == CHIP_KAVERI) {
 			reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_RESUME;
-			pr_debug("kfd:dbgdev: resuming KV\n");
+			pr_debug("Resuming KV\n");
 		} else {
 			reg_sq_cmd.bits_sethalt.cmd  = SQ_IND_CMD_NEW_SETHALT;
 			reg_sq_cmd.bits_sethalt.data = SQ_IND_CMD_DATA_RESUME;
-			pr_debug("kfd:dbgdev: resuming CZ\n");
+			pr_debug("Resuming CZ\n");
 		}
 		break;
 
@@ -870,7 +862,7 @@ int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p)
 	}
 
 	if (vmid > last_vmid_to_scan) {
-		pr_err("amdkfd: didn't find vmid for pasid (%d)\n", p->pasid);
+		pr_err("Didn't find vmid for pasid %d\n", p->pasid);
 		return -EFAULT;
 	}
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c b/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c
index 603cdc3..6a92a74 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_dbgmgr.c
@@ -69,8 +69,7 @@ bool kfd_dbgmgr_create(struct kfd_dbgmgr **ppmgr, struct kfd_dev *pdev)
 
 	new_buff = kfd_alloc_struct(new_buff);
 	if (!new_buff) {
-		dev_err(NULL, "Error! kfd: In func %s >> failed to allocate dbgmgr instance\n",
-				__func__);
+		pr_err("Failed to allocate dbgmgr instance\n");
 		return false;
 	}
 
@@ -78,8 +77,7 @@ bool kfd_dbgmgr_create(struct kfd_dbgmgr **ppmgr, struct kfd_dev *pdev)
 	new_buff->dev = pdev;
 	new_buff->dbgdev = kfd_alloc_struct(new_buff->dbgdev);
 	if (!new_buff->dbgdev) {
-		dev_err(NULL, "Error! kfd: In func %s >> failed to allocate dbgdev\n",
-				__func__);
+		pr_err("Failed to allocate dbgdev\n");
 		kfree(new_buff);
 		return false;
 	}
@@ -94,17 +92,11 @@ bool kfd_dbgmgr_create(struct kfd_dbgmgr **ppmgr, struct kfd_dev *pdev)
 	return true;
 }
 
-/*===========================================================================*/
-
-long
-kfd_dbgmgr_register(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
+long kfd_dbgmgr_register(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
 {
-	if ((!pmgr) || (!pmgr->dev) || (!pmgr->dbgdev)) {
-		dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n",
-				__func__);
-		/*  Invalid Pointer.  */
+	if ((!pmgr) || (!pmgr->dev) || (!pmgr->dbgdev))
 		return -EINVAL;
-	}
+
 	if (pmgr->pasid != 0) {
 		/*  HW debugger is already active.  */
 		return -EBUSY;
@@ -126,19 +118,13 @@ kfd_dbgmgr_register(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
 	return 0;
 }
 
-/* ========================================================================= */
-
-long
-kfd_dbgmgr_unregister(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
+long kfd_dbgmgr_unregister(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
 {
 
 	if ((pmgr == NULL) || (pmgr->dev == NULL) || (pmgr->dbgdev == NULL) ||
-			(p == NULL)) {
-		dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n",
-				__func__);
-		/* Invalid Pointer */
+			(p == NULL))
 		return -EINVAL;
-	}
+
 	if (pmgr->pasid != p->pasid) {
 		/* Is the requests coming from the already registered
 		 * process?
@@ -157,21 +143,13 @@ kfd_dbgmgr_unregister(struct kfd_dbgmgr *pmgr, struct kfd_process *p)
 	return 0;
 }
 
-/* ========================================================================= */
-
-long
-kfd_dbgmgr_wave_control(struct kfd_dbgmgr *pmgr,
+long kfd_dbgmgr_wave_control(struct kfd_dbgmgr *pmgr,
 		struct dbg_wave_control_info *wac_info)
 {
-	dev_info(NULL, "kfd: In func %s\n", __func__);
-
-	if ((!pmgr) || (!pmgr->dev) || (!pmgr->dbgdev) || (!wac_info)
-	    || (wac_info->process == NULL)) {
-		/* Invalid Pointer */
-		dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n",
-				__func__);
+	if ((!pmgr) || (!pmgr->dev) || (!pmgr->dbgdev) || (!wac_info) ||
+			(wac_info->process == NULL))
 		return -EINVAL;
-	}
+
 	/* Is the requests coming from the already registered
 	 * process?
 	 */
@@ -186,21 +164,13 @@ kfd_dbgmgr_wave_control(struct kfd_dbgmgr *pmgr,
 							  wac_info);
 }
 
-/* ========================================================================= */
-
-long
-kfd_dbgmgr_address_watch(struct kfd_dbgmgr *pmgr,
+long kfd_dbgmgr_address_watch(struct kfd_dbgmgr *pmgr,
 		struct dbg_address_watch_info *adw_info)
 {
-	dev_info(NULL, "kfd: In func %s\n", __func__);
-
-	if ((!pmgr) || (!pmgr->dev) || (!pmgr->dbgdev) || (!adw_info)
-	    || (adw_info->process == NULL)) {
-		/* Invalid Pointer */
-		dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n",
-				__func__);
+	if ((!pmgr) || (!pmgr->dev) || (!pmgr->dbgdev) || (!adw_info) ||
+			(adw_info->process == NULL))
 		return -EINVAL;
-	}
+
 	/* Is the requests coming from the already registered
 	 * process?
 	 */
@@ -216,29 +186,21 @@ kfd_dbgmgr_address_watch(struct kfd_dbgmgr *pmgr,
 }
 
 
-/* ========================================================================= */
 /*
  * Handle abnormal process termination
  * if we are in the midst of a debug session, we should kill all pending waves
  * of the debugged process and unregister the process from the Debugger.
  */
-long
-kfd_dbgmgr_abnormal_termination(struct kfd_dbgmgr *pmgr,
+long kfd_dbgmgr_abnormal_termination(struct kfd_dbgmgr *pmgr,
 		struct kfd_process *process)
 {
 	long status = 0;
 	struct dbg_wave_control_info wac_info;
 
-	dev_info(NULL, "kfd: In func %s\n", __func__);
-
-	if ((!pmgr) || (!pmgr->dev) || (!pmgr->dbgdev)) {
-		/* Invalid Pointer */
-		dev_info(NULL, "Error! kfd: In func %s >> Illegal pointers\n",
-				__func__);
+	if ((!pmgr) || (!pmgr->dev) || (!pmgr->dbgdev))
 		return -EINVAL;
-	}
-	/* first, we kill all the wavefronts of this process */
 
+	/* first, we kill all the wavefronts of this process */
 	wac_info.process = process;
 	wac_info.mode = HSA_DBG_WAVEMODE_BROADCAST_PROCESS;
 	wac_info.operand = HSA_DBG_WAVEOP_KILL;
@@ -252,8 +214,7 @@ kfd_dbgmgr_abnormal_termination(struct kfd_dbgmgr *pmgr,
 							  &wac_info);
 
 	if (status != 0) {
-		dev_info(NULL, "Error! kfd: In func %s: wave control failed, status is: %ld\n",
-				__func__, status);
+		pr_err("wave control failed, status is: %ld\n", status);
 		return status;
 	}
 	if (pmgr->pasid == wac_info.process->pasid) {
@@ -264,11 +225,8 @@ kfd_dbgmgr_abnormal_termination(struct kfd_dbgmgr *pmgr,
 		pmgr->pasid = 0;
 	}
 	if (status != 0)
-		dev_info(NULL,
-			"Error! kfd: In func %s: unregister failed, status is: %ld debugger can not be reused\n",
-			__func__, status);
+		pr_err("unregister failed, status is: %ld debugger can not be reused\n",
+				status);
 
 	return status;
 }
-
-/* ///////////////////////////////////////////////////////////////////////// */
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device.c b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
index 4b384c3..5794038 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
@@ -296,7 +296,7 @@ struct kfd_dev *kgd2kfd_probe(struct kgd_dev *kgd,
 		 */
 		if (pci_enable_atomic_ops_to_root(pdev) < 0) {
 			dev_info(kfd_device,
-				"skipped device (%x:%x), PCI rejects atomics",
+				"skipped device %x:%x, PCI rejects atomics",
 				 pdev->vendor, pdev->device);
 			return NULL;
 		}
@@ -340,7 +340,7 @@ static bool device_iommu_pasid_init(struct kfd_dev *kfd)
 	}
 
 	if ((iommu_info.flags & required_iommu_flags) != required_iommu_flags) {
-		dev_err(kfd_device, "error required iommu flags ats(%i), pri(%i), pasid(%i)\n",
+		dev_err(kfd_device, "error required iommu flags ats %i, pri %i, pasid %i\n",
 		       (iommu_info.flags & AMD_IOMMU_DEVICE_FLAG_ATS_SUP) != 0,
 		       (iommu_info.flags & AMD_IOMMU_DEVICE_FLAG_PRI_SUP) != 0,
 		       (iommu_info.flags & AMD_IOMMU_DEVICE_FLAG_PASID_SUP)
@@ -411,7 +411,7 @@ static int kfd_cwsr_init(struct kfd_dev *kfd)
                 }
 
                 if (size > PAGE_SIZE) {
-                        pr_err("amdkfd: wrong CWSR ISA size.\n");
+			pr_err("Wrong CWSR ISA size.\n");
                         return -EINVAL;
                 }
                 kfd->cwsr_size =
@@ -419,7 +419,7 @@ static int kfd_cwsr_init(struct kfd_dev *kfd)
                 kfd->cwsr_pages = alloc_pages(GFP_KERNEL | __GFP_HIGHMEM,
                                         get_order(kfd->cwsr_size));
                 if (!kfd->cwsr_pages) {
-                        pr_err("amdkfd: error alloc CWSR isa memory.\n");
+			pr_err("Failed to allocate CWSR isa memory.\n");
                         return -ENOMEM;
                 }
                 /*Only first page used for cwsr ISA code */
@@ -482,7 +482,7 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
        if ((hws_max_conc_proc < 0)
                        || (hws_max_conc_proc > vmid_num_kfd)) {
                dev_err(kfd_device,
-                       "hws_max_conc_proc (%d) must be between 0 and %d, use %d instead\n",
+			"hws_max_conc_proc %d must be between 0 and %d, use %d instead\n",
                        hws_max_conc_proc, vmid_num_kfd, vmid_num_kfd);
                kfd->max_proc_per_quantum = vmid_num_kfd;
        } else
@@ -510,13 +510,13 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
 			kfd->kgd, size, &kfd->gtt_mem,
 			&kfd->gtt_start_gpu_addr, &kfd->gtt_start_cpu_ptr)){
                 dev_err(kfd_device,
-                        "Could not allocate %d bytes for device (%x:%x)\n",
+                        "Could not allocate %d bytes for device %x:%x\n",
                         size, kfd->pdev->vendor, kfd->pdev->device);
 		goto out;
 	}
 
         dev_info(kfd_device,
-                "Allocated %d bytes on gart for device(%x:%x)\n",
+                "Allocated %d bytes on gart for device%x:%x\n",
                 size, kfd->pdev->vendor, kfd->pdev->device);
 
 	/* Initialize GTT sa with 512 byte chunk size */
@@ -530,14 +530,14 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
 
         if (kfd_topology_add_device(kfd) != 0) {
                 dev_err(kfd_device,
-                        "Error adding device (%x:%x) to topology\n",
+                        "Error adding device %x:%x to topology\n",
                         kfd->pdev->vendor, kfd->pdev->device);	
 		goto kfd_topology_add_device_error;
 	}
 
 	if (kfd_interrupt_init(kfd)) {
                 dev_err(kfd_device,
-                        "Error initializing interrupts for device (%x:%x)\n",
+                        "Error initializing interrupts for device %x:%x\n",
                         kfd->pdev->vendor, kfd->pdev->device);
 		goto kfd_interrupt_error;
 	}
@@ -546,7 +546,7 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
 	kfd->dqm = device_queue_manager_init(kfd);
 	if (!kfd->dqm) {
                 dev_err(kfd_device,
-                        "Error initializing queue manager for device (%x:%x)\n",
+                        "Error initializing queue manager for device %x:%x\n",
                         kfd->pdev->vendor, kfd->pdev->device);
 		goto device_queue_manager_error;
 	}
@@ -555,7 +555,7 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
         if (kfd->device_info->is_need_iommu_device) {
                 if (!device_iommu_pasid_init(kfd)) {
                         dev_err(kfd_device,
-                                "Error initializing iommuv2 for device (%x:%x)\n",
+                                "Error initializing iommuv2 for device %x:%x\n",
                                 kfd->pdev->vendor, kfd->pdev->device);
                         goto device_iommu_pasid_error;
                 }
@@ -576,9 +576,9 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
 	spin_lock_init(&kfd->tlb_invalidation_lock);
 
 	kfd->init_complete = true;
-	dev_info(kfd_device, "added device (%x:%x)\n", kfd->pdev->vendor
+	dev_info(kfd_device, "added device %x:%x\n", kfd->pdev->vendor
 		 kfd->pdev->device);
-        pr_debug("kfd: Starting kfd with the following scheduling policy %d\n",
+	pr_debug("Starting kfd with the following scheduling policy %d\n",
                 kfd->dqm->sched_policy);
 	goto out;
 
@@ -595,7 +595,7 @@ bool kgd2kfd_device_init(struct kfd_dev *kfd,
 kfd_gtt_sa_init_error:
 	kfd->kfd2kgd->free_gtt_mem(kfd->kgd, kfd->gtt_mem);
 	dev_err(kfd_device,
-		"device (%x:%x) NOT added due to errors\n",
+		"device %x:%x NOT added due to errors\n",
 		kfd->pdev->vendor, kfd->pdev->device);
 out:
 	return kfd->init_complete;
@@ -676,7 +676,7 @@ static int kfd_resume(struct kfd_dev *kfd)
        err = kfd->dqm->ops.start(kfd->dqm);
        if (err) {
                dev_err(kfd_device,
-                       "Error starting queue manager for device (%x:%x)\n",
+                       "Error starting queue manager for device %x:%x\n",
                        kfd->pdev->vendor, kfd->pdev->device);
                goto dqm_start_error;
        }
@@ -1007,7 +1007,7 @@ static int kfd_gtt_sa_init(struct kfd_dev *kfd, unsigned int buf_size,
 	if (!kfd->gtt_sa_bitmap)
 		return -ENOMEM;
 
-	pr_debug("kfd: gtt_sa_num_of_chunks = %d, gtt_sa_bitmap = %p\n"
+	pr_debug("gtt_sa_num_of_chunks = %d, gtt_sa_bitmap = %p\n",
 			kfd->gtt_sa_num_of_chunks, kfd->gtt_sa_bitmap);
 
 	mutex_init(&kfd->gtt_sa_lock);
@@ -1053,7 +1053,7 @@ int kfd_gtt_sa_allocate(struct kfd_dev *kfd, unsigned int size,
 	if ((*mem_obj) == NULL)
 		return -ENOMEM;
 
-	pr_debug("kfd: allocated mem_obj = %p for size = %d\n", *mem_obj, size);
+	pr_debug("Allocated mem_obj = %p for size = %d\n", *mem_obj, size);
 
 	start_search = 0;
 
@@ -1065,7 +1065,7 @@ int kfd_gtt_sa_allocate(struct kfd_dev *kfd, unsigned int size,
 					kfd->gtt_sa_num_of_chunks,
 					start_search);
 
-	pr_debug("kfd: found = %d\n", found);
+	pr_debug("Found = %d\n", found);
 
 	/* If there wasn't any free chunk, bail out */
 	if (found == kfd->gtt_sa_num_of_chunks)
@@ -1083,12 +1083,12 @@ int kfd_gtt_sa_allocate(struct kfd_dev *kfd, unsigned int size,
 					found,
 					kfd->gtt_sa_chunk_size);
 
-	pr_debug("kfd: gpu_addr = %p, cpu_addr = %p\n",
+	pr_debug("gpu_addr = %p, cpu_addr = %p\n",
 			(uint64_t *) (*mem_obj)->gpu_addr, (*mem_obj)->cpu_ptr);
 
 	/* If we need only one chunk, mark it as allocated and get out */
 	if (size <= kfd->gtt_sa_chunk_size) {
-		pr_debug("kfd: single bit\n");
+		pr_debug("Single bit\n");
 		set_bit(found, kfd->gtt_sa_bitmap);
 		goto kfd_gtt_out;
 	}
@@ -1123,7 +1123,7 @@ int kfd_gtt_sa_allocate(struct kfd_dev *kfd, unsigned int size,
 
 	} while (cur_size > 0);
 
-	pr_debug("kfd: range_start = %d, range_end = %d\n",
+	pr_debug("range_start = %d, range_end = %d\n",
 		(*mem_obj)->range_start, (*mem_obj)->range_end);
 
 	/* Mark the chunks as allocated */
@@ -1137,7 +1137,7 @@ int kfd_gtt_sa_allocate(struct kfd_dev *kfd, unsigned int size,
 	return 0;
 
 kfd_gtt_no_free_chunk:
-	pr_debug("kfd: allocation failed with mem_obj = %p\n", mem_obj);
+	pr_debug("Allocation failed with mem_obj = %p\n", mem_obj);
 	mutex_unlock(&kfd->gtt_sa_lock);
 	kfree(mem_obj);
 	return -ENOMEM;
@@ -1153,7 +1153,7 @@ int kfd_gtt_sa_free(struct kfd_dev *kfd, struct kfd_mem_obj *mem_obj)
 	if (!mem_obj)
 		return 0;
 
-	pr_debug("kfd: free mem_obj = %p, range_start = %d, range_end = %d\n",
+	pr_debug("Free mem_obj = %p, range_start = %d, range_end = %d\n",
 			mem_obj, mem_obj->range_start, mem_obj->range_end);
 
 	mutex_lock(&kfd->gtt_sa_lock);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
index c36e02f..9a48467 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
@@ -135,7 +135,7 @@ static int allocate_doorbell(struct qcm_process_device *qpd, struct queue *q)
 		found = find_first_zero_bit(qpd->doorbell_bitmap,
 					    KFD_MAX_NUM_OF_QUEUES_PER_PROCESS);
 		if (found >= KFD_MAX_NUM_OF_QUEUES_PER_PROCESS) {
-			pr_debug("amdkfd: No doorbells available");
+			pr_debug("No doorbells available");
 			return -EBUSY;
 		}
 		set_bit(found, qpd->doorbell_bitmap);
@@ -176,7 +176,7 @@ static int allocate_vmid(struct device_queue_manager *dqm,
 	dqm->vmid_bitmap &= ~(1 << bit);
 
 	allocated_vmid = bit + dqm->dev->vm_info.first_vmid_kfd;
-	pr_debug("kfd: vmid allocation %d\n", allocated_vmid);
+	pr_debug("vmid allocation %d\n", allocated_vmid);
 	qpd->vmid = allocated_vmid;
 	q->properties.vmid = allocated_vmid;
 
@@ -219,7 +219,7 @@ static void deallocate_vmid(struct device_queue_manager *dqm,
 	/* On GFX v7, CP doesn't flush TC at dequeue */
 	if (q->device->device_info->asic_family == CHIP_HAWAII)
 		if (flush_texture_cache_nocpsch(q->device, qpd))
-			pr_err("kfd: Failed to flush TC\n");
+			pr_err("Failed to flush TC\n");
 
 	/* Release the vmid mapping */
 	set_pasid_vmid_mapping(dqm, 0, qpd->vmid);
@@ -238,13 +238,12 @@ static int create_queue_nocpsch(struct device_queue_manager *dqm,
 
 	BUG_ON(!dqm || !q || !qpd || !allocated_vmid);
 
-	pr_debug("kfd: In func %s\n", __func__);
 	print_queue(q);
 
 	mutex_lock(&dqm->lock);
 
 	if (dqm->total_queue_count >= max_num_of_queues_per_device) {
-		pr_warn("amdkfd: Can't create new usermode queue because %d queues were already created\n",
+		pr_warn("Can't create new usermode queue because %d queues were already created\n",
 				dqm->total_queue_count);
 		mutex_unlock(&dqm->lock);
 		return -EPERM;
@@ -329,8 +328,7 @@ static int allocate_hqd(struct device_queue_manager *dqm, struct queue *q)
 	if (!set)
 		return -EBUSY;
 
-	pr_debug("kfd: DQM %s hqd slot - pipe (%d) queue(%d)\n",
-				__func__, q->pipe, q->queue);
+	pr_debug("hqd slot - pipe %d, queue %d\n", q->pipe, q->queue);
 	/* horizontal hqd allocation */
 	dqm->next_pipe_to_allocate = (pipe + 1) % get_pipes_per_mec(dqm);
 
@@ -369,9 +367,8 @@ static int create_compute_queue_nocpsch(struct device_queue_manager *dqm,
 	if (retval != 0)
 		goto out_deallocate_doorbell;
 
-	pr_debug("kfd: loading mqd to hqd on pipe (%d) queue (%d)\n",
-			q->pipe,
-			q->queue);
+	pr_debug("Loading mqd to hqd on pipe %d, queue %d\n",
+			q->pipe, q->queue);
 
 	dqm->dev->kfd2kgd->alloc_memory_of_scratch(
 			dqm->dev->kgd, qpd->sh_hidden_private_base, qpd->vmid);
@@ -423,8 +420,8 @@ static int destroy_queue_nocpsch_locked(struct device_queue_manager *dqm,
 		dqm->sdma_queue_count--;
 		deallocate_sdma_queue(dqm, q->sdma_id);
 	} else {
-		pr_debug("q->properties.type is invalid (%d)\n",
-			q->properties.type);
+		pr_debug("q->properties.type %d is invalid\n",
+				q->properties.type);
 		retval = -EINVAL;
 	}
 	dqm->total_queue_count--;
@@ -556,13 +553,13 @@ static struct mqd_manager *get_mqd_manager_nocpsch(
 
 	BUG_ON(!dqm || type >= KFD_MQD_TYPE_MAX);
 
-	pr_debug("kfd: In func %s mqd type %d\n", __func__, type);
+	pr_debug("mqd type %d\n", type);
 
 	mqd = dqm->mqds[type];
 	if (!mqd) {
 		mqd = mqd_manager_init(type, dqm->dev);
 		if (mqd == NULL)
-			pr_err("kfd: mqd manager is NULL");
+			pr_err("mqd manager is NULL");
 		dqm->mqds[type] = mqd;
 	}
 
@@ -648,7 +645,7 @@ int process_restore_queues(struct device_queue_manager *dqm,
 
 	/* Update PD Base in QPD */
 	qpd->page_table_base = pd_base;
-	pr_debug("Updated PD address to 0x%08x in %s\n", pd_base, __func__);
+	pr_debug("Updated PD address to 0x%08x\n", pd_base);
 
 	if (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS &&
 	    !list_empty(&qpd->queues_list)) {
@@ -703,8 +700,6 @@ static int register_process(struct device_queue_manager *dqm,
 
 	BUG_ON(!dqm || !qpd);
 
-	pr_debug("In func %s\n", __func__);
-
 	n = kzalloc(sizeof(struct device_process_node), GFP_KERNEL);
 	if (!n)
 		return -ENOMEM;
@@ -720,7 +715,7 @@ static int register_process(struct device_queue_manager *dqm,
 
 	/* Update PD Base in QPD */
 	qpd->page_table_base = pd_base;
-	pr_debug("Updated PD address to 0x%08x in %s\n", pd_base, __func__);
+	pr_debug("Updated PD address to 0x%08x\n", pd_base);
 
 	retval = dqm->asic_ops.update_qpd(dqm, qpd);
 
@@ -739,8 +734,6 @@ static int unregister_process(struct device_queue_manager *dqm,
 
 	BUG_ON(!dqm || !qpd);
 
-	pr_debug("In func %s\n", __func__);
-
 	pr_debug("qpd->queues_list is %s\n",
 			list_empty(&qpd->queues_list) ? "empty" : "not empty");
 
@@ -803,8 +796,7 @@ static int initialize_nocpsch(struct device_queue_manager *dqm)
 	int i;
 	BUG_ON(!dqm);
 
-        pr_debug("kfd: In func %s num of pipes: %d\n",
-                        __func__, get_pipes_per_mec(dqm));
+	pr_debug("num of pipes: %d\n", get_pipes_per_mec(dqm));
 
         dqm->allocated_queues = kcalloc(get_pipes_per_mec(dqm),
                                        sizeof(unsigned int), GFP_KERNEL);
@@ -898,9 +890,9 @@ static int create_sdma_queue_nocpsch(struct device_queue_manager *dqm,
         if (retval)
                 goto out_deallocate_sdma_queue;
 
-        pr_debug("kfd: sdma id is:    %d\n", q->sdma_id);
-        pr_debug("     sdma queue id: %d\n", q->properties.sdma_queue_id);
-        pr_debug("     sdma engine id: %d\n", q->properties.sdma_engine_id);
+        pr_debug("SDMA id is:    %d\n", q->sdma_id);
+        pr_debug("SDMA queue id: %d\n", q->properties.sdma_queue_id);
+        pr_debug("SDMA engine id: %d\n", q->properties.sdma_engine_id);
 
 	dqm->asic_ops.init_sdma_vm(dqm, q, qpd);
 	retval = mqd->init_mqd(mqd, &q->mqd, &q->mqd_mem_obj,
@@ -936,7 +928,6 @@ static int set_sched_resources(struct device_queue_manager *dqm)
 
         BUG_ON(!dqm);
 
-        pr_debug("kfd: In func %s\n", __func__);
 
 	res.vmid_mask = dqm->dev->shared_resources.compute_vmid_bitmap;
 	
@@ -964,9 +955,9 @@ static int set_sched_resources(struct device_queue_manager *dqm)
 	}
 	res.gws_mask = res.oac_mask = res.gds_heap_base =
 						res.gds_heap_size = 0;
-        pr_debug("kfd: scheduling resources:\n"
-                        "      vmid mask: 0x%8X\n"
-                        "      queue mask: 0x%8llX\n",
+        pr_debug("Scheduling resources:\n"
+                        "vmid mask: 0x%8X\n"
+                        "queue mask: 0x%8llX\n",
 			res.vmid_mask, res.queue_mask);
 
 	return pm_send_set_resources(&dqm->packets, &res);
@@ -978,8 +969,7 @@ static int initialize_cpsch(struct device_queue_manager *dqm)
 
 	BUG_ON(!dqm);
 
-	pr_debug("kfd: In func %s num of pipes: %d\n",
-			__func__, get_pipes_per_mec(dqm));
+	pr_debug("num of pipes: %d\n", get_pipes_per_mec(dqm));
 
 	mutex_init(&dqm->lock);
 	INIT_LIST_HEAD(&dqm->queues);
@@ -1014,7 +1004,7 @@ static int start_cpsch(struct device_queue_manager *dqm)
 	if (retval != 0)
 		goto fail_set_sched_resources;
 
-	pr_debug("kfd: allocating fence memory\n");
+	pr_debug("Allocating fence memory\n");
 
 	/* allocate fence memory on the gart */
 	retval = kfd_gtt_sa_allocate(dqm->dev, sizeof(*dqm->fence_addr),
@@ -1062,11 +1052,10 @@ static int create_kernel_queue_cpsch(struct device_queue_manager *dqm,
 {
 	BUG_ON(!dqm || !kq || !qpd);
 
-	pr_debug("kfd: In func %s\n", __func__);
 
 	mutex_lock(&dqm->lock);
 	if (dqm->total_queue_count >= max_num_of_queues_per_device) {
-		pr_warn("amdkfd: Can't create new kernel queue because %d queues were already created\n",
+		pr_warn("Can't create new kernel queue because %d queues were already created\n",
 				dqm->total_queue_count);
 		mutex_unlock(&dqm->lock);
 		return -EPERM;
@@ -1095,8 +1084,6 @@ static void destroy_kernel_queue_cpsch(struct device_queue_manager *dqm,
 {
 	BUG_ON(!dqm || !kq);
 
-	pr_debug("kfd: In %s\n", __func__);
-
 	 mutex_lock(&dqm->lock);
 	/* here we actually preempt the DIQ */
 	list_del(&kq->list);
@@ -1129,7 +1116,7 @@ static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 	mutex_lock(&dqm->lock);
 
 	if (dqm->total_queue_count >= max_num_of_queues_per_device) {
-		pr_warn("amdkfd: Can't create new usermode queue because %d queues were already created\n",
+		pr_warn("Can't create new usermode queue because %d queues were already created\n",
 				dqm->total_queue_count);
 		retval = -EPERM;
 		goto out_unlock;
@@ -1217,7 +1204,7 @@ int amdkfd_fence_wait_timeout(unsigned int *fence_addr,
 
 	while (*fence_addr != fence_value) {
 		if (time_after(jiffies, end_jiffies)) {
-			pr_err("kfd: qcm fence wait loop timeout expired\n");
+			pr_err("qcm fence wait loop timeout expired\n");
 			return -ETIME;
 		}
 		schedule();
@@ -1273,7 +1260,7 @@ static int unmap_queues_cpsch(struct device_queue_manager *dqm,
 	if (!dqm->active_runlist)
 		return retval;
 
-	pr_debug("kfd: Before destroying queues, sdma queue count is : %u\n",
+	pr_debug("Before destroying queues, sdma queue count is : %u\n",
 		dqm->sdma_queue_count);
 
 	if (dqm->sdma_queue_count > 0) {
@@ -1293,7 +1280,7 @@ static int unmap_queues_cpsch(struct device_queue_manager *dqm,
 	retval = amdkfd_fence_wait_timeout(dqm->fence_addr, KFD_FENCE_COMPLETED,
 				QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS);
 	if (retval != 0) {
-		pr_err("kfd: unmapping queues failed.");
+		pr_err("Unmapping queues failed.");
 		return retval;
 	}
 
@@ -1312,15 +1299,13 @@ static int execute_queues_cpsch(struct device_queue_manager *dqm,
 
 	BUG_ON(!dqm);
 
-	pr_debug("In function %s\n", __func__);
-
 	filter = static_queues_included ?
 			KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES :
 			KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES;
 
 	retval = unmap_queues_cpsch(dqm, filter, 0, false);
 	if (retval != 0) {
-		pr_err("kfd: the cp might be in an unrecoverable state due to an unsuccessful queues preemption");
+		pr_err("The cp might be in an unrecoverable state due to an unsuccessful queues preemption");
 		return retval;
 	}
 
@@ -1414,8 +1399,6 @@ static bool set_cache_memory_policy(struct device_queue_manager *dqm,
 {
 	bool retval = true;
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	if (!dqm->asic_ops.set_cache_memory_policy)
 		return retval;
 
@@ -1463,7 +1446,7 @@ static bool set_cache_memory_policy(struct device_queue_manager *dqm,
 	if ((dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) && (qpd->vmid != 0))
 		program_sh_mem_settings(dqm, qpd);
 
-	pr_debug("kfd: sh_mem_config: 0x%x, ape1_base: 0x%x, ape1_limit: 0x%x\n",
+	pr_debug("sh_mem_config: 0x%x, ape1_base: 0x%x, ape1_limit: 0x%x\n",
 		qpd->sh_mem_config, qpd->sh_mem_ape1_base,
 		qpd->sh_mem_ape1_limit);
 
@@ -1595,7 +1578,7 @@ struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 
 	BUG_ON(!dev);
 
-	pr_debug("kfd: loading device queue manager\n");
+	pr_debug("Loading device queue manager\n");
 
 	dqm = kzalloc(sizeof(struct device_queue_manager), GFP_KERNEL);
 	if (!dqm)
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_cik.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_cik.c
index bf24368..f051e0d 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_cik.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_cik.c
@@ -144,7 +144,7 @@ static int update_qpd_cik(struct device_queue_manager *dqm,
 		qpd->sh_mem_config |= 1  << SH_MEM_CONFIG__PRIVATE_ATC__SHIFT;
 	}
 
-	pr_debug("kfd: is32bit process: %d sh_mem_bases nybble: 0x%X and register 0x%X\n",
+	pr_debug("is32bit process: %d sh_mem_bases nybble: 0x%X and register 0x%X\n",
 		qpd->pqm->process->is_32bit_user_mode, temp, qpd->sh_mem_bases);
 
 	return 0;
@@ -176,7 +176,7 @@ static int update_qpd_cik_hawaii(struct device_queue_manager *dqm,
 	temp = get_sh_mem_bases_nybble_64(pdd);
 	qpd->sh_mem_bases = compute_sh_mem_bases_64bit(temp);
 
-	pr_debug("kfd: is32bit process: %d sh_mem_bases nybble: 0x%X and register 0x%X\n",
+	pr_debug("is32bit process: %d sh_mem_bases nybble: 0x%X and register 0x%X\n",
 		qpd->pqm->process->is_32bit_user_mode, temp, qpd->sh_mem_bases);
 
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_v9.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_v9.c
index 786ea88..dde5882 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_v9.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_v9.c
@@ -72,7 +72,7 @@ static int update_qpd_v9(struct device_queue_manager *dqm,
 
 	qpd->sh_mem_bases = compute_sh_mem_bases_64bit(pdd);
 
-	pr_debug("kfd: sh_mem_bases 0x%X\n", qpd->sh_mem_bases);
+	pr_debug("sh_mem_bases 0x%X\n", qpd->sh_mem_bases);
 
 	return 0;
 }
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_vi.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_vi.c
index 2f37b04..f6c6bea 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_vi.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager_vi.c
@@ -194,7 +194,7 @@ static int update_qpd_vi(struct device_queue_manager *dqm,
 			SH_MEM_CONFIG__PRIVATE_ATC__SHIFT;
 	}
 
-	pr_debug("kfd: is32bit process: %d sh_mem_bases nybble: 0x%X and register 0x%X\n",
+	pr_debug("is32bit process: %d sh_mem_bases nybble: 0x%X and register 0x%X\n",
 		qpd->pqm->process->is_32bit_user_mode, temp, qpd->sh_mem_bases);
 
 	return 0;
@@ -230,7 +230,7 @@ static int update_qpd_vi_tonga(struct device_queue_manager *dqm,
 	temp = get_sh_mem_bases_nybble_64(pdd);
 	qpd->sh_mem_bases = compute_sh_mem_bases_64bit(temp);
 
-	pr_debug("kfd: sh_mem_bases nybble: 0x%X and register 0x%X\n",
+	pr_debug("sh_mem_bases nybble: 0x%X and register 0x%X\n",
 		temp, qpd->sh_mem_bases);
 
 	return 0;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c b/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
index 9a86b98..899fc4c 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
@@ -96,23 +96,23 @@ void kfd_doorbell_init(struct kfd_dev *kfd)
 
 	BUG_ON(!kfd->doorbell_kernel_ptr);
 
-	pr_debug("kfd: doorbell initialization:\n");
-	pr_debug("kfd: doorbell base           == 0x%08lX\n",
+	pr_debug("Doorbell initialization:\n");
+	pr_debug("doorbell base           == 0x%08lX\n",
 			(uintptr_t)kfd->doorbell_base);
 
-	pr_debug("kfd: doorbell_id_offset      == 0x%08lX\n",
+	pr_debug("doorbell_id_offset      == 0x%08lX\n",
 			kfd->doorbell_id_offset);
 
-	pr_debug("kfd: doorbell_process_limit  == 0x%08lX\n",
+	pr_debug("doorbell_process_limit  == 0x%08lX\n",
 			doorbell_process_limit);
 
-	pr_debug("kfd: doorbell_kernel_offset  == 0x%08lX\n",
+	pr_debug("doorbell_kernel_offset  == 0x%08lX\n",
 			(uintptr_t)kfd->doorbell_base);
 
-	pr_debug("kfd: doorbell aperture size  == 0x%08lX\n",
+	pr_debug("doorbell aperture size  == 0x%08lX\n",
 			kfd->shared_resources.doorbell_aperture_size);
 
-	pr_debug("kfd: doorbell kernel address == 0x%08lX\n",
+	pr_debug("doorbell kernel address == 0x%08lX\n",
 			(uintptr_t)kfd->doorbell_kernel_ptr);
 }
 
@@ -136,7 +136,7 @@ int kfd_doorbell_mmap(struct kfd_dev *dev, struct kfd_process *process,
 
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
-	pr_debug("kfd: mapping doorbell page in kfd_doorbell_mmap\n"
+	pr_debug("Mapping doorbell page in kfd_doorbell_mmap\n"
 		 "     target user address == 0x%08llX\n"
 		 "     physical address    == 0x%08llX\n"
 		 "     vm_flags            == 0x%04lX\n"
@@ -181,7 +181,7 @@ void __iomem *kfd_get_kernel_doorbell(struct kfd_dev *kfd,
 	*doorbell_off = KERNEL_DOORBELL_PASID *
 		(kfd_doorbell_process_slice(kfd) / sizeof(u32)) + inx;
 
-	pr_debug("kfd: get kernel queue doorbell\n"
+	pr_debug("Get kernel queue doorbell\n"
 			 "     doorbell offset   == 0x%08X\n"
 			 "     kernel address    == 0x%08lX\n",
 		*doorbell_off, (uintptr_t)(kfd->doorbell_kernel_ptr + inx));
@@ -206,7 +206,7 @@ void write_kernel_doorbell(void __iomem *db, u32 value)
 {
 	if (db) {
 		writel(value, db);
-		pr_debug("writing %d to doorbell address 0x%p\n", value, db);
+		pr_debug("Writing %d to doorbell address 0x%p\n", value, db);
 	}
 }
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_events.c b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
index 18198d8..ac9a463 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_events.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
@@ -113,7 +113,7 @@ allocate_free_slot(struct kfd_process *process,
 			*out_page = page;
 			*out_slot_index = slot;
 
-			pr_debug("allocated event signal slot in page %p, slot %d\n",
+			pr_debug("Allocated event signal slot in page %p, slot %d\n",
 					page, slot);
 
 			return true;
@@ -167,9 +167,9 @@ static bool allocate_signal_page(struct file *devkfd, struct kfd_process *p)
 						   struct signal_page,
 						   event_pages)->page_index + 1;
 
-	pr_debug("allocated new event signal page at %p, for process %p\n",
+	pr_debug("Allocated new event signal page at %p, for process %p\n",
 			page, p);
-	pr_debug("page index is %d\n", page->page_index);
+	pr_debug("Page index is %d\n", page->page_index);
 
 	list_add(&page->event_pages, &p->signal_event_pages);
 
@@ -223,9 +223,9 @@ allocate_signal_page_dgpu(struct kfd_process *p,
 		   struct signal_page,
 		   event_pages)->page_index + 1;
 
-	pr_debug("allocated new event signal page at %p, for process %p\n",
+	pr_debug("Allocated new event signal page at %p, for process %p\n",
 			my_page, p);
-	pr_debug("page index is %d\n", my_page->page_index);
+	pr_debug("Page index is %d\n", my_page->page_index);
 
 	list_add(&my_page->event_pages, &p->signal_event_pages);
 
@@ -274,7 +274,7 @@ allocate_debug_event_notification_slot(struct file *devkfd,
 	*out_page = page;
 	*out_slot_index = slot;
 
-	pr_debug("allocated debug event signal slot in page %p, slot %d\n",
+	pr_debug("Allocated debug event signal slot in page %p, slot %d\n",
 			page, slot);
 
 	return true;
@@ -388,11 +388,11 @@ create_signal_event(struct file *devkfd, struct kfd_process *p,
 {
 	if ((ev->type == KFD_EVENT_TYPE_SIGNAL) &&
 			(p->signal_event_count == KFD_SIGNAL_EVENT_LIMIT)) {
-		pr_warn("amdkfd: Signal event wasn't created because limit was reached\n");
+		pr_warn("Signal event wasn't created because limit was reached\n");
 		return -ENOMEM;
 	} else if ((ev->type == KFD_EVENT_TYPE_DEBUG) &&
 			(p->debug_event_count == KFD_DEBUG_EVENT_LIMIT)) {
-		pr_warn("amdkfd: Debug event wasn't created because limit was reached\n");
+		pr_warn("Debug event wasn't created because limit was reached\n");
 		return -ENOMEM;
 	}
 
@@ -400,7 +400,7 @@ create_signal_event(struct file *devkfd, struct kfd_process *p,
 		if (!allocate_event_notification_slot(devkfd, p,
 						&ev->signal_page,
 						&ev->signal_slot_index)) {
-			pr_warn("amdkfd: Signal event wasn't created because out of kernel memory\n");
+			pr_warn("Signal event wasn't created because out of kernel memory\n");
 			return -ENOMEM;
 		}
 
@@ -414,7 +414,7 @@ create_signal_event(struct file *devkfd, struct kfd_process *p,
 		if (!allocate_debug_event_notification_slot(devkfd, p,
 						&ev->signal_page,
 						&ev->signal_slot_index)) {
-			pr_warn("amdkfd: Debug event wasn't created because out of kernel memory\n");
+			pr_warn("Debug event wasn't created because out of kernel memory\n");
 			return -ENOMEM;
 		}
 
@@ -427,7 +427,7 @@ create_signal_event(struct file *devkfd, struct kfd_process *p,
 	ev->event_id =
 		make_signal_event_id(ev->signal_page, ev->signal_slot_index);
 
-	pr_debug("signal event number %zu created with id %d, address %p\n",
+	pr_debug("Signal event number %zu created with id %d, address %p\n",
 			p->signal_event_count, ev->event_id,
 			ev->user_signal_address);
 
@@ -977,7 +977,7 @@ int kfd_event_mmap(struct kfd_process *p, struct vm_area_struct *vma)
 	/* check required size is logical */
 	if (get_order(KFD_SIGNAL_EVENT_LIMIT * 8) !=
 			get_order(vma->vm_end - vma->vm_start)) {
-		pr_err("amdkfd: event page mmap requested illegal size\n");
+		pr_err("Event page mmap requested illegal size\n");
 		return -EINVAL;
 	}
 
@@ -986,7 +986,7 @@ int kfd_event_mmap(struct kfd_process *p, struct vm_area_struct *vma)
 	page = lookup_signal_page_by_index(p, page_index);
 	if (!page) {
 		/* Probably KFD bug, but mmap is user-accessible. */
-		pr_debug("signal page could not be found for page_index %u\n",
+		pr_debug("Signal page could not be found for page_index %u\n",
 				page_index);
 		return -EINVAL;
 	}
@@ -997,7 +997,7 @@ int kfd_event_mmap(struct kfd_process *p, struct vm_area_struct *vma)
 	vma->vm_flags |= VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_NORESERVE
 		       | VM_DONTDUMP | VM_PFNMAP;
 
-	pr_debug("mapping signal page\n");
+	pr_debug("Mapping signal page\n");
 	pr_debug("     start user address  == 0x%08lx\n", vma->vm_start);
 	pr_debug("     end user address    == 0x%08lx\n", vma->vm_end);
 	pr_debug("     pfn                 == 0x%016lX\n", pfn);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c b/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c
index e119ce7..9654d99 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c
@@ -212,7 +212,7 @@ int kfd_ipc_import_handle(struct kfd_dev *dev, struct kfd_process *p,
 		return -EINVAL;
 	ipc_obj_get(found);
 
-	pr_debug("ipc: found ipc_dma_buf: %p\n", found->data);
+	pr_debug("Found ipc_dma_buf: %p\n", found->data);
 
 	r = kfd_import_dmabuf_create_kfd_bo(dev, p, gpu_id, found->data,
 					    va_addr, handle, mmap_offset,
@@ -244,7 +244,7 @@ int kfd_ipc_export_as_handle(struct kfd_dev *dev, struct kfd_process *p,
 	up_write(&p->lock);
 
 	if (IS_ERR(pdd)) {
-		pr_err("failed to get pdd\n");
+		pr_err("Failed to get pdd\n");
 		return PTR_ERR(pdd);
 	}
 
@@ -253,7 +253,7 @@ int kfd_ipc_export_as_handle(struct kfd_dev *dev, struct kfd_process *p,
 	up_write(&p->lock);
 
 	if (!kfd_bo) {
-		pr_err("failed to get bo");
+		pr_err("Failed to get bo");
 		return -EINVAL;
 	}
 	if (kfd_bo->kfd_ipc_obj) {
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
index 4a67e76..1a21c66 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
@@ -44,8 +44,8 @@ static bool initialize(struct kernel_queue *kq, struct kfd_dev *dev,
 	BUG_ON(!kq || !dev);
 	BUG_ON(type != KFD_QUEUE_TYPE_DIQ && type != KFD_QUEUE_TYPE_HIQ);
 
-	pr_debug("amdkfd: In func %s initializing queue type %d size %d\n",
-			__func__, KFD_QUEUE_TYPE_HIQ, queue_size);
+	pr_debug("Initializing queue type %d size %d\n", KFD_QUEUE_TYPE_HIQ,
+			queue_size);
 
 	memset(&prop, 0, sizeof(prop));
 	memset(&nop, 0, sizeof(nop));
@@ -73,13 +73,13 @@ static bool initialize(struct kernel_queue *kq, struct kfd_dev *dev,
 	prop.doorbell_ptr = kfd_get_kernel_doorbell(dev, &prop.doorbell_off);
 
 	if (prop.doorbell_ptr == NULL) {
-		pr_err("amdkfd: error init doorbell");
+		pr_err("Failed to initialize doorbell");
 		goto err_get_kernel_doorbell;
 	}
 
 	retval = kfd_gtt_sa_allocate(dev, queue_size, &kq->pq);
 	if (retval != 0) {
-		pr_err("amdkfd: error init pq queues size (%d)\n", queue_size);
+		pr_err("Failed to init pq queues size %d\n", queue_size);
 		goto err_pq_allocate_vidmem;
 	}
 
@@ -140,7 +140,7 @@ static bool initialize(struct kernel_queue *kq, struct kfd_dev *dev,
 
 	/* assign HIQ to HQD */
 	if (type == KFD_QUEUE_TYPE_HIQ) {
-		pr_debug("assigning hiq to hqd\n");
+		pr_debug("Assigning hiq to hqd\n");
 		kq->queue->pipe = KFD_CIK_HIQ_PIPE;
 		kq->queue->queue = KFD_CIK_HIQ_QUEUE;
 		kq->mqd->load_mqd(kq->mqd, kq->queue->mqd, kq->queue->pipe,
@@ -227,8 +227,8 @@ static int acquire_packet_buffer(struct kernel_queue *kq,
 	queue_address = (unsigned int *)kq->pq_kernel_addr;
 	queue_size_dwords = kq->queue->properties.queue_size / sizeof(uint32_t);
 
-	pr_debug("amdkfd: In func %s\n rptr: %d\n wptr: %d\n queue_address 0x%p\n",
-			__func__, rptr, wptr, queue_address);
+	pr_debug("rptr: %d\n wptr: %d\n queue_address 0x%p\n", rptr, wptr,
+			queue_address);
 
 	available_size = (rptr + queue_size_dwords - 1 - wptr) %
 							queue_size_dwords;
@@ -362,7 +362,7 @@ struct kernel_queue *kernel_queue_init(struct kfd_dev *dev,
 	}
 
 	if (!kq->ops.initialize(kq, dev, type, KFD_KERNEL_QUEUE_SIZE)) {
-		pr_err("amdkfd: failed to init kernel queue\n");
+		pr_err("Failed to init kernel queue\n");
 		kfree(kq);
 		return NULL;
 	}
@@ -385,7 +385,7 @@ static __attribute__((unused)) void test_kq(struct kfd_dev *dev)
 
 	BUG_ON(!dev);
 
-	pr_err("amdkfd: starting kernel queue test\n");
+	pr_err("Starting kernel queue test\n");
 
 	kq = kernel_queue_init(dev, KFD_QUEUE_TYPE_HIQ);
 	BUG_ON(!kq);
@@ -396,7 +396,7 @@ static __attribute__((unused)) void test_kq(struct kfd_dev *dev)
 		buffer[i] = kq->nop_packet;
 	kq->ops.submit_packet(kq);
 
-	pr_err("amdkfd: ending kernel queue test\n");
+	pr_err("Ending kernel queue test\n");
 }
 
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_cik.c b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_cik.c
index a06b0d4..2e4c6fb 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_cik.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_cik.c
@@ -99,8 +99,6 @@ static int pm_map_process_scratch_cik(struct packet_manager *pm,
 
 	packet = (struct pm4_map_process_scratch_kv *)buffer;
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	memset(buffer, 0, sizeof(struct pm4_map_process_scratch_kv));
 
 	packet->header.u32all = pm_build_pm4_header(IT_MAP_PROCESS,
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_module.c b/drivers/gpu/drm/amd/amdkfd/kfd_module.c
index 2126ec5..71cea02 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_module.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_module.c
@@ -120,7 +120,7 @@ static int __init kfd_module_init(void)
 	/* Verify module parameters */
 	if ((sched_policy < KFD_SCHED_POLICY_HWS) ||
 		(sched_policy > KFD_SCHED_POLICY_NO_HWS)) {
-		pr_err("kfd: sched_policy has invalid value\n");
+		pr_err("sched_policy has invalid value\n");
 		return -1;
 	}
 
@@ -128,7 +128,7 @@ static int __init kfd_module_init(void)
 	if ((max_num_of_queues_per_device < 1) ||
 		(max_num_of_queues_per_device >
 			KFD_MAX_NUM_OF_QUEUES_PER_DEVICE)) {
-		pr_err("kfd: max_num_of_queues_per_device must be between 1 to KFD_MAX_NUM_OF_QUEUES_PER_DEVICE\n");
+		pr_err("max_num_of_queues_per_device must be between 1 to KFD_MAX_NUM_OF_QUEUES_PER_DEVICE\n");
 		return -1;
 	}
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
index 4dff1ec..33367c3 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
@@ -89,7 +89,7 @@ static void update_cu_mask(struct mqd_manager *mm, void *mqd,
 	m->compute_static_thread_mgmt_se2 = se_mask[2];
 	m->compute_static_thread_mgmt_se3 = se_mask[3];
 
-	pr_debug("kfd: update cu mask to %#x %#x %#x %#x\n",
+	pr_debug("Update cu mask to %#x %#x %#x %#x\n",
 		m->compute_static_thread_mgmt_se0,
 		m->compute_static_thread_mgmt_se1,
 		m->compute_static_thread_mgmt_se2,
@@ -114,8 +114,6 @@ static int init_mqd(struct mqd_manager *mm, void **mqd,
 	struct cik_mqd *m;
 	int retval;
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	retval = kfd_gtt_sa_allocate(mm->dev, sizeof(struct cik_mqd),
 					mqd_mem_obj);
 
@@ -243,8 +241,6 @@ static int __update_mqd(struct mqd_manager *mm, void *mqd,
 
 	BUG_ON(!mm || !q || !mqd);
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	m = get_mqd(mqd);
 	m->cp_hqd_pq_control = DEFAULT_RPTR_BLOCK_SIZE |
 				DEFAULT_MIN_AVAIL_SIZE;
@@ -388,8 +384,6 @@ static int init_mqd_hiq(struct mqd_manager *mm, void **mqd,
 
 	BUG_ON(!mm || !q || !mqd || !mqd_mem_obj);
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	retval = kfd_gtt_sa_allocate(mm->dev, sizeof(struct cik_mqd),
 					mqd_mem_obj);
 
@@ -445,8 +439,6 @@ static int update_mqd_hiq(struct mqd_manager *mm, void *mqd,
 
 	BUG_ON(!mm || !q || !mqd);
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	m = get_mqd(mqd);
 	m->cp_hqd_pq_control = DEFAULT_RPTR_BLOCK_SIZE |
 				DEFAULT_MIN_AVAIL_SIZE |
@@ -510,8 +502,6 @@ struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 	BUG_ON(!dev);
 	BUG_ON(type >= KFD_MQD_TYPE_MAX);
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	mqd = kzalloc(sizeof(struct mqd_manager), GFP_NOIO);
 	if (!mqd)
 		return NULL;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c
index 0db4dbf..8e31af3 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c
@@ -85,7 +85,7 @@ static void update_cu_mask(struct mqd_manager *mm, void *mqd,
 	m->compute_static_thread_mgmt_se2 = se_mask[2];
 	m->compute_static_thread_mgmt_se3 = se_mask[3];
 
-	pr_debug("kfd: update cu mask to %#x %#x %#x %#x\n",
+	pr_debug("update cu mask to %#x %#x %#x %#x\n",
 		m->compute_static_thread_mgmt_se0,
 		m->compute_static_thread_mgmt_se1,
 		m->compute_static_thread_mgmt_se2,
@@ -194,14 +194,12 @@ static int update_mqd(struct mqd_manager *mm, void *mqd,
 {
 	struct v9_mqd *m;
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	m = get_mqd(mqd);
 
 	m->cp_hqd_pq_control = 5 << CP_HQD_PQ_CONTROL__RPTR_BLOCK_SIZE__SHIFT;
 	m->cp_hqd_pq_control |=
 			ffs(q->queue_size / sizeof(unsigned int)) - 1 - 1;
-	pr_debug("kfd: cp_hqd_pq_control 0x%x\n", m->cp_hqd_pq_control);
+	pr_debug("cp_hqd_pq_control 0x%x\n", m->cp_hqd_pq_control);
 
 	m->cp_hqd_pq_base_lo = lower_32_bits((uint64_t)q->queue_address >> 8);
 	m->cp_hqd_pq_base_hi = upper_32_bits((uint64_t)q->queue_address >> 8);
@@ -214,7 +212,7 @@ static int update_mqd(struct mqd_manager *mm, void *mqd,
 	m->cp_hqd_pq_doorbell_control =
 		q->doorbell_off <<
 			CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
-	pr_debug("kfd: cp_hqd_pq_doorbell_control 0x%x\n",
+	pr_debug("cp_hqd_pq_doorbell_control 0x%x\n",
 			m->cp_hqd_pq_doorbell_control);
 
 	m->cp_hqd_ib_control = 3 << CP_HQD_IB_CONTROL__MIN_IB_AVAIL_SIZE__SHIFT;
@@ -455,8 +453,6 @@ struct mqd_manager *mqd_manager_init_v9(enum KFD_MQD_TYPE type,
 	if (WARN_ON(type >= KFD_MQD_TYPE_MAX))
 		return NULL;
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	mqd = kzalloc(sizeof(struct mqd_manager), GFP_NOIO);
 	if (!mqd)
 		return NULL;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
index ddca15f..eb7ce43 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
@@ -89,7 +89,7 @@ static void update_cu_mask(struct mqd_manager *mm, void *mqd,
 	m->compute_static_thread_mgmt_se2 = se_mask[2];
 	m->compute_static_thread_mgmt_se3 = se_mask[3];
 
-	pr_debug("kfd: update cu mask to %#x %#x %#x %#x\n",
+	pr_debug("Update cu mask to %#x %#x %#x %#x\n",
 		m->compute_static_thread_mgmt_se0,
 		m->compute_static_thread_mgmt_se1,
 		m->compute_static_thread_mgmt_se2,
@@ -201,8 +201,6 @@ static int __update_mqd(struct mqd_manager *mm, void *mqd,
 
 	BUG_ON(!mm || !q || !mqd);
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	m = get_mqd(mqd);
 
 	m->cp_hqd_pq_control = 5 << CP_HQD_PQ_CONTROL__RPTR_BLOCK_SIZE__SHIFT |
@@ -210,7 +208,7 @@ static int __update_mqd(struct mqd_manager *mm, void *mqd,
 			mtype << CP_HQD_PQ_CONTROL__MTYPE__SHIFT;
 	m->cp_hqd_pq_control |=
 			ffs(q->queue_size / sizeof(unsigned int)) - 1 - 1;
-	pr_debug("kfd: cp_hqd_pq_control 0x%x\n", m->cp_hqd_pq_control);
+	pr_debug("cp_hqd_pq_control 0x%x\n", m->cp_hqd_pq_control);
 
 	m->cp_hqd_pq_base_lo = lower_32_bits((uint64_t)q->queue_address >> 8);
 	m->cp_hqd_pq_base_hi = upper_32_bits((uint64_t)q->queue_address >> 8);
@@ -223,7 +221,7 @@ static int __update_mqd(struct mqd_manager *mm, void *mqd,
 	m->cp_hqd_pq_doorbell_control =
 		q->doorbell_off <<
 			CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
-	pr_debug("kfd: cp_hqd_pq_doorbell_control 0x%x\n",
+	pr_debug("cp_hqd_pq_doorbell_control 0x%x\n",
 			m->cp_hqd_pq_doorbell_control);
 
 	m->cp_hqd_eop_control = atc_bit << CP_HQD_EOP_CONTROL__EOP_ATC__SHIFT |
@@ -478,8 +476,6 @@ struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 	BUG_ON(!dev);
 	BUG_ON(type >= KFD_MQD_TYPE_MAX);
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	mqd = kzalloc(sizeof(struct mqd_manager), GFP_NOIO);
 	if (!mqd)
 		return NULL;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
index ea153bc..386c00e 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
@@ -66,7 +66,7 @@ static void pm_calc_rlib_size(struct packet_manager *pm,
         if ((process_count > max_proc_per_quantum) ||
 		compute_queue_count > get_queues_num(pm->dqm)) {
 		*over_subscription = true;
-		pr_debug("kfd: over subscribed runlist\n");
+		pr_debug("Over subscribed runlist\n");
 	}
 
 	map_queue_size = pm->pmf->get_map_queues_packet_size();
@@ -81,7 +81,7 @@ static void pm_calc_rlib_size(struct packet_manager *pm,
 	if (*over_subscription)
 		*rlib_size += pm->pmf->get_runlist_packet_size();
 
-	pr_debug("kfd: runlist ib size %d\n", *rlib_size);
+	pr_debug("runlist ib size %d\n", *rlib_size);
 }
 
 static int pm_allocate_runlist_ib(struct packet_manager *pm,
@@ -104,7 +104,7 @@ static int pm_allocate_runlist_ib(struct packet_manager *pm,
 					&pm->ib_buffer_obj);
 
 	if (retval != 0) {
-		pr_err("kfd: failed to allocate runlist IB\n");
+		pr_err("Failed to allocate runlist IB\n");
 		mutex_unlock(&pm->lock);
 		return retval;
 	}
@@ -145,8 +145,7 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 	*rl_size_bytes = alloc_size_bytes;
 	pm->ib_size_bytes = alloc_size_bytes;
 
-	pr_debug("kfd: In func %s\n", __func__);
-	pr_debug("kfd: building runlist ib process count: %d queues count %d\n",
+	pr_debug("Building runlist ib process count: %d queues count %d\n",
 		pm->dqm->processes_count, pm->dqm->queue_count);
 
 	/* build the run list ib packet */
@@ -154,7 +153,7 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 		qpd = cur->qpd;
 		/* build map process packet */
 		if (proccesses_mapped >= pm->dqm->processes_count) {
-			pr_debug("kfd: not enough space left in runlist IB\n");
+			pr_debug("Not enough space left in runlist IB\n");
 			pm_release_ib(pm);
 			return -ENOMEM;
 		}
@@ -171,7 +170,7 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 			if (!kq->queue->properties.is_active)
 				continue;
 
-			pr_debug("kfd: static_queue, mapping kernel q %d, is debug status %d\n",
+			pr_debug("static_queue, mapping kernel q %d, is debug status %d\n",
 				kq->queue->queue, qpd->is_debug);
 
 			retval = pm->pmf->map_queues(pm,
@@ -190,7 +189,7 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 			if (!q->properties.is_active)
 				continue;
 
-			pr_debug("kfd: static_queue, mapping user queue %d, is debug status %d\n",
+			pr_debug("static_queue, mapping user queue %d, is debug status %d\n",
 				q->queue, qpd->is_debug);
 
 			retval = pm->pmf->map_queues(pm,
@@ -206,7 +205,7 @@ static int pm_create_runlist_ib(struct packet_manager *pm,
 		}
 	}
 
-	pr_debug("kfd: finished map process and queues to runlist\n");
+	pr_debug("Finished map process and queues to runlist\n");
 
 	if (is_over_subscription)
 		pm->pmf->runlist(pm, &rl_buffer[rl_wptr], *rl_gpu_addr,
@@ -268,8 +267,6 @@ int pm_send_set_resources(struct packet_manager *pm,
 {
 	uint32_t *buffer, size;
 
-	pr_debug("kfd: In func %s\n", __func__);
-
 	size = pm->pmf->get_set_resources_packet_size();
 	mutex_lock(&pm->lock);
 	pm->priv_queue->ops.acquire_packet_buffer(pm->priv_queue,
@@ -277,7 +274,7 @@ int pm_send_set_resources(struct packet_manager *pm,
 				(unsigned int **)&buffer);
 	if (buffer == NULL) {
 		mutex_unlock(&pm->lock);
-		pr_err("kfd: failed to allocate buffer on kernel queue\n");
+		pr_err("Failed to allocate buffer on kernel queue\n");
 		return -ENOMEM;
 	}
 
@@ -304,7 +301,7 @@ int pm_send_runlist(struct packet_manager *pm, struct list_head *dqm_queues)
 	if (retval != 0)
 		goto fail_create_runlist_ib;
 
-	pr_debug("kfd: runlist IB address: 0x%llX\n", rl_gpu_ib_addr);
+	pr_debug("runlist IB address: 0x%llX\n", rl_gpu_ib_addr);
 
 	packet_size_dwords = pm->pmf->get_runlist_packet_size() /
 		sizeof(uint32_t);
@@ -347,7 +344,7 @@ int pm_send_query_status(struct packet_manager *pm, uint64_t fence_address,
 			size / sizeof(uint32_t), (unsigned int **)&buffer);
 	if (buffer == NULL) {
 		mutex_unlock(&pm->lock);
-		pr_err("kfd: failed to allocate buffer on kernel queue\n");
+		pr_err("Failed to allocate buffer on kernel queue\n");
 		return -ENOMEM;
 	}
 	pm->pmf->query_status(pm, buffer, fence_address, fence_value);
@@ -370,7 +367,7 @@ int pm_send_unmap_queue(struct packet_manager *pm, enum kfd_queue_type type,
 			size / sizeof(uint32_t), (unsigned int **)&buffer);
 	if (buffer == NULL) {
 		mutex_unlock(&pm->lock);
-		pr_err("kfd: failed to allocate buffer on kernel queue\n");
+		pr_err("Failed to allocate buffer on kernel queue\n");
 		return -ENOMEM;
 	}
 	pm->pmf->unmap_queues(pm, buffer, type, filter, filter_param, reset,
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c b/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c
index 937c0ac..fa22d47 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c
@@ -159,10 +159,10 @@ static void free_callback(void *client_priv)
 	struct amd_mem_context *mem_context =
 		(struct amd_mem_context *)client_priv;
 
-	pr_debug("free_callback: data 0x%p\n", mem_context);
+	pr_debug("data 0x%p\n", mem_context);
 
 	if (!mem_context) {
-		pr_warn("free_callback: Invalid client context\n");
+		pr_warn("Invalid client context\n");
 		return;
 	}
 
@@ -189,19 +189,19 @@ static int amd_acquire(unsigned long addr, size_t size,
 	/* Get pointer to structure describing current process */
 	pid = get_task_pid(current, PIDTYPE_PID);
 
-	pr_debug("acquire: addr:0x%lx,size:0x%x, pid 0x%p\n",
+	pr_debug("addr:0x%lx,size:0x%x, pid 0x%p\n",
 					addr, (unsigned int)size, pid);
 
 	/* Check if address is handled by AMD GPU driver */
 	ret = rdma_interface->is_gpu_address(addr, pid);
 
 	if (!ret) {
-		pr_debug("acquire: Not GPU Address\n");
+		pr_debug("Not GPU Address\n");
 		/* This is not GPU address */
 		return 0;
 	}
 
-	pr_debug("acquire: GPU address\n");
+	pr_debug("GPU address\n");
 
 	/* Initialize context used for operation with given address */
 	mem_context = kzalloc(sizeof(struct amd_mem_context), GFP_KERNEL);
@@ -218,7 +218,7 @@ static int amd_acquire(unsigned long addr, size_t size,
 	 */
 	mem_context->pid  = pid;
 
-	pr_debug("acquire: Client context %p\n", mem_context);
+	pr_debug("Client context %p\n", mem_context);
 
 	/* Return pointer to allocated context */
 	*client_context = mem_context;
@@ -237,25 +237,25 @@ static int amd_get_pages(unsigned long addr, size_t size, int write, int force,
 	struct amd_mem_context *mem_context =
 		(struct amd_mem_context *)client_context;
 
-	pr_debug("get_pages: addr:0x%lx,size:0x%x, core_context:%p\n",
+	pr_debug("addr:0x%lx,size:0x%x, core_context:%p\n",
 		addr, (unsigned int)size, core_context);
 
 	if (!mem_context) {
-		pr_warn("get_pages: Invalid client context");
+		pr_warn("Invalid client context");
 		return -EINVAL;
 	}
 
-	pr_debug("get_pages: pid :0x%p\n", mem_context->pid);
+	pr_debug("pid :0x%p\n", mem_context->pid);
 
 
 	if (addr != mem_context->va) {
-		pr_warn("get_pages: Context address (0x%llx) is not the same\n",
+		pr_warn("Context address (0x%llx) is not the same\n",
 			mem_context->va);
 		return -EINVAL;
 	}
 
 	if (size != mem_context->size) {
-		pr_warn("get_pages: Context size (0x%llx) is not the same\n",
+		pr_warn("Context size (0x%llx) is not the same\n",
 			mem_context->size);
 		return -EINVAL;
 	}
@@ -304,16 +304,16 @@ static int amd_dma_map(struct sg_table *sg_head, void *client_context,
 	struct amd_mem_context *mem_context =
 		(struct amd_mem_context *)client_context;
 
-	pr_debug("dma_map: Context 0x%p, sg_head 0x%p\n",
+	pr_debug("Context 0x%p, sg_head 0x%p\n",
 			client_context, sg_head);
 
-	pr_debug("dma_map: pid 0x%p, address 0x%llx, size:0x%llx\n",
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
 			mem_context->pid,
 			mem_context->va,
 			mem_context->size);
 
 	if (!mem_context->p2p_info) {
-		pr_err("dma_map: No sg table were allocated\n");
+		pr_err("No sg table were allocated\n");
 		return -EINVAL;
 	}
 
@@ -332,10 +332,10 @@ static int amd_dma_unmap(struct sg_table *sg_head, void *client_context,
 	struct amd_mem_context *mem_context =
 		(struct amd_mem_context *)client_context;
 
-	pr_debug("dma_unmap: Context 0x%p, sg_table 0x%p\n",
+	pr_debug("Context 0x%p, sg_table 0x%p\n",
 			client_context, sg_head);
 
-	pr_debug("dma_unmap: pid 0x%p, address 0x%llx, size:0x%llx\n",
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
 			mem_context->pid,
 			mem_context->va,
 			mem_context->size);
@@ -349,18 +349,18 @@ static void amd_put_pages(struct sg_table *sg_head, void *client_context)
 	struct amd_mem_context *mem_context =
 		(struct amd_mem_context *)client_context;
 
-	pr_debug("put_pages: sg_head %p client_context: 0x%p\n",
+	pr_debug("sg_head %p client_context: 0x%p\n",
 			sg_head, client_context);
-	pr_debug("put_pages: pid 0x%p, address 0x%llx, size:0x%llx\n",
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
 			mem_context->pid,
 			mem_context->va,
 			mem_context->size);
 
-	pr_debug("put_pages: mem_context->p2p_info %p\n",
+	pr_debug("mem_context->p2p_info %p\n",
 				mem_context->p2p_info);
 
 	if (ACCESS_ONCE(mem_context->free_callback_called)) {
-		pr_debug("put_pages: free callback was called\n");
+		pr_debug("Free callback was called\n");
 		return;
 	}
 
@@ -369,10 +369,10 @@ static void amd_put_pages(struct sg_table *sg_head, void *client_context)
 		mem_context->p2p_info = NULL;
 
 		if (ret)
-			pr_err("put_pages failure: %d (callback status %d)\n",
+			pr_err("Failure: %d (callback status %d)\n",
 					ret, mem_context->free_callback_called);
 	} else
-		pr_err("put_pages: Pointer to p2p info is null\n");
+		pr_err("Pointer to p2p info is null\n");
 }
 static unsigned long amd_get_page_size(void *client_context)
 {
@@ -381,8 +381,8 @@ static unsigned long amd_get_page_size(void *client_context)
 	struct amd_mem_context *mem_context =
 		(struct amd_mem_context *)client_context;
 
-	pr_debug("get_page_size: context: %p\n", client_context);
-	pr_debug("get_page_size: pid 0x%p, address 0x%llx, size:0x%llx\n",
+	pr_debug("context: %p\n", client_context);
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
 			mem_context->pid,
 			mem_context->va,
 			mem_context->size);
@@ -410,8 +410,8 @@ static void amd_release(void *client_context)
 	struct amd_mem_context *mem_context =
 		(struct amd_mem_context *)client_context;
 
-	pr_debug("release: context: 0x%p\n", client_context);
-	pr_debug("release: pid 0x%p, address 0x%llx, size:0x%llx\n",
+	pr_debug("context: 0x%p\n", client_context);
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
 			mem_context->pid,
 			mem_context->va,
 			mem_context->size);
@@ -457,7 +457,7 @@ void kfd_init_peer_direct(void)
 
 	if (!pfn_ib_register_peer_memory_client ||
 		!pfn_ib_unregister_peer_memory_client) {
-		pr_debug("amdkfd: PeerDirect interface was not detected\n");
+		pr_debug("PeerDirect interface was not detected\n");
 		/* Do cleanup */
 		kfd_close_peer_direct();
 		return;
@@ -466,8 +466,7 @@ void kfd_init_peer_direct(void)
 	result = amdkfd_query_rdma_interface(&rdma_interface);
 
 	if (result < 0) {
-		pr_err("amdkfd: Cannot get RDMA Interface (result = %d)\n",
-				result);
+		pr_err("Cannot get RDMA Interface (result = %d)\n", result);
 		return;
 	}
 
@@ -478,13 +477,13 @@ void kfd_init_peer_direct(void)
 						&ib_invalidate_callback);
 
 	if (!ib_reg_handle) {
-		pr_err("amdkfd: Cannot register peer memory client\n");
+		pr_err("Cannot register peer memory client\n");
 		/* Do cleanup */
 		kfd_close_peer_direct();
 		return;
 	}
 
-	pr_info("amdkfd: PeerDirect support was initialized successfully\n");
+	pr_info("PeerDirect support was initialized successfully\n");
 }
 
 /**
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process.c b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
index 025ee5d..e34b686 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_process.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
@@ -229,7 +229,7 @@ struct kfd_process *kfd_create_process(struct file *filep)
 	/* A prior open of /dev/kfd could have already created the process. */
 	process = find_process(thread, false);
 	if (process)
-		pr_debug("kfd: process already found\n");
+		pr_debug("Process already found\n");
 	else
 		process = create_process(thread, filep);
 
@@ -741,7 +741,7 @@ struct kfd_process_device *kfd_bind_process_to_device(struct kfd_dev *dev,
 		return pdd;
 
 	if (pdd->bound == PDD_BOUND_SUSPENDED) {
-		pr_err("kfd: binding PDD_BOUND_SUSPENDED pdd is unexpected!\n");
+		pr_err("Binding PDD_BOUND_SUSPENDED pdd is unexpected!\n");
 		return ERR_PTR(-EINVAL);
 	}
 
@@ -780,7 +780,7 @@ int kfd_bind_processes_to_device(struct kfd_dev *dev)
 		err = amd_iommu_bind_pasid(dev->pdev, p->pasid,
 				p->lead_thread);
 		if (err < 0) {
-			pr_err("unexpected pasid %d binding failure\n",
+			pr_err("Unexpected pasid %d binding failure\n",
 					p->pasid);
 			up_write(&p->lock);
 			break;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
index a782941..109bc1a 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
@@ -51,15 +51,13 @@ static int find_available_queue_slot(struct process_queue_manager *pqm,
 
 	BUG_ON(!pqm || !qid);
 
-	pr_debug("kfd: in %s\n", __func__);
-
 	found = find_first_zero_bit(pqm->queue_slot_bitmap,
 			KFD_MAX_NUM_OF_QUEUES_PER_PROCESS);
 
-	pr_debug("kfd: the new slot id %lu\n", found);
+	pr_debug("The new slot id %lu\n", found);
 
 	if (found >= KFD_MAX_NUM_OF_QUEUES_PER_PROCESS) {
-		pr_info("amdkfd: Can not open more queues for process with pasid %d\n",
+		pr_info("Cannot open more queues for process with pasid %d\n",
 				pqm->process->pasid);
 		return -ENOMEM;
 	}
@@ -87,7 +85,7 @@ void kfd_process_dequeue_from_device(struct kfd_process_device *pdd)
 	 * wavefronts. Now we put the check there to be safe.
 	 */
 	if (retval || pdd->reset_wavefronts) {
-		pr_warn("amdkfd: Resetting wave fronts on dev %p\n", dev);
+		pr_warn("Resetting wave fronts on dev %p\n", dev);
 		dbgdev_wave_reset_wavefronts(dev, p);
 		pdd->reset_wavefronts = false;
 	}
@@ -122,8 +120,6 @@ void pqm_uninit(struct process_queue_manager *pqm)
 
 	BUG_ON(!pqm);
 
-	pr_debug("In func %s\n", __func__);
-
 	list_for_each_entry_safe(pqn, next, &pqm->queues, process_queue_list) {
 		uninit_queue(pqn->q);
 		list_del(&pqn->process_queue_list);
@@ -157,7 +153,7 @@ static int create_cp_queue(struct process_queue_manager *pqm,
 	(*q)->device = dev;
 	(*q)->process = pqm->process;
 
-	pr_debug("kfd: PQM After init queue");
+	pr_debug("PQM After init queue");
 
 	return retval;
 
@@ -222,7 +218,7 @@ int pqm_create_queue(struct process_queue_manager *pqm,
 	switch (type) {
 	case KFD_QUEUE_TYPE_SDMA:
                 if (dev->dqm->sdma_queue_count >= CIK_SDMA_QUEUES) {
-                        pr_err("kfd: over-subscription is not allowed for SDMA.\n");
+                        pr_err("Over-subscription is not allowed for SDMA.\n");
                         retval = -EPERM;
                         goto err_create_queue;
                 }
@@ -244,7 +240,7 @@ int pqm_create_queue(struct process_queue_manager *pqm,
 				KFD_SCHED_POLICY_HWS_NO_OVERSUBSCRIPTION) &&
 		((dev->dqm->processes_count >= dev->vm_info.vmid_num_kfd) ||
 		(dev->dqm->queue_count >= get_queues_num(dev->dqm)))) {
-			pr_err("kfd: over-subscription is not allowed in radeon_kfd.sched_policy == 1\n");
+			pr_err("Over-subscription is not allowed in radeon_kfd.sched_policy == 1\n");
 			retval = -EPERM;
 			goto err_create_queue;
 		}
@@ -277,7 +273,7 @@ int pqm_create_queue(struct process_queue_manager *pqm,
 	}
 
 	if (retval != 0) {
-		pr_debug("Error dqm create queue\n");
+		pr_err("DQM create queue failed\n");
 		goto err_create_queue;
 	}
 
@@ -290,12 +286,12 @@ int pqm_create_queue(struct process_queue_manager *pqm,
 			(q->properties.doorbell_off * sizeof(uint32_t)) &
 			(kfd_doorbell_process_slice(dev) - 1);
 
-	pr_debug("kfd: PQM After DQM create queue\n");
+	pr_debug("PQM After DQM create queue\n");
 
 	list_add(&pqn->process_queue_list, &pqm->queues);
 
 	if (q) {
-		pr_debug("kfd: PQM done creating queue\n");
+		pr_debug("PQM done creating queue\n");
 		print_queue_properties(&q->properties);
 	}
 
@@ -325,11 +321,9 @@ int pqm_destroy_queue(struct process_queue_manager *pqm, unsigned int qid)
 	BUG_ON(!pqm);
 	retval = 0;
 
-	pr_debug("kfd: In Func %s\n", __func__);
-
 	pqn = get_queue_by_qid(pqm, qid);
 	if (pqn == NULL) {
-		pr_err("kfd: queue id does not match any known queue\n");
+		pr_err("Queue id does not match any known queue\n");
 		return -EINVAL;
 	}
 
@@ -386,8 +380,7 @@ int pqm_update_queue(struct process_queue_manager *pqm, unsigned int qid,
 
 	pqn = get_queue_by_qid(pqm, qid);
 	if (!pqn) {
-		pr_debug("amdkfd: No queue %d exists for update operation\n",
-				qid);
+		pr_debug("No queue %d exists for update operation\n", qid);
 		return -EFAULT;
 	}
 
@@ -414,8 +407,7 @@ int pqm_set_cu_mask(struct process_queue_manager *pqm, unsigned int qid,
 
 	pqn = get_queue_by_qid(pqm, qid);
 	if (!pqn) {
-		pr_debug("amdkfd: No queue %d exists for update operation\n",
-				qid);
+		pr_debug("No queue %d exists for update operation\n", qid);
 		return -EFAULT;
 	}
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c b/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c
index 2b3c300..bcf0bf2 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c
@@ -76,15 +76,14 @@ static int get_pages(uint64_t address, uint64_t length, struct pid *pid,
 
 	p = kfd_lookup_process_by_pid(pid);
 	if (!p) {
-		pr_err("could not find the process in %s.\n",
-				__func__);
+		pr_err("Could not find the process\n");
 		return -EINVAL;
 	}
 	down_read(&p->lock);
 
 	buf_obj = kfd_process_find_bo_from_interval(p, address, last);
 	if (!buf_obj) {
-		pr_err("can not find a kfd_bo for the range\n");
+		pr_err("Cannot find a kfd_bo for the range\n");
 		ret = -EINVAL;
 		goto out;
 	}
@@ -195,8 +194,7 @@ static int put_pages(struct amd_p2p_info **p_p2p_data)
 
 	p = kfd_lookup_process_by_pid((*p_p2p_data)->pid);
 	if (!p) {
-		pr_err("could not find the process in %s\n",
-				__func__);
+		pr_err("Could not find the process\n");
 		return -EINVAL;
 	}
 
@@ -227,8 +225,7 @@ static int is_gpu_address(uint64_t address, struct pid *pid)
 
 	p = kfd_lookup_process_by_pid(pid);
 	if (!p) {
-		pr_debug("could not find the process in %s.\n",
-				__func__);
+		pr_debug("Could not find the process\n");
 		return 0;
 	}
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_topology.c b/drivers/gpu/drm/amd/amdkfd/kfd_topology.c
index 3e21aef..da91ced 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_topology.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_topology.c
@@ -1239,7 +1239,7 @@ int kfd_topology_add_device(struct kfd_dev *gpu)
 
 	gpu_id = kfd_generate_gpu_id(gpu);
 
-	pr_debug("kfd: Adding new GPU (ID: 0x%x) to topology\n", gpu_id);
+	pr_debug("Adding new GPU (ID: 0x%x) to topology\n", gpu_id);
 
 	proximity_domain = atomic_inc_return(&
 				topology_crat_proximity_domain);
@@ -1321,7 +1321,7 @@ int kfd_topology_add_device(struct kfd_dev *gpu)
 	case CHIP_FIJI:
 	case CHIP_POLARIS10:
 	case CHIP_POLARIS11:
-		pr_debug("amdkfd: adding doorbell packet type capability\n");
+		pr_debug("Adding doorbell packet type capability\n");
 		dev->node_props.capability |= ((HSA_CAP_DOORBELL_TYPE_1_0 <<
 			HSA_CAP_DOORBELL_TYPE_TOTALBITS_SHIFT) &
 			HSA_CAP_DOORBELL_TYPE_TOTALBITS_MASK);
-- 
2.7.4

