From 3ae45b24ded729e708ef0116e6979893af268f05 Mon Sep 17 00:00:00 2001
From: Ayyappa Ch <ayyappa.chandolu@amd.com>
Date: Fri, 27 Jan 2017 13:31:05 +0530
Subject: [PATCH 1619/1722] drm/amdgpu: Support PD table update via CPU
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

If amdgpu.vm_update_context param is set to use CPU, then Page Directory
entries will be updated by CPU.

NOTE: This commit only supports PD. Page Table support to added in a
separate commit

Change-Id: If7216b3530bbf10be437cbf40500a3522d9e38fb
Signed-off-by: Harish Kasiviswanathan <Harish.Kasiviswanathan@amd.com>
Reviewed-by: Christian KÃ¶nig <christian.koenig@amd.com>
Signed-off-by: Ayyappa Ch <ayyappa.chandolu@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c | 144 ++++++++++++++++++++++++---------
 1 file changed, 105 insertions(+), 39 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index ec8266b..f8376d3 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -67,9 +67,46 @@ struct amdgpu_pte_update_params {
 		     uint32_t flags);
 	/* indicate update pt or its shadow */
 	bool shadow;
+	/* Kernel pointer of PD/PT BO that needs to be updated */
+	void *kptr;	
 };
 
 /**
+ * write_pte_using_cpu - update the page tables using CPU
+ *
+ * @adev: amdgpu_device pointer
+ * @vm_update_params: see structure definition
+ * @pe_start: offset from start of the page table
+ * @addr: dst addr to write into pe
+ * @count: number of page entries to update
+ * @incr: increase next addr by incr bytes
+ * @flags: hw access flags
+ *
+ */
+static void write_pte_using_cpu(struct amdgpu_device *adev,
+				struct amdgpu_vm_update_params
+					*vm_update_params,
+				uint64_t pe_start, uint64_t addr,
+				unsigned count, uint32_t incr,
+				uint32_t flags)
+{
+	unsigned int i;
+	uint64_t page_idx, value;
+
+	page_idx = pe_start / 8;
+
+	for (i = 0; i < count; i++) {
+		value = amdgpu_vm_map_gart(vm_update_params->pages_addr, addr);
+		amdgpu_gart_set_pte_pde(adev, vm_update_params->kptr,
+			page_idx, value, flags);
+		addr += incr;
+		page_idx++;
+	}
+	mb();
+	amdgpu_gart_flush_gpu_tlb(adev, 0);
+}
+
+/**
  * amdgpu_vm_num_pde - return the number of page directory entries
  *
  * @adev: amdgpu_device pointer
@@ -494,7 +531,10 @@ static void amdgpu_vm_do_set_ptes(struct amdgpu_pte_update_params *params,
 {
 	trace_amdgpu_vm_set_ptes(pe, addr, count, incr, flags);
 
-	if (count < 3) {
+	if (vm_update_params->kptr) {
+		write_pte_using_cpu(params->adev, params, pe, addr,
+			count, incr, flags);
+	} else if (count < 3) {
 		amdgpu_vm_write_pte(params->adev, params->ib, pe,
 				    addr | flags, count, incr);
 
@@ -570,7 +610,8 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 {
 	struct amdgpu_bo *shadow;
 	struct amdgpu_ring *ring;
-	uint64_t pd_addr, shadow_addr;
+	uint64_t pd_addr = 0;
+	uint64_t shadow_addr = 0;
 	uint32_t incr = AMDGPU_VM_PTE_COUNT * 8;
 	uint64_t last_pde = ~0, last_pt = ~0, last_shadow = ~0;
 	unsigned count = 0, pt_idx, ndw;
@@ -578,7 +619,7 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 	struct amdgpu_pte_update_params params;
 	struct fence *fence = NULL;
 
-	int r;
+	int r = 0;
 
 	ring = container_of(vm->entity.sched, struct amdgpu_ring, sched);
 	shadow = vm->page_directory->shadow;
@@ -589,25 +630,45 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 	/* assume the worst case */
 	ndw += vm->max_pde_used * 6;
 
-	pd_addr = amdgpu_bo_gpu_offset(vm->page_directory);
-	if (shadow) {
-		r = amdgpu_ttm_bind(&shadow->tbo, &shadow->tbo.mem);
+	/* If is_vm_update_mode_cpu flag is set, then try to update PD table
+	 * using CPU. If failed, fallback to use SDMA
+	 */
+	if (vm->is_vm_update_mode_cpu) {
+		struct amdgpu_sync sync;
+		struct amdgpu_bo *pd = vm->page_directory;
+
+		r = amdgpu_bo_kmap(vm->page_directory,
+			&vm_update_params.kptr);
 		if (r)
-			return r;
-		shadow_addr = amdgpu_bo_gpu_offset(shadow);
-		ndw *= 2;
-	} else {
-		shadow_addr = 0;
+			dev_warn(adev->dev, "PD table update using CPU failed. Fallback to SDMA\n");
+		else {
+			amdgpu_sync_create(&sync);
+			amdgpu_sync_resv(adev, &sync, pd->tbo.resv,
+				AMDGPU_FENCE_OWNER_VM);
+			amdgpu_sync_wait(&sync);
+			amdgpu_sync_free(&sync);
+		}
 	}
+	if (r || !vm->is_vm_update_mode_cpu) {
+		pd_addr = amdgpu_bo_gpu_offset(vm->page_directory);
+		if (shadow) {
+			r = amdgpu_ttm_bind(&shadow->tbo, &shadow->tbo.mem);
+			if (r)
+				return r;
+			shadow_addr = amdgpu_bo_gpu_offset(shadow);
+			ndw *= 2;
+		} else {
+			shadow_addr = 0;
+		}
 
-	r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
-	if (r)
-		return r;
-
-	memset(&params, 0, sizeof(params));
-	params.adev = adev;
-	params.ib = &job->ibs[0];
+		r = amdgpu_job_alloc_with_ib(adev, ndw * 4, &job);
+		if (r)
+			return r;
 
+		memset(&params, 0, sizeof(params));
+		params.adev = adev;
+		params.ib = &job->ibs[0];
+	}
 	/* walk over the address space and update the page directory */
 	for (pt_idx = 0; pt_idx <= vm->max_pde_used; ++pt_idx) {
 		struct amdgpu_bo *bo = vm->page_tables[pt_idx].bo;
@@ -667,29 +728,32 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 				      count, incr, AMDGPU_PTE_VALID);
 	}
 
-	if (params.ib->length_dw == 0) {
-		amdgpu_job_free(job);
-		return 0;
-	}
-
-	amdgpu_ring_pad_ib(ring, params.ib);
-	amdgpu_sync_resv(adev, &job->sync, vm->page_directory->tbo.resv,
-			 AMDGPU_FENCE_OWNER_VM);
-	if (shadow)
-		amdgpu_sync_resv(adev, &job->sync, shadow->tbo.resv,
+	/* Submit IB if updating using SDMA */
+	if (vm_update_params.ib) {	
+		if (params.ib->length_dw == 0) {
+			amdgpu_job_free(job);
+			return 0;
+		}
+		amdgpu_ring_pad_ib(ring, params.ib);
+		amdgpu_sync_resv(adev, &job->sync, vm->page_directory->tbo.resv,
 				 AMDGPU_FENCE_OWNER_VM);
+		if (shadow)
+			amdgpu_sync_resv(adev, &job->sync, shadow->tbo.resv,
+					 AMDGPU_FENCE_OWNER_VM);
 
-	WARN_ON(params.ib->length_dw > ndw);
-	r = amdgpu_job_submit(job, ring, &vm->entity,
-			      AMDGPU_FENCE_OWNER_VM, &fence);
-	if (r)
-		goto error_free;
+		WARN_ON(params.ib->length_dw > ndw);
+		r = amdgpu_job_submit(job, ring, &vm->entity,
+				      AMDGPU_FENCE_OWNER_VM, &fence);
+		if (r)
+			goto error_free;
 
-	amdgpu_bo_fence(vm->page_directory, fence, true);
-	fence_put(vm->page_directory_fence);
-	vm->page_directory_fence = fence_get(fence);
-	fence_put(fence);
+		amdgpu_bo_fence(vm->page_directory, fence, true);
+		fence_put(vm->page_directory_fence);
+		vm->page_directory_fence = fence_get(fence);
+		fence_put(fence);
 
+	} else if (vm_update_params.kptr)
+		amdgpu_bo_kunmap(vm->page_directory);
 	return 0;
 
 error_free:
@@ -1518,7 +1582,7 @@ void amdgpu_vm_bo_invalidate(struct amdgpu_device *adev,
  * Init @vm fields.
  */
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-               bool vm_update_mode)
+		bool vm_update_mode)
 {
 	const unsigned align = min(AMDGPU_VM_PTB_ALIGN_SIZE,
 		AMDGPU_VM_PTE_COUNT * 8);
@@ -1566,7 +1630,9 @@ int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 
 	r = amdgpu_bo_create(adev, pd_size, align, true,
 			     AMDGPU_GEM_DOMAIN_VRAM,
-			     AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+			     (vm->is_vm_update_mode_cpu ?
+				 AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED :
+				 AMDGPU_GEM_CREATE_NO_CPU_ACCESS) |
 			     AMDGPU_GEM_CREATE_SHADOW |
 			     AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS |
 			     AMDGPU_GEM_CREATE_VRAM_CLEARED,
-- 
2.7.4

