From b853a07c066a825afc34f558856c8fdee1bb6be8 Mon Sep 17 00:00:00 2001
From: Flora Cui <Flora.Cui@amd.com>
Date: Mon, 10 Oct 2016 17:41:10 +0800
Subject: [PATCH 1584/2094] drm/amd/amdgpu: adapt dgma to the new vam_mgr

Change-Id: Id974e91410107e53a6ae7501855aed31cdf869d4
Signed-off-by: Flora Cui <Flora.Cui@amd.com>
Reviewed-by: Hawking Zhang <Hawking.Zhang@amd.com>

 Conflicts:
	drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
Signed-off-by: Avinash M N <avimn@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c | 46 ++++++++++++++++++++++------------
 1 file changed, 30 insertions(+), 16 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index 6a29477..b6b8a65 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -1468,7 +1468,7 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
  * @vm: requested vm
  * @mapping: mapped range and flags to use for the update
  * @flags: HW flags for the mapping
- * @nodes: array of drm_mm_nodes with the MC addresses
+ * @mem: ttm_mem_reg holding array of drm_mm_nodes with the MC addresses
  * @fence: optional resulting fence
  *
  * Split the mapping into smaller chunks so that each update fits
@@ -1482,9 +1482,10 @@ static int amdgpu_vm_bo_split_mapping(struct amdgpu_device *adev,
 				      struct amdgpu_vm *vm,
 				      struct amdgpu_bo_va_mapping *mapping,
 				      uint64_t flags,
-				      struct drm_mm_node *nodes,
+				      struct ttm_mem_reg *mem,
 				      struct fence **fence)
 {
+	struct drm_mm_node *nodes = mem ? mem->mm_node : NULL;
 	uint64_t pfn, src = 0, start = mapping->start;
 	int r;
 
@@ -1526,21 +1527,36 @@ static int amdgpu_vm_bo_split_mapping(struct amdgpu_device *adev,
 			addr = nodes->start << PAGE_SHIFT;
 			max_entries = (nodes->size - pfn) *
 				(PAGE_SIZE / AMDGPU_GPU_PAGE_SIZE);
+			switch (mem->mem_type) {
+			case AMDGPU_PL_DGMA_IMPORT:
+				pages_addr = (dma_addr_t *)mem->bus.base;
+				addr += adev->mman.bdev.man[mem->mem_type].gpu_offset -
+					adev->mman.bdev.man[TTM_PL_TT].gpu_offset;
+				gtt_flags = flags;
+				/* fall through */
+			case TTM_PL_TT:
+				if (flags == gtt_flags)
+					src = adev->gart.table_addr +
+						(addr >> AMDGPU_GPU_PAGE_SHIFT) * 8;
+				else
+					max_entries = min(max_entries, 16ull * 1024ull);
+				addr = 0;
+				break;
+			case AMDGPU_PL_DGMA:
+				addr += adev->vm_manager.vram_base_offset +
+					adev->mman.bdev.man[mem->mem_type].gpu_offset -
+					adev->mman.bdev.man[TTM_PL_VRAM].gpu_offset;
+				break;
+			case TTM_PL_VRAM:
+				addr += adev->vm_manager.vram_base_offset;
+				break;
+			default:
+				break;
+			}
 		} else {
 			addr = 0;
 			max_entries = S64_MAX;
 		}
-
-		if (pages_addr) {
-			if (flags == gtt_flags)
-				src = adev->gart.table_addr +
-					(addr >> AMDGPU_GPU_PAGE_SHIFT) * 8;
-			else
-				max_entries = min(max_entries, 16ull * 1024ull);
-			addr = 0;
-		} else if (flags & AMDGPU_PTE_VALID) {
-			addr += adev->vm_manager.vram_base_offset;
-		}
 		addr += pfn << PAGE_SHIFT;
 
 		last = min((uint64_t)mapping->last, start + max_entries - 1);
@@ -1610,8 +1626,6 @@ int amdgpu_vm_bo_update(struct amdgpu_device *adev,
 		gtt_flags = (amdgpu_ttm_is_bound(bo_va->bo->tbo.ttm) &&
 			adev == amdgpu_ttm_adev(bo_va->bo->tbo.bdev)) ?
 			flags : 0;
-		if (mem && mem->mem_type == AMDGPU_PL_DGMA_IMPORT)
-			gtt_flags = (adev == amdgpu_ttm_adev(bo_va->bo->tbo.bdev)) ? flags : 0;
 	} else {
 		flags = 0x0;
 		gtt_flags = ~0x0;
@@ -1625,7 +1639,7 @@ int amdgpu_vm_bo_update(struct amdgpu_device *adev,
 	list_for_each_entry(mapping, &bo_va->invalids, list) {
 		r = amdgpu_vm_bo_split_mapping(adev, exclusive,
 					       gtt_flags, pages_addr, vm,
-					       mapping, flags, nodes,
+					       mapping, flags, mem,
 					       &bo_va->last_pt_update);
 		if (r)
 			return r;
-- 
2.7.4

